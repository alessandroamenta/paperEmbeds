[
    {
        "title": "Towards Attack-tolerant Federated Learning via Critical Parameter Analysis",
        "url": "http://arxiv.org/abs/2308.09318",
        "abstract": "Federated learning is used to train a shared model in a decentralized way\nwithout clients sharing private data with each other. Federated learning\nsystems are susceptible to poisoning attacks when malicious clients send false\nupdates to the central server. Existing defense strategies are ineffective\nunder non-IID data settings. This paper proposes a new defense strategy, FedCPA\n(Federated learning with Critical Parameter Analysis). Our attack-tolerant\naggregation method is based on the observation that benign local models have\nsimilar sets of top-k and bottom-k critical parameters, whereas poisoned local\nmodels do not. Experiments with different attack scenarios on multiple datasets\ndemonstrate that our model outperforms existing defense strategies in defending\nagainst poisoning attacks.",
        "authors": [
            "Sungwon Han",
            "Sungwon Park",
            "Fangzhao Wu",
            "Sundong Kim",
            "Bin Zhu",
            "Xing Xie",
            "Meeyoung Cha"
        ]
    },
    {
        "title": "Stochastic Segmentation with Conditional Categorical Diffusion Models",
        "url": "http://arxiv.org/abs/2303.08888",
        "abstract": "Semantic segmentation has made significant progress in recent years thanks to\ndeep neural networks, but the common objective of generating a single\nsegmentation output that accurately matches the image's content may not be\nsuitable for safety-critical domains such as medical diagnostics and autonomous\ndriving. Instead, multiple possible correct segmentation maps may be required\nto reflect the true distribution of annotation maps. In this context,\nstochastic semantic segmentation methods must learn to predict conditional\ndistributions of labels given the image, but this is challenging due to the\ntypically multimodal distributions, high-dimensional output spaces, and limited\nannotation data. To address these challenges, we propose a conditional\ncategorical diffusion model (CCDM) for semantic segmentation based on Denoising\nDiffusion Probabilistic Models. Our model is conditioned to the input image,\nenabling it to generate multiple segmentation label maps that account for the\naleatoric uncertainty arising from divergent ground truth annotations. Our\nexperimental results show that CCDM achieves state-of-the-art performance on\nLIDC, a stochastic semantic segmentation dataset, and outperforms established\nbaselines on the classical segmentation dataset Cityscapes.",
        "authors": [
            "Lukas Zbinden",
            "Lars Doorenbos",
            "Theodoros Pissas",
            "Adrian Thomas Huber",
            "Raphael Sznitman",
            "Pablo M\u00e1rquez-Neila"
        ]
    },
    {
        "title": "Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient",
        "url": "http://arxiv.org/abs/2308.05681",
        "abstract": "Recently, methods for skeleton-based human activity recognition have been\nshown to be vulnerable to adversarial attacks. However, these attack methods\nrequire either the full knowledge of the victim (i.e. white-box attacks),\naccess to training data (i.e. transfer-based attacks) or frequent model queries\n(i.e. black-box attacks). All their requirements are highly restrictive,\nraising the question of how detrimental the vulnerability is. In this paper, we\nshow that the vulnerability indeed exists. To this end, we consider a new\nattack task: the attacker has no access to the victim model or the training\ndata or labels, where we coin the term hard no-box attack. Specifically, we\nfirst learn a motion manifold where we define an adversarial loss to compute a\nnew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our\ngradient contains information of the motion dynamics, which is different from\nexisting gradient-based attack methods that compute the loss gradient assuming\neach dimension in the data is independent. The SMI gradient can augment many\ngradient-based attack methods, leading to a new family of no-box attack\nmethods. Extensive evaluation and comparison show that our method imposes a\nreal threat to existing classifiers. They also show that the SMI gradient\nimproves the transferability and imperceptibility of adversarial samples in\nboth no-box and transfer-based black-box settings.",
        "authors": [
            "Zhengzhi Lu",
            "He Wang",
            "Ziyi Chang",
            "Guoan Yang",
            "Hubert P. H. Shum"
        ]
    },
    {
        "title": "GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving",
        "url": "http://arxiv.org/abs/2303.05760",
        "abstract": "Autonomous vehicles operating in complex real-world environments require\naccurate predictions of interactive behaviors between traffic participants.\nThis paper tackles the interaction prediction problem by formulating it with\nhierarchical game theory and proposing the GameFormer model for its\nimplementation. The model incorporates a Transformer encoder, which effectively\nmodels the relationships between scene elements, alongside a novel hierarchical\nTransformer decoder structure. At each decoding level, the decoder utilizes the\nprediction outcomes from the previous level, in addition to the shared\nenvironmental context, to iteratively refine the interaction process. Moreover,\nwe propose a learning process that regulates an agent's behavior at the current\nlevel to respond to other agents' behaviors from the preceding level. Through\ncomprehensive experiments on large-scale real-world driving datasets, we\ndemonstrate the state-of-the-art accuracy of our model on the Waymo interaction\nprediction task. Additionally, we validate the model's capacity to jointly\nreason about the motion plan of the ego agent and the behaviors of multiple\nagents in both open-loop and closed-loop planning tests, outperforming various\nbaseline methods. Furthermore, we evaluate the efficacy of our model on the\nnuPlan planning benchmark, where it achieves leading performance.",
        "authors": [
            "Zhiyu Huang",
            "Haochen Liu",
            "Chen Lv"
        ]
    },
    {
        "title": "Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels",
        "url": "http://arxiv.org/abs/2304.10539",
        "abstract": "Conventional multi-label classification (MLC) methods assume that all samples\nare fully labeled and identically distributed. Unfortunately, this assumption\nis unrealistic in large-scale MLC data that has long-tailed (LT) distribution\nand partial labels (PL). To address the problem, we introduce a novel task,\nPartial labeling and Long-Tailed Multi-Label Classification (PLT-MLC), to\njointly consider the above two imperfect learning environments. Not\nsurprisingly, we find that most LT-MLC and PL-MLC approaches fail to solve the\nPLT-MLC, resulting in significant performance degradation on the two proposed\nPLT-MLC benchmarks. Therefore, we propose an end-to-end learning framework:\n\\textbf{CO}rrection $\\rightarrow$ \\textbf{M}odificat\\textbf{I}on $\\rightarrow$\nbalan\\textbf{C}e, abbreviated as \\textbf{\\method{}}. Our bootstrapping\nphilosophy is to simultaneously correct the missing labels (Correction) with\nconvinced prediction confidence over a class-aware threshold and to learn from\nthese recall labels during training. We next propose a novel multi-focal\nmodifier loss that simultaneously addresses head-tail imbalance and\npositive-negative imbalance to adaptively modify the attention to different\nsamples (Modification) under the LT class distribution. In addition, we develop\na balanced training strategy by distilling the model's learning effect from\nhead and tail samples, and thus design a balanced classifier (Balance)\nconditioned on the head and tail learning effect to maintain stable performance\nfor all samples. Our experimental study shows that the proposed \\method{}\nsignificantly outperforms general MLC, LT-MLC and PL-MLC methods in terms of\neffectiveness and robustness on our newly created PLT-MLC datasets.",
        "authors": [
            "Wenqiao Zhang",
            "Changshuo Liu",
            "Lingze Zeng",
            "Beng Chin Ooi",
            "Siliang Tang",
            "Yueting Zhuang"
        ]
    },
    {
        "title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance",
        "url": "http://arxiv.org/abs/2309.07403",
        "abstract": "In real-world scenarios, typical visual recognition systems could fail under\ntwo major causes, i.e., the misclassification between known classes and the\nexcusable misbehavior on unknown-class images. To tackle these deficiencies,\nflexible visual recognition should dynamically predict multiple classes when\nthey are unconfident between choices and reject making predictions when the\ninput is entirely out of the training distribution. Two challenges emerge along\nwith this novel task. First, prediction uncertainty should be separately\nquantified as confusion depicting inter-class uncertainties and ignorance\nidentifying out-of-distribution samples. Second, both confusion and ignorance\nshould be comparable between samples to enable effective decision-making. In\nthis paper, we propose to model these two sources of uncertainty explicitly\nwith the theory of Subjective Logic. Regarding recognition as an\nevidence-collecting process, confusion is then defined as conflicting evidence,\nwhile ignorance is the absence of evidence. By predicting Dirichlet\nconcentration parameters for singletons, comprehensive subjective opinions,\nincluding confusion and ignorance, could be achieved via further evidence\ncombinations. Through a series of experiments on synthetic data analysis,\nvisual recognition, and open-set detection, we demonstrate the effectiveness of\nour methods in quantifying two sources of uncertainties and dealing with\nflexible recognition.",
        "authors": [
            "Lei Fan",
            "Bo Liu",
            "Haoxiang Li",
            "Ying Wu",
            "Gang Hua"
        ]
    },
    {
        "title": "Texture Generation on 3D Meshes with Point-UV Diffusion",
        "url": "http://arxiv.org/abs/2308.10490",
        "abstract": "In this work, we focus on synthesizing high-quality textures on 3D meshes. We\npresent Point-UV diffusion, a coarse-to-fine pipeline that marries the\ndenoising diffusion model with UV mapping to generate 3D consistent and\nhigh-quality texture images in UV space. We start with introducing a point\ndiffusion model to synthesize low-frequency texture components with our\ntailored style guidance to tackle the biased color distribution. The derived\ncoarse texture offers global consistency and serves as a condition for the\nsubsequent UV diffusion stage, aiding in regularizing the model to generate a\n3D consistent UV texture image. Then, a UV diffusion model with hybrid\nconditions is developed to enhance the texture fidelity in the 2D UV space. Our\nmethod can process meshes of any genus, generating diversified,\ngeometry-compatible, and high-fidelity textures. Code is available at\nhttps://cvmi-lab.github.io/Point-UV-Diffusion",
        "authors": [
            "Xin Yu",
            "Peng Dai",
            "Wenbo Li",
            "Lan Ma",
            "Zhengzhe Liu",
            "Xiaojuan Qi"
        ]
    },
    {
        "title": "Supervised Homography Learning with Realistic Dataset Generation",
        "url": "http://arxiv.org/abs/2307.15353",
        "abstract": "In this paper, we propose an iterative framework, which consists of two\nphases: a generation phase and a training phase, to generate realistic training\ndata and yield a supervised homography network. In the generation phase, given\nan unlabeled image pair, we utilize the pre-estimated dominant plane masks and\nhomography of the pair, along with another sampled homography that serves as\nground truth to generate a new labeled training pair with realistic motion. In\nthe training phase, the generated data is used to train the supervised\nhomography network, in which the training data is refined via a content\nconsistency module and a quality assessment module. Once an iteration is\nfinished, the trained network is used in the next data generation phase to\nupdate the pre-estimated homography. Through such an iterative strategy, the\nquality of the dataset and the performance of the network can be gradually and\nsimultaneously improved. Experimental results show that our method achieves\nstate-of-the-art performance and existing supervised methods can be also\nimproved based on the generated dataset. Code and dataset are available at\nhttps://github.com/JianghaiSCU/RealSH.",
        "authors": [
            "Hai Jiang",
            "Haipeng Li",
            "Songchen Han",
            "Haoqiang Fan",
            "Bing Zeng",
            "Shuaicheng Liu"
        ]
    },
    {
        "title": "TALL: Thumbnail Layout for Deepfake Video Detection",
        "url": "http://arxiv.org/abs/2307.07494",
        "abstract": "The growing threats of deepfakes to society and cybersecurity have raised\nenormous public concerns, and increasing efforts have been devoted to this\ncritical topic of deepfake video detection. Existing video methods achieve good\nperformance but are computationally intensive. This paper introduces a simple\nyet effective strategy named Thumbnail Layout (TALL), which transforms a video\nclip into a pre-defined layout to realize the preservation of spatial and\ntemporal dependencies. Specifically, consecutive frames are masked in a fixed\nposition in each frame to improve generalization, then resized to sub-images\nand rearranged into a pre-defined layout as the thumbnail. TALL is\nmodel-agnostic and extremely simple by only modifying a few lines of code.\nInspired by the success of vision transformers, we incorporate TALL into Swin\nTransformer, forming an efficient and effective method TALL-Swin. Extensive\nexperiments on intra-dataset and cross-dataset validate the validity and\nsuperiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\\%$ AUC on the\nchallenging cross-dataset task, FaceForensics++ $\\to$ Celeb-DF. The code is\navailable at https://github.com/rainy-xu/TALL4Deepfake.",
        "authors": [
            "Yuting Xu",
            "Jian Liang",
            "Gengyun Jia",
            "Ziming Yang",
            "Yanhao Zhang",
            "Ran He"
        ]
    },
    {
        "title": "Fast Neural Scene Flow",
        "url": "http://arxiv.org/abs/2304.09121",
        "abstract": "Neural Scene Flow Prior (NSFP) is of significant interest to the vision\ncommunity due to its inherent robustness to out-of-distribution (OOD) effects\nand its ability to deal with dense lidar points. The approach utilizes a\ncoordinate neural network to estimate scene flow at runtime, without any\ntraining. However, it is up to 100 times slower than current state-of-the-art\nlearning methods. In other applications such as image, video, and radiance\nfunction reconstruction innovations in speeding up the runtime performance of\ncoordinate networks have centered upon architectural changes. In this paper, we\ndemonstrate that scene flow is different -- with the dominant computational\nbottleneck stemming from the loss function itself (i.e., Chamfer distance).\nFurther, we rediscover the distance transform (DT) as an efficient,\ncorrespondence-free loss function that dramatically speeds up the runtime\noptimization. Our fast neural scene flow (FNSF) approach reports for the first\ntime real-time performance comparable to learning methods, without any training\nor OOD bias on two of the largest open autonomous driving (AV) lidar datasets\nWaymo Open and Argoverse.",
        "authors": [
            "Xueqian Li",
            "Jianqiao Zheng",
            "Francesco Ferroni",
            "Jhony Kaesemodel Pontes",
            "Simon Lucey"
        ]
    },
    {
        "title": "CAME: Contrastive Automated Model Evaluation",
        "url": "http://arxiv.org/abs/2308.11111",
        "abstract": "The Automated Model Evaluation (AutoEval) framework entertains the\npossibility of evaluating a trained machine learning model without resorting to\na labeled testing set. Despite the promise and some decent results, the\nexisting AutoEval methods heavily rely on computing distribution shifts between\nthe unlabelled testing set and the training set. We believe this reliance on\nthe training set becomes another obstacle in shipping this technology to\nreal-world ML development. In this work, we propose Contrastive Automatic Model\nEvaluation (CAME), a novel AutoEval framework that is rid of involving training\nset in the loop. The core idea of CAME bases on a theoretical analysis which\nbonds the model performance with a contrastive loss. Further, with extensive\nempirical validation, we manage to set up a predictable relationship between\nthe two, simply by deducing on the unlabeled/unseen testing set. The resulting\nframework CAME establishes a new SOTA results for AutoEval by surpassing prior\nwork significantly.",
        "authors": [
            "Ru Peng",
            "Qiuyang Duan",
            "Haobo Wang",
            "Jiachen Ma",
            "Yanbo Jiang",
            "Yongjun Tu",
            "Xiu Jiang",
            "Junbo Zhao"
        ]
    },
    {
        "title": "ExposureDiffusion: Learning to Expose for Low-light Image Enhancement",
        "url": "http://arxiv.org/abs/2307.07710",
        "abstract": "Previous raw image-based low-light image enhancement methods predominantly\nrelied on feed-forward neural networks to learn deterministic mappings from\nlow-light to normally-exposed images. However, they failed to capture critical\ndistribution information, leading to visually undesirable results. This work\naddresses the issue by seamlessly integrating a diffusion model with a\nphysics-based exposure model. Different from a vanilla diffusion model that has\nto perform Gaussian denoising, with the injected physics-based exposure model,\nour restoration process can directly start from a noisy image instead of pure\nnoise. As such, our method obtains significantly improved performance and\nreduced inference time compared with vanilla diffusion models. To make full use\nof the advantages of different intermediate steps, we further propose an\nadaptive residual layer that effectively screens out the side-effect in the\niterative refinement when the intermediate results have been already\nwell-exposed. The proposed framework can work with both real-paired datasets,\nSOTA noise models, and different backbone networks. Note that, the proposed\nframework is compatible with real-paired datasets, real/synthetic noise models,\nand different backbone networks. We evaluate the proposed method on various\npublic benchmarks, achieving promising results with consistent improvements\nusing different exposure models and backbones. Besides, the proposed method\nachieves better generalization capacity for unseen amplifying ratios and better\nperformance than a larger feedforward neural model when few parameters are\nadopted.",
        "authors": [
            "Yufei Wang",
            "Yi Yu",
            "Wenhan Yang",
            "Lanqing Guo",
            "Lap-Pui Chau",
            "Alex C. Kot",
            "Bihan Wen"
        ]
    },
    {
        "title": "HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces",
        "url": "http://arxiv.org/abs/2307.10797",
        "abstract": "In this paper, we present our method for neural face reenactment, called\nHyperReenact, that aims to generate realistic talking head images of a source\nidentity, driven by a target facial pose. Existing state-of-the-art face\nreenactment methods train controllable generative models that learn to\nsynthesize realistic facial images, yet producing reenacted faces that are\nprone to significant visual artifacts, especially under the challenging\ncondition of extreme head pose changes, or requiring expensive few-shot\nfine-tuning to better preserve the source identity characteristics. We propose\nto address these limitations by leveraging the photorealistic generation\nability and the disentangled properties of a pretrained StyleGAN2 generator, by\nfirst inverting the real images into its latent space and then using a\nhypernetwork to perform: (i) refinement of the source identity characteristics\nand (ii) facial pose re-targeting, eliminating this way the dependence on\nexternal editing methods that typically produce artifacts. Our method operates\nunder the one-shot setting (i.e., using a single source frame) and allows for\ncross-subject reenactment, without requiring any subject-specific fine-tuning.\nWe compare our method both quantitatively and qualitatively against several\nstate-of-the-art techniques on the standard benchmarks of VoxCeleb1 and\nVoxCeleb2, demonstrating the superiority of our approach in producing\nartifact-free images, exhibiting remarkable robustness even under extreme head\npose changes. We make the code and the pretrained models publicly available at:\nhttps://github.com/StelaBou/HyperReenact .",
        "authors": [
            "Stella Bounareli",
            "Christos Tzelepis",
            "Vasileios Argyriou",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ]
    },
    {
        "title": "Order-preserving Consistency Regularization for Domain Adaptation and Generalization",
        "url": "http://arxiv.org/abs/2309.13258",
        "abstract": "Deep learning models fail on cross-domain challenges if the model is\noversensitive to domain-specific attributes, e.g., lightning, background,\ncamera angle, etc. To alleviate this problem, data augmentation coupled with\nconsistency regularization are commonly adopted to make the model less\nsensitive to domain-specific attributes. Consistency regularization enforces\nthe model to output the same representation or prediction for two views of one\nimage. These constraints, however, are either too strict or not\norder-preserving for the classification probabilities. In this work, we propose\nthe Order-preserving Consistency Regularization (OCR) for cross-domain tasks.\nThe order-preserving property for the prediction makes the model robust to\ntask-irrelevant transformations. As a result, the model becomes less sensitive\nto the domain-specific attributes. The comprehensive experiments show that our\nmethod achieves clear advantages on five different cross-domain tasks.",
        "authors": [
            "Mengmeng Jing",
            "Xiantong Zhen",
            "Jingjing Li",
            "Cees Snoek"
        ]
    },
    {
        "title": "RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D",
        "url": "http://arxiv.org/abs/2308.12035",
        "abstract": "Grounding textual expressions on scene objects from first-person views is a\ntruly demanding capability in developing agents that are aware of their\nsurroundings and behave following intuitive text instructions. Such capability\nis of necessity for glass-devices or autonomous robots to localize referred\nobjects in the real-world. In the conventional referring expression\ncomprehension tasks of images, however, datasets are mostly constructed based\non the web-crawled data and don't reflect diverse real-world structures on the\ntask of grounding textual expressions in diverse objects in the real world.\nRecently, a massive-scale egocentric video dataset of Ego4D was proposed. Ego4D\ncovers around the world diverse real-world scenes including numerous indoor and\noutdoor situations such as shopping, cooking, walking, talking, manufacturing,\netc. Based on egocentric videos of Ego4D, we constructed a broad coverage of\nthe video-based referring expression comprehension dataset: RefEgo. Our dataset\nincludes more than 12k video clips and 41 hours for video-based referring\nexpression comprehension annotation. In experiments, we combine the\nstate-of-the-art 2D referring expression comprehension models with the object\ntracking algorithm, achieving the video-wise referred object tracking even in\ndifficult conditions: the referred object becomes out-of-frame in the middle of\nthe video or multiple similar objects are presented in the video. Codes are\navailable at https://github.com/shuheikurita/RefEgo",
        "authors": [
            "Shuhei Kurita",
            "Naoki Katsura",
            "Eri Onami"
        ]
    },
    {
        "title": "Unified Visual Relationship Detection with Vision and Language Models",
        "url": "http://arxiv.org/abs/2303.08998",
        "abstract": "This work focuses on training a single visual relationship detector\npredicting over the union of label spaces from multiple datasets. Merging\nlabels spanning different datasets could be challenging due to inconsistent\ntaxonomies. The issue is exacerbated in visual relationship detection when\nsecond-order visual semantics are introduced between pairs of objects. To\naddress this challenge, we propose UniVRD, a novel bottom-up method for Unified\nVisual Relationship Detection by leveraging vision and language models (VLMs).\nVLMs provide well-aligned image and text embeddings, where similar\nrelationships are optimized to be close to each other for semantic unification.\nOur bottom-up design enables the model to enjoy the benefit of training with\nboth object detection and visual relationship datasets. Empirical results on\nboth human-object interaction detection and scene-graph generation demonstrate\nthe competitive performance of our model. UniVRD achieves 38.07 mAP on\nHICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP.\nMore importantly, we show that our unified detector performs as well as\ndataset-specific models in mAP, and achieves further improvements when we scale\nup the model. Our code will be made publicly available on GitHub.",
        "authors": [
            "Long Zhao",
            "Liangzhe Yuan",
            "Boqing Gong",
            "Yin Cui",
            "Florian Schroff",
            "Ming-Hsuan Yang",
            "Hartwig Adam",
            "Ting Liu"
        ]
    },
    {
        "title": "Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis",
        "url": "http://arxiv.org/abs/2211.02408",
        "abstract": "While text-to-image synthesis currently enjoys great popularity among\nresearchers and the general public, the security of these models has been\nneglected so far. Many text-guided image generation models rely on pre-trained\ntext encoders from external sources, and their users trust that the retrieved\nmodels will behave as promised. Unfortunately, this might not be the case. We\nintroduce backdoor attacks against text-guided generative models and\ndemonstrate that their text encoders pose a major tampering risk. Our attacks\nonly slightly alter an encoder so that no suspicious model behavior is apparent\nfor image generations with clean prompts. By then inserting a single character\ntrigger into the prompt, e.g., a non-Latin character or emoji, the adversary\ncan trigger the model to either generate images with pre-defined attributes or\nimages following a hidden, potentially malicious description. We empirically\ndemonstrate the high effectiveness of our attacks on Stable Diffusion and\nhighlight that the injection process of a single backdoor takes less than two\nminutes. Besides phrasing our approach solely as an attack, it can also force\nan encoder to forget phrases related to certain concepts, such as nudity or\nviolence, and help to make image generation safer.",
        "authors": [
            "Lukas Struppek",
            "Dominik Hintersdorf",
            "Kristian Kersting"
        ]
    },
    {
        "title": "Downstream-agnostic Adversarial Examples",
        "url": "http://arxiv.org/abs/2307.12280",
        "abstract": "Self-supervised learning usually uses a large amount of unlabeled data to\npre-train an encoder which can be used as a general-purpose feature extractor,\nsuch that downstream users only need to perform fine-tuning operations to enjoy\nthe benefit of \"large model\". Despite this promising prospect, the security of\npre-trained encoder has not been thoroughly investigated yet, especially when\nthe pre-trained encoder is publicly available for commercial use.\n  In this paper, we propose AdvEncoder, the first framework for generating\ndownstream-agnostic universal adversarial examples based on the pre-trained\nencoder. AdvEncoder aims to construct a universal adversarial perturbation or\npatch for a set of natural images that can fool all the downstream tasks\ninheriting the victim pre-trained encoder. Unlike traditional adversarial\nexample works, the pre-trained encoder only outputs feature vectors rather than\nclassification labels. Therefore, we first exploit the high frequency component\ninformation of the image to guide the generation of adversarial examples. Then\nwe design a generative attack framework to construct adversarial\nperturbations/patches by learning the distribution of the attack surrogate\ndataset to improve their attack success rates and transferability. Our results\nshow that an attacker can successfully attack downstream tasks without knowing\neither the pre-training dataset or the downstream dataset. We also tailor four\ndefenses for pre-trained encoders, the results of which further prove the\nattack ability of AdvEncoder.",
        "authors": [
            "Ziqi Zhou",
            "Shengshan Hu",
            "Ruizhi Zhao",
            "Qian Wang",
            "Leo Yu Zhang",
            "Junhui Hou",
            "Hai Jin"
        ]
    },
    {
        "title": "Late Stopping: Avoiding Confidently Learning from Mislabeled Examples",
        "url": "http://arxiv.org/abs/2308.13862",
        "abstract": "Sample selection is a prevalent method in learning with noisy labels, where\nsmall-loss data are typically considered as correctly labeled data. However,\nthis method may not effectively identify clean hard examples with large losses,\nwhich are critical for achieving the model's close-to-optimal generalization\nperformance. In this paper, we propose a new framework, Late Stopping, which\nleverages the intrinsic robust learning ability of DNNs through a prolonged\ntraining process. Specifically, Late Stopping gradually shrinks the noisy\ndataset by removing high-probability mislabeled examples while retaining the\nmajority of clean hard examples in the training set throughout the learning\nprocess. We empirically observe that mislabeled and clean examples exhibit\ndifferences in the number of epochs required for them to be consistently and\ncorrectly classified, and thus high-probability mislabeled examples can be\nremoved. Experimental results on benchmark-simulated and real-world noisy\ndatasets demonstrate that the proposed method outperforms state-of-the-art\ncounterparts.",
        "authors": [
            "Suqin Yuan",
            "Lei Feng",
            "Tongliang Liu"
        ]
    },
    {
        "title": "AerialVLN: Vision-and-Language Navigation for UAVs",
        "url": "http://arxiv.org/abs/2308.06735",
        "abstract": "Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn\nsignificant attention in both computer vision and natural language processing\ncommunities. Existing VLN tasks are built for agents that navigate on the\nground, either indoors or outdoors. However, many tasks require intelligent\nagents to carry out in the sky, such as UAV-based goods delivery,\ntraffic/security patrol, and scenery tour, to name a few. Navigating in the sky\nis more complicated than on the ground because agents need to consider the\nflying height and more complex spatial relationship reasoning. To fill this gap\nand facilitate research in this field, we propose a new task named AerialVLN,\nwhich is UAV-based and towards outdoor environments. We develop a 3D simulator\nrendered by near-realistic pictures of 25 city-level scenarios. Our simulator\nsupports continuous navigation, environment extension and configuration. We\nalso proposed an extended baseline model based on the widely-used\ncross-modal-alignment (CMA) navigation methods. We find that there is still a\nsignificant gap between the baseline model and human performance, which\nsuggests AerialVLN is a new challenging task. Dataset and code is available at\nhttps://github.com/AirVLN/AirVLN.",
        "authors": [
            "Shubo Liu",
            "Hongsheng Zhang",
            "Yuankai Qi",
            "Peng Wang",
            "Yaning Zhang",
            "Qi Wu"
        ]
    },
    {
        "title": "On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion",
        "url": "http://arxiv.org/abs/2308.09942",
        "abstract": "Generalizing deep learning models to unknown target domain distribution with\nlow latency has motivated research into test-time training/adaptation\n(TTT/TTA). Existing approaches often focus on improving test-time training\nperformance under well-curated target domain data. As figured out in this work,\nmany state-of-the-art methods fail to maintain the performance when the target\ndomain is contaminated with strong out-of-distribution (OOD) data, a.k.a.\nopen-world test-time training (OWTTT). The failure is mainly due to the\ninability to distinguish strong OOD samples from regular weak OOD samples. To\nimprove the robustness of OWTTT we first develop an adaptive strong OOD pruning\nwhich improves the efficacy of the self-training TTT method. We further propose\na way to dynamically expand the prototypes to represent strong OOD samples for\nan improved weak/strong OOD data separation. Finally, we regularize\nself-training with distribution alignment and the combination yields the\nstate-of-the-art performance on 5 OWTTT benchmarks. The code is available at\nhttps://github.com/Yushu-Li/OWTTT.",
        "authors": [
            "Yushu Li",
            "Xun Xu",
            "Yongyi Su",
            "Kui Jia"
        ]
    },
    {
        "title": "SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training",
        "url": "http://arxiv.org/abs/2307.08476",
        "abstract": "Skeleton sequence representation learning has shown great advantages for\naction recognition due to its promising ability to model human joints and\ntopology. However, the current methods usually require sufficient labeled data\nfor training computationally expensive models, which is labor-intensive and\ntime-consuming. Moreover, these methods ignore how to utilize the fine-grained\ndependencies among different skeleton joints to pre-train an efficient skeleton\nsequence learning model that can generalize well across different datasets. In\nthis paper, we propose an efficient skeleton sequence learning framework, named\nSkeleton Sequence Learning (SSL). To comprehensively capture the human pose and\nobtain discriminative skeleton sequence representation, we build an asymmetric\ngraph-based encoder-decoder pre-training architecture named SkeletonMAE, which\nembeds skeleton joint sequence into Graph Convolutional Network (GCN) and\nreconstructs the masked skeleton joints and edges based on the prior human\ntopology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated\nwith the Spatial-Temporal Representation Learning (STRL) module to build the\nSSL framework. Extensive experimental results show that our SSL generalizes\nwell across different datasets and outperforms the state-of-the-art\nself-supervised skeleton-based action recognition methods on FineGym, Diving48,\nNTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance to\nsome fully supervised methods. The code is avaliable at\nhttps://github.com/HongYan1123/SkeletonMAE.",
        "authors": [
            "Hong Yan",
            "Yang Liu",
            "Yushen Wei",
            "Zhen Li",
            "Guanbin Li",
            "Liang Lin"
        ]
    },
    {
        "title": "Pose-Free Neural Radiance Fields via Implicit Pose Regularization",
        "url": "http://arxiv.org/abs/2308.15049",
        "abstract": "Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed\nmulti-view images and it has achieved very impressive success in recent years.\nMost existing works share the pipeline of training a coarse pose estimator with\nrendered images at first, followed by a joint optimization of estimated poses\nand neural radiance field. However, as the pose estimator is trained with only\nrendered images, the pose estimation is usually biased or inaccurate for real\nimages due to the domain gap between real images and rendered images, leading\nto poor robustness for the pose estimation of real images and further local\nminima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF\nthat introduces implicit pose regularization to refine pose estimator with\nunposed real images and improve the robustness of the pose estimation for real\nimages. With a collection of 2D images of a specific scene, IR-NeRF constructs\na scene codebook that stores scene features and captures the scene-specific\npose distribution implicitly as priors. Thus, the robustness of pose estimation\ncan be promoted with the scene priors according to the rationale that a 2D real\nimage can be well reconstructed from the scene codebook only when its estimated\npose lies within the pose distribution. Extensive experiments show that IR-NeRF\nachieves superior novel view synthesis and outperforms the state-of-the-art\nconsistently across multiple synthetic and real datasets.",
        "authors": [
            "Jiahui Zhang",
            "Fangneng Zhan",
            "Yingchen Yu",
            "Kunhao Liu",
            "Rongliang Wu",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ]
    },
    {
        "title": "Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive",
        "url": "http://arxiv.org/abs/2305.19862",
        "abstract": "Modern consumer cameras usually employ the rolling shutter (RS) mechanism,\nwhere images are captured by scanning scenes row-by-row, yielding RS\ndistortions for dynamic scenes. To correct RS distortions, existing methods\nadopt a fully supervised learning manner, where high framerate global shutter\n(GS) images should be collected as ground-truth supervision. In this paper, we\npropose a Self-supervised learning framework for Dual reversed RS distortions\nCorrection (SelfDRSC), where a DRSC network can be learned to generate a high\nframerate GS video only based on dual RS images with reversed distortions. In\nparticular, a bidirectional distortion warping module is proposed for\nreconstructing dual reversed RS images, and then a self-supervised loss can be\ndeployed to train DRSC network by enhancing the cycle consistency between input\nand reconstructed dual reversed RS images. Besides start and end RS scanning\ntime, GS images at arbitrary intermediate scanning time can also be supervised\nin SelfDRSC, thus enabling the learned DRSC network to generate a high\nframerate GS video. Moreover, a simple yet effective self-distillation strategy\nis introduced in self-supervised loss for mitigating boundary artifacts in\ngenerated GS images. On synthetic dataset, SelfDRSC achieves better or\ncomparable quantitative metrics in comparison to state-of-the-art methods\ntrained in the full supervision manner. On real-world RS cases, our SelfDRSC\ncan produce high framerate GS videos with finer correction textures and better\ntemporary consistency. The source code and trained models are made publicly\navailable at https://github.com/shangwei5/SelfDRSC. We also provide an\nimplementation in HUAWEI Mindspore at\nhttps://github.com/Hunter-Will/SelfDRSC-mindspore.",
        "authors": [
            "Wei Shang",
            "Dongwei Ren",
            "Chaoyu Feng",
            "Xiaotao Wang",
            "Lei Lei",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.12595",
        "abstract": "Recent advances in semi-supervised semantic segmentation have been heavily\nreliant on pseudo labeling to compensate for limited labeled data, disregarding\nthe valuable relational knowledge among semantic concepts. To bridge this gap,\nwe devise LogicDiag, a brand new neural-logic semi-supervised learning\nframework. Our key insight is that conflicts within pseudo labels, identified\nthrough symbolic knowledge, can serve as strong yet commonly ignored learning\nsignals. LogicDiag resolves such conflicts via reasoning with logic-induced\ndiagnoses, enabling the recovery of (potentially) erroneous pseudo labels,\nultimately alleviating the notorious error accumulation problem. We showcase\nthe practical application of LogicDiag in the data-hungry segmentation\nscenario, where we formalize the structured abstraction of semantic concepts as\na set of logic rules. Extensive experiments on three standard semi-supervised\nsemantic segmentation benchmarks demonstrate the effectiveness and generality\nof LogicDiag. Moreover, LogicDiag highlights the promising opportunities\narising from the systematic integration of symbolic reasoning into the\nprevalent statistical, neural learning approaches.",
        "authors": [
            "Chen Liang",
            "Wenguan Wang",
            "Jiaxu Miao",
            "Yi Yang"
        ]
    },
    {
        "title": "Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network",
        "url": "http://arxiv.org/abs/2308.05605",
        "abstract": "Monocular depth estimation is known as an ill-posed task in which objects in\na 2D image usually do not contain sufficient information to predict their\ndepth. Thus, it acts differently from other tasks (e.g., classification and\nsegmentation) in many ways. In this paper, we find that self-supervised\nmonocular depth estimation shows a direction sensitivity and environmental\ndependency in the feature representation. But the current backbones borrowed\nfrom other tasks pay less attention to handling different types of\nenvironmental information, limiting the overall depth accuracy. To bridge this\ngap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN),\nwhich improves the depth feature representation in two aspects. First, we\npropose a direction-aware module, which can learn to adjust the feature\nextraction in each direction, facilitating the encoding of different types of\ninformation. Secondly, we design a new cumulative convolution to improve the\nefficiency for aggregating important environmental information. Experiments\nshow that our method achieves significant improvements on three widely used\nbenchmarks, KITTI, Cityscapes, and Make3D, setting a new state-of-the-art\nperformance on the popular benchmarks with all three types of self-supervision.",
        "authors": [
            "Wencheng Han",
            "Junbo Yin",
            "Jianbing Shen"
        ]
    },
    {
        "title": "Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories",
        "url": "http://arxiv.org/abs/2306.09224",
        "abstract": "We propose Encyclopedic-VQA, a large scale visual question answering (VQA)\ndataset featuring visual questions about detailed properties of fine-grained\ncategories and instances. It contains 221k unique question+answer pairs each\nmatched with (up to) 5 images, resulting in a total of 1M VQA samples.\nMoreover, our dataset comes with a controlled knowledge base derived from\nWikipedia, marking the evidence to support each answer. Empirically, we show\nthat our dataset poses a hard challenge for large vision+language models as\nthey perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA\n[37], yet it only achieves 13.0% accuracy on our dataset. Moreover, we\nexperimentally show that progress on answering our encyclopedic questions can\nbe achieved by augmenting large models with a mechanism that retrieves relevant\ninformation from the knowledge base. An oracle experiment with perfect\nretrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and\nan automatic retrieval-augmented prototype yields 48.8%. We believe that our\ndataset enables future research on retrieval-augmented vision+language models.\nIt is available at\nhttps://github.com/google-research/google-research/tree/master/encyclopedic_vqa .",
        "authors": [
            "Thomas Mensink",
            "Jasper Uijlings",
            "Lluis Castrejon",
            "Arushi Goel",
            "Felipe Cadar",
            "Howard Zhou",
            "Fei Sha",
            "Andr\u00e9 Araujo",
            "Vittorio Ferrari"
        ]
    },
    {
        "title": "VertexSerum: Poisoning Graph Neural Networks for Link Inference",
        "url": "http://arxiv.org/abs/2308.01469",
        "abstract": "Graph neural networks (GNNs) have brought superb performance to various\napplications utilizing graph structural data, such as social analysis and fraud\ndetection. The graph links, e.g., social relationships and transaction history,\nare sensitive and valuable information, which raises privacy concerns when\nusing GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel\ngraph poisoning attack that increases the effectiveness of graph link stealing\nby amplifying the link connectivity leakage. To infer node adjacency more\naccurately, we propose an attention mechanism that can be embedded into the\nlink detection network. Our experiments demonstrate that VertexSerum\nsignificantly outperforms the SOTA link inference attack, improving the AUC\nscores by an average of $9.8\\%$ across four real-world datasets and three\ndifferent GNN structures. Furthermore, our experiments reveal the effectiveness\nof VertexSerum in both black-box and online learning settings, further\nvalidating its applicability in real-world scenarios.",
        "authors": [
            "Ruyi Ding",
            "Shijin Duan",
            "Xiaolin Xu",
            "Yunsi Fei"
        ]
    },
    {
        "title": "Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception",
        "url": "http://arxiv.org/abs/2307.13929",
        "abstract": "Multi-agent collaborative perception as a potential application for\nvehicle-to-everything communication could significantly improve the perception\nperformance of autonomous vehicles over single-agent perception. However,\nseveral challenges remain in achieving pragmatic information sharing in this\nemerging research. In this paper, we propose SCOPE, a novel collaborative\nperception framework that aggregates the spatio-temporal awareness\ncharacteristics across on-road agents in an end-to-end manner. Specifically,\nSCOPE has three distinct strengths: i) it considers effective semantic cues of\nthe temporal context to enhance current representations of the target agent;\nii) it aggregates perceptually critical spatial information from heterogeneous\nagents and overcomes localization errors via multi-scale feature interactions;\niii) it integrates multi-source representations of the target agent based on\ntheir complementary contributions by an adaptive fusion paradigm. To thoroughly\nevaluate SCOPE, we consider both real-world and simulated scenarios of\ncollaborative 3D object detection tasks on three datasets. Extensive\nexperiments demonstrate the superiority of our approach and the necessity of\nthe proposed components.",
        "authors": [
            "Kun Yang",
            "Dingkang Yang",
            "Jingyu Zhang",
            "Mingcheng Li",
            "Yang Liu",
            "Jing Liu",
            "Hanqi Wang",
            "Peng Sun",
            "Liang Song"
        ]
    },
    {
        "title": "LPFF: A Portrait Dataset for Face Generators Across Large Poses",
        "url": "http://arxiv.org/abs/2303.14407",
        "abstract": "The creation of 2D realistic facial images and 3D face shapes using\ngenerative networks has been a hot topic in recent years. Existing face\ngenerators exhibit exceptional performance on faces in small to medium poses\n(with respect to frontal faces) but struggle to produce realistic results for\nlarge poses. The distorted rendering results on large poses in 3D-aware\ngenerators further show that the generated 3D face shapes are far from the\ndistribution of 3D faces in reality. We find that the above issues are caused\nby the training dataset's pose imbalance.\n  In this paper, we present LPFF, a large-pose Flickr face dataset comprised of\n19,590 high-quality real large-pose portrait images. We utilize our dataset to\ntrain a 2D face generator that can process large-pose face images, as well as a\n3D-aware generator that can generate realistic human face geometry. To better\nvalidate our pose-conditional 3D-aware generators, we develop a new FID measure\nto evaluate the 3D-level performance. Through this novel FID measure and other\nexperiments, we show that LPFF can help 2D face generators extend their latent\nspace and better manipulate the large-pose data, and help 3D-aware face\ngenerators achieve better view consistency and more realistic 3D reconstruction\nresults.",
        "authors": [
            "Yiqian Wu",
            "Jing Zhang",
            "Hongbo Fu",
            "Xiaogang Jin"
        ]
    },
    {
        "title": "Pseudo-label Alignment for Semi-supervised Instance Segmentation",
        "url": "http://arxiv.org/abs/2308.05359",
        "abstract": "Pseudo-labeling is significant for semi-supervised instance segmentation,\nwhich generates instance masks and classes from unannotated images for\nsubsequent training. However, in existing pipelines, pseudo-labels that contain\nvaluable information may be directly filtered out due to mismatches in class\nand mask quality. To address this issue, we propose a novel framework, called\npseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we\ndevise a dynamic aligning loss (DALoss) that adjusts the weights of\nsemi-supervised loss terms with varying class and mask score pairs. Through\nextensive experiments conducted on the COCO and Cityscapes datasets, we\ndemonstrate that PAIS is a promising framework for semi-supervised instance\nsegmentation, particularly in cases where labeled data is severely limited.\nNotably, with just 1\\% labeled data, PAIS achieves 21.2 mAP (based on\nMask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the\ncurrent state-of-the-art model, \\ie, NoisyBoundary with 7.7 mAP, by a margin of\nover 12 points. Code is available at: \\url{https://github.com/hujiecpp/PAIS}.",
        "authors": [
            "Jie Hu",
            "Chen Chen",
            "Liujuan Cao",
            "Shengchuan Zhang",
            "Annan Shu",
            "Guannan Jiang",
            "Rongrong Ji"
        ]
    },
    {
        "title": "MixBag: Bag-Level Data Augmentation for Learning from Label Proportions",
        "url": "http://arxiv.org/abs/2308.08822",
        "abstract": "Learning from label proportions (LLP) is a promising weakly supervised\nlearning problem. In LLP, a set of instances (bag) has label proportions, but\nno instance-level labels are given. LLP aims to train an instance-level\nclassifier by using the label proportions of the bag. In this paper, we propose\na bag-level data augmentation method for LLP called MixBag, based on the key\nobservation from our preliminary experiments; that the instance-level\nclassification accuracy improves as the number of labeled bags increases even\nthough the total number of instances is fixed. We also propose a confidence\ninterval loss designed based on statistical theory to use the augmented bags\neffectively. To the best of our knowledge, this is the first attempt to propose\nbag-level data augmentation for LLP. The advantage of MixBag is that it can be\napplied to instance-level data augmentation techniques and any LLP method that\nuses the proportion loss. Experimental results demonstrate this advantage and\nthe effectiveness of our method.",
        "authors": [
            "Takanori Asanomi",
            "Shinnosuke Matsuo",
            "Daiki Suehiro",
            "Ryoma Bise"
        ]
    },
    {
        "title": "Effective Real Image Editing with Accelerated Iterative Diffusion Inversion",
        "url": "http://arxiv.org/abs/2309.04907",
        "abstract": "Despite all recent progress, it is still challenging to edit and manipulate\nnatural images with modern generative models. When using Generative Adversarial\nNetwork (GAN), one major hurdle is in the inversion process mapping a real\nimage to its corresponding noise vector in the latent space, since its\nnecessary to be able to reconstruct an image to edit its contents. Likewise for\nDenoising Diffusion Implicit Models (DDIM), the linearization assumption in\neach inversion step makes the whole deterministic inversion process unreliable.\nExisting approaches that have tackled the problem of inversion stability often\nincur in significant trade-offs in computational efficiency. In this work we\npropose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that\nsignificantly improves reconstruction accuracy with minimal additional overhead\nin space and time complexity. By using a novel blended guidance technique, we\nshow that effective results can be obtained on a large range of image editing\ntasks without large classifier-free guidance in inversion. Furthermore, when\ncompared with other diffusion inversion based works, our proposed process is\nshown to be more robust for fast image editing in the 10 and 20 diffusion\nsteps' regimes.",
        "authors": [
            "Zhihong Pan",
            "Riccardo Gherardi",
            "Xiufeng Xie",
            "Stephen Huang"
        ]
    },
    {
        "title": "3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation",
        "url": "http://arxiv.org/abs/2308.10123",
        "abstract": "Regression-based methods for 3D human pose estimation directly predict the 3D\npose parameters from a 2D image using deep networks. While achieving\nstate-of-the-art performance on standard benchmarks, their performance degrades\nunder occlusion. In contrast, optimization-based methods fit a parametric body\nmodel to 2D features in an iterative manner. The localized reconstruction loss\ncan potentially make them robust to occlusion, but they suffer from the 2D-3D\nambiguity.\n  Motivated by the recent success of generative models in rigid object pose\nestimation, we propose 3D-aware Neural Body Fitting (3DNBF) - an approximate\nanalysis-by-synthesis approach to 3D human pose estimation with SOTA\nperformance and occlusion robustness. In particular, we propose a generative\nmodel of deep features based on a volumetric human representation with Gaussian\nellipsoidal kernels emitting 3D pose-dependent feature vectors. The neural\nfeatures are trained with contrastive learning to become 3D-aware and hence to\novercome the 2D-3D ambiguity.\n  Experiments show that 3DNBF outperforms other approaches on both occluded and\nstandard benchmarks. Code is available at https://github.com/edz-o/3DNBF",
        "authors": [
            "Yi Zhang",
            "Pengliang Ji",
            "Angtian Wang",
            "Jieru Mei",
            "Adam Kortylewski",
            "Alan Yuille"
        ]
    },
    {
        "title": "Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning",
        "url": "http://arxiv.org/abs/2309.01083",
        "abstract": "Scene text recognition has been studied for decades due to its broad\napplications. However, despite Chinese characters possessing different\ncharacteristics from Latin characters, such as complex inner structures and\nlarge categories, few methods have been proposed for Chinese Text Recognition\n(CTR). Particularly, the characteristic of large categories poses challenges in\ndealing with zero-shot and few-shot Chinese characters. In this paper, inspired\nby the way humans recognize Chinese texts, we propose a two-stage framework for\nCTR. Firstly, we pre-train a CLIP-like model through aligning printed character\nimages and Ideographic Description Sequences (IDS). This pre-training stage\nsimulates humans recognizing Chinese characters and obtains the canonical\nrepresentation of each character. Subsequently, the learned representations are\nemployed to supervise the CTR model, such that traditional single-character\nrecognition can be improved to text-line recognition through image-IDS\nmatching. To evaluate the effectiveness of the proposed method, we conduct\nextensive experiments on both Chinese character recognition (CCR) and CTR. The\nexperimental results demonstrate that the proposed method performs best in CCR\nand outperforms previous methods in most scenarios of the CTR benchmark. It is\nworth noting that the proposed method can recognize zero-shot Chinese\ncharacters in text images without fine-tuning, whereas previous methods require\nfine-tuning when new classes appear. The code is available at\nhttps://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR.",
        "authors": [
            "Haiyang Yu",
            "Xiaocong Wang",
            "Bin Li",
            "Xiangyang Xue"
        ]
    },
    {
        "title": "LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis",
        "url": "http://arxiv.org/abs/2301.04604",
        "abstract": "This work presents an easy-to-use regularizer for GAN training, which helps\nexplicitly link some axes of the latent space to a set of pixels in the\nsynthesized image. Establishing such a connection facilitates a more convenient\nlocal control of GAN generation, where users can alter the image content only\nwithin a spatial area simply by partially resampling the latent code.\nExperimental results confirm four appealing properties of our regularizer,\nwhich we call LinkGAN. (1) The latent-pixel linkage is applicable to either a\nfixed region (\\textit{i.e.}, same for all instances) or a particular semantic\ncategory (i.e., varying across instances), like the sky. (2) Two or multiple\nregions can be independently linked to different latent axes, which further\nsupports joint control. (3) Our regularizer can improve the spatial\ncontrollability of both 2D and 3D-aware GAN models, barely sacrificing the\nsynthesis performance. (4) The models trained with our regularizer are\ncompatible with GAN inversion techniques and maintain editability on real\nimages.",
        "authors": [
            "Jiapeng Zhu",
            "Ceyuan Yang",
            "Yujun Shen",
            "Zifan Shi",
            "Bo Dai",
            "Deli Zhao",
            "Qifeng Chen"
        ]
    },
    {
        "title": "Exploiting Proximity-Aware Tasks for Embodied Social Navigation",
        "url": "http://arxiv.org/abs/2212.00767",
        "abstract": "Learning how to navigate among humans in an occluded and spatially\nconstrained indoor environment, is a key ability required to embodied agent to\nbe integrated into our society. In this paper, we propose an end-to-end\narchitecture that exploits Proximity-Aware Tasks (referred as to Risk and\nProximity Compass) to inject into a reinforcement learning navigation policy\nthe ability to infer common-sense social behaviors. To this end, our tasks\nexploit the notion of immediate and future dangers of collision. Furthermore,\nwe propose an evaluation protocol specifically designed for the Social\nNavigation Task in simulated environments. This is done to capture fine-grained\nfeatures and characteristics of the policy by analyzing the minimal unit of\nhuman-robot spatial interaction, called Encounter. We validate our approach on\nGibson4+ and Habitat-Matterport3D datasets.",
        "authors": [
            "Enrico Cancelli",
            "Tommaso Campari",
            "Luciano Serafini",
            "Angel X. Chang",
            "Lamberto Ballan"
        ]
    },
    {
        "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning",
        "url": "http://arxiv.org/abs/2303.11305",
        "abstract": "Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size compared to existing methods (approximately\n2,200 times fewer parameters compared with vanilla DreamBooth), making it more\npractical for real-world applications.",
        "authors": [
            "Ligong Han",
            "Yinxiao Li",
            "Han Zhang",
            "Peyman Milanfar",
            "Dimitris Metaxas",
            "Feng Yang"
        ]
    },
    {
        "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers",
        "url": "http://arxiv.org/abs/2308.10814",
        "abstract": "Quantization scale and bit-width are the most important parameters when\nconsidering how to quantize a neural network. Prior work focuses on optimizing\nquantization scales in a global manner through gradient methods (gradient\ndescent \\& Hessian analysis). Yet, when applying perturbations to quantization\nscales, we observe a very jagged, highly non-smooth test loss landscape. In\nfact, small perturbations in quantization scale can greatly affect accuracy,\nyielding a $0.5-0.8\\%$ accuracy boost in 4-bit quantized vision transformers\n(ViTs). In this regime, gradient methods break down, since they cannot reliably\nreach local minima. In our work, dubbed Evol-Q, we use evolutionary search to\neffectively traverse the non-smooth landscape. Additionally, we propose using\nan infoNCE loss, which not only helps combat overfitting on the small\ncalibration dataset ($1,000$ images) but also makes traversing such a highly\nnon-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully\nquantized ViT-Base by $10.30\\%$, $0.78\\%$, and $0.15\\%$ for $3$-bit, $4$-bit,\nand $8$-bit weight quantization levels. Extensive experiments on a variety of\nCNN and ViT architectures further demonstrate its robustness in extreme\nquantization scenarios. Our code is available at\nhttps://github.com/enyac-group/evol-q",
        "authors": [
            "Natalia Frumkin",
            "Dibakar Gope",
            "Diana Marculescu"
        ]
    },
    {
        "title": "Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection",
        "url": "http://arxiv.org/abs/2308.14061",
        "abstract": "Effective image restoration with large-size corruptions, such as blind image\ninpainting, entails precise detection of corruption region masks which remains\nextremely challenging due to diverse shapes and patterns of corruptions. In\nthis work, we present a novel method for automatic corruption detection, which\nallows for blind corruption restoration without known corruption masks.\nSpecifically, we develop a hierarchical contrastive learning framework to\ndetect corrupted regions by capturing the intrinsic semantic distinctions\nbetween corrupted and uncorrupted regions. In particular, our model detects the\ncorrupted mask in a coarse-to-fine manner by first predicting a coarse mask by\ncontrastive learning in low-resolution feature space and then refines the\nuncertain area of the mask by high-resolution contrastive learning. A\nspecialized hierarchical interaction mechanism is designed to facilitate the\nknowledge propagation of contrastive learning in different scales, boosting the\nmodeling performance substantially. The detected multi-scale corruption masks\nare then leveraged to guide the corruption restoration. Detecting corrupted\nregions by learning the contrastive distinctions rather than the semantic\npatterns of corruptions, our model has well generalization ability across\ndifferent corruption patterns. Extensive experiments demonstrate following\nmerits of our model: 1) the superior performance over other methods on both\ncorruption detection and various image restoration tasks including blind\ninpainting and watermark removal, and 2) strong generalization across different\ncorruption patterns such as graffiti, random noise or other image content.\nCodes and trained weights are available at https://github.com/xyfJASON/HCL .",
        "authors": [
            "Xin Feng",
            "Yifeng Xu",
            "Guangming Lu",
            "Wenjie Pei"
        ]
    },
    {
        "title": "Learning Optical Flow from Event Camera with Rendered Dataset",
        "url": "http://arxiv.org/abs/2303.11011",
        "abstract": "We study the problem of estimating optical flow from event cameras. One\nimportant issue is how to build a high-quality event-flow dataset with accurate\nevent values and flow labels. Previous datasets are created by either capturing\nreal scenes by event cameras or synthesizing from images with pasted foreground\nobjects. The former case can produce real event values but with calculated flow\nlabels, which are sparse and inaccurate. The later case can generate dense flow\nlabels but the interpolated events are prone to errors. In this work, we\npropose to render a physically correct event-flow dataset using computer\ngraphics models. In particular, we first create indoor and outdoor 3D scenes by\nBlender with rich scene content variations. Second, diverse camera motions are\nincluded for the virtual capturing, producing images and accurate flow labels.\nThird, we render high-framerate videos between images for accurate events. The\nrendered dataset can adjust the density of events, based on which we further\nintroduce an adaptive density module (ADM). Experiments show that our proposed\ndataset can facilitate event-flow learning, whereas previous approaches when\ntrained on our dataset can improve their performances constantly by a\nrelatively large margin. In addition, event-flow pipelines when equipped with\nour ADM can further improve performances.",
        "authors": [
            "Xinglong Luo",
            "Kunming Luo",
            "Ao Luo",
            "Zhengning Wang",
            "Ping Tan",
            "Shuaicheng Liu"
        ]
    },
    {
        "title": "EPiC: Ensemble of Partial Point Clouds for Robust Classification",
        "url": "http://arxiv.org/abs/2303.11419",
        "abstract": "Robust point cloud classification is crucial for real-world applications, as\nconsumer-type 3D sensors often yield partial and noisy data, degraded by\nvarious artifacts. In this work we propose a general ensemble framework, based\non partial point cloud sampling. Each ensemble member is exposed to only\npartial input data. Three sampling strategies are used jointly, two local ones,\nbased on patches and curves, and a global one of random sampling. We\ndemonstrate the robustness of our method to various local and global\ndegradations. We show that our framework significantly improves the robustness\nof top classification netowrks by a large margin. Our experimental setting uses\nthe recently introduced ModelNet-C database by Ren et al.[24], where we reach\nSOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption\nError (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current\nSOTA is 0.57). We analyze and explain these remarkable results through\ndiversity analysis. Our code is available at:\nhttps://github.com/yossilevii100/EPiC",
        "authors": [
            "Meir Yossef Levi",
            "Guy Gilboa"
        ]
    },
    {
        "title": "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability",
        "url": "http://arxiv.org/abs/2307.03135",
        "abstract": "Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood",
        "authors": [
            "Xuanlin Li",
            "Yunhao Fang",
            "Minghua Liu",
            "Zhan Ling",
            "Zhuowen Tu",
            "Hao Su"
        ]
    },
    {
        "title": "Cross-Modal Learning with 3D Deformable Attention for Action Recognition",
        "url": "http://arxiv.org/abs/2212.05638",
        "abstract": "An important challenge in vision-based action recognition is the embedding of\nspatiotemporal features with two or more heterogeneous modalities into a single\nfeature. In this study, we propose a new 3D deformable transformer for action\nrecognition with adaptive spatiotemporal receptive fields and a cross-modal\nlearning scheme. The 3D deformable transformer consists of three attention\nmodules: 3D deformability, local joint stride, and temporal stride attention.\nThe two cross-modal tokens are input into the 3D deformable attention module to\ncreate a cross-attention token with a reflected spatiotemporal correlation.\nLocal joint stride attention is applied to spatially combine attention and pose\ntokens. Temporal stride attention temporally reduces the number of input tokens\nin the attention module and supports temporal expression learning without the\nsimultaneous use of all tokens. The deformable transformer iterates L-times and\ncombines the last cross-modal token for classification. The proposed 3D\ndeformable transformer was tested on the NTU60, NTU120, FineGYM, and PennAction\ndatasets, and showed results better than or similar to pre-trained\nstate-of-the-art methods even without a pre-training process. In addition, by\nvisualizing important joints and correlations during action recognition through\nspatial joint and temporal stride attention, the possibility of achieving an\nexplainable potential for action recognition is presented.",
        "authors": [
            "Sangwon Kim",
            "Dasom Ahn",
            "Byoung Chul Ko"
        ]
    },
    {
        "title": "What do neural networks learn in image classification? A frequency shortcut perspective",
        "url": "http://arxiv.org/abs/2307.09829",
        "abstract": "Frequency analysis is useful for understanding the mechanisms of\nrepresentation learning in neural networks (NNs). Most research in this area\nfocuses on the learning dynamics of NNs for regression tasks, while little for\nclassification. This study empirically investigates the latter and expands the\nunderstanding of frequency shortcuts. First, we perform experiments on\nsynthetic datasets, designed to have a bias in different frequency bands. Our\nresults demonstrate that NNs tend to find simple solutions for classification,\nand what they learn first during training depends on the most distinctive\nfrequency characteristics, which can be either low- or high-frequencies.\nSecond, we confirm this phenomenon on natural images. We propose a metric to\nmeasure class-wise frequency characteristics and a method to identify frequency\nshortcuts. The results show that frequency shortcuts can be texture-based or\nshape-based, depending on what best simplifies the objective. Third, we\nvalidate the transferability of frequency shortcuts on out-of-distribution\n(OOD) test sets. Our results suggest that frequency shortcuts can be\ntransferred across datasets and cannot be fully avoided by larger model\ncapacity and data augmentation. We recommend that future research should focus\non effective training schemes mitigating frequency shortcut learning.",
        "authors": [
            "Shunxin Wang",
            "Raymond Veldhuis",
            "Christoph Brune",
            "Nicola Strisciuglio"
        ]
    },
    {
        "title": "Tracking by 3D Model Estimation of Unknown Objects in Videos",
        "url": "http://arxiv.org/abs/2304.06419",
        "abstract": "Most model-free visual object tracking methods formulate the tracking task as\nobject location estimation given by a 2D segmentation or a bounding box in each\nvideo frame. We argue that this representation is limited and instead propose\nto guide and improve 2D tracking with an explicit object representation, namely\nthe textured 3D shape and 6DoF pose in each video frame. Our representation\ntackles a complex long-term dense correspondence problem between all 3D points\non the object for all video frames, including frames where some points are\ninvisible. To achieve that, the estimation is driven by re-rendering the input\nvideo frames as well as possible through differentiable rendering, which has\nnot been used for tracking before. The proposed optimization minimizes a novel\nloss function to estimate the best 3D shape, texture, and 6DoF pose. We improve\nthe state-of-the-art in 2D segmentation tracking on three different datasets\nwith mostly rigid objects.",
        "authors": [
            "Denys Rozumnyi",
            "Jiri Matas",
            "Marc Pollefeys",
            "Vittorio Ferrari",
            "Martin R. Oswald"
        ]
    },
    {
        "title": "ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering",
        "url": "http://arxiv.org/abs/2305.02103",
        "abstract": "Vision in adverse weather conditions, whether it be snow, rain, or fog is\nchallenging. In these scenarios, scattering and attenuation severly degrades\nimage quality. Handling such inclement weather conditions, however, is\nessential to operate autonomous vehicles, drones and robotic applications where\nhuman performance is impeded the most. A large body of work explores removing\nweather-induced image degradations with dehazing methods. Most methods rely on\nsingle images as input and struggle to generalize from synthetic\nfully-supervised training approaches or to generate high fidelity results from\nunpaired real-world datasets. With data as bottleneck and most of today's\ntraining data relying on good weather conditions with inclement weather as\noutlier, we rely on an inverse rendering approach to reconstruct the scene\ncontent. We introduce ScatterNeRF, a neural rendering method which adequately\nrenders foggy scenes and decomposes the fog-free background from the\nparticipating media-exploiting the multiple views from a short automotive\nsequence without the need for a large training data corpus. Instead, the\nrendering approach is optimized on the multi-view scene itself, which can be\ntypically captured by an autonomous vehicle, robot or drone during operation.\nSpecifically, we propose a disentangled representation for the scattering\nvolume and the scene objects, and learn the scene reconstruction with\nphysics-inspired losses. We validate our method by capturing multi-view\nIn-the-Wild data and controlled captures in a large-scale fog chamber.",
        "authors": [
            "Andrea Ramazzina",
            "Mario Bijelic",
            "Stefanie Walz",
            "Alessandro Sanvito",
            "Dominik Scheuble",
            "Felix Heide"
        ]
    },
    {
        "title": "Sigmoid Loss for Language Image Pre-Training",
        "url": "http://arxiv.org/abs/2303.15343",
        "abstract": "We propose a simple pairwise Sigmoid loss for Language-Image Pre-training\n(SigLIP). Unlike standard contrastive learning with softmax normalization, the\nsigmoid loss operates solely on image-text pairs and does not require a global\nview of the pairwise similarities for normalization. The sigmoid loss\nsimultaneously allows further scaling up the batch size, while also performing\nbetter at smaller batch sizes. Combined with Locked-image Tuning, with only\nfour TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet\nzero-shot accuracy in two days. The disentanglement of the batch size from the\nloss further allows us to study the impact of examples vs pairs and negative to\npositive ratio. Finally, we push the batch size to the extreme, up to one\nmillion, and find that the benefits of growing batch size quickly diminish,\nwith a more reasonable batch size of 32k being sufficient. We release our\nmodels at https://github.com/google-research/big_vision and hope our research\nmotivates further explorations in improving the quality and efficiency of\nlanguage-image pre-training.",
        "authors": [
            "Xiaohua Zhai",
            "Basil Mustafa",
            "Alexander Kolesnikov",
            "Lucas Beyer"
        ]
    },
    {
        "title": "Neural Video Depth Stabilizer",
        "url": "http://arxiv.org/abs/2307.08695",
        "abstract": "Video depth estimation aims to infer temporally consistent depth. Some\nmethods achieve temporal consistency by finetuning a single-image depth model\nduring test time using geometry and re-projection constraints, which is\ninefficient and not robust. An alternative approach is to learn how to enforce\ntemporal consistency from data, but this requires well-designed models and\nsufficient video depth data. To address these challenges, we propose a\nplug-and-play framework called Neural Video Depth Stabilizer (NVDS) that\nstabilizes inconsistent depth estimations and can be applied to different\nsingle-image depth models without extra effort. We also introduce a large-scale\ndataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with\nover two million frames, making it the largest natural-scene video depth\ndataset to our knowledge. We evaluate our method on the VDW dataset as well as\ntwo public benchmarks and demonstrate significant improvements in consistency,\naccuracy, and efficiency compared to previous approaches. Our work serves as a\nsolid baseline and provides a data foundation for learning-based video depth\nmodels. We will release our dataset and code for future research.",
        "authors": [
            "Yiran Wang",
            "Min Shi",
            "Jiaqi Li",
            "Zihao Huang",
            "Zhiguo Cao",
            "Jianming Zhang",
            "Ke Xian",
            "Guosheng Lin"
        ]
    },
    {
        "title": "TrackFlow: Multi-Object tracking with Normalizing Flows",
        "url": "http://arxiv.org/abs/2308.11513",
        "abstract": "The field of multi-object tracking has recently seen a renewed interest in\nthe good old schema of tracking-by-detection, as its simplicity and strong\npriors spare it from the complex design and painful babysitting of\ntracking-by-attention approaches. In view of this, we aim at extending\ntracking-by-detection to multi-modal settings, where a comprehensive cost has\nto be computed from heterogeneous information e.g., 2D motion cues, visual\nappearance, and pose estimates. More precisely, we follow a case study where a\nrough estimate of 3D information is also available and must be merged with\nother traditional metrics (e.g., the IoU). To achieve that, recent approaches\nresort to either simple rules or complex heuristics to balance the contribution\nof each cost. However, i) they require careful tuning of tailored\nhyperparameters on a hold-out set, and ii) they imply these costs to be\nindependent, which does not hold in reality. We address these issues by\nbuilding upon an elegant probabilistic formulation, which considers the cost of\na candidate association as the negative log-likelihood yielded by a deep\ndensity estimator, trained to model the conditional joint probability\ndistribution of correct associations. Our experiments, conducted on both\nsimulated and real benchmarks, show that our approach consistently enhances the\nperformance of several tracking-by-detection algorithms.",
        "authors": [
            "Gianluca Mancusi",
            "Aniello Panariello",
            "Angelo Porrello",
            "Matteo Fabbri",
            "Simone Calderara",
            "Rita Cucchiara"
        ]
    },
    {
        "title": "Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning",
        "url": "http://arxiv.org/abs/2309.01246",
        "abstract": "As advanced image manipulation techniques emerge, detecting the manipulation\nbecomes increasingly important. Despite the success of recent learning-based\napproaches for image manipulation detection, they typically require expensive\npixel-level annotations to train, while exhibiting degraded performance when\ntesting on images that are differently manipulated compared with training\nimages. To address these limitations, we propose weakly-supervised image\nmanipulation detection, such that only binary image-level labels (authentic or\ntampered with) are required for training purpose. Such a weakly-supervised\nsetting can leverage more training images and has the potential to adapt\nquickly to new manipulation techniques. To improve the generalization ability,\nwe propose weakly-supervised self-consistency learning (WSCL) to leverage the\nweakly annotated images. Specifically, two consistency properties are learned:\nmulti-source consistency (MSC) and inter-patch consistency (IPC). MSC exploits\ndifferent content-agnostic information and enables cross-source learning via an\nonline pseudo label generation and refinement process. IPC performs global\npair-wise patch-patch relationship reasoning to discover a complete region of\nmanipulation. Extensive experiments validate that our WSCL, even though is\nweakly supervised, exhibits competitive performance compared with\nfully-supervised counterpart under both in-distribution and out-of-distribution\nevaluations, as well as reasonable manipulation localization ability.",
        "authors": [
            "Yuanhao Zhai",
            "Tianyu Luan",
            "David Doermann",
            "Junsong Yuan"
        ]
    },
    {
        "title": "DeePoint: Visual Pointing Recognition and Direction Estimation",
        "url": "http://arxiv.org/abs/2304.06977",
        "abstract": "In this paper, we realize automatic visual recognition and direction\nestimation of pointing. We introduce the first neural pointing understanding\nmethod based on two key contributions. The first is the introduction of a\nfirst-of-its-kind large-scale dataset for pointing recognition and direction\nestimation, which we refer to as the DP Dataset. DP Dataset consists of more\nthan 2 million frames of 33 people pointing in various styles annotated for\neach frame with pointing timings and 3D directions. The second is DeePoint, a\nnovel deep network model for joint recognition and 3D direction estimation of\npointing. DeePoint is a Transformer-based network which fully leverages the\nspatio-temporal coordination of the body parts, not just the hands. Through\nextensive experiments, we demonstrate the accuracy and efficiency of DeePoint.\nWe believe DP Dataset and DeePoint will serve as a sound foundation for visual\nhuman intention understanding.",
        "authors": [
            "Shu Nakamura",
            "Yasutomo Kawanishi",
            "Shohei Nobuhara",
            "Ko Nishino"
        ]
    },
    {
        "title": "Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation",
        "url": "http://arxiv.org/abs/2303.04991",
        "abstract": "Accurately estimating 3D hand pose is crucial for understanding how humans\ninteract with the world. Despite remarkable progress, existing methods often\nstruggle to generate plausible hand poses when the hand is heavily occluded or\nblurred. In videos, the movements of the hand allow us to observe various parts\nof the hand that may be occluded or blurred in a single frame. To adaptively\nleverage the visual clue before and after the occlusion or blurring for robust\nhand pose estimation, we propose the Deformer: a framework that implicitly\nreasons about the relationship between hand parts within the same image\n(spatial dimension) and different timesteps (temporal dimension). We show that\na naive application of the transformer self-attention mechanism is not\nsufficient because motion blur or occlusions in certain frames can lead to\nheavily distorted hand features and generate imprecise keys and queries. To\naddress this challenge, we incorporate a Dynamic Fusion Module into Deformer,\nwhich predicts the deformation of the hand and warps the hand mesh predictions\nfrom nearby frames to explicitly support the current frame estimation.\nFurthermore, we have observed that errors are unevenly distributed across\ndifferent hand parts, with vertices around fingertips having disproportionately\nhigher errors than those around the palm. We mitigate this issue by introducing\na new loss function called maxMSE that automatically adjusts the weight of\nevery vertex to focus the model on critical hand parts. Extensive experiments\nshow that our method significantly outperforms state-of-the-art methods by 10%,\nand is more robust to occlusions (over 14%).",
        "authors": [
            "Qichen Fu",
            "Xingyu Liu",
            "Ran Xu",
            "Juan Carlos Niebles",
            "Kris M. Kitani"
        ]
    },
    {
        "title": "Online Continual Learning on Hierarchical Label Expansion",
        "url": "http://arxiv.org/abs/2308.14374",
        "abstract": "Continual learning (CL) enables models to adapt to new tasks and environments\nwithout forgetting previously learned knowledge. While current CL setups have\nignored the relationship between labels in the past task and the new task with\nor without small task overlaps, real-world scenarios often involve hierarchical\nrelationships between old and new tasks, posing another challenge for\ntraditional CL approaches. To address this challenge, we propose a novel\nmulti-level hierarchical class incremental task configuration with an online\nlearning constraint, called hierarchical label expansion (HLE). Our\nconfiguration allows a network to first learn coarse-grained classes, with data\nlabels continually expanding to more fine-grained classes in various hierarchy\ndepths. To tackle this new setup, we propose a rehearsal-based method that\nutilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class\ninformation. Additionally, we propose a simple yet effective memory management\nand sampling strategy that selectively adopts samples of newly encountered\nclasses. Our experiments demonstrate that our proposed method can effectively\nuse hierarchy on our HLE setup to improve classification accuracy across all\nlevels of hierarchies, regardless of depth and class imbalance ratio,\noutperforming prior state-of-the-art works by significant margins while also\noutperforming them on the conventional disjoint, blurry and i-Blurry CL setups.",
        "authors": [
            "Byung Hyun Lee",
            "Okchul Jung",
            "Jonghyun Choi",
            "Se Young Chun"
        ]
    },
    {
        "title": "Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning",
        "url": "http://arxiv.org/abs/2303.12745",
        "abstract": "Deception detection in conversations is a challenging yet important task,\nhaving pivotal applications in many fields such as credibility assessment in\nbusiness, multimedia anti-frauds, and custom security. Despite this, deception\ndetection research is hindered by the lack of high-quality deception datasets,\nas well as the difficulties of learning multimodal features effectively. To\naddress this issue, we introduce DOLOS\\footnote {The name ``DOLOS\" comes from\nGreek mythology.}, the largest gameshow deception detection dataset with rich\ndeceptive conversations. DOLOS includes 1,675 video clips featuring 213\nsubjects, and it has been labeled with audio-visual feature annotations. We\nprovide train-test, duration, and gender protocols to investigate the impact of\ndifferent factors. We benchmark our dataset on previously proposed deception\ndetection approaches. To further improve the performance by fine-tuning fewer\nparameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a\nUniform Temporal Adapter (UT-Adapter) explores temporal attention in\ntransformer-based architectures, and a crossmodal fusion module, Plug-in\nAudio-Visual Fusion (PAVF), combines crossmodal information from audio-visual\nfeatures. Based on the rich fine-grained audio-visual annotations on DOLOS, we\nalso exploit multi-task learning to enhance performance by concurrently\npredicting deception and audio-visual features. Experimental results\ndemonstrate the desired quality of the DOLOS dataset and the effectiveness of\nthe PECL. The DOLOS dataset and the source codes are available at\nhttps://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning/tree/main.",
        "authors": [
            "Xiaobao Guo",
            "Nithish Muthuchamy Selvaraj",
            "Zitong Yu",
            "Adams Wai-Kin Kong",
            "Bingquan Shen",
            "Alex Kot"
        ]
    },
    {
        "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification",
        "url": "http://arxiv.org/abs/2307.15254",
        "abstract": "The whole slide image (WSI) classification is often formulated as a multiple\ninstance learning (MIL) problem. Since the positive tissue is only a small\nfraction of the gigapixel WSI, existing MIL methods intuitively focus on\nidentifying salient instances via attention mechanisms. However, this leads to\na bias towards easy-to-classify instances while neglecting hard-to-classify\ninstances. Some literature has revealed that hard examples are beneficial for\nmodeling a discriminative boundary accurately. By applying such an idea at the\ninstance level, we elaborate a novel MIL framework with masked hard instance\nmining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a\nconsistency constraint to explore the potential hard instances. With several\ninstance masking strategies based on attention scores, MHIM-MIL employs a\nmomentum teacher to implicitly mine hard instances for training the student\nmodel, which can be any attention-based MIL model. This counter-intuitive\nstrategy essentially enables the student to learn a better discriminating\nboundary. Moreover, the student is used to update the teacher with an\nexponential moving average (EMA), which in turn identifies new hard instances\nfor subsequent training iterations and stabilizes the optimization.\nExperimental results on the CAMELYON-16 and TCGA Lung Cancer datasets\ndemonstrate that MHIM-MIL outperforms other latest methods in terms of\nperformance and training cost. The code is available at:\nhttps://github.com/DearCaat/MHIM-MIL.",
        "authors": [
            "Wenhao Tang",
            "Sheng Huang",
            "Xiaoxian Zhang",
            "Fengtao Zhou",
            "Yi Zhang",
            "Bo Liu"
        ]
    },
    {
        "title": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models",
        "url": "http://arxiv.org/abs/2306.05357",
        "abstract": "Text-to-image generative models have enabled high-resolution image synthesis\nacross different domains, but require users to specify the content they wish to\ngenerate. In this paper, we consider the inverse problem -- given a collection\nof different images, can we discover the generative concepts that represent\neach image? We present an unsupervised approach to discover generative concepts\nfrom a collection of images, disentangling different art styles in paintings,\nobjects, and lighting from kitchen scenes, and discovering image classes given\nImageNet images. We show how such generative concepts can accurately represent\nthe content of images, be recombined and composed to generate new artistic and\nhybrid images, and be further used as a representation for downstream\nclassification tasks.",
        "authors": [
            "Nan Liu",
            "Yilun Du",
            "Shuang Li",
            "Joshua B. Tenenbaum",
            "Antonio Torralba"
        ]
    },
    {
        "title": "Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes",
        "url": "http://arxiv.org/abs/2307.12101",
        "abstract": "Object detection via inaccurate bounding boxes supervision has boosted a\nbroad interest due to the expensive high-quality annotation data or the\noccasional inevitability of low annotation quality (\\eg tiny objects). The\nprevious works usually utilize multiple instance learning (MIL), which highly\ndepends on category information, to select and refine a low-quality box. Those\nmethods suffer from object drift, group prediction and part domination problems\nwithout exploring spatial information. In this paper, we heuristically propose\na \\textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to mine\nspatial information to refine the inaccurate box in a self-distillation\nfashion. SSD-Det utilizes a Spatial Position Self-Distillation \\textbf{(SPSD)}\nmodule to exploit spatial information and an interactive structure to combine\nspatial information and category information, thus constructing a high-quality\nproposal bag. To further improve the selection procedure, a Spatial Identity\nSelf-Distillation \\textbf{(SISD)} module is introduced in SSD-Det to obtain\nspatial confidence to help select the best proposals. Experiments on MS-COCO\nand VOC datasets with noisy box annotation verify our method's effectiveness\nand achieve state-of-the-art performance. The code is available at\nhttps://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.",
        "authors": [
            "Di Wu",
            "Pengfei Chen",
            "Xuehui Yu",
            "Guorong Li",
            "Zhenjun Han",
            "Jianbin Jiao"
        ]
    },
    {
        "title": "CC3D: Layout-Conditioned Generation of Compositional 3D Scenes",
        "url": "http://arxiv.org/abs/2303.12074",
        "abstract": "In this work, we introduce CC3D, a conditional generative model that\nsynthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained\nusing single-view images. Different from most existing 3D GANs that limit their\napplicability to aligned single objects, we focus on generating complex scenes\nwith multiple objects, by modeling the compositional nature of 3D scenes. By\ndevising a 2D layout-based approach for 3D synthesis and implementing a new 3D\nfield representation with a stronger geometric inductive bias, we have created\na 3D GAN that is both efficient and of high quality, while allowing for a more\ncontrollable generation process. Our evaluations on synthetic 3D-FRONT and\nreal-world KITTI-360 datasets demonstrate that our model generates scenes of\nimproved visual and geometric quality in comparison to previous works.",
        "authors": [
            "Sherwin Bahmani",
            "Jeong Joon Park",
            "Despoina Paschalidou",
            "Xingguang Yan",
            "Gordon Wetzstein",
            "Leonidas Guibas",
            "Andrea Tagliasacchi"
        ]
    },
    {
        "title": "Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy",
        "url": "http://arxiv.org/abs/2307.16867",
        "abstract": "Current state-of-the-art results in computer vision depend in part on\nfine-tuning large pre-trained vision models. However, with the exponential\ngrowth of model sizes, the conventional full fine-tuning, which needs to store\na individual network copy for each tasks, leads to increasingly huge storage\nand transmission overhead. Adapter-based Parameter-Efficient Tuning (PET)\nmethods address this challenge by tuning lightweight adapters inserted into the\nfrozen pre-trained models. In this paper, we investigate how to make adapters\neven more efficient, reaching a new minimum size required to store a\ntask-specific fine-tuned network. Inspired by the observation that the\nparameters of adapters converge at flat local minima, we find that adapters are\nresistant to noise in parameter space, which means they are also resistant to\nlow numerical precision. To train low-precision adapters, we propose a\ncomputational-efficient quantization method which minimizes the quantization\nerror. Through extensive experiments, we find that low-precision adapters\nexhibit minimal performance degradation, and even 1-bit precision is sufficient\nfor adapters. The experimental results demonstrate that 1-bit adapters\noutperform all other PET methods on both the VTAB-1K benchmark and few-shot\nFGVC tasks, while requiring the smallest storage size. Our findings show, for\nthe first time, the significant potential of quantization techniques in PET,\nproviding a general solution to enhance the parameter efficiency of\nadapter-based PET methods. Code: https://github.com/JieShibo/PETL-ViT",
        "authors": [
            "Shibo Jie",
            "Haoqing Wang",
            "Zhi-Hong Deng"
        ]
    },
    {
        "title": "EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization",
        "url": "http://arxiv.org/abs/2307.10554",
        "abstract": "Mixed-Precision Quantization~(MQ) can achieve a competitive\naccuracy-complexity trade-off for models. Conventional training-based search\nmethods require time-consuming candidate training to search optimized per-layer\nbit-width configurations in MQ. Recently, some training-free approaches have\npresented various MQ proxies and significantly improve search efficiency.\nHowever, the correlation between these proxies and quantization accuracy is\npoorly understood. To address the gap, we first build the MQ-Bench-101, which\ninvolves different bit configurations and quantization results. Then, we\nobserve that the existing training-free proxies perform weak correlations on\nthe MQ-Bench-101. To efficiently seek superior proxies, we develop an automatic\nsearch of proxies framework for MQ via evolving algorithms. In particular, we\ndevise an elaborate search space involving the existing proxies and perform an\nevolution search to discover the best correlated MQ proxy. We proposed a\ndiversity-prompting selection strategy and compatibility screening protocol to\navoid premature convergence and improve search efficiency. In this way, our\nEvolving proxies for Mixed-precision Quantization~(EMQ) framework allows the\nauto-generation of proxies without heavy tuning and expert knowledge. Extensive\nexperiments on ImageNet with various ResNet and MobileNet families demonstrate\nthat our EMQ obtains superior performance than state-of-the-art mixed-precision\nmethods at a significantly reduced cost. The code will be released.",
        "authors": [
            "Peijie Dong",
            "Lujun Li",
            "Zimian Wei",
            "Xin Niu",
            "Zhiliang Tian",
            "Hengyue Pan"
        ]
    },
    {
        "title": "Inspecting the Geographical Representativeness of Images from Text-to-Image Models",
        "url": "http://arxiv.org/abs/2305.11080",
        "abstract": "Recent progress in generative models has resulted in models that produce both\nrealistic as well as relevant images for most textual inputs. These models are\nbeing used to generate millions of images everyday, and hold the potential to\ndrastically impact areas such as generative art, digital marketing and data\naugmentation. Given their outsized impact, it is important to ensure that the\ngenerated content reflects the artifacts and surroundings across the globe,\nrather than over-representing certain parts of the world. In this paper, we\nmeasure the geographical representativeness of common nouns (e.g., a house)\ngenerated through DALL.E 2 and Stable Diffusion models using a crowdsourced\nstudy comprising 540 participants across 27 countries. For deliberately\nunderspecified inputs without country names, the generated images most reflect\nthe surroundings of the United States followed by India, and the top\ngenerations rarely reflect surroundings from all other countries (average score\nless than 3 out of 5). Specifying the country names in the input increases the\nrepresentativeness by 1.44 points on average for DALL.E 2 and 0.75 for Stable\nDiffusion, however, the overall scores for many countries still remain low,\nhighlighting the need for future models to be more geographically inclusive.\nLastly, we examine the feasibility of quantifying the geographical\nrepresentativeness of generated images without conducting user studies.",
        "authors": [
            "Abhipsa Basu",
            "R. Venkatesh Babu",
            "Danish Pruthi"
        ]
    },
    {
        "title": "Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing",
        "url": "http://arxiv.org/abs/2304.02051",
        "abstract": "Fashion illustration is used by designers to communicate their vision and to\nbring the design idea from conceptualization to realization, showing how\nclothes interact with the human body. In this context, computer vision can thus\nbe used to improve the fashion design process. Differently from previous works\nthat mainly focused on the virtual try-on of garments, we propose the task of\nmultimodal-conditioned fashion image editing, guiding the generation of\nhuman-centric fashion images by following multimodal prompts, such as text,\nhuman body poses, and garment sketches. We tackle this problem by proposing a\nnew architecture based on latent diffusion models, an approach that has not\nbeen used before in the fashion domain. Given the lack of existing datasets\nsuitable for the task, we also extend two existing fashion datasets, namely\nDress Code and VITON-HD, with multimodal annotations collected in a\nsemi-automatic manner. Experimental results on these new datasets demonstrate\nthe effectiveness of our proposal, both in terms of realism and coherence with\nthe given multimodal inputs. Source code and collected multimodal annotations\nare publicly available at:\nhttps://github.com/aimagelab/multimodal-garment-designer.",
        "authors": [
            "Alberto Baldrati",
            "Davide Morelli",
            "Giuseppe Cartella",
            "Marcella Cornia",
            "Marco Bertini",
            "Rita Cucchiara"
        ]
    },
    {
        "title": "Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks",
        "url": "http://arxiv.org/abs/2308.14153",
        "abstract": "In the real world, image degradations caused by rain often exhibit a\ncombination of rain streaks and raindrops, thereby increasing the challenges of\nrecovering the underlying clean image. Note that the rain streaks and raindrops\nhave diverse shapes, sizes, and locations in the captured image, and thus\nmodeling the correlation relationship between irregular degradations caused by\nrain artifacts is a necessary prerequisite for image deraining. This paper aims\nto present an efficient and flexible mechanism to learn and model degradation\nrelationships in a global view, thereby achieving a unified removal of\nintricate rain scenes. To do so, we propose a Sparse Sampling Transformer based\non Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previous\nmethods, our UDR-S2Former has three merits. First, it can adaptively sample\nrelevant image degradation information to model underlying degradation\nrelationships. Second, explicit application of the uncertainty-driven ranking\nstrategy can facilitate the network to attend to degradation features and\nunderstand the reconstruction process. Finally, experimental results show that\nour UDR-S2Former clearly outperforms state-of-the-art methods for all\nbenchmarks.",
        "authors": [
            "Sixiang Chen",
            "Tian Ye",
            "Jinbin Bai",
            "Erkang Chen",
            "Jun Shi",
            "Lei Zhu"
        ]
    },
    {
        "title": "A Benchmark for Chinese-English Scene Text Image Super-Resolution",
        "url": "http://arxiv.org/abs/2308.03262",
        "abstract": "Scene Text Image Super-resolution (STISR) aims to recover high-resolution\n(HR) scene text images with visually pleasant and readable text content from\nthe given low-resolution (LR) input. Most existing works focus on recovering\nEnglish texts, which have relatively simple character structures, while little\nwork has been done on the more challenging Chinese texts with diverse and\ncomplex character structures. In this paper, we propose a real-world\nChinese-English benchmark dataset, namely Real-CE, for the task of STISR with\nthe emphasis on restoring structurally complex Chinese characters. The\nbenchmark provides 1,935/783 real-world LR-HR text image pairs~(contains 33,789\ntext lines in total) for training/testing in 2$\\times$ and 4$\\times$ zooming\nmodes, complemented by detailed annotations, including detection boxes and text\ntranscripts. Moreover, we design an edge-aware learning method, which provides\nstructural supervision in image and feature domains, to effectively reconstruct\nthe dense structures of Chinese characters. We conduct experiments on the\nproposed Real-CE benchmark and evaluate the existing STISR models with and\nwithout our edge-aware loss. The benchmark, including data and source code, is\navailable at https://github.com/mjq11302010044/Real-CE.",
        "authors": [
            "Jianqi Ma",
            "Zhetong Liang",
            "Wangmeng Xiang",
            "Xi Yang",
            "Lei Zhang"
        ]
    },
    {
        "title": "Replay: Multi-modal Multi-view Acted Videos for Casual Holography",
        "url": "http://arxiv.org/abs/2307.12067",
        "abstract": "We introduce Replay, a collection of multi-view, multi-modal videos of humans\ninteracting socially. Each scene is filmed in high production quality, from\ndifferent viewpoints with several static cameras, as well as wearable action\ncameras, and recorded with a large array of microphones at different positions\nin the room. Overall, the dataset contains over 4000 minutes of footage and\nover 7 million timestamped high-resolution frames annotated with camera poses\nand partially with foreground masks. The Replay dataset has many potential\napplications, such as novel-view synthesis, 3D reconstruction, novel-view\nacoustic synthesis, human body and face analysis, and training generative\nmodels. We provide a benchmark for training and evaluating novel-view\nsynthesis, with two scenarios of different difficulty. Finally, we evaluate\nseveral baseline state-of-the-art methods on the new benchmark.",
        "authors": [
            "Roman Shapovalov",
            "Yanir Kleiman",
            "Ignacio Rocco",
            "David Novotny",
            "Andrea Vedaldi",
            "Changan Chen",
            "Filippos Kokkinos",
            "Ben Graham",
            "Natalia Neverova"
        ]
    },
    {
        "title": "GPGait: Generalized Pose-based Gait Recognition",
        "url": "http://arxiv.org/abs/2303.05234",
        "abstract": "Recent works on pose-based gait recognition have demonstrated the potential\nof using such simple information to achieve results comparable to\nsilhouette-based methods. However, the generalization ability of pose-based\nmethods on different datasets is undesirably inferior to that of\nsilhouette-based ones, which has received little attention but hinders the\napplication of these methods in real-world scenarios. To improve the\ngeneralization ability of pose-based methods across datasets, we propose a\n\\textbf{G}eneralized \\textbf{P}ose-based \\textbf{Gait} recognition\n(\\textbf{GPGait}) framework. First, a Human-Oriented Transformation (HOT) and a\nseries of Human-Oriented Descriptors (HOD) are proposed to obtain a unified\npose representation with discriminative multi-features. Then, given the slight\nvariations in the unified representation after HOT and HOD, it becomes crucial\nfor the network to extract local-global relationships between the keypoints. To\nthis end, a Part-Aware Graph Convolutional Network (PAGCN) is proposed to\nenable efficient graph partition and local-global spatial feature extraction.\nExperiments on four public gait recognition datasets, CASIA-B, OUMVLP-Pose,\nGait3D and GREW, show that our model demonstrates better and more stable\ncross-domain capabilities compared to existing skeleton-based methods,\nachieving comparable recognition results to silhouette-based ones. Code is\navailable at https://github.com/BNU-IVC/FastPoseGait.",
        "authors": [
            "Yang Fu",
            "Shibei Meng",
            "Saihui Hou",
            "Xuecai Hu",
            "Yongzhen Huang"
        ]
    },
    {
        "title": "Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations",
        "url": "http://arxiv.org/abs/2308.08321",
        "abstract": "In recent years, discriminative self-supervised methods have made significant\nstrides in advancing various visual tasks. The central idea of learning a data\nencoder that is robust to data distortions/augmentations is straightforward yet\nhighly effective. Although many studies have demonstrated the empirical success\nof various learning methods, the resulting learned representations can exhibit\ninstability and hinder downstream performance. In this study, we analyze\ndiscriminative self-supervised methods from a causal perspective to explain\nthese unstable behaviors and propose solutions to overcome them. Our approach\ndraws inspiration from prior works that empirically demonstrate the ability of\ndiscriminative self-supervised methods to demix ground truth causal sources to\nsome extent. Unlike previous work on causality-empowered representation\nlearning, we do not apply our solutions during the training process but rather\nduring the inference process to improve time efficiency. Through experiments on\nboth controlled image datasets and realistic image datasets, we show that our\nproposed solutions, which involve tempering a linear transformation with\ncontrolled synthetic data, are effective in addressing these issues.",
        "authors": [
            "Yuewei Yang",
            "Hai Li",
            "Yiran Chen"
        ]
    },
    {
        "title": "ShiftNAS: Improving One-shot NAS via Probability Shift",
        "url": "http://arxiv.org/abs/2307.08300",
        "abstract": "One-shot Neural architecture search (One-shot NAS) has been proposed as a\ntime-efficient approach to obtain optimal subnet architectures and weights\nunder different complexity cases by training only once. However, the subnet\nperformance obtained by weight sharing is often inferior to the performance\nachieved by retraining. In this paper, we investigate the performance gap and\nattribute it to the use of uniform sampling, which is a common approach in\nsupernet training. Uniform sampling concentrates training resources on subnets\nwith intermediate computational resources, which are sampled with high\nprobability. However, subnets with different complexity regions require\ndifferent optimal training strategies for optimal performance. To address the\nproblem of uniform sampling, we propose ShiftNAS, a method that can adjust the\nsampling probability based on the complexity of subnets. We achieve this by\nevaluating the performance variation of subnets with different complexity and\ndesigning an architecture generator that can accurately and efficiently provide\nsubnets with the desired complexity. Both the sampling probability and the\narchitecture generator can be trained end-to-end in a gradient-based manner.\nWith ShiftNAS, we can directly obtain the optimal model architecture and\nparameters for a given computational complexity. We evaluate our approach on\nmultiple visual network models, including convolutional neural networks (CNNs)\nand vision transformers (ViTs), and demonstrate that ShiftNAS is\nmodel-agnostic. Experimental results on ImageNet show that ShiftNAS can improve\nthe performance of one-shot NAS without additional consumption. Source codes\nare available at https://github.com/bestfleer/ShiftNAS.",
        "authors": [
            "Mingyang Zhang",
            "Xinyi Yu",
            "Haodong Zhao",
            "Linlin Ou"
        ]
    },
    {
        "title": "Adaptive Testing of Computer Vision Models",
        "url": "http://arxiv.org/abs/2212.02774",
        "abstract": "Vision models often fail systematically on groups of data that share common\nsemantic characteristics (e.g., rare objects or unusual scenes), but\nidentifying these failure modes is a challenge. We introduce AdaVision, an\ninteractive process for testing vision models which helps users identify and\nfix coherent failure modes. Given a natural language description of a coherent\ngroup, AdaVision retrieves relevant images from LAION-5B with CLIP. The user\nthen labels a small amount of data for model correctness, which is used in\nsuccessive retrieval rounds to hill-climb towards high-error regions, refining\nthe group definition. Once a group is saturated, AdaVision uses GPT-3 to\nsuggest new group descriptions for the user to explore. We demonstrate the\nusefulness and generality of AdaVision in user studies, where users find major\nbugs in state-of-the-art classification, object detection, and image captioning\nmodels. These user-discovered groups have failure rates 2-3x higher than those\nsurfaced by automatic error clustering methods. Finally, finetuning on examples\nfound with AdaVision fixes the discovered bugs when evaluated on unseen\nexamples, without degrading in-distribution accuracy, and while also improving\nperformance on out-of-distribution datasets.",
        "authors": [
            "Irena Gao",
            "Gabriel Ilharco",
            "Scott Lundberg",
            "Marco Tulio Ribeiro"
        ]
    },
    {
        "title": "AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception",
        "url": "http://arxiv.org/abs/2307.13933",
        "abstract": "Driver distraction has become a significant cause of severe traffic accidents\nover the past decade. Despite the growing development of vision-driven driver\nmonitoring systems, the lack of comprehensive perception datasets restricts\nroad safety and traffic security. In this paper, we present an AssIstive\nDriving pErception dataset (AIDE) that considers context information both\ninside and outside the vehicle in naturalistic scenarios. AIDE facilitates\nholistic driver monitoring through three distinctive characteristics, including\nmulti-view settings of driver and scene, multi-modal annotations of face, body,\nposture, and gesture, and four pragmatic task designs for driving\nunderstanding. To thoroughly explore AIDE, we provide experimental benchmarks\non three kinds of baseline frameworks via extensive methods. Moreover, two\nfusion strategies are introduced to give new insights into learning effective\nmulti-stream/modal representations. We also systematically investigate the\nimportance and rationality of the key components in AIDE and benchmarks. The\nproject link is https://github.com/ydk122024/AIDE.",
        "authors": [
            "Dingkang Yang",
            "Shuai Huang",
            "Zhi Xu",
            "Zhenpeng Li",
            "Shunli Wang",
            "Mingcheng Li",
            "Yuzheng Wang",
            "Yang Liu",
            "Kun Yang",
            "Zhaoyu Chen",
            "Yan Wang",
            "Jing Liu",
            "Peixuan Zhang",
            "Peng Zhai",
            "Lihua Zhang"
        ]
    },
    {
        "title": "Self-Supervised Character-to-Character Distillation for Text Recognition",
        "url": "http://arxiv.org/abs/2211.00288",
        "abstract": "When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available at\nhttps://github.com/TongkunGuan/CCD.",
        "authors": [
            "Tongkun Guan",
            "Wei Shen",
            "Xue Yang",
            "Qi Feng",
            "Zekun Jiang",
            "Xiaokang Yang"
        ]
    },
    {
        "title": "MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency",
        "url": "http://arxiv.org/abs/2303.09219",
        "abstract": "3D single object tracking (SOT) is an indispensable part of automated\ndriving. Existing approaches rely heavily on large, densely labeled datasets.\nHowever, annotating point clouds is both costly and time-consuming. Inspired by\nthe great success of cycle tracking in unsupervised 2D SOT, we introduce the\nfirst semi-supervised approach to 3D SOT. Specifically, we introduce two\ncycle-consistency strategies for supervision: 1) Self tracking cycles, which\nleverage labels to help the model converge better in the early stages of\ntraining; 2) forward-backward cycles, which strengthen the tracker's robustness\nto motion variations and the template noise caused by the template update\nstrategy. Furthermore, we propose a data augmentation strategy named SOTMixup\nto improve the tracker's robustness to point cloud diversity. SOTMixup\ngenerates training samples by sampling points in two point clouds with a mixing\nrate and assigns a reasonable loss weight for training according to the mixing\nrate. The resulting MixCycle approach generalizes to appearance matching-based\ntrackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trained\nwith $\\textbf{10\\%}$ labels outperforms P2B trained with $\\textbf{100\\%}$\nlabels, and achieves a $\\textbf{28.4\\%}$ precision improvement when using\n$\\textbf{1\\%}$ labels. Our code will be released at\n\\url{https://github.com/Mumuqiao/MixCycle}.",
        "authors": [
            "Qiao Wu",
            "Jiaqi Yang",
            "Kun Sun",
            "Chu'ai Zhang",
            "Yanning Zhang",
            "Mathieu Salzmann"
        ]
    },
    {
        "title": "Multi-Label Self-Supervised Learning with Scene Images",
        "url": "http://arxiv.org/abs/2308.03286",
        "abstract": "Self-supervised learning (SSL) methods targeting scene images have seen a\nrapid growth recently, and they mostly rely on either a dedicated dense\nmatching mechanism or a costly unsupervised object discovery module. This paper\nshows that instead of hinging on these strenuous operations, quality image\nrepresentations can be learned by treating scene/multi-label image SSL simply\nas a multi-label classification problem, which greatly simplifies the learning\nframework. Specifically, multiple binary pseudo-labels are assigned for each\ninput image by comparing its embeddings with those in two dictionaries, and the\nnetwork is optimized using the binary cross entropy loss. The proposed method\nis named Multi-Label Self-supervised learning (MLS). Visualizations\nqualitatively show that clearly the pseudo-labels by MLS can automatically find\nsemantically similar pseudo-positive pairs across different images to\nfacilitate contrastive learning. MLS learns high quality representations on\nMS-COCO and achieves state-of-the-art results on classification, detection and\nsegmentation benchmarks. At the same time, MLS is much simpler than existing\nmethods, making it easier to deploy and for further exploration.",
        "authors": [
            "Ke Zhu",
            "Minghao Fu",
            "Jianxin Wu"
        ]
    },
    {
        "title": "Domain Adaptive Few-Shot Open-Set Learning",
        "url": "http://arxiv.org/abs/2309.12814",
        "abstract": "Few-shot learning has made impressive strides in addressing the crucial\nchallenges of recognizing unknown samples from novel classes in target query\nsets and managing visual shifts between domains. However, existing techniques\nfall short when it comes to identifying target outliers under domain shifts by\nlearning to reject pseudo-outliers from the source domain, resulting in an\nincomplete solution to both problems. To address these challenges\ncomprehensively, we propose a novel approach called Domain Adaptive Few-Shot\nOpen Set Recognition (DA-FSOS) and introduce a meta-learning-based architecture\nnamed DAFOSNET. During training, our model learns a shared and discriminative\nembedding space while creating a pseudo open-space decision boundary, given a\nfully-supervised source domain and a label-disjoint few-shot target domain. To\nenhance data density, we use a pair of conditional adversarial networks with\ntunable noise variances to augment both domains closed and pseudo-open spaces.\nFurthermore, we propose a domain-specific batch-normalized class prototypes\nalignment strategy to align both domains globally while ensuring\nclass-discriminativeness through novel metric objectives. Our training approach\nensures that DAFOS-NET can generalize well to new scenarios in the target\ndomain. We present three benchmarks for DA-FSOS based on the Office-Home,\nmini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy of\nDAFOS-NET through extensive experimentation",
        "authors": [
            "Debabrata Pal",
            "Deeptej More",
            "Sai Bhargav",
            "Dipesh Tamboli",
            "Vaneet Aggarwal",
            "Biplab Banerjee"
        ]
    },
    {
        "title": "DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion",
        "url": "http://arxiv.org/abs/2305.01921",
        "abstract": "While the community of 3D point cloud generation has witnessed a big growth\nin recent years, there still lacks an effective way to enable intuitive user\ncontrol in the generation process, hence limiting the general utility of such\nmethods. Since an intuitive way of decomposing a shape is through its parts, we\npropose to tackle the task of controllable part-based point cloud generation.\nWe introduce DiffFacto, a novel probabilistic generative model that learns the\ndistribution of shapes with part-level control. We propose a factorization that\nmodels independent part style and part configuration distributions and presents\na novel cross-diffusion network that enables us to generate coherent and\nplausible shapes under our proposed factorization. Experiments show that our\nmethod is able to generate novel shapes with multiple axes of control. It\nachieves state-of-the-art part-level generation quality and generates plausible\nand coherent shapes while enabling various downstream editing applications such\nas shape interpolation, mixing, and transformation editing. Project website:\nhttps://difffacto.github.io/",
        "authors": [
            "Kiyohiro Nakayama",
            "Mikaela Angelina Uy",
            "Jiahui Huang",
            "Shi-Min Hu",
            "Ke Li",
            "Leonidas J Guibas"
        ]
    },
    {
        "title": "Interactive Class-Agnostic Object Counting",
        "url": "http://arxiv.org/abs/2309.05277",
        "abstract": "We propose a novel framework for interactive class-agnostic object counting,\nwhere a human user can interactively provide feedback to improve the accuracy\nof a counter. Our framework consists of two main components: a user-friendly\nvisualizer to gather feedback and an efficient mechanism to incorporate it. In\neach iteration, we produce a density map to show the current prediction result,\nand we segment it into non-overlapping regions with an easily verifiable number\nof objects. The user can provide feedback by selecting a region with obvious\ncounting errors and specifying the range for the estimated number of objects\nwithin it. To improve the counting result, we develop a novel adaptation loss\nto force the visual counter to output the predicted count within the\nuser-specified range. For effective and efficient adaptation, we propose a\nrefinement module that can be used with any density-based visual counter, and\nonly the parameters in the refinement module will be updated during adaptation.\nOur experiments on two challenging class-agnostic object counting benchmarks,\nFSCD-LVIS and FSC-147, show that our method can reduce the mean absolute error\nof multiple state-of-the-art visual counters by roughly 30% to 40% with minimal\nuser input. Our project can be found at\nhttps://yifehuang97.github.io/ICACountProjectPage/.",
        "authors": [
            "Yifeng Huang",
            "Viresh Ranjan",
            "Minh Hoai"
        ]
    },
    {
        "title": "Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization",
        "url": "http://arxiv.org/abs/2304.11823",
        "abstract": "Backdoor defense, which aims to detect or mitigate the effect of malicious\ntriggers introduced by attackers, is becoming increasingly critical for machine\nlearning security and integrity. Fine-tuning based on benign data is a natural\ndefense to erase the backdoor effect in a backdoored model. However, recent\nstudies show that, given limited benign data, vanilla fine-tuning has poor\ndefense performance. In this work, we provide a deep study of fine-tuning the\nbackdoored model from the neuron perspective and find that backdoorrelated\nneurons fail to escape the local minimum in the fine-tuning process. Inspired\nby observing that the backdoorrelated neurons often have larger norms, we\npropose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms\nof backdoor-related neurons by incorporating sharpness-aware minimization with\nfine-tuning. We demonstrate the effectiveness of our method on several\nbenchmark datasets and network architectures, where it achieves\nstate-of-the-art defense performance. Overall, our work provides a promising\navenue for improving the robustness of machine learning models against backdoor\nattacks.",
        "authors": [
            "Mingli Zhu",
            "Shaokui Wei",
            "Li Shen",
            "Yanbo Fan",
            "Baoyuan Wu"
        ]
    },
    {
        "title": "Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training",
        "url": "http://arxiv.org/abs/2308.06689",
        "abstract": "Binarization of neural networks is a dominant paradigm in neural networks\ncompression. The pioneering work BinaryConnect uses Straight Through Estimator\n(STE) to mimic the gradients of the sign function, but it also causes the\ncrucial inconsistency problem. Most of the previous methods design different\nestimators instead of STE to mitigate it. However, they ignore the fact that\nwhen reducing the estimating error, the gradient stability will decrease\nconcomitantly. These highly divergent gradients will harm the model training\nand increase the risk of gradient vanishing and gradient exploding. To fully\ntake the gradient stability into consideration, we present a new perspective to\nthe BNNs training, regarding it as the equilibrium between the estimating error\nand the gradient stability. In this view, we firstly design two indicators to\nquantitatively demonstrate the equilibrium phenomenon. In addition, in order to\nbalance the estimating error and the gradient stability well, we revise the\noriginal straight through estimator and propose a power function based\nestimator, Rectified Straight Through Estimator (ReSTE for short). Comparing to\nother estimators, ReSTE is rational and capable of flexibly balancing the\nestimating error with the gradient stability. Extensive experiments on CIFAR-10\nand ImageNet datasets show that ReSTE has excellent performance and surpasses\nthe state-of-the-art methods without any auxiliary modules or losses.",
        "authors": [
            "Xiao-Ming Wu",
            "Dian Zheng",
            "Zuhao Liu",
            "Wei-Shi Zheng"
        ]
    },
    {
        "title": "Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection",
        "url": "http://arxiv.org/abs/2303.11926",
        "abstract": "In this paper, we propose a long-sequence modeling framework, named\nStreamPETR, for multi-view 3D object detection. Built upon the sparse query\ndesign in the PETR series, we systematically develop an object-centric temporal\nmechanism. The model is performed in an online manner and the long-term\nhistorical information is propagated through object queries frame by frame.\nBesides, we introduce a motion-aware layer normalization to model the movement\nof the objects. StreamPETR achieves significant performance improvements only\nwith negligible computation cost, compared to the single-frame baseline. On the\nstandard nuScenes benchmark, it is the first online multi-view method that\nachieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based\nmethods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming\nthe state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code\nhas been available at https://github.com/exiawsh/StreamPETR.git.",
        "authors": [
            "Shihao Wang",
            "Yingfei Liu",
            "Tiancai Wang",
            "Ying Li",
            "Xiangyu Zhang"
        ]
    },
    {
        "title": "Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities",
        "url": "http://arxiv.org/abs/2302.11154",
        "abstract": "Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit\nstrong generalization on various visual domains and tasks. However, existing\nimage classification benchmarks often evaluate recognition on a specific domain\n(e.g., outdoor images) or a specific task (e.g., classifying plant species),\nwhich falls short of evaluating whether pre-trained foundational models are\nuniversal visual recognizers. To address this, we formally present the task of\nOpen-domain Visual Entity recognitioN (OVEN), where a model need to link an\nimage onto a Wikipedia entity with respect to a text query. We construct\nOVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto\none single label space: Wikipedia entities. OVEN challenges models to select\namong six million possible Wikipedia entities, making it a general visual\nrecognition benchmark with the largest number of labels. Our study on\nstate-of-the-art pre-trained models reveals large headroom in generalizing to\nthe massive-scale label space. We show that a PaLI-based auto-regressive visual\nrecognition model performs surprisingly well, even on Wikipedia entities that\nhave never been seen during fine-tuning. We also find existing pretrained\nmodels yield different strengths: while PaLI-based models obtain higher overall\nperformance, CLIP-based models are better at recognizing tail entities.",
        "authors": [
            "Hexiang Hu",
            "Yi Luan",
            "Yang Chen",
            "Urvashi Khandelwal",
            "Mandar Joshi",
            "Kenton Lee",
            "Kristina Toutanova",
            "Ming-Wei Chang"
        ]
    },
    {
        "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
        "url": "http://arxiv.org/abs/2303.11089",
        "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk",
        "authors": [
            "Ziqiao Peng",
            "Haoyu Wu",
            "Zhenbo Song",
            "Hao Xu",
            "Xiangyu Zhu",
            "Jun He",
            "Hongyan Liu",
            "Zhaoxin Fan"
        ]
    },
    {
        "title": "A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2212.05102",
        "abstract": "Despite significant advances, the performance of state-of-the-art continual\nlearning approaches hinges on the unrealistic scenario of fully labeled data.\nIn this paper, we tackle this challenge and propose an approach for continual\nsemi-supervised learning--a setting where not all the data samples are labeled.\nA primary issue in this scenario is the model forgetting representations of\nunlabeled data and overfitting the labeled samples. We leverage the power of\nnearest-neighbor classifiers to nonlinearly partition the feature space and\nflexibly model the underlying data distribution thanks to its non-parametric\nnature. This enables the model to learn a strong representation for the current\ntask, and distill relevant information from previous tasks. We perform a\nthorough experimental evaluation and show that our method outperforms all the\nexisting approaches by large margins, setting a solid state of the art on the\ncontinual semi-supervised learning paradigm. For example, on CIFAR-100 we\nsurpass several others even when using at least 30 times less supervision (0.8%\nvs. 25% of annotations). Finally, our method works well on both low and high\nresolution images and scales seamlessly to more complex datasets such as\nImageNet-100. The code is publicly available on\nhttps://github.com/kangzhiq/NNCSL",
        "authors": [
            "Zhiqi Kang",
            "Enrico Fini",
            "Moin Nabi",
            "Elisa Ricci",
            "Karteek Alahari"
        ]
    },
    {
        "title": "Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models",
        "url": "http://arxiv.org/abs/2304.01515",
        "abstract": "Token-based masked generative models are gaining popularity for their fast\ninference time with parallel decoding. While recent token-based approaches\nachieve competitive performance to diffusion-based models, their generation\nperformance is still suboptimal as they sample multiple tokens simultaneously\nwithout considering the dependence among them. We empirically investigate this\nproblem and propose a learnable sampling model, Text-Conditioned Token\nSelection (TCTS), to select optimal tokens via localized supervision with text\ninformation. TCTS improves not only the image quality but also the semantic\nalignment of the generated images with the given texts. To further improve the\nimage quality, we introduce a cohesive sampling strategy, Frequency Adaptive\nSampling (FAS), to each group of tokens divided according to the self-attention\nmaps. We validate the efficacy of TCTS combined with FAS with various\ngenerative tasks, demonstrating that it significantly outperforms the baselines\nin image-text alignment and image quality. Our text-conditioned sampling\nframework further reduces the original inference time by more than 50% without\nmodifying the original generative model.",
        "authors": [
            "Jaewoong Lee",
            "Sangwon Jang",
            "Jaehyeong Jo",
            "Jaehong Yoon",
            "Yunji Kim",
            "Jin-Hwa Kim",
            "Jung-Woo Ha",
            "Sung Ju Hwang"
        ]
    },
    {
        "title": "Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations",
        "url": "http://arxiv.org/abs/2308.10554",
        "abstract": "Training deep generative models usually requires a large amount of data. To\nalleviate the data collection cost, the task of zero-shot GAN adaptation aims\nto reuse well-trained generators to synthesize images of an unseen target\ndomain without any further training samples. Due to the data absence, the\ntextual description of the target domain and the vision-language models, e.g.,\nCLIP, are utilized to effectively guide the generator. However, with only a\nsingle representative text feature instead of real images, the synthesized\nimages gradually lose diversity as the model is optimized, which is also known\nas mode collapse. To tackle the problem, we propose a novel method to find\nsemantic variations of the target text in the CLIP space. Specifically, we\nexplore diverse semantic variations based on the informative text feature of\nthe target domain while regularizing the uncontrolled deviation of the semantic\ninformation. With the obtained variations, we design a novel directional moment\nloss that matches the first and second moments of image and text direction\ndistributions. Moreover, we introduce elastic weight consolidation and a\nrelation consistency loss to effectively preserve valuable content information\nfrom the source domain, e.g., appearances. Through extensive experiments, we\ndemonstrate the efficacy of the proposed methods in ensuring sample diversity\nin various scenarios of zero-shot GAN adaptation. We also conduct ablation\nstudies to validate the effect of each proposed component. Notably, our model\nachieves a new state-of-the-art on zero-shot GAN adaptation in terms of both\ndiversity and quality.",
        "authors": [
            "Seogkyu Jeon",
            "Bei Liu",
            "Pilhyeon Lee",
            "Kibeom Hong",
            "Jianlong Fu",
            "Hyeran Byun"
        ]
    },
    {
        "title": "Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents",
        "url": "http://arxiv.org/abs/2308.07241",
        "abstract": "Accomplishing household tasks requires to plan step-by-step actions\nconsidering the consequences of previous actions. However, the state-of-the-art\nembodied agents often make mistakes in navigating the environment and\ninteracting with proper objects due to imperfect learning by imitating experts\nor algorithmic planners without such knowledge. To improve both visual\nnavigation and object interaction, we propose to consider the consequence of\ntaken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory)\nthat incorporates semantic context (e.g., appropriate objects to interact with)\nin a sequence of actions, and the changed spatial arrangement and states of\ninteracted objects (e.g., location that the object has been moved to) in\ninferring the subsequent actions. We empirically show that the agent with the\nproposed CAPEAM achieves state-of-the-art performance in various metrics using\na challenging interactive instruction following benchmark in both seen and\nunseen environments by large margins (up to +10.70% in unseen env.).",
        "authors": [
            "Byeonghwi Kim",
            "Jinyeon Kim",
            "Yuyeong Kim",
            "Cheolhong Min",
            "Jonghyun Choi"
        ]
    },
    {
        "title": "Inverse Problem Regularization with Hierarchical Variational Autoencoders",
        "url": "http://arxiv.org/abs/2303.11217",
        "abstract": "In this paper, we propose to regularize ill-posed inverse problems using a\ndeep hierarchical variational autoencoder (HVAE) as an image prior. The\nproposed method synthesizes the advantages of i) denoiser-based Plug \\& Play\napproaches and ii) generative model based approaches to inverse problems.\nFirst, we exploit VAE properties to design an efficient algorithm that benefits\nfrom convergence guarantees of Plug-and-Play (PnP) methods. Second, our\napproach is not restricted to specialized datasets and the proposed PnP-HVAE\nmodel is able to solve image restoration problems on natural images of any\nsize. Our experiments show that the proposed PnP-HVAE method is competitive\nwith both SOTA denoiser-based PnP approaches, and other SOTA restoration\nmethods based on generative models.",
        "authors": [
            "Jean Prost",
            "Antoine Houdard",
            "Andr\u00e9s Almansa",
            "Nicolas Papadakis"
        ]
    },
    {
        "title": "Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map",
        "url": "http://arxiv.org/abs/2308.13245",
        "abstract": "While impressive progress has recently been made in image-oriented facial\nattribute translation, shape-oriented 3D facial attribute translation remains\nan unsolved issue. This is primarily limited by the lack of 3D generative\nmodels and ineffective usage of 3D facial data. We propose a learning framework\nfor 3D facial attribute translation to relieve these limitations. Firstly, we\ncustomize a novel geometric map for 3D shape representation and embed it in an\nend-to-end generative adversarial network. The geometric map represents 3D\nshapes symmetrically on a square image grid, while preserving the neighboring\nrelationship of 3D vertices in a local least-square sense. This enables\neffective learning for the latent representation of data with different\nattributes. Secondly, we employ a unified and unpaired learning framework for\nmulti-domain attribute translation. It not only makes effective usage of data\ncorrelation from multiple domains, but also mitigates the constraint for hardly\naccessible paired data. Finally, we propose a hierarchical architecture for the\ndiscriminator to guarantee robust results against both global and local\nartifacts. We conduct extensive experiments to demonstrate the advantage of the\nproposed framework over the state-of-the-art in generating high-fidelity facial\nshapes. Given an input 3D facial shape, the proposed framework is able to\nsynthesize novel shapes of different attributes, which covers some downstream\napplications, such as expression transfer, gender translation, and aging. Code\nat https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.",
        "authors": [
            "Zhenfeng Fan",
            "Zhiheng Zhang",
            "Shuang Yang",
            "Chongyang Zhong",
            "Min Cao",
            "Shihong Xia"
        ]
    },
    {
        "title": "ETran: Energy-Based Transferability Estimation",
        "url": "http://arxiv.org/abs/2308.02027",
        "abstract": "This paper addresses the problem of ranking pre-trained models for object\ndetection and image classification. Selecting the best pre-trained model by\nfine-tuning is an expensive and time-consuming task. Previous works have\nproposed transferability estimation based on features extracted by the\npre-trained models. We argue that quantifying whether the target dataset is\nin-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is\nan important factor in the transferability estimation. To this end, we propose\nETran, an energy-based transferability assessment metric, which includes three\nscores: 1) energy score, 2) classification score, and 3) regression score. We\nuse energy-based models to determine whether the target dataset is OOD or IND\nfor the pre-trained model. In contrast to the prior works, ETran is applicable\nto a wide range of tasks including classification, regression, and object\ndetection (classification+regression). This is the first work that proposes\ntransferability estimation for object detection task. Our extensive experiments\non four benchmarks and two tasks show that ETran outperforms previous works on\nobject detection and classification benchmarks by an average of 21% and 12%,\nrespectively, and achieves SOTA in transferability assessment.",
        "authors": [
            "Mohsen Gholami",
            "Mohammad Akbari",
            "Xinglu Wang",
            "Behnam Kamranian",
            "Yong Zhang"
        ]
    },
    {
        "title": "Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images",
        "url": "http://arxiv.org/abs/2306.08528",
        "abstract": "Recent camera-based 3D object detection methods have introduced sequential\nframes to improve the detection performance hoping that multiple frames would\nmitigate the large depth estimation error. Despite improved detection\nperformance, prior works rely on naive fusion methods (e.g., concatenation) or\nare limited to static scenes (e.g., temporal stereo), neglecting the importance\nof the motion cue of objects. These approaches do not fully exploit the\npotential of sequential images and show limited performance improvements. To\naddress this limitation, we propose a novel 3D object detection model, P2D\n(Predict to Detect), that integrates a prediction scheme into a detection\nframework to explicitly extract and leverage motion features. P2D predicts\nobject information in the current frame using solely past frames to learn\ntemporal motion features. We then introduce a novel temporal feature\naggregation method that attentively exploits Bird's-Eye-View (BEV) features\nbased on predicted object information, resulting in accurate 3D object\ndetection. Experimental results demonstrate that P2D improves mAP and NDS by\n3.0% and 3.7% compared to the sequential image-based baseline, illustrating\nthat incorporating a prediction scheme can significantly improve detection\naccuracy.",
        "authors": [
            "Sanmin Kim",
            "Youngseok Kim",
            "In-Jae Lee",
            "Dongsuk Kum"
        ]
    },
    {
        "title": "Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection",
        "url": "http://arxiv.org/abs/2308.10155",
        "abstract": "Anomaly detection (AD), aiming to find samples that deviate from the training\ndistribution, is essential in safety-critical applications. Though recent\nself-supervised learning based attempts achieve promising results by creating\nvirtual outliers, their training objectives are less faithful to AD which\nrequires a concentrated inlier distribution as well as a dispersive outlier\ndistribution. In this paper, we propose Unilaterally Aggregated Contrastive\nLearning with Hierarchical Augmentation (UniCon-HA), taking into account both\nthe requirements above. Specifically, we explicitly encourage the concentration\nof inliers and the dispersion of virtual outliers via supervised and\nunsupervised contrastive losses, respectively. Considering that standard\ncontrastive data augmentation for generating positive views may induce\noutliers, we additionally introduce a soft mechanism to re-weight each\naugmented inlier according to its deviation from the inlier distribution, to\nensure a purified concentration. Moreover, to prompt a higher concentration,\ninspired by curriculum learning, we adopt an easy-to-hard hierarchical\naugmentation strategy and perform contrastive aggregation at different depths\nof the network based on the strengths of data augmentation. Our method is\nevaluated under three AD settings including unlabeled one-class, unlabeled\nmulti-class, and labeled multi-class, demonstrating its consistent superiority\nover other competitors.",
        "authors": [
            "Guodong Wang",
            "Yunhong Wang",
            "Jie Qin",
            "Dongming Zhang",
            "Xiuguo Bao",
            "Di Huang"
        ]
    },
    {
        "title": "Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration",
        "url": "http://arxiv.org/abs/2306.06513",
        "abstract": "Recent work on discrete generative priors, in the form of codebooks, has\nshown exciting performance for image reconstruction and restoration, as the\ndiscrete prior space spanned by the codebooks increases the robustness against\ndiverse image degradations. Nevertheless, these methods require separate\ntraining of codebooks for different image categories, which limits their use to\nspecific image categories only (e.g. face, architecture, etc.), and fail to\nhandle arbitrary natural images. In this paper, we propose AdaCode for learning\nimage-adaptive codebooks for class-agnostic image restoration. Instead of\nlearning a single codebook for each image category, we learn a set of basis\ncodebooks. For a given input image, AdaCode learns a weight map with which we\ncompute a weighted combination of these basis codebooks for adaptive image\nrestoration. Intuitively, AdaCode is a more flexible and expressive discrete\ngenerative prior than previous work. Experimental results demonstrate that\nAdaCode achieves state-of-the-art performance on image reconstruction and\nrestoration tasks, including image super-resolution and inpainting.",
        "authors": [
            "Kechun Liu",
            "Yitong Jiang",
            "Inchang Choi",
            "Jinwei Gu"
        ]
    },
    {
        "title": "3D Segmentation of Humans in Point Clouds with Synthetic Data",
        "url": "http://arxiv.org/abs/2212.00786",
        "abstract": "Segmenting humans in 3D indoor scenes has become increasingly important with\nthe rise of human-centered robotics and AR/VR applications. To this end, we\npropose the task of joint 3D human semantic segmentation, instance segmentation\nand multi-human body-part segmentation. Few works have attempted to directly\nsegment humans in cluttered 3D scenes, which is largely due to the lack of\nannotated training data of humans interacting with 3D scenes. We address this\nchallenge and propose a framework for generating training data of synthetic\nhumans interacting with real 3D scenes. Furthermore, we propose a novel\ntransformer-based model, Human3D, which is the first end-to-end model for\nsegmenting multiple human instances and their body-parts in a unified manner.\nThe key advantage of our synthetic data generation framework is its ability to\ngenerate diverse and realistic human-scene interactions, with highly accurate\nground truth. Our experiments show that pre-training on synthetic data improves\nperformance on a wide variety of 3D human segmentation tasks. Finally, we\ndemonstrate that Human3D outperforms even task-specific state-of-the-art 3D\nsegmentation methods.",
        "authors": [
            "Ay\u00e7a Takmaz",
            "Jonas Schult",
            "Irem Kaftan",
            "Mertcan Ak\u00e7ay",
            "Bastian Leibe",
            "Robert Sumner",
            "Francis Engelmann",
            "Siyu Tang"
        ]
    },
    {
        "title": "Mastering Spatial Graph Prediction of Road Networks",
        "url": "http://arxiv.org/abs/2210.00828",
        "abstract": "Accurately predicting road networks from satellite images requires a global\nunderstanding of the network topology. We propose to capture such high-level\ninformation by introducing a graph-based framework that simulates the addition\nof sequences of graph edges using a reinforcement learning (RL) approach. In\nparticular, given a partially generated graph associated with a satellite\nimage, an RL agent nominates modifications that maximize a cumulative reward.\nAs opposed to standard supervised techniques that tend to be more restricted to\ncommonly used surrogate losses, these rewards can be based on various complex,\npotentially non-continuous, metrics of interest. This yields more power and\nflexibility to encode problem-dependent knowledge. Empirical results on several\nbenchmark datasets demonstrate enhanced performance and increased high-level\nreasoning about the graph topology when using a tree-based search. We further\nhighlight the superiority of our approach under substantial occlusions by\nintroducing a new synthetic benchmark dataset for this task.",
        "authors": [
            "Sotiris Anagnostidis",
            "Aurelien Lucchi",
            "Thomas Hofmann"
        ]
    },
    {
        "title": "Domain Generalization via Rationale Invariance",
        "url": "http://arxiv.org/abs/2308.11158",
        "abstract": "This paper offers a new perspective to ease the challenge of domain\ngeneralization, which involves maintaining robust results even in unseen\nenvironments. Our design focuses on the decision-making process in the final\nclassifier layer. Specifically, we propose treating the element-wise\ncontributions to the final results as the rationale for making a decision and\nrepresenting the rationale for each sample as a matrix. For a well-generalized\nmodel, we suggest the rationale matrices for samples belonging to the same\ncategory should be similar, indicating the model relies on domain-invariant\nclues to make decisions, thereby ensuring robust results. To implement this\nidea, we introduce a rationale invariance loss as a simple regularization\ntechnique, requiring only a few lines of code. Our experiments demonstrate that\nthe proposed approach achieves competitive results across various datasets,\ndespite its simplicity. Code is available at\n\\url{https://github.com/liangchen527/RIDG}.",
        "authors": [
            "Liang Chen",
            "Yong Zhang",
            "Yibing Song",
            "Anton van den Hengel",
            "Lingqiao Liu"
        ]
    },
    {
        "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models",
        "url": "http://arxiv.org/abs/2307.00398",
        "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find\ncorrespondences between images and text. Through the standard deterministic\nmapping process, an image or a text sample is mapped to a single vector in the\nembedding space. This is problematic: as multiple samples (images or text) can\nabstract the same concept in the physical world, deterministic embeddings do\nnot reflect the inherent ambiguity in the embedding space. We propose ProbVLM,\na probabilistic adapter that estimates probability distributions for the\nembeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc\nmanner without needing large-scale datasets or computing. On four challenging\ndatasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the\nmulti-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify\nthe calibration of embedding uncertainties in retrieval tasks and show that\nProbVLM outperforms other methods. Furthermore, we propose active learning and\nmodel selection as two real-world downstream tasks for VLMs and show that the\nestimated uncertainty aids both tasks. Lastly, we present a novel technique for\nvisualizing the embedding distributions using a large-scale pre-trained latent\ndiffusion model. Code is available at https://github.com/ExplainableML/ProbVLM.",
        "authors": [
            "Uddeshya Upadhyay",
            "Shyamgopal Karthik",
            "Massimiliano Mancini",
            "Zeynep Akata"
        ]
    },
    {
        "title": "Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization",
        "url": "http://arxiv.org/abs/2308.06879",
        "abstract": "Test-time adaptation (TTA) methods, which generally rely on the model's\npredictions (e.g., entropy minimization) to adapt the source pretrained model\nto the unlabeled target domain, suffer from noisy signals originating from 1)\nincorrect or 2) open-set predictions. Long-term stable adaptation is hampered\nby such noisy signals, so training models without such error accumulation is\ncrucial for practical TTA. To address these issues, including open-set TTA, we\npropose a simple yet effective sample selection method inspired by the\nfollowing crucial empirical finding. While entropy minimization compels the\nmodel to increase the probability of its predicted label (i.e., confidence\nvalues), we found that noisy samples rather show decreased confidence values.\nTo be more specific, entropy minimization attempts to raise the confidence\nvalues of an individual sample's prediction, but individual confidence values\nmay rise or fall due to the influence of signals from numerous other\npredictions (i.e., wisdom of crowds). Due to this fact, noisy signals\nmisaligned with such 'wisdom of crowds', generally found in the correct\nsignals, fail to raise the individual confidence values of wrong samples,\ndespite attempts to increase them. Based on such findings, we filter out the\nsamples whose confidence values are lower in the adapted model than in the\noriginal model, as they are likely to be noisy. Our method is widely applicable\nto existing TTA methods and improves their long-term adaptation performance in\nboth image classification (e.g., 49.4% reduced error rates with TENT) and\nsemantic segmentation (e.g., 11.7% gain in mIoU with TENT).",
        "authors": [
            "Jungsoo Lee",
            "Debasmit Das",
            "Jaegul Choo",
            "Sungha Choi"
        ]
    },
    {
        "title": "Long-Range Grouping Transformer for Multi-View 3D Reconstruction",
        "url": "http://arxiv.org/abs/2308.08724",
        "abstract": "Nowadays, transformer networks have demonstrated superior performance in many\ncomputer vision tasks. In a multi-view 3D reconstruction algorithm following\nthis paradigm, self-attention processing has to deal with intricate image\ntokens including massive information when facing heavy amounts of view input.\nThe curse of information content leads to the extreme difficulty of model\nlearning. To alleviate this problem, recent methods compress the token number\nrepresenting each view or discard the attention operations between the tokens\nfrom different views. Obviously, they give a negative impact on performance.\nTherefore, we propose long-range grouping attention (LGA) based on the\ndivide-and-conquer principle. Tokens from all views are grouped for separate\nattention operations. The tokens in each group are sampled from all views and\ncan provide macro representation for the resided view. The richness of feature\nlearning is guaranteed by the diversity among different groups. An effective\nand efficient encoder can be established which connects inter-view features\nusing LGA and extract intra-view features using the standard self-attention\nlayer. Moreover, a novel progressive upsampling decoder is also designed for\nvoxel generation with relatively high resolution. Hinging on the above, we\nconstruct a powerful transformer-based network, called LRGT. Experimental\nresults on ShapeNet verify our method achieves SOTA accuracy in multi-view\nreconstruction. Code will be available at\nhttps://github.com/LiyingCV/Long-Range-Grouping-Transformer.",
        "authors": [
            "Liying Yang",
            "Zhenwei Zhu",
            "Xuxin Lin",
            "Jian Nong",
            "Yanyan Liang"
        ]
    },
    {
        "title": "DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization",
        "url": "http://arxiv.org/abs/2208.09708",
        "abstract": "Efficiently deploying deep neural networks on low-resource edge devices is\nchallenging due to their ever-increasing resource requirements. To address this\nissue, researchers have proposed multiplication-free neural networks, such as\nPower-of-Two quantization, or also known as Shift networks, which aim to reduce\nmemory usage and simplify computation. However, existing low-bit Shift networks\nare not as accurate as their full-precision counterparts, typically suffering\nfrom limited weight range encoding schemes and quantization loss. In this\npaper, we propose the DenseShift network, which significantly improves the\naccuracy of Shift networks, achieving competitive performance to full-precision\nnetworks for vision and speech applications. In addition, we introduce a method\nto deploy an efficient DenseShift network using non-quantized floating-point\nactivations, while obtaining 1.6X speed-up over existing methods. To achieve\nthis, we demonstrate that zero-weight values in low-bit Shift networks do not\ncontribute to model capacity and negatively impact inference computation. To\naddress this issue, we propose a zero-free shifting mechanism that simplifies\ninference and increases model capacity. We further propose a sign-scale\ndecomposition design to enhance training efficiency and a low-variance random\ninitialization strategy to improve the model's transfer learning performance.\nOur extensive experiments on various computer vision and speech tasks\ndemonstrate that DenseShift outperforms existing low-bit multiplication-free\nnetworks and achieves competitive performance compared to full-precision\nnetworks. Furthermore, our proposed approach exhibits strong transfer learning\nperformance without a drop in accuracy. Our code was released on GitHub.",
        "authors": [
            "Xinlin Li",
            "Bang Liu",
            "Rui Heng Yang",
            "Vanessa Courville",
            "Chao Xing",
            "Vahid Partovi Nia"
        ]
    },
    {
        "title": "Efficient Computation Sharing for Multi-Task Visual Scene Understanding",
        "url": "http://arxiv.org/abs/2303.09663",
        "abstract": "Solving multiple visual tasks using individual models can be\nresource-intensive, while multi-task learning can conserve resources by sharing\nknowledge across different tasks. Despite the benefits of multi-task learning,\nsuch techniques can struggle with balancing the loss for each task, leading to\npotential performance degradation. We present a novel computation- and\nparameter-sharing framework that balances efficiency and accuracy to perform\nmultiple visual tasks utilizing individually-trained single-task transformers.\nOur method is motivated by transfer learning schemes to reduce computational\nand parameter storage costs while maintaining the desired performance. Our\napproach involves splitting the tasks into a base task and the other sub-tasks,\nand sharing a significant portion of activations and parameters/weights between\nthe base and sub-tasks to decrease inter-task redundancies and enhance\nknowledge sharing. The evaluation conducted on NYUD-v2 and PASCAL-context\ndatasets shows that our method is superior to the state-of-the-art\ntransformer-based multi-task learning techniques with higher accuracy and\nreduced computational resources. Moreover, our method is extended to video\nstream inputs, further reducing computational costs by efficiently sharing\ninformation across the temporal domain as well as the task domain. Our codes\nand models will be publicly available.",
        "authors": [
            "Sara Shoouri",
            "Mingyu Yang",
            "Zichen Fan",
            "Hun-Seok Kim"
        ]
    },
    {
        "title": "Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation",
        "url": "http://arxiv.org/abs/2303.15932",
        "abstract": "Automatic radiology report generation has attracted enormous research\ninterest due to its practical value in reducing the workload of radiologists.\nHowever, simultaneously establishing global correspondences between the image\n(e.g., Chest X-ray) and its related report and local alignments between image\npatches and keywords remains challenging. To this end, we propose an Unify,\nAlign and then Refine (UAR) approach to learn multi-level cross-modal\nalignments and introduce three novel modules: Latent Space Unifier (LSU),\nCross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).\nSpecifically, LSU unifies multimodal data into discrete tokens, making it\nflexible to learn common knowledge among modalities with a shared network. The\nmodality-agnostic CRA learns discriminative features via a set of orthonormal\nbasis and a dual-gate mechanism first and then globally aligns visual and\ntextual representations under a triplet contrastive loss. TIR boosts\ntoken-level local alignment via calibrating text-to-image attention with a\nlearnable mask. Additionally, we design a two-stage training procedure to make\nUAR gradually grasp cross-modal alignments at different levels, which imitates\nradiologists' workflow: writing sentence by sentence first and then checking\nword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR\nbenchmark datasets demonstrate the superiority of our UAR against varied\nstate-of-the-art methods.",
        "authors": [
            "Yaowei Li",
            "Bang Yang",
            "Xuxin Cheng",
            "Zhihong Zhu",
            "Hongxiang Li",
            "Yuexian Zou"
        ]
    },
    {
        "title": "Synthesizing Diverse Human Motions in 3D Indoor Scenes",
        "url": "http://arxiv.org/abs/2305.12411",
        "abstract": "We present a novel method for populating 3D indoor scenes with virtual humans\nthat can navigate in the environment and interact with objects in a realistic\nmanner. Existing approaches rely on training sequences that contain captured\nhuman motions and the 3D scenes they interact with. However, such interaction\ndata are costly, difficult to capture, and can hardly cover all plausible\nhuman-scene interactions in complex environments. To address these challenges,\nwe propose a reinforcement learning-based approach that enables virtual humans\nto navigate in 3D scenes and interact with objects realistically and\nautonomously, driven by learned motion control policies. The motion control\npolicies employ latent motion action spaces, which correspond to realistic\nmotion primitives and are learned from large-scale motion capture data using a\npowerful generative motion model. For navigation in a 3D environment, we\npropose a scene-aware policy with novel state and reward designs for collision\navoidance. Combined with navigation mesh-based path-finding algorithms to\ngenerate intermediate waypoints, our approach enables the synthesis of diverse\nhuman motions navigating in 3D indoor scenes and avoiding obstacles. To\ngenerate fine-grained human-object interactions, we carefully curate\ninteraction goal guidance using a marker-based body representation and leverage\nfeatures based on the signed distance field (SDF) to encode human-scene\nproximity relations. Our method can synthesize realistic and diverse\nhuman-object interactions (e.g.,~sitting on a chair and then getting up) even\nfor out-of-distribution test scenarios with different object shapes,\norientations, starting body positions, and poses. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in terms of\nboth motion naturalness and diversity. Code and video results are available at:\nhttps://zkf1997.github.io/DIMOS.",
        "authors": [
            "Kaifeng Zhao",
            "Yan Zhang",
            "Shaofei Wang",
            "Thabo Beeler",
            "Siyu Tang"
        ]
    },
    {
        "title": "Scene-Aware Feature Matching",
        "url": "http://arxiv.org/abs/2308.09949",
        "abstract": "Current feature matching methods focus on point-level matching, pursuing\nbetter representation learning of individual features, but lacking further\nunderstanding of the scene. This results in significant performance degradation\nwhen handling challenging scenes such as scenes with large viewpoint and\nillumination changes. To tackle this problem, we propose a novel model named\nSAM, which applies attentional grouping to guide Scene-Aware feature Matching.\nSAM handles multi-level features, i.e., image tokens and group tokens, with\nattention layers, and groups the image tokens with the proposed token grouping\nmodule. Our model can be trained by ground-truth matches only and produce\nreasonable grouping results. With the sense-aware grouping guidance, SAM is not\nonly more accurate and robust but also more interpretable than conventional\nfeature matching models. Sufficient experiments on various applications,\nincluding homography estimation, pose estimation, and image matching,\ndemonstrate that our model achieves state-of-the-art performance.",
        "authors": [
            "Xiaoyong Lu",
            "Yaping Yan",
            "Tong Wei",
            "Songlin Du"
        ]
    },
    {
        "title": "Tuning Pre-trained Model via Moment Probing",
        "url": "http://arxiv.org/abs/2307.11342",
        "abstract": "Recently, efficient fine-tuning of large-scale pre-trained models has\nattracted increasing research interests, where linear probing (LP) as a\nfundamental module is involved in exploiting the final representations for\ntask-dependent classification. However, most of the existing methods focus on\nhow to effectively introduce a few of learnable parameters, and little work\npays attention to the commonly used LP module. In this paper, we propose a\nnovel Moment Probing (MP) method to further explore the potential of LP.\nDistinguished from LP which builds a linear classification head based on the\nmean of final features (e.g., word tokens for ViT) or classification tokens,\nour MP performs a linear classifier on feature distribution, which provides the\nstronger representation ability by exploiting richer statistical information\ninherent in features. Specifically, we represent feature distribution by its\ncharacteristic function, which is efficiently approximated by using first- and\nsecond-order moments of features. Furthermore, we propose a multi-head\nconvolutional cross-covariance (MHC$^3$) to compute second-order moments in an\nefficient and effective manner. By considering that MP could affect feature\nlearning, we introduce a partially shared module to learn two recalibrating\nparameters (PSRP) for backbones based on MP, namely MP$_{+}$. Extensive\nexperiments on ten benchmarks using various models show that our MP\nsignificantly outperforms LP and is competitive with counterparts at less\ntraining cost, while our MP$_{+}$ achieves state-of-the-art performance.",
        "authors": [
            "Mingze Gao",
            "Qilong Wang",
            "Zhenyi Lin",
            "Pengfei Zhu",
            "Qinghua Hu",
            "Jingbo Zhou"
        ]
    },
    {
        "title": "Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration",
        "url": "http://arxiv.org/abs/2309.01131",
        "abstract": "We propose a novel end-to-end document understanding model called SeRum\n(SElective Region Understanding Model) for extracting meaningful information\nfrom document images, including document analysis, retrieval, and office\nautomation.\n  Unlike state-of-the-art approaches that rely on multi-stage technical schemes\nand are computationally expensive,\n  SeRum converts document image understanding and recognition tasks into a\nlocal decoding process of the visual tokens of interest, using a content-aware\ntoken merge module.\n  This mechanism enables the model to pay more attention to regions of interest\ngenerated by the query decoder, improving the model's effectiveness and\nspeeding up the decoding speed of the generative scheme.\n  We also designed several pre-training tasks to enhance the understanding and\nlocal awareness of the model.\n  Experimental results demonstrate that SeRum achieves state-of-the-art\nperformance on document understanding tasks and competitive results on text\nspotting tasks.\n  SeRum represents a substantial advancement towards enabling efficient and\neffective end-to-end document understanding.",
        "authors": [
            "Haoyu Cao",
            "Changcun Bao",
            "Chaohu Liu",
            "Huang Chen",
            "Kun Yin",
            "Hao Liu",
            "Yinsong Liu",
            "Deqiang Jiang",
            "Xing Sun"
        ]
    },
    {
        "title": "Task Agnostic Restoration of Natural Video Dynamics",
        "url": "http://arxiv.org/abs/2206.03753",
        "abstract": "In many video restoration/translation tasks, image processing operations are\nna\\\"ively extended to the video domain by processing each frame independently,\ndisregarding the temporal connection of the video frames. This disregard for\nthe temporal connection often leads to severe temporal inconsistencies.\nState-Of-The-Art (SOTA) techniques that address these inconsistencies rely on\nthe availability of unprocessed videos to implicitly siphon and utilize\nconsistent video dynamics to restore the temporal consistency of frame-wise\nprocessed videos which often jeopardizes the translation effect. We propose a\ngeneral framework for this task that learns to infer and utilize consistent\nmotion dynamics from inconsistent videos to mitigate the temporal flicker while\npreserving the perceptual quality for both the temporally neighboring and\nrelatively distant frames without requiring the raw videos at test time. The\nproposed framework produces SOTA results on two benchmark datasets, DAVIS and\nvidevo.net, processed by numerous image processing applications. The code and\nthe trained models are available at\n\\url{https://github.com/MKashifAli/TARONVD}.",
        "authors": [
            "Muhammad Kashif Ali",
            "Dongjin Kim",
            "Tae Hyun Kim"
        ]
    },
    {
        "title": "TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis",
        "url": "http://arxiv.org/abs/2305.00976",
        "abstract": "In this paper, we present TMR, a simple yet effective approach for text to 3D\nhuman motion retrieval. While previous work has only treated retrieval as a\nproxy evaluation metric, we tackle it as a standalone task. Our method extends\nthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a\ncontrastive loss to better structure the cross-modal latent space. We show that\nmaintaining the motion generation loss, along with the contrastive training, is\ncrucial to obtain good performance. We introduce a benchmark for evaluation and\nprovide an in-depth analysis by reporting results on several protocols. Our\nextensive experiments on the KIT-ML and HumanML3D datasets show that TMR\noutperforms the prior work by a significant margin, for example reducing the\nmedian rank from 54 to 19. Finally, we showcase the potential of our approach\non moment retrieval. Our code and models are publicly available at\nhttps://mathis.petrovich.fr/tmr.",
        "authors": [
            "Mathis Petrovich",
            "Michael J. Black",
            "G\u00fcl Varol"
        ]
    },
    {
        "title": "3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation",
        "url": "http://arxiv.org/abs/2302.03744",
        "abstract": "The ability to perceive and understand 3D scenes is crucial for many\napplications in computer vision and robotics. Inverse graphics is an appealing\napproach to 3D scene understanding that aims to infer the 3D scene structure\nfrom 2D images. In this paper, we introduce probabilistic modeling to the\ninverse graphics framework to quantify uncertainty and achieve robustness in 6D\npose estimation tasks. Specifically, we propose 3D Neural Embedding Likelihood\n(3DNEL) as a unified probabilistic model over RGB-D images, and develop\nefficient inference procedures on 3D scene descriptions. 3DNEL effectively\ncombines learned neural embeddings from RGB with depth information to improve\nrobustness in sim-to-real 6D object pose estimation from RGB-D images.\nPerformance on the YCB-Video dataset is on par with state-of-the-art yet is\nmuch more robust in challenging regimes. In contrast to discriminative\napproaches, 3DNEL's probabilistic generative formulation jointly models\nmultiple objects in a scene, quantifies uncertainty in a principled way, and\nhandles object pose tracking under heavy occlusion. Finally, 3DNEL provides a\nprincipled framework for incorporating prior knowledge about the scene and\nobjects, which allows natural extension to additional tasks like camera pose\ntracking from video.",
        "authors": [
            "Guangyao Zhou",
            "Nishad Gothoskar",
            "Lirui Wang",
            "Joshua B. Tenenbaum",
            "Dan Gutfreund",
            "Miguel L\u00e1zaro-Gredilla",
            "Dileep George",
            "Vikash K. Mansinghka"
        ]
    },
    {
        "title": "Towards Robust Model Watermark via Reducing Parametric Vulnerability",
        "url": "http://arxiv.org/abs/2309.04777",
        "abstract": "Deep neural networks are valuable assets considering their commercial\nbenefits and huge demands for costly annotation and computation resources. To\nprotect the copyright of DNNs, backdoor-based ownership verification becomes\npopular recently, in which the model owner can watermark the model by embedding\na specific backdoor behavior before releasing it. The defenders (usually the\nmodel owners) can identify whether a suspicious third-party model is ``stolen''\nfrom them based on the presence of the behavior. Unfortunately, these\nwatermarks are proven to be vulnerable to removal attacks even like\nfine-tuning. To further explore this vulnerability, we investigate the\nparameter space and find there exist many watermark-removed models in the\nvicinity of the watermarked one, which may be easily used by removal attacks.\nInspired by this finding, we propose a mini-max formulation to find these\nwatermark-removed models and recover their watermark behavior. Extensive\nexperiments demonstrate that our method improves the robustness of the model\nwatermarking against parametric changes and numerous watermark-removal attacks.\nThe codes for reproducing our main experiments are available at\n\\url{https://github.com/GuanhaoGan/robust-model-watermarking}.",
        "authors": [
            "Guanhao Gan",
            "Yiming Li",
            "Dongxian Wu",
            "Shu-Tao Xia"
        ]
    },
    {
        "title": "SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection",
        "url": "http://arxiv.org/abs/2309.07084",
        "abstract": "In this paper, we propose a novel training strategy called SupFusion, which\nprovides an auxiliary feature level supervision for effective LiDAR-Camera\nfusion and significantly boosts detection performance. Our strategy involves a\ndata enhancement method named Polar Sampling, which densifies sparse objects\nand trains an assistant model to generate high-quality features as the\nsupervision. These features are then used to train the LiDAR-Camera fusion\nmodel, where the fusion feature is optimized to simulate the generated\nhigh-quality features. Furthermore, we propose a simple yet effective deep\nfusion module, which contiguously gains superior performance compared with\nprevious fusion methods with SupFusion strategy. In such a manner, our proposal\nshares the following advantages. Firstly, SupFusion introduces auxiliary\nfeature-level supervision which could boost LiDAR-Camera detection performance\nwithout introducing extra inference costs. Secondly, the proposed deep fusion\ncould continuously improve the detector's abilities. Our proposed SupFusion and\ndeep fusion module is plug-and-play, we make extensive experiments to\ndemonstrate its effectiveness. Specifically, we gain around 2% 3D mAP\nimprovements on KITTI benchmark based on multiple LiDAR-Camera 3D detectors.",
        "authors": [
            "Yiran Qin",
            "Chaoqun Wang",
            "Zijian Kang",
            "Ningning Ma",
            "Zhen Li",
            "Ruimao Zhang"
        ]
    },
    {
        "title": "Rethinking Vision Transformers for MobileNet Size and Speed",
        "url": "http://arxiv.org/abs/2212.08059",
        "abstract": "With the success of Vision Transformers (ViTs) in computer vision tasks,\nrecent arts try to optimize the performance and complexity of ViTs to enable\nefficient deployment on mobile devices. Multiple approaches are proposed to\naccelerate attention mechanism, improve inefficient designs, or incorporate\nmobile-friendly lightweight convolutions to form hybrid architectures. However,\nViT and its variants still have higher latency or considerably more parameters\nthan lightweight CNNs, even true for the years-old MobileNet. In practice,\nlatency and size are both crucial for efficient deployment on\nresource-constraint hardware. In this work, we investigate a central question,\ncan transformer models run as fast as MobileNet and maintain a similar size? We\nrevisit the design choices of ViTs and propose a novel supernet with low\nlatency and high parameter efficiency. We further introduce a novel\nfine-grained joint search strategy for transformer models that can find\nefficient architectures by optimizing latency and number of parameters\nsimultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher\ntop-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and\nparameters. This work demonstrate that properly designed and optimized vision\ntransformers can achieve high performance even with MobileNet-level size and\nspeed.",
        "authors": [
            "Yanyu Li",
            "Ju Hu",
            "Yang Wen",
            "Georgios Evangelidis",
            "Kamyar Salahi",
            "Yanzhi Wang",
            "Sergey Tulyakov",
            "Jian Ren"
        ]
    },
    {
        "title": "Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation",
        "url": "http://arxiv.org/abs/2307.09906",
        "abstract": "Talking head video generation aims to animate a human face in a still image\nwith dynamic poses and expressions using motion information derived from a\ntarget-driving video, while maintaining the person's identity in the source\nimage. However, dramatic and complex motions in the driving video cause\nambiguous generation, because the still source image cannot provide sufficient\nappearance information for occluded regions or delicate expression variations,\nwhich produces severe artifacts and significantly degrades the generation\nquality. To tackle this problem, we propose to learn a global facial\nrepresentation space, and design a novel implicit identity representation\nconditioned memory compensation network, coined as MCNet, for high-fidelity\ntalking head generation.~Specifically, we devise a network module to learn a\nunified spatial facial meta-memory bank from all training samples, which can\nprovide rich facial structure and appearance priors to compensate warped source\nfacial features for the generation. Furthermore, we propose an effective query\nmechanism based on implicit identity representations learned from the discrete\nkeypoints of the source image. It can greatly facilitate the retrieval of more\ncorrelated information from the memory bank for the compensation. Extensive\nexperiments demonstrate that MCNet can learn representative and complementary\nfacial memory, and can clearly outperform previous state-of-the-art talking\nhead generation methods on VoxCeleb1 and CelebV datasets. Please check our\n\\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.",
        "authors": [
            "Fa-Ting Hong",
            "Dan Xu"
        ]
    },
    {
        "title": "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks",
        "url": "http://arxiv.org/abs/2307.07742",
        "abstract": "Large Pre-trained Transformers exhibit an intriguing capacity for in-context\nlearning. Without gradient updates, these models can rapidly construct new\npredictors from demonstrations presented in the inputs. Recent works promote\nthis ability in the vision-language domain by incorporating visual information\ninto large language models that can already make in-context predictions.\nHowever, these methods could inherit issues in the language domain, such as\ntemplate sensitivity and hallucination. Also, the scale of these language\nmodels raises a significant demand for computations, making learning and\noperating these models resource-intensive. To this end, we raise a question:\n``How can we enable in-context learning without relying on the intrinsic\nin-context ability of large language models?\". To answer it, we propose a\nsuccinct and general framework, Self-supervised IN-Context learning (SINC),\nthat introduces a meta-model to learn on self-supervised prompts consisting of\ntailored demonstrations. The learned models can be transferred to downstream\ntasks for making in-context predictions on-the-fly. Extensive experiments show\nthat SINC outperforms gradient-based methods in various vision-language tasks\nunder few-shot settings. Furthermore, the designs of SINC help us investigate\nthe benefits of in-context learning across different tasks, and the analysis\nfurther reveals the essential components for the emergence of in-context\nlearning in the vision-language domain.",
        "authors": [
            "Yi-Syuan Chen",
            "Yun-Zhu Song",
            "Cheng Yu Yeo",
            "Bei Liu",
            "Jianlong Fu",
            "Hong-Han Shuai"
        ]
    },
    {
        "title": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models",
        "url": "http://arxiv.org/abs/2305.11870",
        "abstract": "We propose a 3D generation pipeline that uses diffusion models to generate\nrealistic human digital avatars. Due to the wide variety of human identities,\nposes, and stochastic details, the generation of 3D human meshes has been a\nchallenging problem. To address this, we decompose the problem into 2D normal\nmap generation and normal map-based 3D reconstruction. Specifically, we first\nsimultaneously generate realistic normal maps for the front and backside of a\nclothed human, dubbed dual normal maps, using a pose-conditional diffusion\nmodel. For 3D reconstruction, we \"carve\" the prior SMPL-X mesh to a detailed 3D\nmesh according to the normal maps through mesh optimization. To further enhance\nthe high-frequency details, we present a diffusion resampling scheme on both\nbody and facial regions, thus encouraging the generation of realistic digital\navatars. We also seamlessly incorporate a recent text-to-image diffusion model\nto support text-based human identity control. Our method, namely, Chupa, is\ncapable of generating realistic 3D clothed humans with better perceptual\nquality and identity variety.",
        "authors": [
            "Byungjun Kim",
            "Patrick Kwon",
            "Kwangho Lee",
            "Myunggi Lee",
            "Sookwan Han",
            "Daesik Kim",
            "Hanbyul Joo"
        ]
    },
    {
        "title": "Unsupervised Domain Adaptive Detection with Network Stability Analysis",
        "url": "http://arxiv.org/abs/2308.08182",
        "abstract": "Domain adaptive detection aims to improve the generality of a detector,\nlearned from the labeled source domain, on the unlabeled target domain. In this\nwork, drawing inspiration from the concept of stability from the control theory\nthat a robust system requires to remain consistent both externally and\ninternally regardless of disturbances, we propose a novel framework that\nachieves unsupervised domain adaptive detection through stability analysis. In\nspecific, we treat discrepancies between images and regions from different\ndomains as disturbances, and introduce a novel simple but effective Network\nStability Analysis (NSA) framework that considers various disturbances for\ndomain adaptation. Particularly, we explore three types of perturbations\nincluding heavy and light image-level disturbances and instancelevel\ndisturbance. For each type, NSA performs external consistency analysis on the\noutputs from raw and perturbed images and/or internal consistency analysis on\ntheir features, using teacher-student models. By integrating NSA into Faster\nR-CNN, we immediately achieve state-of-the-art results. In particular, we set a\nnew record of 52.7% mAP on Cityscapes-to-FoggyCityscapes, showing the potential\nof NSA for domain adaptive detection. It is worth noticing, our NSA is designed\nfor general purpose, and thus applicable to one-stage detection model (e.g.,\nFCOS) besides the adopted one, as shown by experiments.\nhttps://github.com/tiankongzhang/NSA.",
        "authors": [
            "Wenzhang Zhou",
            "Heng Fan",
            "Tiejian Luo",
            "Libo Zhang"
        ]
    },
    {
        "title": "Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation",
        "url": "http://arxiv.org/abs/2303.09152",
        "abstract": "Implicit neural rendering, which uses signed distance function (SDF)\nrepresentation with geometric priors (such as depth or surface normal), has led\nto impressive progress in the surface reconstruction of large-scale scenes.\nHowever, applying this method to reconstruct a room-level scene from images may\nmiss structures in low-intensity areas or small and thin objects. We conducted\nexperiments on three datasets to identify limitations of the original color\nrendering loss and priors-embedded SDF scene representation.\n  We found that the color rendering loss results in optimization bias against\nlow-intensity areas, causing gradient vanishing and leaving these areas\nunoptimized. To address this issue, we propose a feature-based color rendering\nloss that utilizes non-zero feature values to bring back optimization signals.\nAdditionally, the SDF representation can be influenced by objects along a ray\npath, disrupting the monotonic change of SDF values when a single object is\npresent. To counteract this, we explore using the occupancy representation,\nwhich encodes each point separately and is unaffected by objects along a\nquerying ray. Our experimental results demonstrate that the joint forces of the\nfeature-based rendering loss and Occ-SDF hybrid representation scheme can\nprovide high-quality reconstruction results, especially in challenging\nroom-level scenarios. The code would be released.",
        "authors": [
            "Xiaoyang Lyu",
            "Peng Dai",
            "Zizhang Li",
            "Dongyu Yan",
            "Yi Lin",
            "Yifan Peng",
            "Xiaojuan Qi"
        ]
    },
    {
        "title": "Spatially and Spectrally Consistent Deep Functional Maps",
        "url": "http://arxiv.org/abs/2308.08871",
        "abstract": "Cycle consistency has long been exploited as a powerful prior for jointly\noptimizing maps within a collection of shapes. In this paper, we investigate\nits utility in the approaches of Deep Functional Maps, which are considered\nstate-of-the-art in non-rigid shape matching. We first justify that under\ncertain conditions, the learned maps, when represented in the spectral domain,\nare already cycle consistent. Furthermore, we identify the discrepancy that\nspectrally consistent maps are not necessarily spatially, or point-wise,\nconsistent. In light of this, we present a novel design of unsupervised Deep\nFunctional Maps, which effectively enforces the harmony of learned maps under\nthe spectral and the point-wise representation. By taking advantage of cycle\nconsistency, our framework produces state-of-the-art results in mapping shapes\neven under significant distortions. Beyond that, by independently estimating\nmaps in both spectral and spatial domains, our method naturally alleviates\nover-fitting in network training, yielding superior generalization performance\nand accuracy within an array of challenging tests for both near-isometric and\nnon-isometric datasets. Codes are available at\nhttps://github.com/rqhuang88/Spatiallyand-Spectrally-Consistent-Deep-Functional-Maps.",
        "authors": [
            "Mingze Sun",
            "Shiwei Mao",
            "Puhua Jiang",
            "Maks Ovsjanikov",
            "Ruqi Huang"
        ]
    },
    {
        "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data",
        "url": "http://arxiv.org/abs/2303.17590",
        "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.",
        "authors": [
            "Paola Cascante-Bonilla",
            "Khaled Shehada",
            "James Seale Smith",
            "Sivan Doveh",
            "Donghyun Kim",
            "Rameswar Panda",
            "G\u00fcl Varol",
            "Aude Oliva",
            "Vicente Ordonez",
            "Rogerio Feris",
            "Leonid Karlinsky"
        ]
    },
    {
        "title": "Continual Zero-Shot Learning through Semantically Guided Generative Random Walks",
        "url": "http://arxiv.org/abs/2308.12366",
        "abstract": "Learning novel concepts, remembering previous knowledge, and adapting it to\nfuture tasks occur simultaneously throughout a human's lifetime. To model such\ncomprehensive abilities, continual zero-shot learning (CZSL) has recently been\nintroduced. However, most existing methods overused unseen semantic information\nthat may not be continually accessible in realistic settings. In this paper, we\naddress the challenge of continual zero-shot learning where unseen information\nis not provided during training, by leveraging generative modeling. The heart\nof the generative-based methods is to learn quality representations from seen\nclasses to improve the generative understanding of the unseen visual space.\nMotivated by this, we introduce generalization-bound tools and provide the\nfirst theoretical explanation for the benefits of generative modeling to CZSL\ntasks. Guided by the theoretical analysis, we then propose our learning\nalgorithm that employs a novel semantically guided Generative Random Walk (GRW)\nloss. The GRW loss augments the training by continually encouraging the model\nto generate realistic and characterized samples to represent the unseen space.\nOur algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN\ndatasets, surpassing existing CZSL methods by 3-7\\%. The code has been made\navailable here \\url{https://github.com/wx-zhang/IGCZSL}",
        "authors": [
            "Wenxuan Zhang",
            "Paul Janson",
            "Kai Yi",
            "Ivan Skorokhodov",
            "Mohamed Elhoseiny"
        ]
    },
    {
        "title": "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions",
        "url": "http://arxiv.org/abs/2308.08544",
        "abstract": "This paper strives for motion expressions guided video segmentation, which\nfocuses on segmenting objects in video content based on a sentence describing\nthe motion of the objects. Existing referring video object datasets typically\nfocus on salient objects and use language expressions that contain excessive\nstatic attributes that could potentially enable the target object to be\nidentified in a single frame. These datasets downplay the importance of motion\nin video content for language-guided video object segmentation. To investigate\nthe feasibility of using motion expressions to ground and segment objects in\nvideos, we propose a large-scale dataset called MeViS, which contains numerous\nmotion expressions to indicate target objects in complex environments. We\nbenchmarked 5 existing referring video object segmentation (RVOS) methods and\nconducted a comprehensive comparison on the MeViS dataset. The results show\nthat current RVOS methods cannot effectively address motion expression-guided\nvideo segmentation. We further analyze the challenges and propose a baseline\napproach for the proposed MeViS dataset. The goal of our benchmark is to\nprovide a platform that enables the development of effective language-guided\nvideo segmentation algorithms that leverage motion expressions as a primary cue\nfor object segmentation in complex video scenes. The proposed MeViS dataset has\nbeen released at https://henghuiding.github.io/MeViS.",
        "authors": [
            "Henghui Ding",
            "Chang Liu",
            "Shuting He",
            "Xudong Jiang",
            "Chen Change Loy"
        ]
    },
    {
        "title": "OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions",
        "url": "http://arxiv.org/abs/2210.05557",
        "abstract": "The pretrain-finetune paradigm in modern computer vision facilitates the\nsuccess of self-supervised learning, which tends to achieve better\ntransferability than supervised learning. However, with the availability of\nmassive labeled data, a natural question emerges: how to train a better model\nwith both self and full supervision signals? In this paper, we propose\nOmni-suPErvised Representation leArning with hierarchical supervisions (OPERA)\nas a solution. We provide a unified perspective of supervisions from labeled\nand unlabeled data and propose a unified framework of fully supervised and\nself-supervised learning. We extract a set of hierarchical proxy\nrepresentations for each image and impose self and full supervisions on the\ncorresponding proxy representations. Extensive experiments on both\nconvolutional neural networks and vision transformers demonstrate the\nsuperiority of OPERA in image classification, segmentation, and object\ndetection. Code is available at: https://github.com/wangck20/OPERA.",
        "authors": [
            "Chengkun Wang",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Jie Zhou",
            "Jiwen Lu"
        ]
    },
    {
        "title": "GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning",
        "url": "http://arxiv.org/abs/2308.10279",
        "abstract": "Federated Learning (FL) is popular for its privacy-preserving and\ncollaborative learning capabilities. Recently, personalized FL (pFL) has\nreceived attention for its ability to address statistical heterogeneity and\nachieve personalization in FL. However, from the perspective of feature\nextraction, most existing pFL methods only focus on extracting global or\npersonalized feature information during local training, which fails to meet the\ncollaborative learning and personalization goals of pFL. To address this, we\npropose a new pFL method, named GPFL, to simultaneously learn global and\npersonalized feature information on each client. We conduct extensive\nexperiments on six datasets in three statistically heterogeneous settings and\nshow the superiority of GPFL over ten state-of-the-art methods regarding\neffectiveness, scalability, fairness, stability, and privacy. Besides, GPFL\nmitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.",
        "authors": [
            "Jianqing Zhang",
            "Yang Hua",
            "Hao Wang",
            "Tao Song",
            "Zhengui Xue",
            "Ruhui Ma",
            "Jian Cao",
            "Haibing Guan"
        ]
    },
    {
        "title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer",
        "url": "http://arxiv.org/abs/2303.08622",
        "abstract": "Diffusion models have shown great promise in text-guided image style\ntransfer, but there is a trade-off between style transformation and content\npreservation due to their stochastic nature. Existing methods require\ncomputationally expensive fine-tuning of diffusion models or additional neural\nnetwork. To address this, here we propose a zero-shot contrastive loss for\ndiffusion models that doesn't require additional fine-tuning or auxiliary\nnetworks. By leveraging patch-wise contrastive loss between generated samples\nand original image embeddings in the pre-trained diffusion model, our method\ncan generate images with the same semantic content as the source image in a\nzero-shot manner. Our approach outperforms existing methods while preserving\ncontent and requiring no additional training, not only for image style transfer\nbut also for image-to-image translation and manipulation. Our experimental\nresults validate the effectiveness of our proposed method.",
        "authors": [
            "Serin Yang",
            "Hyunmin Hwang",
            "Jong Chul Ye"
        ]
    },
    {
        "title": "Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis",
        "url": "http://arxiv.org/abs/2307.09323",
        "abstract": "This paper presents ER-NeRF, a novel conditional Neural Radiance Fields\n(NeRF) based architecture for talking portrait synthesis that can concurrently\nachieve fast convergence, real-time rendering, and state-of-the-art performance\nwith small model size. Our idea is to explicitly exploit the unequal\ncontribution of spatial regions to guide talking portrait modeling.\nSpecifically, to improve the accuracy of dynamic head reconstruction, a compact\nand expressive NeRF-based Tri-Plane Hash Representation is introduced by\npruning empty spatial regions with three planar hash encoders. For speech\naudio, we propose a Region Attention Module to generate region-aware condition\nfeature via an attention mechanism. Different from existing methods that\nutilize an MLP-based encoder to learn the cross-modal relation implicitly, the\nattention mechanism builds an explicit connection between audio features and\nspatial regions to capture the priors of local motions. Moreover, a direct and\nfast Adaptive Pose Encoding is introduced to optimize the head-torso separation\nproblem by mapping the complex transformation of the head pose into spatial\ncoordinates. Extensive experiments demonstrate that our method renders better\nhigh-fidelity and audio-lips synchronized talking portrait videos, with\nrealistic details and high efficiency compared to previous methods.",
        "authors": [
            "Jiahe Li",
            "Jiawei Zhang",
            "Xiao Bai",
            "Jun Zhou",
            "Lin Gu"
        ]
    },
    {
        "title": "Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network",
        "url": "http://arxiv.org/abs/2308.08220",
        "abstract": "This paper presents a novel network structure with illumination-aware gamma\ncorrection and complete image modelling to solve the low-light image\nenhancement problem. Low-light environments usually lead to less informative\nlarge-scale dark areas, directly learning deep representations from low-light\nimages is insensitive to recovering normal illumination. We propose to\nintegrate the effectiveness of gamma correction with the strong modelling\ncapacities of deep networks, which enables the correction factor gamma to be\nlearned in a coarse to elaborate manner via adaptively perceiving the deviated\nillumination. Because exponential operation introduces high computational\ncomplexity, we propose to use Taylor Series to approximate gamma correction,\naccelerating the training and inference speed. Dark areas usually occupy large\nscales in low-light images, common local modelling structures, e.g., CNN,\nSwinIR, are thus insufficient to recover accurate illumination across whole\nlow-light images. We propose a novel Transformer block to completely simulate\nthe dependencies of all pixels across images via a local-to-global hierarchical\nattention mechanism, so that dark areas could be inferred by borrowing the\ninformation from far informative regions in a highly effective manner.\nExtensive experiments on several benchmark datasets demonstrate that our\napproach outperforms state-of-the-art methods.",
        "authors": [
            "Yinglong Wang",
            "Zhen Liu",
            "Jianzhuang Liu",
            "Songcen Xu",
            "Shuaicheng Liu"
        ]
    },
    {
        "title": "Exploring the Benefits of Visual Prompting in Differential Privacy",
        "url": "http://arxiv.org/abs/2303.12247",
        "abstract": "Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration. Our code is available at\n(https://github.com/EzzzLi/Prompt-PATE).",
        "authors": [
            "Yizhe Li",
            "Yu-Lin Tsai",
            "Xuebin Ren",
            "Chia-Mu Yu",
            "Pin-Yu Chen"
        ]
    },
    {
        "title": "Single Image Reflection Separation via Component Synergy",
        "url": "http://arxiv.org/abs/2308.10027",
        "abstract": "The reflection superposition phenomenon is complex and widely distributed in\nthe real world, which derives various simplified linear and nonlinear\nformulations of the problem. In this paper, based on the investigation of the\nweaknesses of existing models, we propose a more general form of the\nsuperposition model by introducing a learnable residue term, which can\neffectively capture residual information during decomposition, guiding the\nseparated layers to be complete. In order to fully capitalize on its\nadvantages, we further design the network structure elaborately, including a\nnovel dual-stream interaction mechanism and a powerful decomposition network\nwith a semantic pyramid encoder. Extensive experiments and ablation studies are\nconducted to verify our superiority over state-of-the-art approaches on\nmultiple real-world benchmark datasets. Our code is publicly available at\nhttps://github.com/mingcv/DSRNet.",
        "authors": [
            "Qiming Hu",
            "Xiaojie Guo"
        ]
    },
    {
        "title": "Mining bias-target Alignment from Voronoi Cells",
        "url": "http://arxiv.org/abs/2305.03691",
        "abstract": "Despite significant research efforts, deep neural networks are still\nvulnerable to biases: this raises concerns about their fairness and limits\ntheir generalization. In this paper, we propose a bias-agnostic approach to\nmitigate the impact of bias in deep neural networks. Unlike traditional\ndebiasing approaches, we rely on a metric to quantify ``bias\nalignment/misalignment'' on target classes, and use this information to\ndiscourage the propagation of bias-target alignment information through the\nnetwork. We conduct experiments on several commonly used datasets for debiasing\nand compare our method to supervised and bias-specific approaches. Our results\nindicate that the proposed method achieves comparable performance to\nstate-of-the-art supervised approaches, although it is bias-agnostic, even in\npresence of multiple biases in the same sample.",
        "authors": [
            "R\u00e9mi Nahon",
            "Van-Tam Nguyen",
            "Enzo Tartaglione"
        ]
    },
    {
        "title": "DIFFGUARD: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-Trained Diffusion Models",
        "url": "http://arxiv.org/abs/2308.07687",
        "abstract": "Given a classifier, the inherent property of semantic Out-of-Distribution\n(OOD) samples is that their contents differ from all legal classes in terms of\nsemantics, namely semantic mismatch. There is a recent work that directly\napplies it to OOD detection, which employs a conditional Generative Adversarial\nNetwork (cGAN) to enlarge semantic mismatch in the image space. While achieving\nremarkable OOD detection performance on small datasets, it is not applicable to\nImageNet-scale datasets due to the difficulty in training cGANs with both input\nimages and labels as conditions. As diffusion models are much easier to train\nand amenable to various conditions compared to cGANs, in this work, we propose\nto directly use pre-trained diffusion models for semantic mismatch-guided OOD\ndetection, named DiffGuard. Specifically, given an OOD input image and the\npredicted label from the classifier, we try to enlarge the semantic difference\nbetween the reconstructed OOD image under these conditions and the original\ninput image. We also present several test-time techniques to further strengthen\nsuch differences. Experimental results show that DiffGuard is effective on both\nCifar-10 and hard cases of the large-scale ImageNet, and it can be easily\ncombined with existing OOD detection techniques to achieve state-of-the-art OOD\ndetection results.",
        "authors": [
            "Ruiyuan Gao",
            "Chenchen Zhao",
            "Lanqing Hong",
            "Qiang Xu"
        ]
    },
    {
        "title": "Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-Identification",
        "url": "http://arxiv.org/abs/2308.08887",
        "abstract": "This paper aims to learn a domain-generalizable (DG) person re-identification\n(ReID) representation from large-scale videos \\textbf{without any annotation}.\nPrior DG ReID methods employ limited labeled data for training due to the high\ncost of annotation, which restricts further advances. To overcome the barriers\nof data and annotation, we propose to utilize large-scale unsupervised data for\ntraining. The key issue lies in how to mine identity information. To this end,\nwe propose an Identity-seeking Self-supervised Representation learning (ISR)\nmethod. ISR constructs positive pairs from inter-frame images by modeling the\ninstance association as a maximum-weight bipartite matching problem. A\nreliability-guided contrastive loss is further presented to suppress the\nadverse impact of noisy positive pairs, ensuring that reliable positive pairs\ndominate the learning process. The training cost of ISR scales approximately\nlinearly with the data size, making it feasible to utilize large-scale data for\ntraining. The learned representation exhibits superior generalization ability.\n\\textbf{Without human annotation and fine-tuning, ISR achieves 87.0\\% Rank-1 on\nMarket-1501 and 56.4\\% Rank-1 on MSMT17}, outperforming the best supervised\ndomain-generalizable method by 5.0\\% and 19.5\\%, respectively. In the\npre-training$\\rightarrow$fine-tuning scenario, ISR achieves state-of-the-art\nperformance, with 88.4\\% Rank-1 on MSMT17. The code is at\n\\url{https://github.com/dcp15/ISR_ICCV2023_Oral}.",
        "authors": [
            "Zhaopeng Dou",
            "Zhongdao Wang",
            "Yali Li",
            "Shengjin Wang"
        ]
    },
    {
        "title": "3D-Aware Generative Model for Improved Side-View Image Synthesis",
        "url": "http://arxiv.org/abs/2309.10388",
        "abstract": "While recent 3D-aware generative models have shown photo-realistic image\nsynthesis with multi-view consistency, the synthesized image quality degrades\ndepending on the camera pose (e.g., a face with a blurry and noisy boundary at\na side viewpoint). Such degradation is mainly caused by the difficulty of\nlearning both pose consistency and photo-realism simultaneously from a dataset\nwith heavily imbalanced poses. In this paper, we propose SideGAN, a novel 3D\nGAN training method to generate photo-realistic images irrespective of the\ncamera pose, especially for faces of side-view angles. To ease the challenging\nproblem of learning photo-realistic and pose-consistent image synthesis, we\nsplit the problem into two subproblems, each of which can be solved more\neasily. Specifically, we formulate the problem as a combination of two simple\ndiscrimination problems, one of which learns to discriminate whether a\nsynthesized image looks real or not, and the other learns to discriminate\nwhether a synthesized image agrees with the camera pose. Based on this, we\npropose a dual-branched discriminator with two discrimination branches. We also\npropose a pose-matching loss to learn the pose consistency of 3D GANs. In\naddition, we present a pose sampling strategy to increase learning\nopportunities for steep angles in a pose-imbalanced dataset. With extensive\nvalidation, we demonstrate that our approach enables 3D GANs to generate\nhigh-quality geometries and photo-realistic images irrespective of the camera\npose.",
        "authors": [
            "Kyungmin Jo",
            "Wonjoon Jin",
            "Jaegul Choo",
            "Hyunjoon Lee",
            "Sunghyun Cho"
        ]
    },
    {
        "title": "Tracking Anything with Decoupled Video Segmentation",
        "url": "http://arxiv.org/abs/2309.03903",
        "abstract": "Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA",
        "authors": [
            "Ho Kei Cheng",
            "Seoung Wug Oh",
            "Brian Price",
            "Alexander Schwing",
            "Joon-Young Lee"
        ]
    },
    {
        "title": "EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation",
        "url": "http://arxiv.org/abs/2304.14291",
        "abstract": "With autonomous industries on the rise, domain adaptation of the visual\nperception stack is an important research direction due to the cost savings\npromise. Much prior art was dedicated to domain-adaptive semantic segmentation\nin the synthetic-to-real context. Despite being a crucial output of the\nperception stack, panoptic segmentation has been largely overlooked by the\ndomain adaptation community. Therefore, we revisit well-performing domain\nadaptation strategies from other fields, adapt them to panoptic segmentation,\nand show that they can effectively enhance panoptic domain adaptation. Further,\nwe study the panoptic network design and propose a novel architecture (EDAPS)\ndesigned explicitly for domain-adaptive panoptic segmentation. It uses a\nshared, domain-robust transformer encoder to facilitate the joint adaptation of\nsemantic and instance features, but task-specific decoders tailored for the\nspecific requirements of both domain-adaptive semantic and instance\nsegmentation. As a result, the performance gap seen in challenging panoptic\nbenchmarks is substantially narrowed. EDAPS significantly improves the\nstate-of-the-art performance for panoptic segmentation UDA by a large margin of\n20% on SYNTHIA-to-Cityscapes and even 72% on the more challenging\nSYNTHIA-to-Mapillary Vistas. The implementation is available at\nhttps://github.com/susaha/edaps.",
        "authors": [
            "Suman Saha",
            "Lukas Hoyer",
            "Anton Obukhov",
            "Dengxin Dai",
            "Luc Van Gool"
        ]
    },
    {
        "title": "Parallax-Tolerant Unsupervised Deep Image Stitching",
        "url": "http://arxiv.org/abs/2302.08207",
        "abstract": "Traditional image stitching approaches tend to leverage increasingly complex\ngeometric features (point, line, edge, etc.) for better performance. However,\nthese hand-crafted features are only suitable for specific natural scenes with\nadequate geometric structures. In contrast, deep stitching schemes overcome the\nadverse conditions by adaptively learning robust semantic features, but they\ncannot handle large-parallax cases due to homography-based registration. To\nsolve these issues, we propose UDIS++, a parallax-tolerant unsupervised deep\nimage stitching technique. First, we propose a robust and flexible warp to\nmodel the image registration from global homography to local thin-plate spline\nmotion. It provides accurate alignment for overlapping regions and shape\npreservation for non-overlapping regions by joint optimization concerning\nalignment and distortion. Subsequently, to improve the generalization\ncapability, we design a simple but effective iterative strategy to enhance the\nwarp adaption in cross-dataset and cross-resolution applications. Finally, to\nfurther eliminate the parallax artifacts, we propose to composite the stitched\nimage seamlessly by unsupervised learning for seam-driven composition masks.\nCompared with existing methods, our solution is parallax-tolerant and free from\nlaborious designs of complicated geometric features for specific scenes.\nExtensive experiments show our superiority over the SoTA methods, both\nquantitatively and qualitatively. The code is available at\nhttps://github.com/nie-lang/UDIS2.",
        "authors": [
            "Lang Nie",
            "Chunyu Lin",
            "Kang Liao",
            "Shuaicheng Liu",
            "Yao Zhao"
        ]
    },
    {
        "title": "SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis",
        "url": "http://arxiv.org/abs/2303.15965",
        "abstract": "To represent the biological variability of clinical neuroimaging populations,\nit is vital to be able to combine data across scanners and studies. However,\ndifferent MRI scanners produce images with different characteristics, resulting\nin a domain shift known as the `harmonisation problem'. Additionally,\nneuroimaging data is inherently personal in nature, leading to data privacy\nconcerns when sharing the data. To overcome these barriers, we propose an\nUnsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through\nmodelling the imaging features as a Gaussian Mixture Model and minimising an\nadapted Bhattacharyya distance between the source and target features, we can\ncreate a model that performs well for the target data whilst having a shared\nfeature representation across the data domains, without needing access to the\nsource data for adaptation or target labels. We demonstrate the performance of\nour method on simulated and real domain shifts, showing that the approach is\napplicable to classification, segmentation and regression tasks, requiring no\nchanges to the algorithm. Our method outperforms existing SFDA approaches\nacross a range of realistic data scenarios, demonstrating the potential utility\nof our approach for MRI harmonisation and general SFDA problems. Our code is\navailable at \\url{https://github.com/nkdinsdale/SFHarmony}.",
        "authors": [
            "Nicola K Dinsdale",
            "Mark Jenkinson",
            "Ana IL Namburete"
        ]
    },
    {
        "title": "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",
        "url": "http://arxiv.org/abs/2308.07498",
        "abstract": "VLN-CE is a recently released embodied task, where AI agents need to navigate\na freely traversable environment to reach a distant target location, given\nlanguage instructions. It poses great challenges due to the huge space of\npossible strategies. Driven by the belief that the ability to anticipate the\nconsequences of future actions is crucial for the emergence of intelligent and\ninterpretable planning behavior, we propose DREAMWALKER -- a world model based\nVLN-CE agent. The world model is built to summarize the visual, topological,\nand dynamic properties of the complicated continuous environment into a\ndiscrete, structured, and compact representation. DREAMWALKER can simulate and\nevaluate possible plans entirely in such internal abstract world, before\nexecuting costly actions. As opposed to existing model-free VLN-CE agents\nsimply making greedy decisions in the real world, which easily results in\nshortsighted behaviors, DREAMWALKER is able to make strategic planning through\nlarge amounts of ``mental experiments.'' Moreover, the imagined future\nscenarios reflect our agent's intention, making its decision-making process\nmore transparent. Extensive experiments and ablation studies on VLN-CE dataset\nconfirm the effectiveness of the proposed approach and outline fruitful\ndirections for future work.",
        "authors": [
            "Hanqing Wang",
            "Wei Liang",
            "Luc Van Gool",
            "Wenguan Wang"
        ]
    },
    {
        "title": "Agglomerative Transformer for Human-Object Interaction Detection",
        "url": "http://arxiv.org/abs/2308.08370",
        "abstract": "We propose an agglomerative Transformer (AGER) that enables Transformer-based\nhuman-object interaction (HOI) detectors to flexibly exploit extra\ninstance-level cues in a single-stage and end-to-end manner for the first time.\nAGER acquires instance tokens by dynamically clustering patch tokens and\naligning cluster centers to instances with textual guidance, thus enjoying two\nbenefits: 1) Integrality: each instance token is encouraged to contain all\ndiscriminative feature regions of an instance, which demonstrates a significant\nimprovement in the extraction of different instance-level cues and subsequently\nleads to a new state-of-the-art performance of HOI detection with 36.75 mAP on\nHICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER to\ngenerate instance tokens jointly with the feature learning of the Transformer\nencoder, eliminating the need of an additional object detector or instance\ndecoder in prior methods, thus allowing the extraction of desirable extra cues\nfor HOI detection in a single-stage and end-to-end pipeline. Concretely, AGER\nreduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanilla\nDETR-like pipeline without extra cue extraction.",
        "authors": [
            "Danyang Tu",
            "Wei Sun",
            "Guangtao Zhai",
            "Wei Shen"
        ]
    },
    {
        "title": "P1AC: Revisiting Absolute Pose From a Single Affine Correspondence",
        "url": "http://arxiv.org/abs/2011.08790",
        "abstract": "Affine correspondences have traditionally been used to improve feature\nmatching over wide baselines. While recent work has successfully used affine\ncorrespondences to solve various relative camera pose estimation problems, less\nattention has been given to their use in absolute pose estimation. We introduce\nthe first general solution to the problem of estimating the pose of a\ncalibrated camera given a single observation of an oriented point and an affine\ncorrespondence. The advantage of our approach (P1AC) is that it requires only a\nsingle correspondence, in comparison to the traditional point-based approach\n(P3P), significantly reducing the combinatorics in robust estimation. P1AC\nprovides a general solution that removes restrictive assumptions made in prior\nwork and is applicable to large-scale image-based localization. We propose a\nminimal solution to the P1AC problem and evaluate our novel solver on synthetic\ndata, showing its numerical stability and performance under various types of\nnoise. On standard image-based localization benchmarks we show that P1AC\nachieves more accurate results than the widely used P3P algorithm. Code for our\nmethod is available at https://github.com/jonathanventura/P1AC/ .",
        "authors": [
            "Jonathan Ventura",
            "Zuzana Kukelova",
            "Torsten Sattler",
            "D\u00e1niel Bar\u00e1th"
        ]
    },
    {
        "title": "Unsupervised Manifold Linearizing and Clustering",
        "url": "http://arxiv.org/abs/2301.01805",
        "abstract": "We consider the problem of simultaneously clustering and learning a linear\nrepresentation of data lying close to a union of low-dimensional manifolds, a\nfundamental task in machine learning and computer vision. When the manifolds\nare assumed to be linear subspaces, this reduces to the classical problem of\nsubspace clustering, which has been studied extensively over the past two\ndecades. Unfortunately, many real-world datasets such as natural images can not\nbe well approximated by linear subspaces. On the other hand, numerous works\nhave attempted to learn an appropriate transformation of the data, such that\ndata is mapped from a union of general non-linear manifolds to a union of\nlinear subspaces (with points from the same manifold being mapped to the same\nsubspace). However, many existing works have limitations such as assuming\nknowledge of the membership of samples to clusters, requiring high sampling\ndensity, or being shown theoretically to learn trivial representations. In this\npaper, we propose to optimize the Maximal Coding Rate Reduction metric with\nrespect to both the data representation and a novel doubly stochastic cluster\nmembership, inspired by state-of-the-art subspace clustering results. We give a\nparameterization of such a representation and membership, allowing efficient\nmini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100,\nand TinyImageNet-200 datasets show that the proposed method is much more\naccurate and scalable than state-of-the-art deep clustering methods, and\nfurther learns a latent linear representation of the data.",
        "authors": [
            "Tianjiao Ding",
            "Shengbang Tong",
            "Kwan Ho Ryan Chan",
            "Xili Dai",
            "Yi Ma",
            "Benjamin D. Haeffele"
        ]
    },
    {
        "title": "MMVP: Motion-Matrix-Based Video Prediction",
        "url": "http://arxiv.org/abs/2308.16154",
        "abstract": "A central challenge of video prediction lies where the system has to reason\nthe objects' future motions from image frames while simultaneously maintaining\nthe consistency of their appearances across frames. This work introduces an\nend-to-end trainable two-stream video prediction framework, Motion-Matrix-based\nVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods that\nusually handle motion prediction and appearance maintenance within the same set\nof modules, MMVP decouples motion and appearance information by constructing\nappearance-agnostic motion matrices. The motion matrices represent the temporal\nsimilarity of each and every pair of feature patches in the input frames, and\nare the sole input of the motion prediction module in MMVP. This design\nimproves video prediction in both accuracy and efficiency, and reduces the\nmodel size. Results of extensive experiments demonstrate that MMVP outperforms\nstate-of-the-art systems on public data sets by non-negligible large margins\n(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the\nsize or smaller).",
        "authors": [
            "Yiqi Zhong",
            "Luming Liang",
            "Ilya Zharkov",
            "Ulrich Neumann"
        ]
    },
    {
        "title": "Human Preference Score: Better Aligning Text-to-Image Models with Human Preference",
        "url": "http://arxiv.org/abs/2303.14420",
        "abstract": "Recent years have witnessed a rapid growth of deep generative models, with\ntext-to-image models gaining significant attention from the public. However,\nexisting models often generate images that do not align well with human\npreferences, such as awkward combinations of limbs and facial expressions. To\naddress this issue, we collect a dataset of human choices on generated images\nfrom the Stable Foundation Discord channel. Our experiments demonstrate that\ncurrent evaluation metrics for generative models do not correlate well with\nhuman choices. Thus, we train a human preference classifier with the collected\ndataset and derive a Human Preference Score (HPS) based on the classifier.\nUsing HPS, we propose a simple yet effective method to adapt Stable Diffusion\nto better align with human preferences. Our experiments show that HPS\noutperforms CLIP in predicting human choices and has good generalization\ncapability toward images generated from other models. By tuning Stable\nDiffusion with the guidance of HPS, the adapted model is able to generate\nimages that are more preferred by human users. The project page is available\nhere: https://tgxs002.github.io/align_sd_web/ .",
        "authors": [
            "Xiaoshi Wu",
            "Keqiang Sun",
            "Feng Zhu",
            "Rui Zhao",
            "Hongsheng Li"
        ]
    },
    {
        "title": "AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose",
        "url": "http://arxiv.org/abs/2309.08942",
        "abstract": "How human interact with objects depends on the functional roles of the target\nobjects, which introduces the problem of affordance-aware hand-object\ninteraction. It requires a large number of human demonstrations for the\nlearning and understanding of plausible and appropriate hand-object\ninteractions. In this work, we present AffordPose, a large-scale dataset of\nhand-object interactions with affordance-driven hand pose. We first annotate\nthe specific part-level affordance labels for each object, e.g. twist, pull,\nhandle-grasp, etc, instead of the general intents such as use or handover, to\nindicate the purpose and guide the localization of the hand-object\ninteractions. The fine-grained hand-object interactions reveal the influence of\nhand-centered affordances on the detailed arrangement of the hand poses, yet\nalso exhibit a certain degree of diversity. We collect a total of 26.7K\nhand-object interactions, each including the 3D object shape, the part-level\naffordance label, and the manually adjusted hand poses. The comprehensive data\nanalysis shows the common characteristics and diversity of hand-object\ninteractions per affordance via the parameter statistics and contacting\ncomputation. We also conduct experiments on the tasks of hand-object affordance\nunderstanding and affordance-oriented hand-object interaction generation, to\nvalidate the effectiveness of our dataset in learning the fine-grained\nhand-object interactions. Project page:\nhttps://github.com/GentlesJan/AffordPose.",
        "authors": [
            "Juntao Jian",
            "Xiuping Liu",
            "Manyi Li",
            "Ruizhen Hu",
            "Jian Liu"
        ]
    },
    {
        "title": "NDDepth: Normal-Distance Assisted Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2309.10592",
        "abstract": "Monocular depth estimation has drawn widespread attention from the vision\ncommunity due to its broad applications. In this paper, we propose a novel\nphysics (geometry)-driven deep learning framework for monocular depth\nestimation by assuming that 3D scenes are constituted by piece-wise planes.\nParticularly, we introduce a new normal-distance head that outputs pixel-level\nsurface normal and plane-to-origin distance for deriving depth at each\nposition. Meanwhile, the normal and distance are regularized by a developed\nplane-aware consistency constraint. We further integrate an additional depth\nhead to improve the robustness of the proposed framework. To fully exploit the\nstrengths of these two heads, we develop an effective contrastive iterative\nrefinement module that refines depth in a complementary manner according to the\ndepth uncertainty. Extensive experiments indicate that the proposed method\nexceeds previous state-of-the-art competitors on the NYU-Depth-v2, KITTI and\nSUN RGB-D datasets. Notably, it ranks 1st among all submissions on the KITTI\ndepth prediction online benchmark at the submission time.",
        "authors": [
            "Shuwei Shao",
            "Zhongcai Pei",
            "Weihai Chen",
            "Xingming Wu",
            "Zhengguo Li"
        ]
    },
    {
        "title": "DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models",
        "url": "http://arxiv.org/abs/2303.11681",
        "abstract": "Collecting and annotating images with pixel-wise labels is time-consuming and\nlaborious. In contrast, synthetic data can be freely available using a\ngenerative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that\nit is possible to automatically obtain accurate semantic masks of synthetic\nimages generated by the Off-the-shelf Stable Diffusion model, which uses only\ntext-image pairs during training. Our approach, called DiffuMask, exploits the\npotential of the cross-attention map between text and image, which is natural\nand seamless to extend the text-driven image synthesis to semantic mask\ngeneration. DiffuMask uses text-guided cross-attention information to localize\nclass/word-specific regions, which are combined with practical techniques to\ncreate a novel high-resolution and class-discriminative pixel-wise mask. The\nmethods help to reduce data collection and annotation costs obviously.\nExperiments demonstrate that the existing segmentation methods trained on\nsynthetic data of DiffuMask can achieve a competitive performance over the\ncounterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),\nDiffuMask presents promising performance, close to the stateof-the-art result\nof real data (within 3% mIoU gap). Moreover, in the open-vocabulary\nsegmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on\nUnseen class of VOC 2012. The project website can be found at\nhttps://weijiawu.github.io/DiffusionMask/.",
        "authors": [
            "Weijia Wu",
            "Yuzhong Zhao",
            "Mike Zheng Shou",
            "Hong Zhou",
            "Chunhua Shen"
        ]
    },
    {
        "title": "NSF: Neural Surface Fields for Human Modeling from Monocular Depth",
        "url": "http://arxiv.org/abs/2308.14847",
        "abstract": "Obtaining personalized 3D animatable avatars from a monocular camera has\nseveral real world applications in gaming, virtual try-on, animation, and\nVR/XR, etc. However, it is very challenging to model dynamic and fine-grained\nclothing deformations from such sparse data. Existing methods for modeling 3D\nhumans from depth data have limitations in terms of computational efficiency,\nmesh coherency, and flexibility in resolution and topology. For instance,\nreconstructing shapes using implicit functions and extracting explicit meshes\nper frame is computationally expensive and cannot ensure coherent meshes across\nframes. Moreover, predicting per-vertex deformations on a pre-designed human\ntemplate with a discrete surface lacks flexibility in resolution and topology.\nTo overcome these limitations, we propose a novel method Neural Surface Fields\nfor modeling 3D clothed humans from monocular depth. NSF defines a neural field\nsolely on the base surface which models a continuous and flexible displacement\nfield. NSF can be adapted to the base surface with different resolution and\ntopology without retraining at inference time. Compared to existing approaches,\nour method eliminates the expensive per-frame surface extraction while\nmaintaining mesh coherency, and is capable of reconstructing meshes with\narbitrary resolution without retraining. To foster research in this direction,\nwe release our code in project page at: https://yuxuan-xue.com/nsf.",
        "authors": [
            "Yuxuan Xue",
            "Bharat Lal Bhatnagar",
            "Riccardo Marin",
            "Nikolaos Sarafianos",
            "Yuanlu Xu",
            "Gerard Pons-Moll",
            "Tony Tung"
        ]
    },
    {
        "title": "Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers",
        "url": "http://arxiv.org/abs/2308.14152",
        "abstract": "Generating 3D images of complex objects conditionally from a few 2D views is\na difficult synthesis problem, compounded by issues such as domain gap and\ngeometric misalignment. For instance, a unified framework such as Generative\nAdversarial Networks cannot achieve this unless they explicitly define both a\ndomain-invariant and geometric-invariant joint latent distribution, whereas\nNeural Radiance Fields are generally unable to handle both issues as they\noptimize at the pixel level. By contrast, we propose a simple and novel 2D to\n3D synthesis approach based on conditional diffusion with vector-quantized\ncodes. Operating in an information-rich code space enables high-resolution 3D\nsynthesis via full-coverage attention across the views. Specifically, we\ngenerate the 3D codes (e.g. for CT images) conditional on previously generated\n3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative\nand quantitative results demonstrate state-of-the-art performance over\nspecialized methods across varied evaluation criteria, including fidelity\nmetrics such as density, coverage, and distortion metrics for two complex\nvolumetric imagery datasets from in real-world scenarios.",
        "authors": [
            "Abril Corona-Figueroa",
            "Sam Bond-Taylor",
            "Neelanjan Bhowmik",
            "Yona Falinie A. Gaus",
            "Toby P. Breckon",
            "Hubert P. H. Shum",
            "Chris G. Willcocks"
        ]
    },
    {
        "title": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation",
        "url": "http://arxiv.org/abs/2212.10229",
        "abstract": "Domain adaptation of GANs is a problem of fine-tuning GAN models pretrained\non a large dataset (e.g. StyleGAN) to a specific domain with few samples (e.g.\npainting faces, sketches, etc.). While there are many methods that tackle this\nproblem in different ways, there are still many important questions that remain\nunanswered. In this paper, we provide a systematic and in-depth analysis of the\ndomain adaptation problem of GANs, focusing on the StyleGAN model. We perform a\ndetailed exploration of the most important parts of StyleGAN that are\nresponsible for adapting the generator to a new domain depending on the\nsimilarity between the source and target domains. As a result of this study, we\npropose new efficient and lightweight parameterizations of StyleGAN for domain\nadaptation. Particularly, we show that there exist directions in StyleSpace\n(StyleDomain directions) that are sufficient for adapting to similar domains.\nFor dissimilar domains, we propose Affine+ and AffineLight+ parameterizations\nthat allows us to outperform existing baselines in few-shot adaptation while\nhaving significantly less training parameters. Finally, we examine StyleDomain\ndirections and discover their many surprising properties that we apply for\ndomain mixing and cross-domain image morphing. Source code can be found at\nhttps://github.com/AIRI-Institute/StyleDomain.",
        "authors": [
            "Aibek Alanov",
            "Vadim Titov",
            "Maksim Nakhodnov",
            "Dmitry Vetrov"
        ]
    },
    {
        "title": "RankMixup: Ranking-Based Mixup Training for Network Calibration",
        "url": "http://arxiv.org/abs/2308.11990",
        "abstract": "Network calibration aims to accurately estimate the level of confidences,\nwhich is particularly important for employing deep neural networks in\nreal-world systems. Recent approaches leverage mixup to calibrate the network's\npredictions during training. However, they do not consider the problem that\nmixtures of labels in mixup may not accurately represent the actual\ndistribution of augmented samples. In this paper, we present RankMixup, a novel\nmixup-based framework alleviating the problem of the mixture of labels for\nnetwork calibration. To this end, we propose to use an ordinal ranking\nrelationship between raw and mixup-augmented samples as an alternative\nsupervisory signal to the label mixtures for network calibration. We\nhypothesize that the network should estimate a higher level of confidence for\nthe raw samples than the augmented ones (Fig.1). To implement this idea, we\nintroduce a mixup-based ranking loss (MRL) that encourages lower confidences\nfor augmented samples compared to raw ones, maintaining the ranking\nrelationship. We also propose to leverage the ranking relationship among\nmultiple mixup-augmented samples to further improve the calibration capability.\nAugmented samples with larger mixing coefficients are expected to have higher\nconfidences and vice versa (Fig.1). That is, the order of confidences should be\naligned with that of mixing coefficients. To this end, we introduce a novel\nloss, M-NDCG, in order to reduce the number of misaligned pairs of the\ncoefficients and confidences. Extensive experimental results on standard\nbenchmarks for network calibration demonstrate the effectiveness of RankMixup.",
        "authors": [
            "Jongyoun Noh",
            "Hyekang Park",
            "Junghyup Lee",
            "Bumsub Ham"
        ]
    },
    {
        "title": "Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction",
        "url": "http://arxiv.org/abs/2308.00799",
        "abstract": "While 3D body reconstruction methods have made remarkable progress recently,\nit remains difficult to acquire the sufficiently accurate and numerous 3D\nsupervisions required for training. In this paper, we propose \\textbf{KNOWN}, a\nframework that effectively utilizes body \\textbf{KNOW}ledge and\nu\\textbf{N}certainty modeling to compensate for insufficient 3D supervisions.\nKNOWN exploits a comprehensive set of generic body constraints derived from\nwell-established body knowledge. These generic constraints precisely and\nexplicitly characterize the reconstruction plausibility and enable 3D\nreconstruction models to be trained without any 3D data. Moreover, existing\nmethods typically use images from multiple datasets during training, which can\nresult in data noise (\\textit{e.g.}, inconsistent joint annotation) and data\nimbalance (\\textit{e.g.}, minority images representing unusual poses or\ncaptured from challenging camera views). KNOWN solves these problems through a\nnovel probabilistic framework that models both aleatoric and epistemic\nuncertainty. Aleatoric uncertainty is encoded in a robust Negative\nLog-Likelihood (NLL) training loss, while epistemic uncertainty is used to\nguide model refinement. Experiments demonstrate that KNOWN's body\nreconstruction outperforms prior weakly-supervised approaches, particularly on\nthe challenging minority images.",
        "authors": [
            "Yufei Zhang",
            "Hanjing Wang",
            "Jeffrey O. Kephart",
            "Qiang Ji"
        ]
    },
    {
        "title": "Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning",
        "url": "http://arxiv.org/abs/2212.08663",
        "abstract": "Self-supervised representation learning follows a paradigm of withholding\nsome part of the data and tasking the network to predict it from the remaining\npart. Among many techniques, data augmentation lies at the core for creating\nthe information gap. Towards this end, masking has emerged as a generic and\npowerful tool where content is withheld along the sequential dimension, e.g.,\nspatial in images, temporal in audio, and syntactic in language. In this paper,\nwe explore the orthogonal channel dimension for generic data augmentation by\nexploiting precision redundancy. The data for each channel is quantized through\na non-uniform quantizer, with the quantized value sampled randomly within\nrandomly sampled quantization bins. From another perspective, quantization is\nanalogous to channel-wise masking, as it removes the information within each\nbin, but preserves the information across bins. Our approach significantly\nsurpasses existing generic data augmentation methods, while showing on par\nperformance against modality-specific augmentations. We comprehensively\nevaluate our approach on vision, audio, 3D point clouds, as well as the DABS\nbenchmark which is comprised of various data modalities. The code is available\nat https: //github.com/microsoft/random_quantize.",
        "authors": [
            "Huimin Wu",
            "Chenyang Lei",
            "Xiao Sun",
            "Peng-Shuai Wang",
            "Qifeng Chen",
            "Kwang-Ting Cheng",
            "Stephen Lin",
            "Zhirong Wu"
        ]
    },
    {
        "title": "Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis",
        "url": "http://arxiv.org/abs/2308.08157",
        "abstract": "Existing text-to-image generation approaches have set high standards for\nphotorealism and text-image correspondence, largely benefiting from web-scale\ntext-image datasets, which can include up to 5~billion pairs. However,\ntext-to-image generation models trained on domain-specific datasets, such as\nurban scenes, medical images, and faces, still suffer from low text-image\ncorrespondence due to the lack of text-image pairs. Additionally, collecting\nbillions of text-image pairs for a specific domain can be time-consuming and\ncostly. Thus, ensuring high text-image correspondence without relying on\nweb-scale text-image datasets remains a challenging task. In this paper, we\npresent a novel approach for enhancing text-image correspondence by leveraging\navailable semantic layouts. Specifically, we propose a Gaussian-categorical\ndiffusion process that simultaneously generates both images and corresponding\nlayout pairs. Our experiments reveal that we can guide text-to-image generation\nmodels to be aware of the semantics of different image regions, by training the\nmodel to generate semantic labels for each pixel. We demonstrate that our\napproach achieves higher text-image correspondence compared to existing\ntext-to-image generation approaches in the Multi-Modal CelebA-HQ and the\nCityscapes dataset, where text-image pairs are scarce. Codes are available in\nthis https://pmh9960.github.io/research/GCDP",
        "authors": [
            "Minho Park",
            "Jooyeol Yun",
            "Seunghwan Choi",
            "Jaegul Choo"
        ]
    },
    {
        "title": "Erasing Concepts from Diffusion Models",
        "url": "http://arxiv.org/abs/2303.07345",
        "abstract": "Motivated by recent advancements in text-to-image diffusion, we study erasure\nof specific concepts from the model's weights. While Stable Diffusion has shown\npromise in producing explicit or realistic artwork, it has raised concerns\nregarding its potential for misuse. We propose a fine-tuning method that can\nerase a visual concept from a pre-trained diffusion model, given only the name\nof the style and using negative guidance as a teacher. We benchmark our method\nagainst previous approaches that remove sexually explicit content and\ndemonstrate its effectiveness, performing on par with Safe Latent Diffusion and\ncensored training. To evaluate artistic style removal, we conduct experiments\nerasing five modern artists from the network and conduct a user study to assess\nthe human perception of the removed styles. Unlike previous methods, our\napproach can remove concepts from a diffusion model permanently rather than\nmodifying the output at the inference time, so it cannot be circumvented even\nif a user has access to model weights. Our code, data, and results are\navailable at https://erasing.baulab.info/",
        "authors": [
            "Rohit Gandikota",
            "Joanna Materzynska",
            "Jaden Fiotto-Kaufman",
            "David Bau"
        ]
    },
    {
        "title": "ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion",
        "url": "http://arxiv.org/abs/2308.07009",
        "abstract": "Adversarial camouflage has garnered attention for its ability to attack\nobject detectors from any viewpoint by covering the entire object's surface.\nHowever, universality and robustness in existing methods often fall short as\nthe transferability aspect is often overlooked, thus restricting their\napplication only to a specific target with limited performance. To address\nthese challenges, we present Adversarial Camouflage for Transferable and\nIntensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage\nattack framework designed to generate universal and robust adversarial\ncamouflage capable of concealing any 3D vehicle from detectors. Our framework\nincorporates innovative techniques to enhance universality and robustness,\nincluding a refined texture rendering that enables common texture application\nto different vehicles without being constrained to a specific texture map, a\nnovel stealth loss that renders the vehicle undetectable, and a smooth and\ncamouflage loss to enhance the naturalness of the adversarial camouflage. Our\nextensive experiments on 15 different models show that ACTIVE consistently\noutperforms existing works on various public detectors, including the latest\nYOLOv7. Notably, our universality evaluations reveal promising transferability\nto other vehicle classes, tasks (segmentation models), and the real world, not\njust other vehicles.",
        "authors": [
            "Naufal Suryanto",
            "Yongsu Kim",
            "Harashta Tatimma Larasati",
            "Hyoeun Kang",
            "Thi-Thu-Huong Le",
            "Yoonyoung Hong",
            "Hunmin Yang",
            "Se-Yoon Oh",
            "Howon Kim"
        ]
    },
    {
        "title": "Learning Adaptive Neighborhoods for Graph Neural Networks",
        "url": "http://arxiv.org/abs/2307.09065",
        "abstract": "Graph convolutional networks (GCNs) enable end-to-end learning on graph\nstructured data. However, many works assume a given graph structure. When the\ninput graph is noisy or unavailable, one approach is to construct or learn a\nlatent graph structure. These methods typically fix the choice of node degree\nfor the entire graph, which is suboptimal. Instead, we propose a novel\nend-to-end differentiable graph generator which builds graph topologies where\neach node selects both its neighborhood and its size. Our module can be readily\nintegrated into existing pipelines involving graph convolution operations,\nreplacing the predetermined or existing adjacency matrix with one that is\nlearned, and optimized, as part of the general objective. As such it is\napplicable to any GCN. We integrate our module into trajectory prediction,\npoint cloud classification and node classification pipelines resulting in\nimproved accuracy over other structure-learning methods across a wide range of\ndatasets and GCN backbones.",
        "authors": [
            "Avishkar Saha",
            "Oscar Mendez",
            "Chris Russell",
            "Richard Bowden"
        ]
    },
    {
        "title": "Equivariant Similarity for Vision-Language Foundation Models",
        "url": "http://arxiv.org/abs/2303.14465",
        "abstract": "This study explores the concept of equivariance in vision-language foundation\nmodels (VLMs), focusing specifically on the multimodal similarity function that\nis not only the major training objective but also the core delivery to support\ndownstream tasks. Unlike the existing image-text similarity objective which\nonly categorizes matched pairs as similar and unmatched pairs as dissimilar,\nequivariance also requires similarity to vary faithfully according to the\nsemantic changes. This allows VLMs to generalize better to nuanced and unseen\nmultimodal compositions. However, modeling equivariance is challenging as the\nground truth of semantic change is difficult to collect. For example, given an\nimage-text pair about a dog, it is unclear to what extent the similarity\nchanges when the pixel is changed from dog to cat? To this end, we propose\nEqSim, a regularization loss that can be efficiently calculated from any two\nmatched training pairs and easily pluggable into existing image-text retrieval\nfine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we\npresent a new challenging benchmark EqBen. Compared to the existing evaluation\nsets, EqBen is the first to focus on \"visual-minimal change\". Extensive\nexperiments show the lack of equivariance in current VLMs and validate the\neffectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen.",
        "authors": [
            "Tan Wang",
            "Kevin Lin",
            "Linjie Li",
            "Chung-Ching Lin",
            "Zhengyuan Yang",
            "Hanwang Zhang",
            "Zicheng Liu",
            "Lijuan Wang"
        ]
    },
    {
        "title": "ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2308.13229",
        "abstract": "Multi-Camera Multi-Object Tracking (MC-MOT) utilizes information from\nmultiple views to better handle problems with occlusion and crowded scenes.\nRecently, the use of graph-based approaches to solve tracking problems has\nbecome very popular. However, many current graph-based methods do not\neffectively utilize information regarding spatial and temporal consistency.\nInstead, they rely on single-camera trackers as input, which are prone to\nfragmentation and ID switch errors. In this paper, we propose a novel\nreconfigurable graph model that first associates all detected objects across\ncameras spatially before reconfiguring it into a temporal graph for Temporal\nAssociation. This two-stage association approach enables us to extract robust\nspatial and temporal-aware features and address the problem with fragmented\ntracklets. Furthermore, our model is designed for online tracking, making it\nsuitable for real-world applications. Experimental results show that the\nproposed graph model is able to extract more discriminating features for object\ntracking, and our model achieves state-of-the-art performance on several public\ndatasets.",
        "authors": [
            "Cheng-Che Cheng",
            "Min-Xuan Qiu",
            "Chen-Kuo Chiang",
            "Shang-Hong Lai"
        ]
    },
    {
        "title": "Too Large; Data Reduction for Vision-Language Pre-Training",
        "url": "http://arxiv.org/abs/2305.20087",
        "abstract": "This paper examines the problems of severe image-text misalignment and high\nredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)\ndatasets. To address these issues, we propose an efficient and straightforward\nVision-Language learning algorithm called TL;DR, which aims to compress the\nexisting large VLP data into a small, high-quality set. Our approach consists\nof two major steps. First, a codebook-based encoder-decoder captioner is\ndeveloped to select representative samples. Second, a new caption is generated\nto complement the original captions for selected samples, mitigating the\ntext-image misalignment problem while maintaining uniqueness. As the result,\nTL;DR enables us to reduce the large dataset into a small set of high-quality\ndata, which can serve as an alternative pre-training dataset. This algorithm\nsignificantly speeds up the time-consuming pretraining process. Specifically,\nTL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce\nwell-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M\nfrom 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP\nmodels over seven downstream tasks show that VLP model trained on the\ncompressed dataset provided by TL;DR can perform similar or even better results\ncompared with training on the full-scale dataset. The code will be made\navailable at \\url{https://github.com/showlab/datacentric.vlp}.",
        "authors": [
            "Alex Jinpeng Wang",
            "Kevin Qinghong Lin",
            "David Junhao Zhang",
            "Stan Weixian Lei",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning",
        "url": "http://arxiv.org/abs/2307.14786",
        "abstract": "Depth-aware panoptic segmentation is an emerging topic in computer vision\nwhich combines semantic and geometric understanding for more robust scene\ninterpretation. Recent works pursue unified frameworks to tackle this challenge\nbut mostly still treat it as two individual learning tasks, which limits their\npotential for exploring cross-domain information. We propose a deeply unified\nframework for depth-aware panoptic segmentation, which performs joint\nsegmentation and depth estimation both in a per-segment manner with identical\nobject queries. To narrow the gap between the two tasks, we further design a\ngeometric query enhancement method, which is able to integrate scene geometry\ninto object queries using latent representations. In addition, we propose a\nbi-directional guidance learning approach to facilitate cross-task feature\nlearning by taking advantage of their mutual relations. Our method sets the new\nstate of the art for depth-aware panoptic segmentation on both Cityscapes-DVPS\nand SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown\nto deliver performance improvement even under incomplete supervision labels.",
        "authors": [
            "Junwen He",
            "Yifan Wang",
            "Lijun Wang",
            "Huchuan Lu",
            "Jun-Yan He",
            "Jin-Peng Lan",
            "Bin Luo",
            "Yifeng Geng",
            "Xuansong Xie"
        ]
    },
    {
        "title": "Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation",
        "url": "http://arxiv.org/abs/2307.14709",
        "abstract": "The success of automated medical image analysis depends on large-scale and\nexpert-annotated training sets. Unsupervised domain adaptation (UDA) has been\nraised as a promising approach to alleviate the burden of labeled data\ncollection. However, they generally operate under the closed-set adaptation\nsetting assuming an identical label set between the source and target domains,\nwhich is over-restrictive in clinical practice where new classes commonly exist\nacross datasets due to taxonomic inconsistency. While several methods have been\npresented to tackle both domain shifts and incoherent label sets, none of them\ntake into account the common characteristics of the two issues and consider the\nlearning dynamics along network training. In this work, we propose optimization\ntrajectory distillation, a unified approach to address the two technical\nchallenges from a new perspective. It exploits the low-rank nature of gradient\nspace and devises a dual-stream distillation algorithm to regularize the\nlearning dynamics of insufficiently annotated domain and classes with the\nexternal guidance obtained from reliable sources. Our approach resolves the\nissue of inadequate navigation along network optimization, which is the major\nobstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluate\nthe proposed method extensively on several tasks towards various endpoints with\nclinical and open-world significance. The results demonstrate its effectiveness\nand improvements over previous methods.",
        "authors": [
            "Jianan Fan",
            "Dongnan Liu",
            "Hang Chang",
            "Heng Huang",
            "Mei Chen",
            "Weidong Cai"
        ]
    },
    {
        "title": "DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion",
        "url": "http://arxiv.org/abs/2303.14863",
        "abstract": "We propose a new formulation of temporal action detection (TAD) with\ndenoising diffusion, DiffTAD in short. Taking as input random temporal\nproposals, it can yield action proposals accurately given an untrimmed long\nvideo. This presents a generative modeling perspective, against previous\ndiscriminative learning manners. This capability is achieved by first diffusing\nthe ground-truth proposals to random ones (i.e., the forward/noising process)\nand then learning to reverse the noising process (i.e., the backward/denoising\nprocess). Concretely, we establish the denoising process in the Transformer\ndecoder (e.g., DETR) by introducing a temporal location query design with\nfaster convergence in training. We further propose a cross-step selective\nconditioning algorithm for inference acceleration. Extensive evaluations on\nActivityNet and THUMOS show that our DiffTAD achieves top performance compared\nto previous art alternatives. The code will be made available at\nhttps://github.com/sauradip/DiffusionTAD.",
        "authors": [
            "Sauradip Nag",
            "Xiatian Zhu",
            "Jiankang Deng",
            "Yi-Zhe Song",
            "Tao Xiang"
        ]
    },
    {
        "title": "Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation",
        "url": "http://arxiv.org/abs/2304.13681",
        "abstract": "Multi-view image generation attracts particular attention these days due to\nits promising 3D-related applications, e.g., image viewpoint editing. Most\nexisting methods follow a paradigm where a 3D representation is first\nsynthesized, and then rendered into 2D images to ensure photo-consistency\nacross viewpoints. However, such explicit bias for photo-consistency sacrifices\nphoto-realism, causing geometry artifacts and loss of fine-scale details when\nthese methods are applied to edit real images. To address this issue, we\npropose ray conditioning, a geometry-free alternative that relaxes the\nphoto-consistency constraint. Our method generates multi-view images by\nconditioning a 2D GAN on a light field prior. With explicit viewpoint control,\nstate-of-the-art photo-realism and identity consistency, our method is\nparticularly suited for the viewpoint editing task.",
        "authors": [
            "Eric Ming Chen",
            "Sidhanth Holalkere",
            "Ruyu Yan",
            "Kai Zhang",
            "Abe Davis"
        ]
    },
    {
        "title": "SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap",
        "url": "http://arxiv.org/abs/2309.12382",
        "abstract": "Inspired by the great success of language model (LM)-based pre-training,\nrecent studies in visual document understanding have explored LM-based\npre-training methods for modeling text within document images. Among them,\npre-training that reads all text from an image has shown promise, but often\nexhibits instability and even fails when applied to broader domains, such as\nthose involving both visual documents and scene text images. This is a\nsubstantial limitation for real-world scenarios, where the processing of text\nimage inputs in diverse domains is essential. In this paper, we investigate\neffective pre-training tasks in the broader domains and also propose a novel\npre-training method called SCOB that leverages character-wise supervised\ncontrastive learning with online text rendering to effectively pre-train\ndocument and scene text domains by bridging the domain gap. Moreover, SCOB\nenables weakly supervised learning, significantly reducing annotation costs.\nExtensive benchmarks demonstrate that SCOB generally improves vanilla\npre-training methods and achieves comparable performance to state-of-the-art\nmethods. Our findings suggest that SCOB can be served generally and effectively\nfor read-type pre-training methods. The code will be available at\nhttps://github.com/naver-ai/scob.",
        "authors": [
            "Daehee Kim",
            "Yoonsik Kim",
            "DongHyun Kim",
            "Yumin Lim",
            "Geewook Kim",
            "Taeho Kil"
        ]
    },
    {
        "title": "Point-Query Quadtree for Crowd Counting, Localization, and More",
        "url": "http://arxiv.org/abs/2308.13814",
        "abstract": "We show that crowd counting can be viewed as a decomposable point querying\nprocess. This formulation enables arbitrary points as input and jointly reasons\nwhether the points are crowd and where they locate. The querying processing,\nhowever, raises an underlying problem on the number of necessary querying\npoints. Too few imply underestimation; too many increase computational\noverhead. To address this dilemma, we introduce a decomposable structure, i.e.,\nthe point-query quadtree, and propose a new counting model, termed Point quEry\nTransformer (PET). PET implements decomposable point querying via\ndata-dependent quadtree splitting, where each querying point could split into\nfour new points when necessary, thus enabling dynamic processing of sparse and\ndense regions. Such a querying process yields an intuitive, universal modeling\nof crowd as both the input and output are interpretable and steerable. We\ndemonstrate the applications of PET on a number of crowd-related tasks,\nincluding fully-supervised crowd counting and localization, partial annotation\nlearning, and point annotation refinement, and also report state-of-the-art\nperformance. For the first time, we show that a single counting model can\naddress multiple crowd-related tasks across different learning paradigms. Code\nis available at https://github.com/cxliu0/PET.",
        "authors": [
            "Chengxin Liu",
            "Hao Lu",
            "Zhiguo Cao",
            "Tongliang Liu"
        ]
    },
    {
        "title": "Domain Generalization of 3D Semantic Segmentation in Autonomous Driving",
        "url": "http://arxiv.org/abs/2212.04245",
        "abstract": "Using deep learning, 3D autonomous driving semantic segmentation has become a\nwell-studied subject, with methods that can reach very high performance.\nNonetheless, because of the limited size of the training datasets, these models\ncannot see every type of object and scene found in real-world applications. The\nability to be reliable in these various unknown environments is called\n\\textup{domain generalization}.\n  Despite its importance, domain generalization is relatively unexplored in the\ncase of 3D autonomous driving semantic segmentation. To fill this gap, this\npaper presents the first benchmark for this application by testing\nstate-of-the-art methods and discussing the difficulty of tackling Laser\nImaging Detection and Ranging (LiDAR) domain shifts.\n  We also propose the first method designed to address this domain\ngeneralization, which we call 3DLabelProp. This method relies on leveraging the\ngeometry and sequentiality of the LiDAR data to enhance its generalization\nperformances by working on partially accumulated point clouds. It reaches a\nmean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on\nPandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it\nthe state-of-the-art method for generalization (+5% and +33% better,\nrespectively, than the second best method).\n  The code for this method is available on GitHub:\nhttps://github.com/JulesSanchez/3DLabelProp.",
        "authors": [
            "Jules Sanchez",
            "Jean-Emmanuel Deschaud",
            "Francois Goulette"
        ]
    },
    {
        "title": "HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning",
        "url": "http://arxiv.org/abs/2302.00988",
        "abstract": "Recent advancements in 3D hand pose estimation have shown promising results,\nbut its effectiveness has primarily relied on the availability of large-scale\nannotated datasets, the creation of which is a laborious and costly process. To\nalleviate the label-hungry limitation, we propose a self-supervised learning\nframework, HaMuCo, that learns a single-view hand pose estimator from\nmulti-view pseudo 2D labels. However, one of the main challenges of\nself-supervised learning is the presence of noisy labels and the ``groupthink''\neffect from multiple views. To overcome these issues, we introduce a cross-view\ninteraction network that distills the single-view estimator by utilizing the\ncross-view correlated features and enforcing multi-view consistency to achieve\ncollaborative learning. Both the single-view estimator and the cross-view\ninteraction network are trained jointly in an end-to-end manner. Extensive\nexperiments show that our method can achieve state-of-the-art performance on\nmulti-view self-supervised hand pose estimation. Furthermore, the proposed\ncross-view interaction network can also be applied to hand pose estimation from\nmulti-view input and outperforms previous methods under the same settings.",
        "authors": [
            "Xiaozheng Zheng",
            "Chao Wen",
            "Zhou Xue",
            "Pengfei Ren",
            "Jingyu Wang"
        ]
    },
    {
        "title": "Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation",
        "url": "http://arxiv.org/abs/2308.15367",
        "abstract": "Federated learning (FL) emerges as a decentralized learning framework which\ntrains models from multiple distributed clients without sharing their data to\npreserve privacy. Recently, large-scale pre-trained models (e.g., Vision\nTransformer) have shown a strong capability of deriving robust representations.\nHowever, the data heterogeneity among clients, the limited computation\nresources, and the communication bandwidth restrict the deployment of\nlarge-scale models in FL frameworks. To leverage robust representations from\nlarge-scale models while enabling efficient model personalization for\nheterogeneous clients, we propose a novel personalized FL framework of\nclient-specific Prompt Generation (pFedPG), which learns to deploy a\npersonalized prompt generator at the server for producing client-specific\nvisual prompts that efficiently adapts frozen backbones to local data\ndistributions. Our proposed framework jointly optimizes the stages of\npersonalized prompt adaptation locally and personalized prompt generation\nglobally. The former aims to train visual prompts that adapt foundation models\nto each client, while the latter observes local optimization directions to\ngenerate personalized prompts for all clients. Through extensive experiments on\nbenchmark datasets, we show that our pFedPG is favorable against\nstate-of-the-art personalized FL methods under various types of data\nheterogeneity, allowing computation and communication efficient model\npersonalization.",
        "authors": [
            "Fu-En Yang",
            "Chien-Yi Wang",
            "Yu-Chiang Frank Wang"
        ]
    },
    {
        "title": "Dual Aggregation Transformer for Image Super-Resolution",
        "url": "http://arxiv.org/abs/2308.03364",
        "abstract": "Transformer has recently gained considerable popularity in low-level vision\ntasks, including image super-resolution (SR). These networks utilize\nself-attention along different dimensions, spatial or channel, and achieve\nimpressive performance. This inspires us to combine the two dimensions in\nTransformer for a more powerful representation capability. Based on the above\nidea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT),\nfor image SR. Our DAT aggregates features across spatial and channel\ndimensions, in the inter-block and intra-block dual manner. Specifically, we\nalternately apply spatial and channel self-attention in consecutive Transformer\nblocks. The alternate strategy enables DAT to capture the global context and\nrealize inter-block feature aggregation. Furthermore, we propose the adaptive\ninteraction module (AIM) and the spatial-gate feed-forward network (SGFN) to\nachieve intra-block feature aggregation. AIM complements two self-attention\nmechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional\nnon-linear spatial information in the feed-forward network. Extensive\nexperiments show that our DAT surpasses current methods. Code and models are\nobtainable at https://github.com/zhengchen1999/DAT.",
        "authors": [
            "Zheng Chen",
            "Yulun Zhang",
            "Jinjin Gu",
            "Linghe Kong",
            "Xiaokang Yang",
            "Fisher Yu"
        ]
    },
    {
        "title": "Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2306.13754",
        "abstract": "Large-scale text-to-image diffusion models have significantly improved the\nstate of the art in generative image modelling and allow for an intuitive and\npowerful user interface to drive the image generation process. Expressing\nspatial constraints, e.g. to position specific objects in particular locations,\nis cumbersome using text; and current text-based image generation models are\nnot able to accurately follow such instructions. In this paper we consider\nimage generation from text associated with segments on the image canvas, which\ncombines an intuitive natural language interface with precise spatial control\nover the generated content. We propose ZestGuide, a zero-shot segmentation\nguidance approach that can be plugged into pre-trained text-to-image diffusion\nmodels, and does not require any additional training. It leverages implicit\nsegmentation maps that can be extracted from cross-attention layers, and uses\nthem to align the generation with input masks. Our experimental results combine\nhigh image quality with accurate alignment of generated content with input\nsegmentations, and improve over prior work both quantitatively and\nqualitatively, including methods that require training on images with\ncorresponding segmentations. Compared to Paint with Words, the previous\nstate-of-the art in image generation with zero-shot segmentation conditioning,\nwe improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores.",
        "authors": [
            "Guillaume Couairon",
            "Marl\u00e8ne Careil",
            "Matthieu Cord",
            "St\u00e9phane Lathuili\u00e8re",
            "Jakob Verbeek"
        ]
    },
    {
        "title": "Semantify: Simplifying the Control of 3D Morphable Models Using CLIP",
        "url": "http://arxiv.org/abs/2308.07415",
        "abstract": "We present Semantify: a self-supervised method that utilizes the semantic\npower of CLIP language-vision foundation model to simplify the control of 3D\nmorphable models. Given a parametric model, training data is created by\nrandomly sampling the model's parameters, creating various shapes and rendering\nthem. The similarity between the output images and a set of word descriptors is\ncalculated in CLIP's latent space. Our key idea is first to choose a small set\nof semantically meaningful and disentangled descriptors that characterize the\n3DMM, and then learn a non-linear mapping from scores across this set to the\nparametric coefficients of the given 3DMM. The non-linear mapping is defined by\ntraining a neural network without a human-in-the-loop. We present results on\nnumerous 3DMMs: body shape models, face shape and expression models, as well as\nanimal shapes. We demonstrate how our method defines a simple slider interface\nfor intuitive modeling, and show how the mapping can be used to instantly fit a\n3D parametric body shape to in-the-wild images.",
        "authors": [
            "Omer Gralnik",
            "Guy Gafni",
            "Ariel Shamir"
        ]
    },
    {
        "title": "From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal",
        "url": "http://arxiv.org/abs/2308.03867",
        "abstract": "Learning-based image deraining methods have made great progress. However, the\nlack of large-scale high-quality paired training samples is the main bottleneck\nto hamper the real image deraining (RID). To address this dilemma and advance\nRID, we construct a Large-scale High-quality Paired real rain benchmark\n(LHP-Rain), including 3000 video sequences with 1 million high-resolution\n(1920*1080) frame pairs. The advantages of the proposed dataset over the\nexisting ones are three-fold: rain with higher-diversity and larger-scale,\nimage with higher-resolution and higher-quality ground-truth. Specifically, the\nreal rains in LHP-Rain not only contain the classical rain\nstreak/veiling/occlusion in the sky, but also the \\textbf{splashing on the\nground} overlooked by deraining community. Moreover, we propose a novel robust\nlow-rank tensor recovery model to generate the GT with better separating the\nstatic background from the dynamic rain. In addition, we design a simple\ntransformer-based single image deraining baseline, which simultaneously utilize\nthe self-attention and cross-layer attention within the image and rain layer\nwith discriminative feature representation. Extensive experiments verify the\nsuperiority of the proposed dataset and deraining method over state-of-the-art.",
        "authors": [
            "Yun Guo",
            "Xueyao Xiao",
            "Yi Chang",
            "Shumin Deng",
            "Luxin Yan"
        ]
    },
    {
        "title": "Knowledge Restore and Transfer for Multi-Label Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2302.13334",
        "abstract": "Current class-incremental learning research mainly focuses on single-label\nclassification tasks while multi-label class-incremental learning (MLCIL) with\nmore practical application scenarios is rarely studied. Although there have\nbeen many anti-forgetting methods to solve the problem of catastrophic\nforgetting in class-incremental learning, these methods have difficulty in\nsolving the MLCIL problem due to label absence and information dilution. In\nthis paper, we propose a knowledge restore and transfer (KRT) framework for\nMLCIL, which includes a dynamic pseudo-label (DPL) module to restore the old\nclass knowledge and an incremental cross-attention(ICA) module to save\nsession-specific knowledge and transfer old class knowledge to the new model\nsufficiently. Besides, we propose a token loss to jointly optimize the\nincremental cross-attention module. Experimental results on MS-COCO and PASCAL\nVOC datasets demonstrate the effectiveness of our method for improving\nrecognition performance and mitigating forgetting on multi-label\nclass-incremental learning tasks.",
        "authors": [
            "Songlin Dong",
            "Haoyu Luo",
            "Yuhang He",
            "Xing Wei",
            "Yihong Gong"
        ]
    },
    {
        "title": "DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders",
        "url": "http://arxiv.org/abs/2212.11613",
        "abstract": "Image colorization is a challenging problem due to multi-modal uncertainty\nand high ill-posedness. Directly training a deep neural network usually leads\nto incorrect semantic colors and low color richness. While transformer-based\nmethods can deliver better results, they often rely on manually designed\npriors, suffer from poor generalization ability, and introduce color bleeding\neffects. To address these issues, we propose DDColor, an end-to-end method with\ndual decoders for image colorization. Our approach includes a pixel decoder and\na query-based color decoder. The former restores the spatial resolution of the\nimage, while the latter utilizes rich visual features to refine color queries,\nthus avoiding hand-crafted priors. Our two decoders work together to establish\ncorrelations between color and multi-scale semantic representations via\ncross-attention, significantly alleviating the color bleeding effect.\nAdditionally, a simple yet effective colorfulness loss is introduced to enhance\nthe color richness. Extensive experiments demonstrate that DDColor achieves\nsuperior performance to existing state-of-the-art works both quantitatively and\nqualitatively. The codes and models are publicly available at\nhttps://github.com/piddnad/DDColor.",
        "authors": [
            "Xiaoyang Kang",
            "Tao Yang",
            "Wenqi Ouyang",
            "Peiran Ren",
            "Lingzhi Li",
            "Xuansong Xie"
        ]
    },
    {
        "title": "PanFlowNet: A Flow-Based Deep Network for Pan-Sharpening",
        "url": "http://arxiv.org/abs/2305.07774",
        "abstract": "Pan-sharpening aims to generate a high-resolution multispectral (HRMS) image\nby integrating the spectral information of a low-resolution multispectral\n(LRMS) image with the texture details of a high-resolution panchromatic (PAN)\nimage. It essentially inherits the ill-posed nature of the super-resolution\n(SR) task that diverse HRMS images can degrade into an LRMS image. However,\nexisting deep learning-based methods recover only one HRMS image from the LRMS\nimage and PAN image using a deterministic mapping, thus ignoring the diversity\nof the HRMS image. In this paper, to alleviate this ill-posed issue, we propose\na flow-based pan-sharpening network (PanFlowNet) to directly learn the\nconditional distribution of HRMS image given LRMS image and PAN image instead\nof learning a deterministic mapping. Specifically, we first transform this\nunknown conditional distribution into a given Gaussian distribution by an\ninvertible network, and the conditional distribution can thus be explicitly\ndefined. Then, we design an invertible Conditional Affine Coupling Block (CACB)\nand further build the architecture of PanFlowNet by stacking a series of CACBs.\nFinally, the PanFlowNet is trained by maximizing the log-likelihood of the\nconditional distribution given a training set and can then be used to predict\ndiverse HRMS images. The experimental results verify that the proposed\nPanFlowNet can generate various HRMS images given an LRMS image and a PAN\nimage. Additionally, the experimental results on different kinds of satellite\ndatasets also demonstrate the superiority of our PanFlowNet compared with other\nstate-of-the-art methods both visually and quantitatively.",
        "authors": [
            "Gang Yang",
            "Xiangyong Cao",
            "Wenzhe Xiao",
            "Man Zhou",
            "Aiping Liu",
            "Xun chen",
            "Deyu Meng"
        ]
    },
    {
        "title": "Domain Generalization via Balancing Training Difficulty and Model Capability",
        "url": "http://arxiv.org/abs/2309.00844",
        "abstract": "Domain generalization (DG) aims to learn domain-generalizable models from one\nor multiple source domains that can perform well in unseen target domains.\nDespite its recent progress, most existing work suffers from the misalignment\nbetween the difficulty level of training samples and the capability of\ncontemporarily trained models, leading to over-fitting or under-fitting in the\ntrained generalization model. We design MoDify, a Momentum Difficulty framework\nthat tackles the misalignment by balancing the seesaw between the model's\ncapability and the samples' difficulties along the training process. MoDify\nconsists of two novel designs that collaborate to fight against the\nmisalignment while learning domain-generalizable models. The first is\nMoDify-based Data Augmentation which exploits an RGB Shuffle technique to\ngenerate difficulty-aware training samples on the fly. The second is\nMoDify-based Network Optimization which dynamically schedules the training\nsamples for balanced and smooth learning with appropriate difficulty. Without\nbells and whistles, a simple implementation of MoDify achieves superior\nperformance across multiple benchmarks. In addition, MoDify can complement\nexisting methods as a plug-in, and it is generic and can work for different\nvisual recognition tasks.",
        "authors": [
            "Xueying Jiang",
            "Jiaxing Huang",
            "Sheng Jin",
            "Shijian Lu"
        ]
    },
    {
        "title": "JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery",
        "url": "http://arxiv.org/abs/2307.16377",
        "abstract": "In this study, we focus on the problem of 3D human mesh recovery from a\nsingle image under obscured conditions. Most state-of-the-art methods aim to\nimprove 2D alignment technologies, such as spatial averaging and 2D joint\nsampling. However, they tend to neglect the crucial aspect of 3D alignment by\nimproving 3D representations. Furthermore, recent methods struggle to separate\nthe target human from occlusion or background in crowded scenes as they\noptimize the 3D space of target human with 3D joint coordinates as local\nsupervision. To address these issues, a desirable method would involve a\nframework for fusing 2D and 3D features and a strategy for optimizing the 3D\nspace globally. Therefore, this paper presents 3D JOint contrastive learning\nwith TRansformers (JOTR) framework for handling occluded 3D human mesh\nrecovery. Our method includes an encoder-decoder transformer architecture to\nfuse 2D and 3D representations for achieving 2D$\\&$3D aligned results in a\ncoarse-to-fine manner and a novel 3D joint contrastive learning approach for\nadding explicitly global supervision for the 3D feature space. The contrastive\nlearning approach includes two contrastive losses: joint-to-joint contrast for\nenhancing the similarity of semantically similar voxels (i.e., human joints),\nand joint-to-non-joint contrast for ensuring discrimination from others (e.g.,\nocclusions and background). Qualitative and quantitative analyses demonstrate\nthat our method outperforms state-of-the-art competitors on both\nocclusion-specific and standard benchmarks, significantly improving the\nreconstruction of occluded humans.",
        "authors": [
            "Jiahao Li",
            "Zongxin Yang",
            "Xiaohan Wang",
            "Jianxin Ma",
            "Chang Zhou",
            "Yi Yang"
        ]
    },
    {
        "title": "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection",
        "url": "http://arxiv.org/abs/2301.00785",
        "abstract": "An increasing number of public datasets have shown a marked impact on\nautomated organ segmentation and tumor detection. However, due to the small\nsize and partially labeled problem of each dataset, as well as a limited\ninvestigation of diverse types of tumors, the resulting models are often\nlimited to segmenting specific organs/tumors and ignore the semantics of\nanatomical structures, nor can they be extended to novel domains. To address\nthese issues, we propose the CLIP-Driven Universal Model, which incorporates\ntext embedding learned from Contrastive Language-Image Pre-training (CLIP) to\nsegmentation models. This CLIP-based label encoding captures anatomical\nrelationships, enabling the model to learn a structured feature embedding and\nsegment 25 organs and 6 types of tumors. The proposed model is developed from\nan assembly of 14 datasets, using a total of 3,410 CT scans for training and\nthen evaluated on 6,162 external CT scans from 3 additional datasets. We rank\nfirst on the Medical Segmentation Decathlon (MSD) public leaderboard and\nachieve state-of-the-art results on Beyond The Cranial Vault (BTCV).\nAdditionally, the Universal Model is computationally more efficient (6x faster)\ncompared with dataset-specific models, generalized better to CT scans from\nvarying sites, and shows stronger transfer learning performance on novel tasks.",
        "authors": [
            "Jie Liu",
            "Yixiao Zhang",
            "Jie-Neng Chen",
            "Junfei Xiao",
            "Yongyi Lu",
            "Bennett A. Landman",
            "Yixuan Yuan",
            "Alan Yuille",
            "Yucheng Tang",
            "Zongwei Zhou"
        ]
    },
    {
        "title": "VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis",
        "url": "http://arxiv.org/abs/2309.04800",
        "abstract": "Unsupervised learning of 3D-aware generative adversarial networks has lately\nmade much progress. Some recent work demonstrates promising results of learning\nhuman generative models using neural articulated radiance fields, yet their\ngeneralization ability and controllability lag behind parametric human models,\ni.e., they do not perform well when generalizing to novel pose/shape and are\nnot part controllable. To solve these problems, we propose VeRi3D, a generative\nhuman vertex-based radiance field parameterized by vertices of the parametric\nhuman template, SMPL. We map each 3D point to the local coordinate system\ndefined on its neighboring vertices, and use the corresponding vertex feature\nand local coordinates for mapping it to color and density values. We\ndemonstrate that our simple approach allows for generating photorealistic human\nimages with free control over camera pose, human pose, shape, as well as\nenabling part-level editing.",
        "authors": [
            "Xinya Chen",
            "Jiaxin Huang",
            "Yanrui Bin",
            "Lu Yu",
            "Yiyi Liao"
        ]
    },
    {
        "title": "MOSE: A New Dataset for Video Object Segmentation in Complex Scenes",
        "url": "http://arxiv.org/abs/2302.01872",
        "abstract": "Video object segmentation (VOS) aims at segmenting a particular object\nthroughout the entire video clip sequence. The state-of-the-art VOS methods\nhave achieved excellent performance (e.g., 90+% J&F) on existing datasets.\nHowever, since the target objects in these existing datasets are usually\nrelatively salient, dominant, and isolated, VOS under complex scenes has rarely\nbeen studied. To revisit VOS and make it more applicable in the real world, we\ncollect a new VOS dataset called coMplex video Object SEgmentation (MOSE) to\nstudy the tracking and segmenting objects in complex environments. MOSE\ncontains 2,149 video clips and 5,200 objects from 36 categories, with 431,725\nhigh-quality object segmentation masks. The most notable feature of MOSE\ndataset is complex scenes with crowded and occluded objects. The target objects\nin the videos are commonly occluded by others and disappear in some frames. To\nanalyze the proposed MOSE dataset, we benchmark 18 existing VOS methods under 4\ndifferent settings on the proposed MOSE dataset and conduct comprehensive\ncomparisons. The experiments show that current VOS algorithms cannot well\nperceive objects in complex scenes. For example, under the semi-supervised VOS\nsetting, the highest J&F by existing state-of-the-art VOS methods is only 59.4%\non MOSE, much lower than their ~90% J&F performance on DAVIS. The results\nreveal that although excellent performance has been achieved on existing\nbenchmarks, there are unresolved challenges under complex scenes and more\nefforts are desired to explore these challenges in the future. The proposed\nMOSE dataset has been released at https://henghuiding.github.io/MOSE.",
        "authors": [
            "Henghui Ding",
            "Chang Liu",
            "Shuting He",
            "Xudong Jiang",
            "Philip H. S. Torr",
            "Song Bai"
        ]
    },
    {
        "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.01692",
        "abstract": "Recently, transformer-based methods have dominated 3D instance segmentation,\nwhere mask attention is commonly involved. Specifically, object queries are\nguided by the initial instance masks in the first cross-attention, and then\niteratively refine themselves in a similar manner. However, we observe that the\nmask-attention pipeline usually leads to slow convergence due to low-recall\ninitial instance masks. Therefore, we abandon the mask attention design and\nresort to an auxiliary center regression task instead. Through center\nregression, we effectively overcome the low-recall issue and perform\ncross-attention by imposing positional prior. To reach this goal, we develop a\nseries of position-aware designs. First, we learn a spatial distribution of 3D\nlocations as the initial position queries. They spread over the 3D space\ndensely, and thus can easily capture the objects in a scene with a high recall.\nMoreover, we present relative position encoding for the cross-attention and\niterative refinement for more accurate position queries. Experiments show that\nour approach converges 4x faster than existing work, sets a new state of the\nart on ScanNetv2 3D instance segmentation benchmark, and also demonstrates\nsuperior performance across various datasets. Code and models are available at\nhttps://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
        "authors": [
            "Xin Lai",
            "Yuhui Yuan",
            "Ruihang Chu",
            "Yukang Chen",
            "Han Hu",
            "Jiaya Jia"
        ]
    },
    {
        "title": "SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors",
        "url": "http://arxiv.org/abs/2309.05810",
        "abstract": "We present SHIFT3D, a differentiable pipeline for generating 3D shapes that\nare structurally plausible yet challenging to 3D object detectors. In\nsafety-critical applications like autonomous driving, discovering such novel\nchallenging objects can offer insight into unknown vulnerabilities of 3D\ndetectors. By representing objects with a signed distanced function (SDF), we\nshow that gradient error signals allow us to smoothly deform the shape or pose\nof a 3D object in order to confuse a downstream 3D detector. Importantly, the\nobjects generated by SHIFT3D physically differ from the baseline object yet\nretain a semantically recognizable shape. Our approach provides interpretable\nfailure modes for modern 3D object detectors, and can aid in preemptive\ndiscovery of potential safety risks within 3D perception systems before these\nrisks become critical failures.",
        "authors": [
            "Hongge Chen",
            "Zhao Chen",
            "Gregory P. Meyer",
            "Dennis Park",
            "Carl Vondrick",
            "Ashish Shrivastava",
            "Yuning Chai"
        ]
    },
    {
        "title": "EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries",
        "url": "http://arxiv.org/abs/2212.06969",
        "abstract": "With the recent advances in video and 3D understanding, novel 4D\nspatio-temporal methods fusing both concepts have emerged. Towards this\ndirection, the Ego4D Episodic Memory Benchmark proposed a task for Visual\nQueries with 3D Localization (VQ3D). Given an egocentric video clip and an\nimage crop depicting a query object, the goal is to localize the 3D position of\nthe center of that query object with respect to the camera pose of a query\nframe. Current methods tackle the problem of VQ3D by unprojecting the 2D\nlocalization results of the sibling task Visual Queries with 2D Localization\n(VQ2D) into 3D predictions. Yet, we point out that the low number of camera\nposes caused by camera re-localization from previous VQ3D methods severally\nhinders their overall success rate. In this work, we formalize a pipeline (we\ndub EgoLoc) that better entangles 3D multiview geometry with 2D object\nretrieval from egocentric videos. Our approach involves estimating more robust\ncamera poses and aggregating multi-view 3D displacements by leveraging the 2D\ndetection confidence, which enhances the success rate of object queries and\nleads to a significant improvement in the VQ3D baseline performance.\nSpecifically, our approach achieves an overall success rate of up to 87.12%,\nwhich sets a new state-of-the-art result in the VQ3D task. We provide a\ncomprehensive empirical analysis of the VQ3D task and existing solutions, and\nhighlight the remaining challenges in VQ3D. The code is available at\nhttps://github.com/Wayne-Mai/EgoLoc.",
        "authors": [
            "Jinjie Mai",
            "Abdullah Hamdi",
            "Silvio Giancola",
            "Chen Zhao",
            "Bernard Ghanem"
        ]
    },
    {
        "title": "Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos",
        "url": "http://arxiv.org/abs/2308.10334",
        "abstract": "Multi-person 3D mesh recovery from videos is a critical first step towards\nautomatic perception of group behavior in virtual reality, physical therapy and\nbeyond. However, existing approaches rely on multi-stage paradigms, where the\nperson detection and tracking stages are performed in a multi-person setting,\nwhile temporal dynamics are only modeled for one person at a time.\nConsequently, their performance is severely limited by the lack of inter-person\ninteractions in the spatial-temporal mesh recovery, as well as by detection and\ntracking defects. To address these challenges, we propose the Coordinate\ntransFormer (CoordFormer) that directly models multi-person spatial-temporal\nrelations and simultaneously performs multi-mesh recovery in an end-to-end\nmanner. Instead of partitioning the feature map into coarse-scale patch-wise\ntokens, CoordFormer leverages a novel Coordinate-Aware Attention to preserve\npixel-level spatial-temporal coordinate information. Additionally, we propose a\nsimple, yet effective Body Center Attention mechanism to fuse position\ninformation. Extensive experiments on the 3DPW dataset demonstrate that\nCoordFormer significantly improves the state-of-the-art, outperforming the\npreviously best results by 4.2%, 8.8% and 4.7% according to the MPJPE, PAMPJPE,\nand PVE metrics, respectively, while being 40% faster than recent video-based\napproaches. The released code can be found at\nhttps://github.com/Li-Hao-yuan/CoordFormer.",
        "authors": [
            "Haoyuan Li",
            "Haoye Dong",
            "Hanchao Jia",
            "Dong Huang",
            "Michael C. Kampffmeyer",
            "Liang Lin",
            "Xiaodan Liang"
        ]
    },
    {
        "title": "FLatten Transformer: Vision Transformer using Focused Linear Attention",
        "url": "http://arxiv.org/abs/2308.00442",
        "abstract": "The quadratic computation complexity of self-attention has been a persistent\nchallenge when applying Transformer models to vision tasks. Linear attention,\non the other hand, offers a much more efficient alternative with its linear\ncomplexity by approximating the Softmax operation through carefully designed\nmapping functions. However, current linear attention approaches either suffer\nfrom significant performance degradation or introduce additional computation\noverhead from the mapping functions. In this paper, we propose a novel Focused\nLinear Attention module to achieve both high efficiency and expressiveness.\nSpecifically, we first analyze the factors contributing to the performance\ndegradation of linear attention from two perspectives: the focus ability and\nfeature diversity. To overcome these limitations, we introduce a simple yet\neffective mapping function and an efficient rank restoration module to enhance\nthe expressiveness of self-attention while maintaining low computation\ncomplexity. Extensive experiments show that our linear attention module is\napplicable to a variety of advanced vision Transformers, and achieves\nconsistently improved performances on multiple benchmarks. Code is available at\nhttps://github.com/LeapLabTHU/FLatten-Transformer.",
        "authors": [
            "Dongchen Han",
            "Xuran Pan",
            "Yizeng Han",
            "Shiji Song",
            "Gao Huang"
        ]
    },
    {
        "title": "Robustifying Token Attention for Vision Transformers",
        "url": "http://arxiv.org/abs/2303.11126",
        "abstract": "Despite the success of vision transformers (ViTs), they still suffer from\nsignificant drops in accuracy in the presence of common corruptions, such as\nnoise or blur. Interestingly, we observe that the attention mechanism of ViTs\ntends to rely on few important tokens, a phenomenon we call token overfocusing.\nMore critically, these tokens are not robust to corruptions, often leading to\nhighly diverging attention patterns. In this paper, we intend to alleviate this\noverfocusing issue and make attention more stable through two general\ntechniques: First, our Token-aware Average Pooling (TAP) module encourages the\nlocal neighborhood of each token to take part in the attention mechanism.\nSpecifically, TAP learns average pooling schemes for each token such that the\ninformation of potentially important tokens in the neighborhood can adaptively\nbe taken into account. Second, we force the output tokens to aggregate\ninformation from a diverse set of input tokens rather than focusing on just a\nfew by using our Attention Diversification Loss (ADL). We achieve this by\npenalizing high cosine similarity between the attention vectors of different\ntokens. In experiments, we apply our methods to a wide range of transformer\narchitectures and improve robustness significantly. For example, we improve\ncorruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4%\nbased on state-of-the-art robust architecture FAN. Also, when fine-tuning on\nsemantic segmentation tasks, we improve robustness on CityScapes-C by 2.4% and\nACDC by 3.0%. Our code is available at https://github.com/guoyongcs/TAPADL.",
        "authors": [
            "Yong Guo",
            "David Stutz",
            "Bernt Schiele"
        ]
    },
    {
        "title": "ADNet: Lane Shape Prediction via Anchor Decomposition",
        "url": "http://arxiv.org/abs/2308.10481",
        "abstract": "In this paper, we revisit the limitations of anchor-based lane detection\nmethods, which have predominantly focused on fixed anchors that stem from the\nedges of the image, disregarding their versatility and quality. To overcome the\ninflexibility of anchors, we decompose them into learning the heat map of\nstarting points and their associated directions. This decomposition removes the\nlimitations on the starting point of anchors, making our algorithm adaptable to\ndifferent lane types in various datasets. To enhance the quality of anchors, we\nintroduce the Large Kernel Attention (LKA) for Feature Pyramid Network (FPN).\nThis significantly increases the receptive field, which is crucial in capturing\nthe sufficient context as lane lines typically run throughout the entire image.\nWe have named our proposed system the Anchor Decomposition Network (ADNet).\nAdditionally, we propose the General Lane IoU (GLIoU) loss, which significantly\nimproves the performance of ADNet in complex scenarios. Experimental results on\nthree widely used lane detection benchmarks, VIL-100, CULane, and TuSimple,\ndemonstrate that our approach outperforms the state-of-the-art methods on\nVIL-100 and exhibits competitive accuracy on CULane and TuSimple. Code and\nmodels will be released on https://github.com/ Sephirex-X/ADNet.",
        "authors": [
            "Lingyu Xiao",
            "Xiang Li",
            "Sen Yang",
            "Wankou Yang"
        ]
    },
    {
        "title": "UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase",
        "url": "http://arxiv.org/abs/2309.05573",
        "abstract": "Point-, voxel-, and range-views are three representative forms of point\nclouds. All of them have accurate 3D measurements but lack color and texture\ninformation. RGB images are a natural complement to these point cloud views and\nfully utilizing the comprehensive information of them benefits more robust\nperceptions. In this paper, we present a unified multi-modal LiDAR segmentation\nnetwork, termed UniSeg, which leverages the information of RGB images and three\nviews of the point cloud, and accomplishes semantic segmentation and panoptic\nsegmentation simultaneously. Specifically, we first design the Learnable\ncross-Modal Association (LMA) module to automatically fuse voxel-view and\nrange-view features with image features, which fully utilize the rich semantic\ninformation of images and are robust to calibration errors. Then, the enhanced\nvoxel-view and range-view features are transformed to the point space,where\nthree views of point cloud features are further fused adaptively by the\nLearnable cross-View Association module (LVA). Notably, UniSeg achieves\npromising results in three public benchmarks, i.e., SemanticKITTI, nuScenes,\nand Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks,\nincluding the LiDAR semantic segmentation challenge of nuScenes and panoptic\nsegmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg\ncodebase, which is the largest and most comprehensive outdoor LiDAR\nsegmentation codebase. It contains most of the popular outdoor LiDAR\nsegmentation algorithms and provides reproducible implementations. The\nOpenPCSeg codebase will be made publicly available at\nhttps://github.com/PJLab-ADG/PCSeg.",
        "authors": [
            "Youquan Liu",
            "Runnan Chen",
            "Xin Li",
            "Lingdong Kong",
            "Yuchen Yang",
            "Zhaoyang Xia",
            "Yeqi Bai",
            "Xinge Zhu",
            "Yuexin Ma",
            "Yikang Li",
            "Yu Qiao",
            "Yuenan Hou"
        ]
    },
    {
        "title": "Sign Language Translation with Iterative Prototype",
        "url": "http://arxiv.org/abs/2308.12191",
        "abstract": "This paper presents IP-SLT, a simple yet effective framework for sign\nlanguage translation (SLT). Our IP-SLT adopts a recurrent structure and\nenhances the semantic representation (prototype) of the input sign language\nvideo via an iterative refinement manner. Our idea mimics the behavior of human\nreading, where a sentence can be digested repeatedly, till reaching accurate\nunderstanding. Technically, IP-SLT consists of feature extraction, prototype\ninitialization, and iterative prototype refinement. The initialization module\ngenerates the initial prototype based on the visual feature extracted by the\nfeature extraction module. Then, the iterative refinement module leverages the\ncross-attention mechanism to polish the previous prototype by aggregating it\nwith the original video feature. Through repeated refinement, the prototype\nfinally converges to a more stable and accurate state, leading to a fluent and\nappropriate translation. In addition, to leverage the sequential dependence of\nprototypes, we further propose an iterative distillation loss to compress the\nknowledge of the final iteration into previous ones. As the autoregressive\ndecoding process is executed only once in inference, our IP-SLT is ready to\nimprove various SLT systems with acceptable overhead. Extensive experiments are\nconducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.",
        "authors": [
            "Huijie Yao",
            "Wengang Zhou",
            "Hao Feng",
            "Hezhen Hu",
            "Hao Zhou",
            "Houqiang Li"
        ]
    },
    {
        "title": "Pixel-Wise Contrastive Distillation",
        "url": "http://arxiv.org/abs/2211.00218",
        "abstract": "We present a simple but effective pixel-level self-supervised distillation\nframework friendly to dense prediction tasks. Our method, called Pixel-Wise\nContrastive Distillation (PCD), distills knowledge by attracting the\ncorresponding pixels from student's and teacher's output feature maps. PCD\nincludes a novel design called SpatialAdaptor which ``reshapes'' a part of the\nteacher network while preserving the distribution of its output features. Our\nablation experiments suggest that this reshaping behavior enables more\ninformative pixel-to-pixel distillation. Moreover, we utilize a plug-in\nmulti-head self-attention module that explicitly relates the pixels of\nstudent's feature maps to enhance the effective receptive field, leading to a\nmore competitive student. PCD \\textbf{outperforms} previous self-supervised\ndistillation methods on various dense prediction tasks. A backbone of\n\\mbox{ResNet-18-FPN} distilled by PCD achieves $37.4$ AP$^\\text{bbox}$ and\n$34.0$ AP$^\\text{mask}$ on COCO dataset using the detector of \\mbox{Mask\nR-CNN}. We hope our study will inspire future research on how to pre-train a\nsmall model friendly to dense prediction tasks in a self-supervised fashion.",
        "authors": [
            "Junqiang Huang",
            "Zichao Guo"
        ]
    },
    {
        "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
        "url": "http://arxiv.org/abs/2305.20091",
        "abstract": "We present an approach to reconstruct humans and track them over time. At the\ncore of our approach, we propose a fully \"transformerized\" version of a network\nfor human mesh recovery. This network, HMR 2.0, advances the state of the art\nand shows the capability to analyze unusual poses that have in the past been\ndifficult to reconstruct from single images. To analyze video, we use 3D\nreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.\nThis enables us to deal with multiple people and maintain identities through\nocclusion events. Our complete approach, 4DHumans, achieves state-of-the-art\nresults for tracking people from monocular video. Furthermore, we demonstrate\nthe effectiveness of HMR 2.0 on the downstream task of action recognition,\nachieving significant improvements over previous pose-based action recognition\napproaches. Our code and models are available on the project website:\nhttps://shubham-goel.github.io/4dhumans/.",
        "authors": [
            "Shubham Goel",
            "Georgios Pavlakos",
            "Jathushan Rajasegaran",
            "Angjoo Kanazawa",
            "Jitendra Malik"
        ]
    },
    {
        "title": "Ponder: Point Cloud Pre-training via Neural Rendering",
        "url": "http://arxiv.org/abs/2301.00157",
        "abstract": "We propose a novel approach to self-supervised learning of point cloud\nrepresentations by differentiable neural rendering. Motivated by the fact that\ninformative point cloud features should be able to encode rich geometry and\nappearance cues and render realistic images, we train a point-cloud encoder\nwithin a devised point-based neural renderer by comparing the rendered images\nwith real images on massive RGB-D data. The learned point-cloud encoder can be\neasily integrated into various downstream tasks, including not only high-level\ntasks like 3D detection and segmentation, but low-level tasks like 3D\nreconstruction and image synthesis. Extensive experiments on various tasks\ndemonstrate the superiority of our approach compared to existing pre-training\nmethods.",
        "authors": [
            "Di Huang",
            "Sida Peng",
            "Tong He",
            "Honghui Yang",
            "Xiaowei Zhou",
            "Wanli Ouyang"
        ]
    },
    {
        "title": "HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation",
        "url": "http://arxiv.org/abs/2308.10122",
        "abstract": "Neural radiance fields (NeRF) have garnered significant attention, with\nrecent works such as Instant-NGP accelerating NeRF training and evaluation\nthrough a combination of hashgrid-based positional encoding and neural\nnetworks. However, effectively leveraging the spatial sparsity of 3D scenes\nremains a challenge. To cull away unnecessary regions of the feature grid,\nexisting solutions rely on prior knowledge of object shape or periodically\nestimate object shape during training by repeated model evaluations, which are\ncostly and wasteful.\n  To address this issue, we propose HollowNeRF, a novel compression solution\nfor hashgrid-based NeRF which automatically sparsifies the feature grid during\nthe training phase. Instead of directly compressing dense features, HollowNeRF\ntrains a coarse 3D saliency mask that guides efficient feature pruning, and\nemploys an alternating direction method of multipliers (ADMM) pruner to\nsparsify the 3D saliency mask during training. By exploiting the sparsity in\nthe 3D scene to redistribute hash collisions, HollowNeRF improves rendering\nquality while using a fraction of the parameters of comparable state-of-the-art\nsolutions, leading to a better cost-accuracy trade-off. Our method delivers\ncomparable rendering quality to Instant-NGP, while utilizing just 31% of the\nparameters. In addition, our solution can achieve a PSNR accuracy gain of up to\n1dB using only 56% of the parameters.",
        "authors": [
            "Xiufeng Xie",
            "Riccardo Gherardi",
            "Zhihong Pan",
            "Stephen Huang"
        ]
    },
    {
        "title": "Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising",
        "url": "http://arxiv.org/abs/2308.04682",
        "abstract": "Real-world single image denoising is crucial and practical in computer\nvision. Bayesian inversions combined with score priors now have proven\neffective for single image denoising but are limited to white Gaussian noise.\nMoreover, applying existing score-based methods for real-world denoising\nrequires not only the explicit train of score priors on the target domain but\nalso the careful design of sampling procedures for posterior inference, which\nis complicated and impractical. To address these limitations, we propose a\nscore priors-guided deep variational inference, namely ScoreDVI, for practical\nreal-world denoising. By considering the deep variational image posterior with\na Gaussian form, score priors are extracted based on easily accessible minimum\nMSE Non-$i.i.d$ Gaussian denoisers and variational samples, which in turn\nfacilitate optimizing the variational image posterior. Such a procedure\nadaptively applies cheap score priors to denoising. Additionally, we exploit a\nNon-$i.i.d$ Gaussian mixture model and variational noise posterior to model the\nreal-world noise. This scheme also enables the pixel-wise fusion of multiple\nimage priors and variational image posteriors. Besides, we develop a\nnoise-aware prior assignment strategy that dynamically adjusts the weight of\nimage priors in the optimization. Our method outperforms other single\nimage-based real-world denoising methods and achieves comparable performance to\ndataset-based unsupervised methods.",
        "authors": [
            "Jun Cheng",
            "Tao Liu",
            "Shan Tan"
        ]
    },
    {
        "title": "Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints",
        "url": "http://arxiv.org/abs/2303.02885",
        "abstract": "Learning robust local image feature matching is a fundamental low-level\nvision task, which has been widely explored in the past few years. Recently,\ndetector-free local feature matchers based on transformers have shown promising\nresults, which largely outperform pure Convolutional Neural Network (CNN) based\nones. But correlations produced by transformer-based methods are spatially\nlimited to the center of source views' coarse patches, because of the costly\nattention learning. In this work, we rethink this issue and find that such\nmatching formulation degrades pose estimation, especially for low-resolution\nimages. So we propose a transformer-based cascade matching model -- Cascade\nfeature Matching TRansformer (CasMTR), to efficiently learn dense feature\ncorrelations, which allows us to choose more reliable matching pairs for the\nrelative pose estimation. Instead of re-training a new detector, we use a\nsimple yet effective Non-Maximum Suppression (NMS) post-process to filter\nkeypoints through the confidence map, and largely improve the matching\nprecision. CasMTR achieves state-of-the-art performance in indoor and outdoor\npose estimation as well as visual localization. Moreover, thorough ablations\nshow the efficacy of the proposed components and techniques.",
        "authors": [
            "Chenjie Cao",
            "Yanwei Fu"
        ]
    },
    {
        "title": "Controllable Guide-Space for Generalizable Face Forgery Detection",
        "url": "http://arxiv.org/abs/2307.14039",
        "abstract": "Recent studies on face forgery detection have shown satisfactory performance\nfor methods involved in training datasets, but are not ideal enough for unknown\ndomains. This motivates many works to improve the generalization, but\nforgery-irrelevant information, such as image background and identity, still\nexists in different domain features and causes unexpected clustering, limiting\nthe generalization. In this paper, we propose a controllable guide-space (GS)\nmethod to enhance the discrimination of different forgery domains, so as to\nincrease the forgery relevance of features and thereby improve the\ngeneralization. The well-designed guide-space can simultaneously achieve both\nthe proper separation of forgery domains and the large distance between\nreal-forgery domains in an explicit and controllable manner. Moreover, for\nbetter discrimination, we use a decoupling module to weaken the interference of\nforgery-irrelevant correlations between domains. Furthermore, we make\nadjustments to the decision boundary manifold according to the clustering\ndegree of the same domain features within the neighborhood. Extensive\nexperiments in multiple in-domain and cross-domain settings confirm that our\nmethod can achieve state-of-the-art generalization.",
        "authors": [
            "Ying Guo",
            "Cheng Zhen",
            "Pengfei Yan"
        ]
    },
    {
        "title": "Calibrating Uncertainty for Semi-Supervised Crowd Counting",
        "url": "http://arxiv.org/abs/2308.09887",
        "abstract": "Semi-supervised crowd counting is an important yet challenging task. A\npopular approach is to iteratively generate pseudo-labels for unlabeled data\nand add them to the training set. The key is to use uncertainty to select\nreliable pseudo-labels. In this paper, we propose a novel method to calibrate\nmodel uncertainty for crowd counting. Our method takes a supervised uncertainty\nestimation strategy to train the model through a surrogate function. This\nensures the uncertainty is well controlled throughout the training. We propose\na matching-based patch-wise surrogate function to better approximate\nuncertainty for crowd counting tasks. The proposed method pays a sufficient\namount of attention to details, while maintaining a proper granularity.\nAltogether our method is able to generate reliable uncertainty estimation, high\nquality pseudolabels, and achieve state-of-the-art performance in\nsemisupervised crowd counting.",
        "authors": [
            "Chen Li",
            "Xiaoling Hu",
            "Shahira Abousamra",
            "Chao Chen"
        ]
    },
    {
        "title": "MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers",
        "url": "http://arxiv.org/abs/2308.11096",
        "abstract": "Quantum machine learning and vision have come to the fore recently, with\nhardware advances enabling rapid advancement in the capabilities of quantum\nmachines. Recently, quantum image generation has been explored with many\npotential advantages over non-quantum techniques; however, previous techniques\nhave suffered from poor quality and robustness. To address these problems, we\nintroduce, MosaiQ, a high-quality quantum image generation GAN framework that\ncan be executed on today's Near-term Intermediate Scale Quantum (NISQ)\ncomputers.",
        "authors": [
            "Daniel Silver",
            "Tirthak Patel",
            "William Cutler",
            "Aditya Ranjan",
            "Harshitta Gandhi",
            "Devesh Tiwari"
        ]
    },
    {
        "title": "DVIS: Decoupled Video Instance Segmentation Framework",
        "url": "http://arxiv.org/abs/2306.03413",
        "abstract": "Video instance segmentation (VIS) is a critical task with diverse\napplications, including autonomous driving and video editing. Existing methods\noften underperform on complex and long videos in real world, primarily due to\ntwo factors. Firstly, offline methods are limited by the tightly-coupled\nmodeling paradigm, which treats all frames equally and disregards the\ninterdependencies between adjacent frames. Consequently, this leads to the\nintroduction of excessive noise during long-term temporal alignment. Secondly,\nonline methods suffer from inadequate utilization of temporal information. To\ntackle these challenges, we propose a decoupling strategy for VIS by dividing\nit into three independent sub-tasks: segmentation, tracking, and refinement.\nThe efficacy of the decoupling strategy relies on two crucial elements: 1)\nattaining precise long-term alignment outcomes via frame-by-frame association\nduring tracking, and 2) the effective utilization of temporal information\npredicated on the aforementioned accurate alignment outcomes during refinement.\nWe introduce a novel referring tracker and temporal refiner to construct the\n\\textbf{D}ecoupled \\textbf{VIS} framework (\\textbf{DVIS}). DVIS achieves new\nSOTA performance in both VIS and VPS, surpassing the current SOTA methods by\n7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the most\nchallenging and realistic benchmarks. Moreover, thanks to the decoupling\nstrategy, the referring tracker and temporal refiner are super light-weight\n(only 1.69\\% of the segmenter FLOPs), allowing for efficient training and\ninference on a single GPU with 11G memory. The code is available at\n\\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.",
        "authors": [
            "Tao Zhang",
            "Xingye Tian",
            "Yu Wu",
            "Shunping Ji",
            "Xuebo Wang",
            "Yuan Zhang",
            "Pengfei Wan"
        ]
    },
    {
        "title": "Segmentation of Tubular Structures Using Iterative Training with Tailored Samples",
        "url": "http://arxiv.org/abs/2309.08727",
        "abstract": "We propose a minimal path method to simultaneously compute segmentation masks\nand extract centerlines of tubular structures with line-topology. Minimal path\nmethods are commonly used for the segmentation of tubular structures in a wide\nvariety of applications. Recent methods use features extracted by CNNs, and\noften outperform methods using hand-tuned features. However, for CNN-based\nmethods, the samples used for training may be generated inappropriately, so\nthat they can be very different from samples encountered during inference. We\napproach this discrepancy by introducing a novel iterative training scheme,\nwhich enables generating better training samples specifically tailored for the\nminimal path methods without changing existing annotations. In our method,\nsegmentation masks and centerlines are not determined after one another by\npost-processing, but obtained using the same steps. Our method requires only\nvery few annotated training images. Comparison with seven previous approaches\non three public datasets, including satellite images and medical images, shows\nthat our method achieves state-of-the-art results both for segmentation masks\nand centerlines.",
        "authors": [
            "Wei Liao"
        ]
    },
    {
        "title": "Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction",
        "url": "http://arxiv.org/abs/2303.18125",
        "abstract": "This paper addresses the problem of rolling shutter correction in complex\nnonlinear and dynamic scenes with extreme occlusion. Existing methods suffer\nfrom two main drawbacks. Firstly, they face challenges in estimating the\naccurate correction field due to the uniform velocity assumption, leading to\nsignificant image correction errors under complex motion. Secondly, the drastic\nocclusion in dynamic scenes prevents current solutions from achieving better\nimage quality because of the inherent difficulties in aligning and aggregating\nmultiple frames. To tackle these challenges, we model the curvilinear\ntrajectory of pixels analytically and propose a geometry-based Quadratic\nRolling Shutter (QRS) motion solver, which precisely estimates the high-order\ncorrection field of individual pixels. Besides, to reconstruct high-quality\nocclusion frames in dynamic scenes, we present a 3D video architecture that\neffectively Aligns and Aggregates multi-frame context, namely, RSA2-Net. We\nevaluate our method across a broad range of cameras and video sequences,\ndemonstrating its significant superiority. Specifically, our method surpasses\nthe state-of-the-art by +4.98, +0.77, and +4.33 of PSNR on Carla-RS, Fastec-RS,\nand BS-RSC datasets, respectively. Code is available at\nhttps://github.com/DelinQu/qrsc.",
        "authors": [
            "Delin Qu",
            "Yizhen Lao",
            "Zhigang Wang",
            "Dong Wang",
            "Bin Zhao",
            "Xuelong Li"
        ]
    },
    {
        "title": "Surface Extraction from Neural Unsigned Distance Fields",
        "url": "http://arxiv.org/abs/2309.08878",
        "abstract": "We propose a method, named DualMesh-UDF, to extract a surface from unsigned\ndistance functions (UDFs), encoded by neural networks, or neural UDFs. Neural\nUDFs are becoming increasingly popular for surface representation because of\ntheir versatility in presenting surfaces with arbitrary topologies, as opposed\nto the signed distance function that is limited to representing a closed\nsurface. However, the applications of neural UDFs are hindered by the notorious\ndifficulty in extracting the target surfaces they represent. Recent methods for\nsurface extraction from a neural UDF suffer from significant geometric errors\nor topological artifacts due to two main difficulties: (1) A UDF does not\nexhibit sign changes; and (2) A neural UDF typically has substantial\napproximation errors. DualMesh-UDF addresses these two difficulties.\nSpecifically, given a neural UDF encoding a target surface $\\bar{S}$ to be\nrecovered, we first estimate the tangent planes of $\\bar{S}$ at a set of sample\npoints close to $\\bar{S}$. Next, we organize these sample points into local\nclusters, and for each local cluster, solve a linear least squares problem to\ndetermine a final surface point. These surface points are then connected to\ncreate the output mesh surface, which approximates the target surface. The\nrobust estimation of the tangent planes of the target surface and the\nsubsequent minimization problem constitute our core strategy, which contributes\nto the favorable performance of DualMesh-UDF over other competing methods. To\nefficiently implement this strategy, we employ an adaptive Octree. Within this\nframework, we estimate the location of a surface point in each of the octree\ncells identified as containing part of the target surface. Extensive\nexperiments show that our method outperforms existing methods in terms of\nsurface reconstruction quality while maintaining comparable computational\nefficiency.",
        "authors": [
            "Congyi Zhang",
            "Guying Lin",
            "Lei Yang",
            "Xin Li",
            "Taku Komura",
            "Scott Schaefer",
            "John Keyser",
            "Wenping Wang"
        ]
    },
    {
        "title": "CBA: Improving Online Continual Learning via Continual Bias Adaptor",
        "url": "http://arxiv.org/abs/2308.06925",
        "abstract": "Online continual learning (CL) aims to learn new knowledge and consolidate\npreviously learned knowledge from non-stationary data streams. Due to the\ntime-varying training setting, the model learned from a changing distribution\neasily forgets the previously learned knowledge and biases toward the newly\nreceived task. To address this problem, we propose a Continual Bias Adaptor\n(CBA) module to augment the classifier network to adapt to catastrophic\ndistribution change during training, such that the classifier network is able\nto learn a stable consolidation of previously learned tasks. In the testing\nstage, CBA can be removed which introduces no additional computation cost and\nmemory overhead. We theoretically reveal the reason why the proposed method can\neffectively alleviate catastrophic distribution shifts, and empirically\ndemonstrate its effectiveness through extensive experiments based on four\nrehearsal-based baselines and three public continual learning benchmarks.",
        "authors": [
            "Quanziang Wang",
            "Renzhen Wang",
            "Yichen Wu",
            "Xixi Jia",
            "Deyu Meng"
        ]
    },
    {
        "title": "GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation",
        "url": "http://arxiv.org/abs/2309.11145",
        "abstract": "Echocardiogram video segmentation plays an important role in cardiac disease\ndiagnosis. This paper studies the unsupervised domain adaption (UDA) for\nechocardiogram video segmentation, where the goal is to generalize the model\ntrained on the source domain to other unlabelled target domains. Existing UDA\nsegmentation methods are not suitable for this task because they do not model\nlocal information and the cyclical consistency of heartbeat. In this paper, we\nintroduce a newly collected CardiacUDA dataset and a novel GraphEcho method for\ncardiac structure segmentation. Our GraphEcho comprises two innovative modules,\nthe Spatial-wise Cross-domain Graph Matching (SCGM) and the Temporal Cycle\nConsistency (TCC) module, which utilize prior knowledge of echocardiogram\nvideos, i.e., consistent cardiac structure across patients and centers and the\nheartbeat cyclical consistency, respectively. These two modules can better\nalign global and local features from source and target domains, improving UDA\nsegmentation results. Experimental results showed that our GraphEcho\noutperforms existing state-of-the-art UDA segmentation methods. Our collected\ndataset and code will be publicly released upon acceptance. This work will lay\na new and solid cornerstone for cardiac structure segmentation from\nechocardiogram videos. Code and dataset are available at:\nhttps://github.com/xmed-lab/GraphEcho",
        "authors": [
            "Jiewen Yang",
            "Xinpeng Ding",
            "Ziyang Zheng",
            "Xiaowei Xu",
            "Xiaomeng Li"
        ]
    },
    {
        "title": "Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation",
        "url": "http://arxiv.org/abs/2309.13248",
        "abstract": "Video amodal segmentation is a particularly challenging task in computer\nvision, which requires to deduce the full shape of an object from the visible\nparts of it. Recently, some studies have achieved promising performance by\nusing motion flow to integrate information across frames under a\nself-supervised setting. However, motion flow has a clear limitation by the two\nfactors of moving cameras and object deformation. This paper presents a\nrethinking to previous works. We particularly leverage the supervised signals\nwith object-centric representation in \\textit{real-world scenarios}. The\nunderlying idea is the supervision signal of the specific object and the\nfeatures from different views can mutually benefit the deduction of the full\nmask in any specific frame. We thus propose an Efficient object-centric\nRepresentation amodal Segmentation (EoRaS). Specially, beyond solely relying on\nsupervision signals, we design a translation module to project image features\ninto the Bird's-Eye View (BEV), which introduces 3D information to improve\ncurrent feature quality. Furthermore, we propose a multi-view fusion layer\nbased temporal module which is equipped with a set of object slots and\ninteracts with features from different views by attention mechanism to fulfill\nsufficient object representation completion. As a result, the full mask of the\nobject can be decoded from image features updated by object slots. Extensive\nexperiments on both real-world and synthetic benchmarks demonstrate the\nsuperiority of our proposed method, achieving state-of-the-art performance. Our\ncode will be released at \\url{https://github.com/kfan21/EoRaS}.",
        "authors": [
            "Ke Fan",
            "Jingshi Lei",
            "Xuelin Qian",
            "Miaopeng Yu",
            "Tianjun Xiao",
            "Tong He",
            "Zheng Zhang",
            "Yanwei Fu"
        ]
    },
    {
        "title": "Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection",
        "url": "http://arxiv.org/abs/2307.12427",
        "abstract": "In incremental learning, replaying stored samples from previous tasks\ntogether with current task samples is one of the most efficient approaches to\naddress catastrophic forgetting. However, unlike incremental classification,\nimage replay has not been successfully applied to incremental object detection\n(IOD). In this paper, we identify the overlooked problem of foreground shift as\nthe main reason for this. Foreground shift only occurs when replaying images of\nprevious tasks and refers to the fact that their background might contain\nforeground objects of the current task. To overcome this problem, a novel and\nefficient Augmented Box Replay (ABR) method is developed that only stores and\nreplays foreground objects and thereby circumvents the foreground shift\nproblem. In addition, we propose an innovative Attentive RoI Distillation loss\nthat uses spatial attention from region-of-interest (RoI) features to constrain\ncurrent model to focus on the most important information from old model. ABR\nsignificantly reduces forgetting of previous classes while maintaining high\nplasticity in current classes. Moreover, it considerably reduces the storage\nrequirements when compared to standard image replay. Comprehensive experiments\non Pascal-VOC and COCO datasets support the state-of-the-art performance of our\nmodel.",
        "authors": [
            "Liu Yuyang",
            "Cong Yang",
            "Goswami Dipam",
            "Liu Xialei",
            "Joost van de Weijer"
        ]
    },
    {
        "title": "Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning",
        "url": "http://arxiv.org/abs/2303.00404",
        "abstract": "Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new\ncompositions of seen attributes and objects. In OW-CZSL, methods built on the\nconventional closed-world setting degrade severely due to the unconstrained OW\ntest space. While previous works alleviate the issue by pruning compositions\naccording to external knowledge or correlations in seen pairs, they introduce\nbiases that harm the generalization. Some methods thus predict state and object\nwith independently constructed and trained classifiers, ignoring that\nattributes are highly context-dependent and visually entangled with objects. In\nthis paper, we propose a novel Distilled Reverse Attention Network to address\nthe challenges. We also model attributes and objects separately but with\ndifferent motivations, capturing contextuality and locality, respectively. We\nfurther design a reverse-and-distill strategy that learns disentangled\nrepresentations of elementary components in training data supervised by reverse\nattention and knowledge distillation. We conduct experiments on three datasets\nand consistently achieve state-of-the-art (SOTA) performance.",
        "authors": [
            "Yun Li",
            "Zhe Liu",
            "Saurav Jha",
            "Sally Cripps",
            "Lina Yao"
        ]
    },
    {
        "title": "Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising",
        "url": "http://arxiv.org/abs/2308.03448",
        "abstract": "Explicit calibration-based methods have dominated RAW image denoising under\nextremely low-light environments. However, these methods are impeded by several\ncritical limitations: a) the explicit calibration process is both labor- and\ntime-intensive, b) challenge exists in transferring denoisers across different\ncamera models, and c) the disparity between synthetic and real noise is\nexacerbated by digital gain. To address these issues, we introduce a\ngroundbreaking pipeline named Lighting Every Darkness (LED), which is effective\nregardless of the digital gain or the camera sensor. LED eliminates the need\nfor explicit noise model calibration, instead utilizing an implicit fine-tuning\nprocess that allows quick deployment and requires minimal data. Structural\nmodifications are also included to reduce the discrepancy between synthetic and\nreal noise without extra computational demands. Our method surpasses existing\nmethods in various camera models, including new ones not in public datasets,\nwith just a few pairs per digital gain and only 0.5% of the typical iterations.\nFurthermore, LED also allows researchers to focus more on deep learning\nadvancements while still utilizing sensor engineering benefits. Code and\nrelated materials can be found in https://srameo.github.io/projects/led-iccv23/ .",
        "authors": [
            "Xin Jin",
            "Jia-Wen Xiao",
            "Ling-Hao Han",
            "Chunle Guo",
            "Xialei Liu",
            "Chongyi Li",
            "Ming-Ming Cheng"
        ]
    },
    {
        "title": "MotionBERT: A Unified Perspective on Learning Human Motion Representations",
        "url": "http://arxiv.org/abs/2210.06551",
        "abstract": "We present a unified perspective on tackling various human-centric video\ntasks by learning human motion representations from large-scale and\nheterogeneous data resources. Specifically, we propose a pretraining stage in\nwhich a motion encoder is trained to recover the underlying 3D motion from\nnoisy partial 2D observations. The motion representations acquired in this way\nincorporate geometric, kinematic, and physical knowledge about human motion,\nwhich can be easily transferred to multiple downstream tasks. We implement the\nmotion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)\nneural network. It could capture long-range spatio-temporal relationships among\nthe skeletal joints comprehensively and adaptively, exemplified by the lowest\n3D pose estimation error so far when trained from scratch. Furthermore, our\nproposed framework achieves state-of-the-art performance on all three\ndownstream tasks by simply finetuning the pretrained motion encoder with a\nsimple regression head (1-2 layers), which demonstrates the versatility of the\nlearned motion representations. Code and models are available at\nhttps://motionbert.github.io/",
        "authors": [
            "Wentao Zhu",
            "Xiaoxuan Ma",
            "Zhaoyang Liu",
            "Libin Liu",
            "Wayne Wu",
            "Yizhou Wang"
        ]
    },
    {
        "title": "PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization",
        "url": "http://arxiv.org/abs/2212.00979",
        "abstract": "Synthetic data offers the promise of cheap and bountiful training data for\nsettings where labeled real-world data is scarce. However, models trained on\nsynthetic data significantly underperform when evaluated on real-world data. In\nthis paper, we propose Proportional Amplitude Spectrum Training Augmentation\n(PASTA), a simple and effective augmentation strategy to improve out-of-the-box\nsynthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the\namplitude spectra of synthetic images in the Fourier domain to generate\naugmented views. Specifically, with PASTA we propose a structured perturbation\nstrategy where high-frequency components are perturbed relatively more than the\nlow-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real),\nobject detection (Sim10K-to-Real), and object recognition (VisDA-C\nSyn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA\noutperforms more complex state-of-the-art generalization methods while being\ncomplementary to the same.",
        "authors": [
            "Prithvijit Chattopadhyay",
            "Kartik Sarangmath",
            "Vivek Vijaykumar",
            "Judy Hoffman"
        ]
    },
    {
        "title": "EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding",
        "url": "http://arxiv.org/abs/2309.02423",
        "abstract": "With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),\nlarge-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.\nHowever, most current research is built on resources derived from third-person\nvideo action recognition. This inherent domain gap between first- and\nthird-person action videos, which have not been adequately addressed before,\nmakes current Ego-HOI suboptimal. This paper rethinks and proposes a new\nframework as an infrastructure to advance Ego-HOI recognition by Probing,\nCuration and Adaption (EgoPCA). We contribute comprehensive pre-train sets,\nbalanced test sets and a new baseline, which are complete with a\ntraining-finetuning strategy. With our new framework, we not only achieve\nstate-of-the-art performance on Ego-HOI benchmarks but also build several new\nand effective mechanisms and settings to advance further research. We believe\nour data and the findings will pave a new way for Ego-HOI understanding. Code\nand data are available at https://mvig-rhos.com/ego_pca",
        "authors": [
            "Yue Xu",
            "Yong-Lu Li",
            "Zhemin Huang",
            "Michael Xu Liu",
            "Cewu Lu",
            "Yu-Wing Tai",
            "Chi-Keung Tang"
        ]
    },
    {
        "title": "Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image",
        "url": "http://arxiv.org/abs/2307.10984",
        "abstract": "Reconstructing accurate 3D scenes from images is a long-standing vision task.\nDue to the ill-posedness of the single-image reconstruction problem, most\nwell-established methods are built upon multi-view geometry. State-of-the-art\n(SOTA) monocular metric depth estimation methods can only handle a single\ncamera model and are unable to perform mixed-data training due to the metric\nambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets\nachieve zero-shot generalization by learning affine-invariant depths, which\ncannot recover real-world metrics. In this work, we show that the key to a\nzero-shot single-view metric depth model lies in the combination of large-scale\ndata training and resolving the metric ambiguity from various camera models. We\npropose a canonical camera space transformation module, which explicitly\naddresses the ambiguity problems and can be effortlessly plugged into existing\nmonocular models. Equipped with our module, monocular models can be stably\ntrained with over 8 million images with thousands of camera models, resulting\nin zero-shot generalization to in-the-wild images with unseen camera settings.\nExperiments demonstrate SOTA performance of our method on 7 zero-shot\nbenchmarks. Notably, our method won the championship in the 2nd Monocular Depth\nEstimation Challenge. Our method enables the accurate recovery of metric 3D\nstructures on randomly collected internet images, paving the way for plausible\nsingle-image metrology. The potential benefits extend to downstream tasks,\nwhich can be significantly improved by simply plugging in our model. For\nexample, our model relieves the scale drift issues of monocular-SLAM (Fig. 1),\nleading to high-quality metric scale dense mapping. The code is available at\nhttps://github.com/YvanYin/Metric3D.",
        "authors": [
            "Wei Yin",
            "Chi Zhang",
            "Hao Chen",
            "Zhipeng Cai",
            "Gang Yu",
            "Kaixuan Wang",
            "Xiaozhi Chen",
            "Chunhua Shen"
        ]
    },
    {
        "title": "RANA: Relightable Articulated Neural Avatars",
        "url": "http://arxiv.org/abs/2212.03237",
        "abstract": "We propose RANA, a relightable and articulated neural avatar for the\nphotorealistic synthesis of humans under arbitrary viewpoints, body poses, and\nlighting. We only require a short video clip of the person to create the avatar\nand assume no knowledge about the lighting environment. We present a novel\nframework to model humans while disentangling their geometry, texture, and also\nlighting environment from monocular RGB videos. To simplify this otherwise\nill-posed task we first estimate the coarse geometry and texture of the person\nvia SMPL+D model fitting and then learn an articulated neural representation\nfor photorealistic image generation. RANA first generates the normal and albedo\nmaps of the person in any given target body pose and then uses spherical\nharmonics lighting to generate the shaded image in the target lighting\nenvironment. We also propose to pretrain RANA using synthetic images and\ndemonstrate that it leads to better disentanglement between geometry and\ntexture while also improving robustness to novel body poses. Finally, we also\npresent a new photorealistic synthetic dataset, Relighting Humans, to\nquantitatively evaluate the performance of the proposed approach.",
        "authors": [
            "Umar Iqbal",
            "Akin Caliskan",
            "Koki Nagano",
            "Sameh Khamis",
            "Pavlo Molchanov",
            "Jan Kautz"
        ]
    },
    {
        "title": "Memory-and-Anticipation Transformer for Online Action Understanding",
        "url": "http://arxiv.org/abs/2308.07893",
        "abstract": "Most existing forecasting systems are memory-based methods, which attempt to\nmimic human forecasting ability by employing various memory mechanisms and have\nprogressed in temporal modeling for memory dependency. Nevertheless, an obvious\nweakness of this paradigm is that it can only model limited historical\ndependence and can not transcend the past. In this paper, we rethink the\ntemporal dependence of event evolution and propose a novel\nmemory-anticipation-based paradigm to model an entire temporal structure,\nincluding the past, present, and future. Based on this idea, we present\nMemory-and-Anticipation Transformer (MAT), a memory-anticipation-based\napproach, to address the online action detection and anticipation tasks. In\naddition, owing to the inherent superiority of MAT, it can process online\naction detection and anticipation tasks in a unified manner. The proposed MAT\nmodel is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and\nEPIC-Kitchens-100, for online action detection and anticipation tasks, and it\nsignificantly outperforms all existing methods. Code is available at\nhttps://github.com/Echo0125/Memory-and-Anticipation-Transformer.",
        "authors": [
            "Jiahao Wang",
            "Guo Chen",
            "Yifei Huang",
            "Limin Wang",
            "Tong Lu"
        ]
    },
    {
        "title": "Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search",
        "url": "http://arxiv.org/abs/2302.12986",
        "abstract": "Weakly supervised person search aims to jointly detect and match persons with\nonly bounding box annotations. Existing approaches typically focus on improving\nthe features by exploring relations of persons. However, scale variation\nproblem is a more severe obstacle and under-studied that a person often owns\nimages with different scales (resolutions). On the one hand, small-scale images\ncontain less information of a person, thus affecting the accuracy of the\ngenerated pseudo labels. On the other hand, the similarity of cross-scale\nimages is often smaller than that of images with the same scale for a person,\nwhich will increase the difficulty of matching. In this paper, we address this\nproblem by proposing a novel one-step framework, named Self-similarity driven\nScale-invariant Learning (SSL). Scale invariance can be explored based on the\nself-similarity prior that it shows the same statistical properties of an image\nat different scales. To this end, we introduce a Multi-scale Exemplar Branch to\nguide the network in concentrating on the foreground and learning\nscale-invariant features by hard exemplars mining. To enhance the\ndiscriminative power of the features in an unsupervised manner, we introduce a\ndynamic multi-label prediction which progressively seeks true labels for\ntraining. It is adaptable to different types of unlabeled data and serves as a\ncompensation for clustering based strategy. Experiments on PRW and CUHK-SYSU\ndatabases demonstrate the effectiveness of our method.",
        "authors": [
            "Benzhi Wang",
            "Yang Yang",
            "Jinlin Wu",
            "Guo-jun Qi",
            "Zhen Lei"
        ]
    },
    {
        "title": "MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions",
        "url": "http://arxiv.org/abs/2307.10008",
        "abstract": "Audio-driven portrait animation aims to synthesize portrait videos that are\nconditioned by given audio. Animating high-fidelity and multimodal video\nportraits has a variety of applications. Previous methods have attempted to\ncapture different motion modes and generate high-fidelity portrait videos by\ntraining different models or sampling signals from given videos. However,\nlacking correlation learning between lip-sync and other movements (e.g., head\npose/eye blinking) usually leads to unnatural results. In this paper, we\npropose a unified system for multi-person, diverse, and high-fidelity talking\nportrait generation. Our method contains three stages, i.e., 1) Mapping-Once\nnetwork with Dual Attentions (MODA) generates talking representation from given\naudio. In MODA, we design a dual-attention module to encode accurate mouth\nmovements and diverse modalities. 2) Facial composer network generates dense\nand detailed face landmarks, and 3) temporal-guided renderer syntheses stable\nvideos. Extensive evaluations demonstrate that the proposed system produces\nmore natural and realistic video portraits compared to previous methods.",
        "authors": [
            "Yunfei Liu",
            "Lijian Lin",
            "Fei Yu",
            "Changyin Zhou",
            "Yu Li"
        ]
    },
    {
        "title": "Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling",
        "url": "http://arxiv.org/abs/2308.08855",
        "abstract": "To bridge the physical and virtual worlds for rapidly developed VR/AR\napplications, the ability to realistically drive 3D full-body avatars is of\ngreat significance. Although real-time body tracking with only the head-mounted\ndisplays (HMDs) and hand controllers is heavily under-constrained, a carefully\ndesigned end-to-end neural network is of great potential to solve the problem\nby learning from large-scale motion data. To this end, we propose a two-stage\nframework that can obtain accurate and smooth full-body motions with the three\ntracking signals of head and hands only. Our framework explicitly models the\njoint-level features in the first stage and utilizes them as spatiotemporal\ntokens for alternating spatial and temporal transformer blocks to capture\njoint-level correlations in the second stage. Furthermore, we design a set of\nloss terms to constrain the task of a high degree of freedom, such that we can\nexploit the potential of our joint-level modeling. With extensive experiments\non the AMASS motion dataset and real-captured data, we validate the\neffectiveness of our designs and show our proposed method can achieve more\naccurate and smooth motion compared to existing approaches.",
        "authors": [
            "Xiaozheng Zheng",
            "Zhuo Su",
            "Chao Wen",
            "Zhou Xue",
            "Xiaojie Jin"
        ]
    },
    {
        "title": "MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces",
        "url": "http://arxiv.org/abs/2309.08113",
        "abstract": "Due to their highly structured characteristics, faces are easier to recover\nthan natural scenes for blind image super-resolution. Therefore, we can extract\nthe degradation representation of an image from the low-quality and recovered\nface pairs. Using the degradation representation, realistic low-quality images\ncan then be synthesized to fine-tune the super-resolution model for the\nreal-world low-quality image. However, such a procedure is time-consuming and\nlaborious, and the gaps between recovered faces and the ground-truths further\nincrease the optimization uncertainty. To facilitate efficient model adaptation\ntowards image-specific degradations, we propose a method dubbed MetaF2N, which\nleverages the contained Faces to fine-tune model parameters for adapting to the\nwhole Natural image in a Meta-learning framework. The degradation extraction\nand low-quality image synthesis steps are thus circumvented in our MetaF2N, and\nit requires only one fine-tuning step to get decent performance. Considering\nthe gaps between the recovered faces and ground-truths, we further deploy a\nMaskNet for adaptively predicting loss weights at different positions to reduce\nthe impact of low-confidence areas. To evaluate our proposed MetaF2N, we have\ncollected a real-world low-quality dataset with one or multiple faces in each\nimage, and our MetaF2N achieves superior performance on both synthetic and\nreal-world datasets. Source code, pre-trained models, and collected datasets\nare available at https://github.com/yinzhicun/MetaF2N.",
        "authors": [
            "Zhicun Yin",
            "Ming Liu",
            "Xiaoming Li",
            "Hui Yang",
            "Longan Xiao",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "Lighting up NeRF via Unsupervised Decomposition and Enhancement",
        "url": "http://arxiv.org/abs/2307.10664",
        "abstract": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novel\nviews, given a set of images and the corresponding camera poses of a scene.\nHowever, images photographed from a low-light scene can hardly be used to train\na NeRF model to produce high-quality results, due to their low pixel\nintensities, heavy noise, and color distortion. Combining existing low-light\nimage enhancement methods with NeRF methods also does not work well due to the\nview inconsistency caused by the individual 2D enhancement process. In this\npaper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to\nenhance the scene representation and synthesize normal-light novel views\ndirectly from sRGB low-light images in an unsupervised manner. The core of our\napproach is a decomposition of radiance field learning, which allows us to\nenhance the illumination, reduce noise and correct the distorted colors jointly\nwith the NeRF optimization process. Our method is able to produce novel view\nimages with proper lighting and vivid colors and details, given a collection of\ncamera-finished low dynamic range (8-bits/channel) images from a low-light\nscene. Experiments demonstrate that our method outperforms existing low-light\nenhancement methods and NeRF methods.",
        "authors": [
            "Haoyuan Wang",
            "Xiaogang Xu",
            "Ke Xu",
            "Rynson WH. Lau"
        ]
    },
    {
        "title": "ViM: Vision Middleware for Unified Downstream Transferring",
        "url": "http://arxiv.org/abs/2303.06911",
        "abstract": "Foundation models are pre-trained on massive data and transferred to\ndownstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a\nnew learning paradigm that targets unified transferring from a single\nfoundation model to a variety of downstream tasks. ViM consists of a zoo of\nlightweight plug-in modules, each of which is independently learned on a\nmidstream dataset with a shared frozen backbone. Downstream tasks can then\nbenefit from an adequate aggregation of the module zoo thanks to the rich\nknowledge inherited from midstream tasks. There are three major advantages of\nsuch a design. From the efficiency aspect, the upstream backbone can be trained\nonly once and reused for all downstream tasks without tuning. From the\nscalability aspect, we can easily append additional modules to ViM with no\ninfluence on existing modules. From the performance aspect, ViM can include as\nmany midstream tasks as possible, narrowing the task gap between upstream and\ndownstream. Considering these benefits, we believe that ViM, which the\ncommunity could maintain and develop together, would serve as a powerful tool\nto assist foundation models.",
        "authors": [
            "Yutong Feng",
            "Biao Gong",
            "Jianwen Jiang",
            "Yiliang Lv",
            "Yujun Shen",
            "Deli Zhao",
            "Jingren Zhou"
        ]
    },
    {
        "title": "DIRE for Diffusion-Generated Image Detection",
        "url": "http://arxiv.org/abs/2303.09295",
        "abstract": "Diffusion models have shown remarkable success in visual synthesis, but have\nalso raised concerns about potential abuse for malicious purposes. In this\npaper, we seek to build a detector for telling apart real images from\ndiffusion-generated images. We find that existing detectors struggle to detect\nimages generated by diffusion models, even if we include generated images from\na specific diffusion model in their training data. To address this issue, we\npropose a novel image representation called DIffusion Reconstruction Error\n(DIRE), which measures the error between an input image and its reconstruction\ncounterpart by a pre-trained diffusion model. We observe that\ndiffusion-generated images can be approximately reconstructed by a diffusion\nmodel while real images cannot. It provides a hint that DIRE can serve as a\nbridge to distinguish generated and real images. DIRE provides an effective way\nto detect images generated by most diffusion models, and it is general for\ndetecting generated images from unseen diffusion models and robust to various\nperturbations. Furthermore, we establish a comprehensive diffusion-generated\nbenchmark including images generated by eight diffusion models to evaluate the\nperformance of diffusion-generated image detectors. Extensive experiments on\nour collected benchmark demonstrate that DIRE exhibits superiority over\nprevious generated-image detectors. The code and dataset are available at\nhttps://github.com/ZhendongWang6/DIRE.",
        "authors": [
            "Zhendong Wang",
            "Jianmin Bao",
            "Wengang Zhou",
            "Weilun Wang",
            "Hezhen Hu",
            "Hong Chen",
            "Houqiang Li"
        ]
    },
    {
        "title": "Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction",
        "url": "http://arxiv.org/abs/2307.09004",
        "abstract": "Ordinal regression refers to classifying object instances into ordinal\ncategories. It has been widely studied in many scenarios, such as medical\ndisease grading, movie rating, etc. Known methods focused only on learning\ninter-class ordinal relationships, but still incur limitations in\ndistinguishing adjacent categories thus far. In this paper, we propose a simple\nsequence prediction framework for ordinal regression called Ord2Seq, which, for\nthe first time, transforms each ordinal category label into a special label\nsequence and thus regards an ordinal regression task as a sequence prediction\nprocess. In this way, we decompose an ordinal regression task into a series of\nrecursive binary classification steps, so as to subtly distinguish adjacent\ncategories. Comprehensive experiments show the effectiveness of distinguishing\nadjacent categories for performance improvement and our new approach exceeds\nstate-of-the-art performances in four different scenarios. Codes are available\nat https://github.com/wjh892521292/Ord2Seq.",
        "authors": [
            "Jinhong Wang",
            "Yi Cheng",
            "Jintai Chen",
            "Tingting Chen",
            "Danny Chen",
            "Jian Wu"
        ]
    },
    {
        "title": "Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video",
        "url": "http://arxiv.org/abs/2308.10305",
        "abstract": "Despite significant progress in single image-based 3D human mesh recovery,\naccurately and smoothly recovering 3D human motion from a video remains\nchallenging. Existing video-based methods generally recover human mesh by\nestimating the complex pose and shape parameters from coupled image features,\nwhose high complexity and low representation ability often result in\ninconsistent pose motion and limited shape patterns. To alleviate this issue,\nwe introduce 3D pose as the intermediary and propose a Pose and Mesh\nCo-Evolution network (PMCE) that decouples this task into two parts: 1)\nvideo-based 3D human pose estimation and 2) mesh vertices regression from the\nestimated 3D pose and temporal image feature. Specifically, we propose a\ntwo-stream encoder that estimates mid-frame 3D pose and extracts a temporal\nimage feature from the input image sequence. In addition, we design a\nco-evolution decoder that performs pose and mesh interactions with the\nimage-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the\nhuman body shape. Extensive experiments demonstrate that the proposed PMCE\noutperforms previous state-of-the-art methods in terms of both per-frame\naccuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,\nand MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.",
        "authors": [
            "Yingxuan You",
            "Hong Liu",
            "Ti Wang",
            "Wenhao Li",
            "Runwei Ding",
            "Xia Li"
        ]
    },
    {
        "title": "Controllable Visual-Tactile Synthesis",
        "url": "http://arxiv.org/abs/2305.03051",
        "abstract": "Deep generative models have various content creation applications such as\ngraphic design, e-commerce, and virtual Try-on. However, current works mainly\nfocus on synthesizing realistic visual outputs, often ignoring other sensory\nmodalities, such as touch, which limits physical interaction with users. In\nthis work, we leverage deep generative models to create a multi-sensory\nexperience where users can touch and see the synthesized object when sliding\ntheir fingers on a haptic surface. The main challenges lie in the significant\nscale discrepancy between vision and touch sensing and the lack of explicit\nmapping from touch sensing data to a haptic rendering device. To bridge this\ngap, we collect high-resolution tactile data with a GelSight sensor and create\na new visuotactile clothing dataset. We then develop a conditional generative\nmodel that synthesizes both visual and tactile outputs from a single sketch. We\nevaluate our method regarding image quality and tactile rendering accuracy.\nFinally, we introduce a pipeline to render high-quality visual and tactile\noutputs on an electroadhesion-based haptic device for an immersive experience,\nallowing for challenging materials and editable sketch inputs.",
        "authors": [
            "Ruihan Gao",
            "Wenzhen Yuan",
            "Jun-Yan Zhu"
        ]
    },
    {
        "title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?",
        "url": "http://arxiv.org/abs/2309.06891",
        "abstract": "Convolutional networks and vision transformers have different forms of\npairwise interactions, pooling across layers and pooling at the end of the\nnetwork. Does the latter really need to be different? As a by-product of\npooling, vision transformers provide spatial attention for free, but this is\nmost often of low quality unless self-supervised, which is not well studied. Is\nsupervision really the problem?\n  In this work, we develop a generic pooling framework and then we formulate a\nnumber of existing methods as instantiations. By discussing the properties of\neach group of methods, we derive SimPool, a simple attention-based pooling\nmechanism as a replacement of the default one for both convolutional and\ntransformer encoders. We find that, whether supervised or self-supervised, this\nimproves performance on pre-training and downstream tasks and provides\nattention maps delineating object boundaries in all cases. One could thus call\nSimPool universal. To our knowledge, we are the first to obtain attention maps\nin supervised transformers of at least as good quality as self-supervised,\nwithout explicit losses or modifying the architecture. Code at:\nhttps://github.com/billpsomas/simpool.",
        "authors": [
            "Bill Psomas",
            "Ioannis Kakogeorgiou",
            "Konstantinos Karantzalos",
            "Yannis Avrithis"
        ]
    },
    {
        "title": "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling",
        "url": "http://arxiv.org/abs/2303.17368",
        "abstract": "Synthetic data has emerged as a promising source for 3D human research as it\noffers low-cost access to large-scale human datasets. To advance the diversity\nand annotation quality of human models, we introduce a new synthetic dataset,\nSynBody, with three appealing features: 1) a clothed parametric human model\nthat can generate a diverse range of subjects; 2) the layered human\nrepresentation that naturally offers high-quality 3D annotations to support\nmultiple tasks; 3) a scalable system for producing realistic data to facilitate\nreal-world tasks. The dataset comprises 1.2M images with corresponding accurate\n3D annotations, covering 10,000 human body models, 1,187 actions, and various\nviewpoints. The dataset includes two subsets for human pose and shape\nestimation as well as human neural rendering. Extensive experiments on SynBody\nindicate that it substantially enhances both SMPL and SMPL-X estimation.\nFurthermore, the incorporation of layered annotations offers a valuable\ntraining resource for investigating the Human Neural Radiance Fields (NeRF).",
        "authors": [
            "Zhitao Yang",
            "Zhongang Cai",
            "Haiyi Mei",
            "Shuai Liu",
            "Zhaoxi Chen",
            "Weiye Xiao",
            "Yukun Wei",
            "Zhongfei Qing",
            "Chen Wei",
            "Bo Dai",
            "Wayne Wu",
            "Chen Qian",
            "Dahua Lin",
            "Ziwei Liu",
            "Lei Yang"
        ]
    },
    {
        "title": "LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models",
        "url": "http://arxiv.org/abs/2309.01155",
        "abstract": "Prompt engineering is a powerful tool used to enhance the performance of\npre-trained models on downstream tasks. For example, providing the prompt\n\"Let's think step by step\" improved GPT-3's reasoning accuracy to 63% on\nMutiArith while prompting \"a photo of\" filled with a class name enables CLIP to\nachieve $80$\\% zero-shot accuracy on ImageNet. While previous research has\nexplored prompt learning for the visual modality, analyzing what constitutes a\ngood visual prompt specifically for image recognition is limited. In addition,\nexisting visual prompt tuning methods' generalization ability is worse than\ntext-only prompting tuning. This paper explores our key insight: synthetic text\nimages are good visual prompts for vision-language models! To achieve that, we\npropose our LoGoPrompt, which reformulates the classification objective to the\nvisual prompt selection and addresses the chicken-and-egg challenge of first\nadding synthetic text images as class-wise visual prompts or predicting the\nclass first. Without any trainable visual prompt parameters, experimental\nresults on 16 datasets demonstrate that our method consistently outperforms\nstate-of-the-art methods in few-shot learning, base-to-new generalization, and\ndomain generalization.",
        "authors": [
            "Cheng Shi",
            "Sibei Yang"
        ]
    },
    {
        "title": "FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision",
        "url": "http://arxiv.org/abs/2308.03594",
        "abstract": "Extracting useful visual cues for the downstream tasks is especially\nchallenging under low-light vision. Prior works create enhanced representations\nby either correlating visual quality with machine perception or designing\nillumination-degrading transformation methods that require pre-training on\nsynthetic datasets. We argue that optimizing enhanced image representation\npertaining to the loss of the downstream task can result in more expressive\nrepresentations. Therefore, in this work, we propose a novel module,\nFeatEnHancer, that hierarchically combines multiscale features using\nmultiheaded attention guided by task-related loss function to create suitable\nrepresentations. Furthermore, our intra-scale enhancement improves the quality\nof features extracted at each scale or level, as well as combines features from\ndifferent scales in a way that reflects their relative importance for the task\nat hand. FeatEnHancer is a general-purpose plug-and-play module and can be\nincorporated into any low-light vision pipeline. We show with extensive\nexperimentation that the enhanced representation produced with FeatEnHancer\nsignificantly and consistently improves results in several low-light vision\ntasks, including dark object detection (+5.7 mAP on ExDark), face detection\n(+1.5 mAPon DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC ),\nand video object detection (+1.8 mAP on DarkVision), highlighting the\neffectiveness of enhancing hierarchical features under low-light vision.",
        "authors": [
            "Khurram Azeem Hashmi",
            "Goutham Kallempudi",
            "Didier Stricker",
            "Muhammamd Zeshan Afzal"
        ]
    },
    {
        "title": "SOAR: Scene-debiasing Open-set Action Recognition",
        "url": "http://arxiv.org/abs/2309.01265",
        "abstract": "Deep learning models have a risk of utilizing spurious clues to make\npredictions, such as recognizing actions based on the background scene. This\nissue can severely degrade the open-set action recognition performance when the\ntesting samples have different scene distributions from the training samples.\nTo mitigate this problem, we propose a novel method, called Scene-debiasing\nOpen-set Action Recognition (SOAR), which features an adversarial scene\nreconstruction module and an adaptive adversarial scene classification module.\nThe former prevents the decoder from reconstructing the video background given\nvideo features, and thus helps reduce the background information in feature\nlearning. The latter aims to confuse scene type classification given video\nfeatures, with a specific emphasis on the action foreground, and helps to learn\nscene-invariant information. In addition, we design an experiment to quantify\nthe scene bias. The results indicate that the current open-set action\nrecognizers are biased toward the scene, and our proposed SOAR method better\nmitigates such bias. Furthermore, our extensive experiments demonstrate that\nour method outperforms state-of-the-art methods, and the ablation studies\nconfirm the effectiveness of our proposed modules.",
        "authors": [
            "Yuanhao Zhai",
            "Ziyi Liu",
            "Zhenyu Wu",
            "Yi Wu",
            "Chunluan Zhou",
            "David Doermann",
            "Junsong Yuan",
            "Gang Hua"
        ]
    },
    {
        "title": "FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis",
        "url": "http://arxiv.org/abs/2306.17723",
        "abstract": "Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis\nwith its remarkable quality of rendered images and simple architecture.\nAlthough NeRF has been developed in various directions improving continuously\nits performance, the necessity of a dense set of multi-view images still exists\nas a stumbling block to progress for practical application. In this work, we\npropose FlipNeRF, a novel regularization method for few-shot novel view\nsynthesis by utilizing our proposed flipped reflection rays. The flipped\nreflection rays are explicitly derived from the input ray directions and\nestimated normal vectors, and play a role of effective additional training rays\nwhile enabling to estimate more accurate surface normals and learn the 3D\ngeometry effectively. Since the surface normal and the scene depth are both\nderived from the estimated densities along a ray, the accurate surface normal\nleads to more exact depth estimation, which is a key factor for few-shot novel\nview synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss\nand Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more\nreliable outputs with reducing floating artifacts effectively across the\ndifferent scene structures, and enhance the feature-level consistency between\nthe pair of the rays cast toward the photo-consistent pixels without any\nadditional feature extractor, respectively. Our FlipNeRF achieves the SOTA\nperformance on the multiple benchmarks across all the scenarios.",
        "authors": [
            "Seunghyeon Seo",
            "Yeonjin Chang",
            "Nojun Kwak"
        ]
    },
    {
        "title": "Discovering Spatio-Temporal Rationales for Video Question Answering",
        "url": "http://arxiv.org/abs/2307.12058",
        "abstract": "This paper strives to solve complex video question answering (VideoQA) which\nfeatures long video containing multiple objects and events at different time.\nTo tackle the challenge, we highlight the importance of identifying\nquestion-critical temporal moments and spatial objects from the vast amount of\nvideo content. Towards this, we propose a Spatio-Temporal Rationalization\n(STR), a differentiable selection module that adaptively collects\nquestion-critical moments and objects using cross-modal interaction. The\ndiscovered video moments and objects are then served as grounded rationales to\nsupport answer reasoning. Based on STR, we further propose TranSTR, a\nTransformer-style neural network architecture that takes STR as the core and\nadditionally underscores a novel answer interaction mechanism to coordinate STR\nfor answer decoding. Experiments on four datasets show that TranSTR achieves\nnew state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which\nfeature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\\%\nand 6.8\\%, respectively. We then conduct extensive studies to verify the\nimportance of STR as well as the proposed answer interaction mechanism. With\nthe success of TranSTR and our comprehensive analysis, we hope this work can\nspark more future efforts in complex VideoQA. Code will be released at\nhttps://github.com/yl3800/TranSTR.",
        "authors": [
            "Yicong Li",
            "Junbin Xiao",
            "Chun Feng",
            "Xiang Wang",
            "Tat-Seng Chua"
        ]
    },
    {
        "title": "Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution",
        "url": "http://arxiv.org/abs/2303.09650",
        "abstract": "Image super-resolution (SR) has witnessed extensive neural network designs\nfrom CNN to transformer architectures. However, prevailing SR models suffer\nfrom prohibitive memory footprint and intensive computations, which limits\nfurther deployment on edge devices. This work investigates the potential of\nnetwork pruning for super-resolution to take advantage of off-the-shelf network\ndesigns and reduce the underlying computational overhead. Two main challenges\nremain in applying pruning methods for SR. First, the widely-used filter\npruning technique reflects limited granularity and restricted adaptability to\ndiverse network structures. Second, existing pruning methods generally operate\nupon a pre-trained network for the sparse structure determination, hard to get\nrid of dense model training in the traditional SR paradigm. To address these\nchallenges, we adopt unstructured pruning with sparse models directly trained\nfrom scratch. Specifically, we propose a novel Iterative Soft\nShrinkage-Percentage (ISS-P) method by optimizing the sparse structure of a\nrandomly initialized network at each iteration and tweaking unimportant weights\nwith a small amount proportional to the magnitude scale on-the-fly. We observe\nthat the proposed ISS-P can dynamically learn sparse structures adapting to the\noptimization process and preserve the sparse model's trainability by yielding a\nmore regularized gradient throughput. Experiments on benchmark datasets\ndemonstrate the effectiveness of the proposed ISS-P over diverse network\narchitectures. Code is available at\nhttps://github.com/Jiamian-Wang/Iterative-Soft-Shrinkage-SR",
        "authors": [
            "Jiamian Wang",
            "Huan Wang",
            "Yulun Zhang",
            "Yun Fu",
            "Zhiqiang Tao"
        ]
    },
    {
        "title": "G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory",
        "url": "http://arxiv.org/abs/2307.14277",
        "abstract": "The recent video grounding works attempt to introduce vanilla contrastive\nlearning into video grounding. However, we claim that this naive solution is\nsuboptimal. Contrastive learning requires two key properties: (1)\n\\emph{alignment} of features of similar samples, and (2) \\emph{uniformity} of\nthe induced distribution of the normalized features on the hypersphere. Due to\ntwo annoying issues in video grounding: (1) the co-existence of some visual\nentities in both ground truth and other moments, \\ie semantic overlapping; (2)\nonly a few moments in the video are annotated, \\ie sparse annotation dilemma,\nvanilla contrastive learning is unable to model the correlations between\ntemporally distant moments and learned inconsistent video representations. Both\ncharacteristics lead to vanilla contrastive learning being unsuitable for video\ngrounding. In this paper, we introduce Geodesic and Game Localization (G2L), a\nsemantically aligned and uniform video grounding framework via geodesic and\ngame theory. We quantify the correlations among moments leveraging the geodesic\ndistance that guides the model to learn the correct cross-modal\nrepresentations. Furthermore, from the novel perspective of game theory, we\npropose semantic Shapley interaction based on geodesic distance sampling to\nlearn fine-grained semantic alignment in similar moments. Experiments on three\nbenchmarks demonstrate the effectiveness of our method.",
        "authors": [
            "Hongxiang Li",
            "Meng Cao",
            "Xuxin Cheng",
            "Yaowei Li",
            "Zhihong Zhu",
            "Yuexian Zou"
        ]
    },
    {
        "title": "TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation",
        "url": "http://arxiv.org/abs/2303.06937",
        "abstract": "This paper focuses on an under-explored yet important problem: Federated\nClass-Continual Learning (FCCL), where new classes are dynamically added in\nfederated learning. Existing FCCL works suffer from various limitations, such\nas requiring additional datasets or storing the private data from previous\ntasks. In response, we first demonstrate that non-IID data exacerbates\ncatastrophic forgetting issue in FL. Then we propose a novel method called\nTARGET (federat\\textbf{T}ed cl\\textbf{A}ss-continual lea\\textbf{R}nin\\textbf{G}\nvia \\textbf{E}xemplar-free dis\\textbf{T}illation), which alleviates\ncatastrophic forgetting in FCCL while preserving client data privacy. Our\nproposed method leverages the previously trained global model to transfer\nknowledge of old tasks to the current task at the model level. Moreover, a\ngenerator is trained to produce synthetic data to simulate the global\ndistribution of data on each client at the data level. Compared to previous\nFCCL methods, TARGET does not require any additional datasets or storing real\ndata from previous tasks, which makes it ideal for data-sensitive scenarios.",
        "authors": [
            "Jie Zhang",
            "Chen Chen",
            "Weiming Zhuang",
            "Lingjuan Lv"
        ]
    },
    {
        "title": "FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory",
        "url": "http://arxiv.org/abs/2308.10170",
        "abstract": "Multi-turn textual feedback-based fashion image retrieval focuses on a\nreal-world setting, where users can iteratively provide information to refine\nretrieval results until they find an item that fits all their requirements. In\nthis work, we present a novel memory-based method, called FashionNTM, for such\na multi-turn system. Our framework incorporates a new Cascaded Memory Neural\nTuring Machine (CM-NTM) approach for implicit state management, thereby\nlearning to integrate information across all past turns to retrieve new images,\nfor a given turn. Unlike vanilla Neural Turing Machine (NTM), our CM-NTM\noperates on multiple inputs, which interact with their respective memories via\nindividual read and write heads, to learn complex relationships. Extensive\nevaluation results show that our proposed method outperforms the previous\nstate-of-the-art algorithm by 50.5%, on Multi-turn FashionIQ -- the only\nexisting multi-turn fashion dataset currently, in addition to having a relative\nimprovement of 12.6% on Multi-turn Shoes -- an extension of the single-turn\nShoes dataset that we created in this work. Further analysis of the model in a\nreal-world interactive setting demonstrates two important capabilities of our\nmodel -- memory retention across turns, and agnosticity to turn order for\nnon-contradictory feedback. Finally, user study results show that images\nretrieved by FashionNTM were favored by 83.1% over other multi-turn models.\nProject page: https://sites.google.com/eng.ucsd.edu/fashionntm",
        "authors": [
            "Anwesan Pal",
            "Sahil Wadhwa",
            "Ayush Jaiswal",
            "Xu Zhang",
            "Yue Wu",
            "Rakesh Chada",
            "Pradeep Natarajan",
            "Henrik I. Christensen"
        ]
    },
    {
        "title": "MolGrapher: Graph-based Visual Recognition of Chemical Structures",
        "url": "http://arxiv.org/abs/2308.12234",
        "abstract": "The automatic analysis of chemical literature has immense potential to\naccelerate the discovery of new materials and drugs. Much of the critical\ninformation in patent documents and scientific articles is contained in\nfigures, depicting the molecule structures. However, automatically parsing the\nexact chemical structure is a formidable challenge, due to the amount of\ndetailed information, the diversity of drawing styles, and the need for\ntraining data. In this work, we introduce MolGrapher to recognize chemical\nstructures visually. First, a deep keypoint detector detects the atoms. Second,\nwe treat all candidate atoms and bonds as nodes and put them in a graph. This\nconstruct allows a natural graph representation of the molecule. Last, we\nclassify atom and bond nodes in the graph with a Graph Neural Network. To\naddress the lack of real training data, we propose a synthetic data generation\npipeline producing diverse and realistic results. In addition, we introduce a\nlarge-scale benchmark of annotated real molecule images, USPTO-30K, to spur\nresearch on this critical topic. Extensive experiments on five datasets show\nthat our approach significantly outperforms classical and learning-based\nmethods in most settings. Code, models, and datasets are available.",
        "authors": [
            "Lucas Morin",
            "Martin Danelljan",
            "Maria Isabel Agea",
            "Ahmed Nassar",
            "Valery Weber",
            "Ingmar Meijer",
            "Peter Staar",
            "Fisher Yu"
        ]
    },
    {
        "title": "SAMPLING: Scene-adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image",
        "url": "http://arxiv.org/abs/2309.06323",
        "abstract": "Recent novel view synthesis methods obtain promising results for relatively\nsmall scenes, e.g., indoor environments and scenes with a few objects, but tend\nto fail for unbounded outdoor scenes with a single image as input. In this\npaper, we introduce SAMPLING, a Scene-adaptive Hierarchical Multiplane Images\nRepresentation for Novel View Synthesis from a Single Image based on improved\nmultiplane images (MPI). Observing that depth distribution varies significantly\nfor unbounded outdoor scenes, we employ an adaptive-bins strategy for MPI to\narrange planes in accordance with each scene image. To represent intricate\ngeometry and multi-scale details, we further introduce a hierarchical\nrefinement branch, which results in high-quality synthesized novel views. Our\nmethod demonstrates considerable performance gains in synthesizing large-scale\nunbounded outdoor scenes using a single image on the KITTI dataset and\ngeneralizes well to the unseen Tanks and Temples dataset.The code and models\nwill soon be made available.",
        "authors": [
            "Xiaoyu Zhou",
            "Zhiwei Lin",
            "Xiaojun Shan",
            "Yongtao Wang",
            "Deqing Sun",
            "Ming-Hsuan Yang"
        ]
    },
    {
        "title": "DiffV2S: Diffusion-Based Video-to-Speech Synthesis with Vision-Guided Speaker Embedding",
        "url": "http://arxiv.org/abs/2308.07787",
        "abstract": "Recent research has demonstrated impressive results in video-to-speech\nsynthesis which involves reconstructing speech solely from visual input.\nHowever, previous works have struggled to accurately synthesize speech due to a\nlack of sufficient guidance for the model to infer the correct content with the\nappropriate sound. To resolve the issue, they have adopted an extra speaker\nembedding as a speaking style guidance from a reference auditory information.\nNevertheless, it is not always possible to obtain the audio information from\nthe corresponding video input, especially during the inference time. In this\npaper, we present a novel vision-guided speaker embedding extractor using a\nself-supervised pre-trained model and prompt tuning technique. In doing so, the\nrich speaker embedding information can be produced solely from input visual\ninformation, and the extra audio information is not necessary during the\ninference time. Using the extracted vision-guided speaker embedding\nrepresentations, we further develop a diffusion-based video-to-speech synthesis\nmodel, so called DiffV2S, conditioned on those speaker embeddings and the\nvisual representation extracted from the input video. The proposed DiffV2S not\nonly maintains phoneme details contained in the input video frames, but also\ncreates a highly intelligible mel-spectrogram in which the speaker identities\nof the multiple speakers are all preserved. Our experimental results show that\nDiffV2S achieves the state-of-the-art performance compared to the previous\nvideo-to-speech synthesis technique.",
        "authors": [
            "Jeongsoo Choi",
            "Joanna Hong",
            "Yong Man Ro"
        ]
    },
    {
        "title": "PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking",
        "url": "http://arxiv.org/abs/2307.15055",
        "abstract": "We introduce PointOdyssey, a large-scale synthetic dataset, and data\ngeneration framework, for the training and evaluation of long-term fine-grained\ntracking algorithms. Our goal is to advance the state-of-the-art by placing\nemphasis on long videos with naturalistic motion. Toward the goal of\nnaturalism, we animate deformable characters using real-world motion capture\ndata, we build 3D scenes to match the motion capture environments, and we\nrender camera viewpoints using trajectories mined via structure-from-motion on\nreal videos. We create combinatorial diversity by randomizing character\nappearance, motion profiles, materials, lighting, 3D assets, and atmospheric\neffects. Our dataset currently includes 104 videos, averaging 2,000 frames\nlong, with orders of magnitude more correspondence annotations than prior work.\nWe show that existing methods can be trained from scratch in our dataset and\noutperform the published variants. Finally, we introduce modifications to the\nPIPs point tracking method, greatly widening its temporal receptive field,\nwhich improves its performance on PointOdyssey as well as on two real-world\nbenchmarks. Our data and code are publicly available at:\nhttps://pointodyssey.com",
        "authors": [
            "Yang Zheng",
            "Adam W. Harley",
            "Bokui Shen",
            "Gordon Wetzstein",
            "Leonidas J. Guibas"
        ]
    },
    {
        "title": "The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining",
        "url": "http://arxiv.org/abs/2303.13496",
        "abstract": "This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and\nzero-shot transfer on Food-101 (96.2%). Our study reveals that model\ninitialization plays a significant role, even for web-scale pretraining with\nbillions of images, and our models are available publicly.",
        "authors": [
            "Mannat Singh",
            "Quentin Duval",
            "Kalyan Vasudev Alwala",
            "Haoqi Fan",
            "Vaibhav Aggarwal",
            "Aaron Adcock",
            "Armand Joulin",
            "Piotr Doll\u00e1r",
            "Christoph Feichtenhofer",
            "Ross Girshick",
            "Rohit Girdhar",
            "Ishan Misra"
        ]
    },
    {
        "title": "TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering",
        "url": "http://arxiv.org/abs/2307.12291",
        "abstract": "In this paper, we focus on the task of generalizable neural human rendering\nwhich trains conditional Neural Radiance Fields (NeRF) from multi-view videos\nof different characters. To handle the dynamic human motion, previous methods\nhave primarily used a SparseConvNet (SPC)-based human representation to process\nthe painted SMPL. However, such SPC-based representation i) optimizes under the\nvolatile observation space which leads to the pose-misalignment between\ntraining and inference stages, and ii) lacks the global relationships among\nhuman parts that is critical for handling the incomplete painted SMPL. Tackling\nthese issues, we present a brand-new framework named TransHuman, which learns\nthe painted SMPL under the canonical space and captures the global\nrelationships between human parts with transformers. Specifically, TransHuman\nis mainly composed of Transformer-based Human Encoding (TransHE), Deformable\nPartial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI).\nTransHE first processes the painted SMPL under the canonical space via\ntransformers for capturing the global relationships between human parts. Then,\nDPaRF binds each output token with a deformable radiance field for encoding the\nquery point under the observation space. Finally, the FDI is employed to\nfurther integrate fine-grained information from reference images. Extensive\nexperiments on ZJU-MoCap and H36M show that our TransHuman achieves a\nsignificantly new state-of-the-art performance with high efficiency. Project\npage: https://pansanity666.github.io/TransHuman/",
        "authors": [
            "Xiao Pan",
            "Zongxin Yang",
            "Jianxin Ma",
            "Chang Zhou",
            "Yi Yang"
        ]
    },
    {
        "title": "Random Sub-Samples Generation for Self-Supervised Real Image Denoising",
        "url": "http://arxiv.org/abs/2307.16825",
        "abstract": "With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.",
        "authors": [
            "Yizhong Pan",
            "Xiao Liu",
            "Xiangyu Liao",
            "Yuanzhouhan Cao",
            "Chao Ren"
        ]
    },
    {
        "title": "Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts",
        "url": "http://arxiv.org/abs/2306.07282",
        "abstract": "The visual classification performance of vision-language models such as CLIP\nhas been shown to benefit from additional semantic knowledge from large\nlanguage models (LLMs) such as GPT-3. In particular, averaging over\nLLM-generated class descriptors, e.g. \"waffle, which has a round shape\", can\nnotably improve generalization performance. In this work, we critically study\nthis behavior and propose WaffleCLIP, a framework for zero-shot visual\nclassification which simply replaces LLM-generated descriptors with random\ncharacter and word descriptors. Without querying external models, we achieve\ncomparable performance gains on a large number of visual classification tasks.\nThis allows WaffleCLIP to both serve as a low-cost alternative, as well as a\nsanity check for any future LLM-based vision-language model extensions. We\nconduct an extensive experimental study on the impact and shortcomings of\nadditional semantics introduced with LLM-generated descriptors, and showcase\nhow - if available - semantic context is better leveraged by querying LLMs for\nhigh-level concepts, which we show can be done to jointly resolve potential\nclass name ambiguities. Code is available here:\nhttps://github.com/ExplainableML/WaffleCLIP.",
        "authors": [
            "Karsten Roth",
            "Jae Myung Kim",
            "A. Sophia Koepke",
            "Oriol Vinyals",
            "Cordelia Schmid",
            "Zeynep Akata"
        ]
    },
    {
        "title": "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance",
        "url": "http://arxiv.org/abs/2309.12314",
        "abstract": "In this paper, we propose a novel cross-modal distillation method, called\nTinyCLIP, for large-scale language-image pre-trained models. The method\nintroduces two core techniques: affinity mimicking and weight inheritance.\nAffinity mimicking explores the interaction between modalities during\ndistillation, enabling student models to mimic teachers' behavior of learning\ncross-modal feature alignment in a visual-linguistic affinity space. Weight\ninheritance transmits the pre-trained weights from the teacher models to their\nstudent counterparts to improve distillation efficiency. Moreover, we extend\nthe method into a multi-stage progressive distillation to mitigate the loss of\ninformative weights during extreme compression. Comprehensive experiments\ndemonstrate the efficacy of TinyCLIP, showing that it can reduce the size of\nthe pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot\nperformance. While aiming for comparable performance, distillation with weight\ninheritance can speed up the training by 1.4 - 7.8 $\\times$ compared to\ntraining from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,\nachieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,\nsurpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%\nparameters. Finally, we demonstrate the good transferability of TinyCLIP in\nvarious downstream tasks. Code and models will be open-sourced at\nhttps://aka.ms/tinyclip.",
        "authors": [
            "Kan Wu",
            "Houwen Peng",
            "Zhenghong Zhou",
            "Bin Xiao",
            "Mengchen Liu",
            "Lu Yuan",
            "Hong Xuan",
            "Michael Valenzuela",
            " Xi",
            " Chen",
            "Xinggang Wang",
            "Hongyang Chao",
            "Han Hu"
        ]
    },
    {
        "title": "AG3D: Learning to Generate 3D Avatars from 2D Image Collections",
        "url": "http://arxiv.org/abs/2305.02312",
        "abstract": "While progress in 2D generative models of human appearance has been rapid,\nmany applications require 3D avatars that can be animated and rendered.\nUnfortunately, most existing methods for learning generative models of 3D\nhumans with diverse shape and appearance require 3D training data, which is\nlimited and expensive to acquire. The key to progress is hence to learn\ngenerative models of 3D avatars from abundant unstructured 2D image\ncollections. However, learning realistic and complete 3D appearance and\ngeometry in this under-constrained setting remains challenging, especially in\nthe presence of loose clothing such as dresses. In this paper, we propose a new\nadversarial generative model of realistic 3D people from 2D images. Our method\ncaptures shape and deformation of the body and loose clothing by adopting a\nholistic 3D generator and integrating an efficient and flexible articulation\nmodule. To improve realism, we train our model using multiple discriminators\nwhile also integrating geometric cues in the form of predicted 2D normal maps.\nWe experimentally find that our method outperforms previous 3D- and\narticulation-aware methods in terms of geometry and appearance. We validate the\neffectiveness of our model and the importance of each component via systematic\nablation studies.",
        "authors": [
            "Zijian Dong",
            "Xu Chen",
            "Jinlong Yang",
            "Michael J. Black",
            "Otmar Hilliges",
            "Andreas Geiger"
        ]
    },
    {
        "title": "KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection",
        "url": "http://arxiv.org/abs/2307.07942",
        "abstract": "Achieving a reliable LiDAR-based object detector in autonomous driving is\nparamount, but its success hinges on obtaining large amounts of precise 3D\nannotations. Active learning (AL) seeks to mitigate the annotation burden\nthrough algorithms that use fewer labels and can attain performance comparable\nto fully supervised learning. Although AL has shown promise, current approaches\nprioritize the selection of unlabeled point clouds with high uncertainty and/or\ndiversity, leading to the selection of more instances for labeling and reduced\ncomputational efficiency. In this paper, we resort to a novel kernel coding\nrate maximization (KECOR) strategy which aims to identify the most informative\npoint clouds to acquire labels through the lens of information theory. Greedy\nsearch is applied to seek desired point clouds that can maximize the minimal\nnumber of bits required to encode the latent features. To determine the\nuniqueness and informativeness of the selected samples from the model\nperspective, we construct a proxy network of the 3D detector head and compute\nthe outer product of Jacobians from all proxy layers to form the empirical\nneural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,\nSECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the\nclassification entropy maximization and well trade-off between detection\nperformance and the total number of bounding boxes selected for annotation.\nExtensive experiments conducted on two 3D benchmarks and a 2D detection dataset\nevidence the superiority and versatility of the proposed approach. Our results\nshow that approximately 44% box-level annotation costs and 26% computational\ntime are reduced compared to the state-of-the-art AL method, without\ncompromising detection performance.",
        "authors": [
            "Yadan Luo",
            "Zhuoxiao Chen",
            "Zhen Fang",
            "Zheng Zhang",
            "Zi Huang",
            "Mahsa Baktashmotlagh"
        ]
    },
    {
        "title": "Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-spectral Image Fusion",
        "url": "http://arxiv.org/abs/2308.16083",
        "abstract": "The success of deep neural networks for pan-sharpening is commonly in a form\nof black box, lacking transparency and interpretability. To alleviate this\nissue, we propose a novel model-driven deep unfolding framework with image\nreasoning prior tailored for the pan-sharpening task. Different from existing\nunfolding solutions that deliver the proximal operator networks as the\nuncertain and vague priors, our framework is motivated by the content reasoning\nability of masked autoencoders (MAE) with insightful designs. Specifically, the\npre-trained MAE with spatial masking strategy, acting as intrinsic reasoning\nprior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE\nwith spatial-spectral masking strategy is treated as the regularization term\nwithin loss function to constrain the spatial-spectral consistency. Such\ndesigns penetrate the image reasoning prior into deep unfolding networks while\nimproving its interpretability and representation capability. The uniqueness of\nour framework is that the holistic learning process is explicitly integrated\nwith the inherent physical mechanism underlying the pan-sharpening task.\nExtensive experiments on multiple satellite datasets demonstrate the\nsuperiority of our method over the existing state-of-the-art approaches. Code\nwill be released at \\url{https://manman1995.github.io/}.",
        "authors": [
            "Man Zhou",
            "Jie Huang",
            "Naishan Zheng",
            "Chongyi Li"
        ]
    },
    {
        "title": "Representation Disparity-aware Distillation for 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.10308",
        "abstract": "In this paper, we focus on developing knowledge distillation (KD) for compact\n3D detectors. We observe that off-the-shelf KD methods manifest their efficacy\nonly when the teacher model and student counterpart share similar intermediate\nfeature representations. This might explain why they are less effective in\nbuilding extreme-compact 3D detectors where significant representation\ndisparity arises due primarily to the intrinsic sparsity and irregularity in 3D\npoint clouds. This paper presents a novel representation disparity-aware\ndistillation (RDD) method to address the representation disparity issue and\nreduce performance gap between compact students and over-parameterized\nteachers. This is accomplished by building our RDD from an innovative\nperspective of information bottleneck (IB), which can effectively minimize the\ndisparity of proposal region pairs from student and teacher in features and\nlogits. Extensive experiments are performed to demonstrate the superiority of\nour RDD over existing KD methods. For example, our RDD increases mAP of\nCP-Voxel-S to 57.1% on nuScenes dataset, which even surpasses teacher\nperformance while taking up only 42% FLOPs.",
        "authors": [
            "Yanjing Li",
            "Sheng Xu",
            "Mingbao Lin",
            "Jihao Yin",
            "Baochang Zhang",
            "Xianbin Cao"
        ]
    },
    {
        "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects",
        "url": "http://arxiv.org/abs/2305.14345",
        "abstract": "Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.\nhttps://taeksuu.github.io/ncho/",
        "authors": [
            "Taeksoo Kim",
            "Shunsuke Saito",
            "Hanbyul Joo"
        ]
    },
    {
        "title": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation",
        "url": "http://arxiv.org/abs/2011.11233",
        "abstract": "Albeit being a prevalent architecture searching approach, differentiable\narchitecture search (DARTS) is largely hindered by its substantial memory cost\nsince the entire supernet resides in the memory. This is where the single-path\nDARTS comes in, which only chooses a single-path submodel at each step. While\nbeing memory-friendly, it also comes with low computational costs. Nonetheless,\nwe discover a critical issue of single-path DARTS that has not been primarily\nnoticed. Namely, it also suffers from severe performance collapse since too\nmany parameter-free operations like skip connections are derived, just like\nDARTS does. In this paper, we propose a new algorithm called RObustifying\nMemory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology\nsearch from the operation search to make searching and evaluation consistent.\nWe then adopt Gumbel-Top2 reparameterization and gradient accumulation to\nrobustify the unwieldy bi-level optimization. We verify ROME extensively across\n15 benchmarks to demonstrate its effectiveness and robustness.",
        "authors": [
            "Xiaoxing Wang",
            "Xiangxiang Chu",
            "Yuda Fan",
            "Zhexi Zhang",
            "Bo Zhang",
            "Xiaokang Yang",
            "Junchi Yan"
        ]
    },
    {
        "title": "3D-aware Image Generation using 2D Diffusion Models",
        "url": "http://arxiv.org/abs/2303.17905",
        "abstract": "In this paper, we introduce a novel 3D-aware image generation method that\nleverages 2D diffusion models. We formulate the 3D-aware image generation task\nas multiview 2D image set generation, and further to a sequential\nunconditional-conditional multiview image generation process. This allows us to\nutilize 2D diffusion models to boost the generative modeling power of the\nmethod. Additionally, we incorporate depth information from monocular depth\nestimators to construct the training data for the conditional diffusion model\nusing only still images. We train our method on a large-scale dataset, i.e.,\nImageNet, which is not addressed by previous methods. It produces high-quality\nimages that significantly outperform prior methods. Furthermore, our approach\nshowcases its capability to generate instances with large view angles, even\nthough the training images are diverse and unaligned, gathered from\n\"in-the-wild\" real-world environments.",
        "authors": [
            "Jianfeng Xiang",
            "Jiaolong Yang",
            "Binbin Huang",
            "Xin Tong"
        ]
    },
    {
        "title": "Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution",
        "url": "http://arxiv.org/abs/2302.08058",
        "abstract": "Exploiting spatial-angular correlation is crucial to light field (LF) image\nsuper-resolution (SR), but is highly challenging due to its non-local property\ncaused by the disparities among LF images. Although many deep neural networks\n(DNNs) have been developed for LF image SR and achieved continuously improved\nperformance, existing methods cannot well leverage the long-range\nspatial-angular correlation and thus suffer a significant performance drop when\nhandling scenes with large disparity variations. In this paper, we propose a\nsimple yet effective method to learn the non-local spatial-angular correlation\nfor LF image SR. In our method, we adopt the epipolar plane image (EPI)\nrepresentation to project the 4D spatial-angular correlation onto multiple 2D\nEPI planes, and then develop a Transformer network with repetitive\nself-attention operations to learn the spatial-angular correlation by modeling\nthe dependencies between each pair of EPI pixels. Our method can fully\nincorporate the information from all angular views while achieving a global\nreceptive field along the epipolar line. We conduct extensive experiments with\ninsightful visualizations to validate the effectiveness of our method.\nComparative results on five public datasets show that our method not only\nachieves state-of-the-art SR performance, but also performs robust to disparity\nvariations. Code is publicly available at\nhttps://github.com/ZhengyuLiang24/EPIT.",
        "authors": [
            "Zhengyu Liang",
            "Yingqian Wang",
            "Longguang Wang",
            "Jungang Yang",
            "Shilin Zhou",
            "Yulan Guo"
        ]
    },
    {
        "title": "SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation",
        "url": "http://arxiv.org/abs/2308.11568",
        "abstract": "Recent studies show that self-attentions behave like low-pass filters (as\nopposed to convolutions) and enhancing their high-pass filtering capability\nimproves model performance. Contrary to this idea, we investigate existing\nconvolution-based models with spectral analysis and observe that improving the\nlow-pass filtering in convolution operations also leads to performance\nimprovement. To account for this observation, we hypothesize that utilizing\noptimal token mixers that capture balanced representations of both high- and\nlow-frequency components can enhance the performance of models. We verify this\nby decomposing visual features into the frequency domain and combining them in\na balanced manner. To handle this, we replace the balancing problem with a mask\nfiltering problem in the frequency domain. Then, we introduce a novel\ntoken-mixer named SPAM and leverage it to derive a MetaFormer model termed as\nSPANet. Experimental results show that the proposed method provides a way to\nachieve this balance, and the balanced representations of both high- and\nlow-frequency components can improve the performance of models on multiple\ncomputer vision tasks. Our code is available at\n$\\href{https://doranlyong.github.io/projects/spanet/}{\\text{https://doranlyong.github.io/projects/spanet/}}$.",
        "authors": [
            "Guhnoo Yun",
            "Juhan Yoo",
            "Kijung Kim",
            "Jeongho Lee",
            "Dong Hwan Kim"
        ]
    },
    {
        "title": "ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation",
        "url": "http://arxiv.org/abs/2308.09242",
        "abstract": "Recent sparse detectors with multiple, e.g. six, decoder layers achieve\npromising performance but much inference time due to complex heads. Previous\nworks have explored using dense priors as initialization and built\none-decoder-layer detectors. Although they gain remarkable acceleration, their\nperformance still lags behind their six-decoder-layer counterparts by a large\nmargin. In this work, we aim to bridge this performance gap while retaining\nfast speed. We find that the architecture discrepancy between dense and sparse\ndetectors leads to feature conflict, hampering the performance of\none-decoder-layer detectors. Thus we propose Adaptive Sparse Anchor Generator\n(ASAG) which predicts dynamic anchors on patches rather than grids in a sparse\nway so that it alleviates the feature conflict problem. For each image, ASAG\ndynamically selects which feature maps and which locations to predict, forming\na fully adaptive way to generate image-specific anchors. Further, a simple and\neffective Query Weighting method eases the training instability from\nadaptiveness. Extensive experiments show that our method outperforms\ndense-initialized ones and achieves a better speed-accuracy trade-off. The code\nis available at \\url{https://github.com/iSEE-Laboratory/ASAG}.",
        "authors": [
            "Shenghao Fu",
            "Junkai Yan",
            "Yipeng Gao",
            "Xiaohua Xie",
            "Wei-Shi Zheng"
        ]
    },
    {
        "title": "MGMAE: Motion Guided Masking for Video Masked Autoencoding",
        "url": "http://arxiv.org/abs/2308.10794",
        "abstract": "Masked autoencoding has shown excellent performance on self-supervised video\nrepresentation learning. Temporal redundancy has led to a high masking ratio\nand customized masking strategy in VideoMAE. In this paper, we aim to further\nimprove the performance of video masked autoencoding by introducing a motion\nguided masking strategy. Our key insight is that motion is a general and unique\nprior in video, which should be taken into account during masked pre-training.\nOur motion guided masking explicitly incorporates motion information to build\ntemporal consistent masking volume. Based on this masking volume, we can track\nthe unmasked tokens in time and sample a set of temporal consistent cubes from\nvideos. These temporal aligned unmasked tokens will further relieve the\ninformation leakage issue in time and encourage the MGMAE to learn more useful\nstructure information. We implement our MGMAE with an online efficient optical\nflow estimator and backward masking map warping strategy. We perform\nexperiments on the datasets of Something-Something V2 and Kinetics-400,\ndemonstrating the superior performance of our MGMAE to the original VideoMAE.\nIn addition, we provide the visualization analysis to illustrate that our MGMAE\ncan sample temporal consistent cubes in a motion-adaptive manner for more\neffective video pre-training.",
        "authors": [
            "Bingkun Huang",
            "Zhiyu Zhao",
            "Guozhen Zhang",
            "Yu Qiao",
            "Limin Wang"
        ]
    },
    {
        "title": "The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning",
        "url": "http://arxiv.org/abs/2211.00453",
        "abstract": "Semi-supervised machine learning (SSL) is gaining popularity as it reduces\nthe cost of training ML models. It does so by using very small amounts of\n(expensive, well-inspected) labeled data and large amounts of (cheap,\nnon-inspected) unlabeled data. SSL has shown comparable or even superior\nperformances compared to conventional fully-supervised ML techniques.\n  In this paper, we show that the key feature of SSL that it can learn from\n(non-inspected) unlabeled data exposes SSL to strong poisoning attacks. In\nfact, we argue that, due to its reliance on non-inspected unlabeled data,\npoisoning is a much more severe problem in SSL than in conventional\nfully-supervised ML.\n  Specifically, we design a backdoor poisoning attack on SSL that can be\nconducted by a weak adversary with no knowledge of target SSL pipeline. This is\nunlike prior poisoning attacks in fully-supervised settings that assume strong\nadversaries with practically-unrealistic capabilities. We show that by\npoisoning only 0.2% of the unlabeled training data, our attack can cause\nmisclassification of more than 80% of test inputs (when they contain the\nadversary's backdoor trigger). Our attacks remain effective across twenty\ncombinations of benchmark datasets and SSL algorithms, and even circumvent the\nstate-of-the-art defenses against backdoor attacks. Our work raises significant\nconcerns about the practical utility of existing SSL algorithms.",
        "authors": [
            "Virat Shejwalkar",
            "Lingjuan Lyu",
            "Amir Houmansadr"
        ]
    },
    {
        "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models",
        "url": "http://arxiv.org/abs/2308.07863",
        "abstract": "Content and style (C-S) disentanglement is a fundamental problem and critical\nchallenge of style transfer. Existing approaches based on explicit definitions\n(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable\nnor easy to control, resulting in entangled representations and less satisfying\nresults. In this paper, we propose a new C-S disentangled framework for style\ntransfer without using previous assumptions. The key insight is to explicitly\nextract the content information and implicitly learn the complementary style\ninformation, yielding interpretable and controllable C-S disentanglement and\nstyle transfer. A simple yet effective CLIP-based style disentanglement loss\ncoordinated with a style reconstruction prior is introduced to disentangle C-S\nin the CLIP image space. By further leveraging the powerful style removal and\ngenerative ability of diffusion models, our framework achieves superior results\nthan state of the art and flexible C-S disentanglement and trade-off control.\nOur work provides new insights into the C-S disentanglement in style transfer\nand demonstrates the potential of diffusion models for learning\nwell-disentangled C-S characteristics.",
        "authors": [
            "Zhizhong Wang",
            "Lei Zhao",
            "Wei Xing"
        ]
    },
    {
        "title": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding",
        "url": "http://arxiv.org/abs/2303.16894",
        "abstract": "Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,\nand +1.35% on Sr3D, Nr3D, and ScanRefer. Code is released at\nhttps://github.com/Ivan-Tang-3D/ViewRefer3D.",
        "authors": [
            "Zoey Guo",
            "Yiwen Tang",
            "Ray Zhang",
            "Dong Wang",
            "Zhigang Wang",
            "Bin Zhao",
            "Xuelong Li"
        ]
    },
    {
        "title": "CaPhy: Capturing Physical Properties for Animatable Human Avatars",
        "url": "http://arxiv.org/abs/2308.05925",
        "abstract": "We present CaPhy, a novel method for reconstructing animatable human avatars\nwith realistic dynamic properties for clothing. Specifically, we aim for\ncapturing the geometric and physical properties of the clothing from real\nobservations. This allows us to apply novel poses to the human avatar with\nphysically correct deformations and wrinkles of the clothing. To this end, we\ncombine unsupervised training with physics-based losses and 3D-supervised\ntraining using scanned data to reconstruct a dynamic model of clothing that is\nphysically realistic and conforms to the human scans. We also optimize the\nphysical parameters of the underlying physical model from the scans by\nintroducing gradient constraints of the physics-based losses. In contrast to\nprevious work on 3D avatar reconstruction, our method is able to generalize to\nnovel poses with realistic dynamic cloth deformations. Experiments on several\nsubjects demonstrate that our method can estimate the physical properties of\nthe garments, resulting in superior quantitative and qualitative results\ncompared with previous methods.",
        "authors": [
            "Zhaoqi Su",
            "Liangxiao Hu",
            "Siyou Lin",
            "Hongwen Zhang",
            "Shengping Zhang",
            "Justus Thies",
            "Yebin Liu"
        ]
    },
    {
        "title": "Cross-Modal Orthogonal High-Rank Augmentation for RGB-Event Transformer-Trackers",
        "url": "http://arxiv.org/abs/2307.04129",
        "abstract": "This paper addresses the problem of cross-modal object tracking from RGB\nvideos and event data. Rather than constructing a complex cross-modal fusion\nnetwork, we explore the great potential of a pre-trained vision Transformer\n(ViT). Particularly, we delicately investigate plug-and-play training\naugmentations that encourage the ViT to bridge the vast distribution gap\nbetween the two modalities, enabling comprehensive cross-modal information\ninteraction and thus enhancing its ability. Specifically, we propose a mask\nmodeling strategy that randomly masks a specific modality of some tokens to\nenforce the interaction between tokens from different modalities interacting\nproactively. To mitigate network oscillations resulting from the masking\nstrategy and further amplify its positive effect, we then theoretically propose\nan orthogonal high-rank loss to regularize the attention matrix. Extensive\nexperiments demonstrate that our plug-and-play training augmentation techniques\ncan significantly boost state-of-the-art one-stream and twostream trackers to a\nlarge extent in terms of both tracking precision and success rate. Our new\nperspective and findings will potentially bring insights to the field of\nleveraging powerful pre-trained ViTs to model cross-modal data. The code will\nbe publicly available.",
        "authors": [
            "Zhiyu Zhu",
            "Junhui Hou",
            "Dapeng Oliver Wu"
        ]
    },
    {
        "title": "Open-vocabulary Panoptic Segmentation with Embedding Modulation",
        "url": "http://arxiv.org/abs/2303.11324",
        "abstract": "Open-vocabulary image segmentation is attracting increasing attention due to\nits critical applications in the real world. Traditional closed-vocabulary\nsegmentation methods are not able to characterize novel objects, whereas\nseveral recent open-vocabulary attempts obtain unsatisfactory results, i.e.,\nnotable performance reduction on the closed vocabulary and massive demand for\nextra data. To this end, we propose OPSNet, an omnipotent and data-efficient\nframework for Open-vocabulary Panoptic Segmentation. Specifically, the\nexquisitely designed Embedding Modulation module, together with several\nmeticulous components, enables adequate embedding enhancement and information\nexchange between the segmentation model and the visual-linguistic well-aligned\nCLIP encoder, resulting in superior segmentation performance under both open-\nand closed-vocabulary settings with much fewer need of additional data.\nExtensive experimental evaluations are conducted across multiple datasets\n(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various\ncircumstances, where the proposed OPSNet achieves state-of-the-art results,\nwhich demonstrates the effectiveness and generality of the proposed approach.\nThe code and trained models will be made publicly available.",
        "authors": [
            "Xi Chen",
            "Shuang Li",
            "Ser-Nam Lim",
            "Antonio Torralba",
            "Hengshuang Zhao"
        ]
    },
    {
        "title": "Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models",
        "url": "http://arxiv.org/abs/2304.07221",
        "abstract": "Pre-trained point cloud models have found extensive applications in 3D\nunderstanding tasks like object classification and part segmentation. However,\nthe prevailing strategy of full fine-tuning in downstream tasks leads to large\nper-task storage overhead for model parameters, which limits the efficiency\nwhen applying large-scale pre-trained models. Inspired by the recent success of\nvisual prompt tuning (VPT), this paper attempts to explore prompt tuning on\npre-trained point cloud models, to pursue an elegant balance between\nperformance and parameter efficiency. We find while instance-agnostic static\nprompting, e.g. VPT, shows some efficacy in downstream transfer, it is\nvulnerable to the distribution diversity caused by various types of noises in\nreal-world point cloud data. To conquer this limitation, we propose a novel\nInstance-aware Dynamic Prompt Tuning (IDPT) strategy for pre-trained point\ncloud models. The essence of IDPT is to develop a dynamic prompt generation\nmodule to perceive semantic prior features of each point cloud instance and\ngenerate adaptive prompt tokens to enhance the model's robustness. Notably,\nextensive experiments demonstrate that IDPT outperforms full fine-tuning in\nmost tasks with a mere 7% of the trainable parameters, providing a promising\nsolution to parameter-efficient learning for pre-trained point cloud models.\nCode is available at \\url{https://github.com/zyh16143998882/ICCV23-IDPT}.",
        "authors": [
            "Yaohua Zha",
            "Jinpeng Wang",
            "Tao Dai",
            "Bin Chen",
            "Zhi Wang",
            "Shu-Tao Xia"
        ]
    },
    {
        "title": "Text2Tex: Text-driven Texture Synthesis via Diffusion Models",
        "url": "http://arxiv.org/abs/2303.11396",
        "abstract": "We present Text2Tex, a novel method for generating high-quality textures for\n3D meshes from the given text prompts. Our method incorporates inpainting into\na pre-trained depth-aware image diffusion model to progressively synthesize\nhigh resolution partial textures from multiple viewpoints. To avoid\naccumulating inconsistent and stretched artifacts across views, we dynamically\nsegment the rendered view into a generation mask, which represents the\ngeneration status of each visible texel. This partitioned view representation\nguides the depth-aware inpainting model to generate and update partial textures\nfor the corresponding regions. Furthermore, we propose an automatic view\nsequence generation scheme to determine the next best view for updating the\npartial texture. Extensive experiments demonstrate that our method\nsignificantly outperforms the existing text-driven approaches and GAN-based\nmethods.",
        "authors": [
            "Dave Zhenyu Chen",
            "Yawar Siddiqui",
            "Hsin-Ying Lee",
            "Sergey Tulyakov",
            "Matthias Nie\u00dfner"
        ]
    },
    {
        "title": "Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models",
        "url": "http://arxiv.org/abs/2212.14306",
        "abstract": "Curating datasets for object segmentation is a difficult task. With the\nadvent of large-scale pre-trained generative models, conditional image\ngeneration has been given a significant boost in result quality and ease of\nuse. In this paper, we present a novel method that enables the generation of\ngeneral foreground-background segmentation models from simple textual\ndescriptions, without requiring segmentation labels. We leverage and explore\npre-trained latent diffusion models, to automatically generate weak\nsegmentation masks for concepts and objects. The masks are then used to\nfine-tune the diffusion model on an inpainting task, which enables fine-grained\nremoval of the object, while at the same time providing a synthetic foreground\nand background dataset. We demonstrate that using this method beats previous\nmethods in both discriminative and generative performance and closes the gap\nwith fully supervised training while requiring no pixel-wise object labels. We\nshow results on the task of segmenting four different objects (humans, dogs,\ncars, birds) and a use case scenario in medical image analysis. The code is\navailable at https://github.com/MischaD/fobadiffusion.",
        "authors": [
            "Mischa Dombrowski",
            "Hadrien Reynaud",
            "Matthew Baugh",
            "Bernhard Kainz"
        ]
    },
    {
        "title": "ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting",
        "url": "http://arxiv.org/abs/2303.13022",
        "abstract": "Recent advances in neural rendering have shown great potential for\nreconstructing scenes from multiview images. However, accurately representing\nobjects with glossy surfaces remains a challenge for existing methods. In this\nwork, we introduce ENVIDR, a rendering and modeling framework for high-quality\nrendering and reconstruction of surfaces with challenging specular reflections.\nTo achieve this, we first propose a novel neural renderer with decomposed\nrendering components to learn the interaction between surface and environment\nlighting. This renderer is trained using existing physically based renderers\nand is decoupled from actual scene representations. We then propose an\nSDF-based neural surface model that leverages this learned neural renderer to\nrepresent general scenes. Our model additionally synthesizes indirect\nilluminations caused by inter-reflections from shiny surfaces by marching\nsurface-reflected rays. We demonstrate that our method outperforms state-of-art\nmethods on challenging shiny scenes, providing high-quality rendering of\nspecular reflections while also enabling material editing and scene relighting.",
        "authors": [
            "Ruofan Liang",
            "Huiting Chen",
            "Chunlin Li",
            "Fan Chen",
            "Selvakumar Panneer",
            "Nandita Vijaykumar"
        ]
    },
    {
        "title": "Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation",
        "url": "http://arxiv.org/abs/2307.08448",
        "abstract": "Conditional diffusion models have demonstrated impressive performance in\nimage manipulation tasks. The general pipeline involves adding noise to the\nimage and then denoising it. However, this method faces a trade-off problem:\nadding too much noise affects the fidelity of the image while adding too little\naffects its editability. This largely limits their practical applicability. In\nthis paper, we propose a novel framework, Selective Diffusion Distillation\n(SDD), that ensures both the fidelity and editability of images. Instead of\ndirectly editing images with a diffusion model, we train a feedforward image\nmanipulation network under the guidance of the diffusion model. Besides, we\npropose an effective indicator to select the semantic-related timestep to\nobtain the correct semantic guidance from the diffusion model. This approach\nsuccessfully avoids the dilemma caused by the diffusion process. Our extensive\nexperiments demonstrate the advantages of our framework. Code is released at\nhttps://github.com/AndysonYs/Selective-Diffusion-Distillation.",
        "authors": [
            "Luozhou Wang",
            "Shuai Yang",
            "Shu Liu",
            "Ying-cong Chen"
        ]
    },
    {
        "title": "SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage",
        "url": "http://arxiv.org/abs/2303.11114",
        "abstract": "We need billion-scale images to achieve more generalizable and\nground-breaking vision models, as well as massive dataset storage to ship the\nimages (e.g., the LAION-4B dataset needs 240TB storage space). However, it has\nbecome challenging to deal with unlimited dataset storage with limited storage\ninfrastructure. A number of storage-efficient training methods have been\nproposed to tackle the problem, but they are rarely scalable or suffer from\nsevere damage to performance. In this paper, we propose a storage-efficient\ntraining strategy for vision classifiers for large-scale datasets (e.g.,\nImageNet) that only uses 1024 tokens per instance without using the raw level\npixels; our token storage only needs <1% of the original JPEG-compressed raw\npixels. We also propose token augmentations and a Stem-adaptor module to make\nour approach able to use the same architecture as pixel-based approaches with\nonly minimal modifications on the stem layer and the carefully tuned\noptimization settings. Our experimental results on ImageNet-1k show that our\nmethod significantly outperforms other storage-efficient training methods with\na large gap. We further show the effectiveness of our method in other practical\nscenarios, storage-efficient pre-training, and continual learning. Code is\navailable at https://github.com/naver-ai/seit",
        "authors": [
            "Song Park",
            "Sanghyuk Chun",
            "Byeongho Heo",
            "Wonjae Kim",
            "Sangdoo Yun"
        ]
    },
    {
        "title": "ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption",
        "url": "http://arxiv.org/abs/2308.08428",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) has significantly boosted the\nperformance of various vision-language tasks by scaling up the dataset with\nimage-text pairs collected from the web. However, the presence of intrinsic\nnoise and unmatched image-text pairs in web data can potentially affect the\nperformance of representation learning. To address this issue, we first utilize\nthe OFA model to generate synthetic captions that focus on the image content.\nThe generated captions contain complementary information that is beneficial for\npre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP),\na bi-path model that integrates supervision from both raw text and synthetic\ncaption. As the core components of ALIP, the Language Consistency Gate (LCG)\nand Description Consistency Gate (DCG) dynamically adjust the weights of\nsamples and image-text/caption pairs during the training process. Meanwhile,\nthe adaptive contrastive loss can effectively reduce the impact of noise data\nand enhances the efficiency of pre-training data. We validate ALIP with\nexperiments on different scales of models and pre-training datasets.\nExperiments results show that ALIP achieves state-of-the-art performance on\nmultiple downstream tasks including zero-shot image-text retrieval and linear\nprobe. To facilitate future research, the code and pre-trained models are\nreleased at https://github.com/deepglint/ALIP.",
        "authors": [
            "Kaicheng Yang",
            "Jiankang Deng",
            "Xiang An",
            "Jiawei Li",
            "Ziyong Feng",
            "Jia Guo",
            "Jing Yang",
            "Tongliang Liu"
        ]
    },
    {
        "title": "GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation",
        "url": "http://arxiv.org/abs/2211.16762",
        "abstract": "We present a learning-based method, namely GeoUDF,to tackle the long-standing\nand challenging problem of reconstructing a discrete surface from a sparse\npoint cloud.To be specific, we propose a geometry-guided learning method for\nUDF and its gradient estimation that explicitly formulates the unsigned\ndistance of a query point as the learnable affine averaging of its distances to\nthe tangent planes of neighboring points on the surface. Besides,we model the\nlocal geometric structure of the input point clouds by explicitly learning a\nquadratic polynomial for each point. This not only facilitates upsampling the\ninput sparse point cloud but also naturally induces unoriented normal, which\nfurther augments UDF estimation. Finally, to extract triangle meshes from the\npredicted UDF we propose a customized edge-based marching cube module. We\nconduct extensive experiments and ablation studies to demonstrate the\nsignificant advantages of our method over state-of-the-art methods in terms of\nreconstruction accuracy, efficiency, and generality. The source code is\npublicly available at https://github.com/rsy6318/GeoUDF.",
        "authors": [
            "Siyu Ren",
            "Junhui Hou",
            "Xiaodong Chen",
            "Ying He",
            "Wenping Wang"
        ]
    },
    {
        "title": "CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training",
        "url": "http://arxiv.org/abs/2210.01055",
        "abstract": "Pre-training across 3D vision and language remains under development because\nof limited training data. Recent works attempt to transfer vision-language\npre-training models to 3D vision. PointCLIP converts point cloud data to\nmulti-view depth maps, adopting CLIP for shape classification. However, its\nperformance is restricted by the domain gap between rendered depth maps and\nimages, as well as the diversity of depth distributions. To address this issue,\nwe propose CLIP2Point, an image-depth pre-training method by contrastive\nlearning to transfer CLIP to the 3D domain, and adapt it to point cloud\nclassification. We introduce a new depth rendering setting that forms a better\nvisual effect, and then render 52,460 pairs of images and depth maps from\nShapeNet for pre-training. The pre-training scheme of CLIP2Point combines\ncross-modality learning to enforce the depth features for capturing expressive\nvisual and textual features and intra-modality learning to enhance the\ninvariance of depth aggregation. Additionally, we propose a novel Dual-Path\nAdapter (DPA) module, i.e., a dual-path structure with simplified adapters for\nfew-shot learning. The dual-path structure allows the joint use of CLIP and\nCLIP2Point, and the simplified adapter can well fit few-shot tasks without\npost-search. Experimental results show that CLIP2Point is effective in\ntransferring CLIP knowledge to 3D vision. Our CLIP2Point outperforms PointCLIP\nand other self-supervised 3D networks, achieving state-of-the-art results on\nzero-shot and few-shot classification.",
        "authors": [
            "Tianyu Huang",
            "Bowen Dong",
            "Yunhan Yang",
            "Xiaoshui Huang",
            "Rynson W. H. Lau",
            "Wanli Ouyang",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "Parametric Classification for Generalized Category Discovery: A Baseline Study",
        "url": "http://arxiv.org/abs/2211.11727",
        "abstract": "Generalized Category Discovery (GCD) aims to discover novel categories in\nunlabelled datasets using knowledge learned from labelled samples. Previous\nstudies argued that parametric classifiers are prone to overfitting to seen\ncategories, and endorsed using a non-parametric classifier formed with\nsemi-supervised k-means. However, in this study, we investigate the failure of\nparametric classifiers, verify the effectiveness of previous design choices\nwhen high-quality supervision is available, and identify unreliable\npseudo-labels as a key problem. We demonstrate that two prediction biases\nexist: the classifier tends to predict seen classes more often, and produces an\nimbalanced distribution across seen and novel categories. Based on these\nfindings, we propose a simple yet effective parametric classification method\nthat benefits from entropy regularisation, achieves state-of-the-art\nperformance on multiple GCD benchmarks and shows strong robustness to unknown\nclass numbers. We hope the investigation and proposed simple framework can\nserve as a strong baseline to facilitate future studies in this field. Our code\nis available at: https://github.com/CVMI-Lab/SimGCD.",
        "authors": [
            "Xin Wen",
            "Bingchen Zhao",
            "Xiaojuan Qi"
        ]
    },
    {
        "title": "MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2307.15700",
        "abstract": "As a video task, Multiple Object Tracking (MOT) is expected to capture\ntemporal information of targets effectively. Unfortunately, most existing\nmethods only explicitly exploit the object features between adjacent frames,\nwhile lacking the capacity to model long-term temporal information. In this\npaper, we propose MeMOTR, a long-term memory-augmented Transformer for\nmulti-object tracking. Our method is able to make the same object's track\nembedding more stable and distinguishable by leveraging long-term memory\ninjection with a customized memory-attention layer. This significantly improves\nthe target association ability of our model. Experimental results on DanceTrack\nshow that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and\n13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also\noutperforms other Transformer-based methods on association performance on MOT17\nand generalizes well on BDD100K. Code is available at\nhttps://github.com/MCG-NJU/MeMOTR.",
        "authors": [
            "Ruopeng Gao",
            "Limin Wang"
        ]
    },
    {
        "title": "RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image",
        "url": "http://arxiv.org/abs/2309.02020",
        "abstract": "High dynamic range (HDR) images capture much more intensity levels than\nstandard ones. Current methods predominantly generate HDR images from 8-bit low\ndynamic range (LDR) sRGB images that have been degraded by the camera\nprocessing pipeline. However, it becomes a formidable task to retrieve\nextremely high dynamic range scenes from such limited bit-depth data. Unlike\nexisting methods, the core idea of this work is to incorporate more informative\nRaw sensor data to generate HDR images, aiming to recover scene information in\nhard regions (the darkest and brightest areas of an HDR scene). To this end, we\npropose a model tailor-made for Raw images, harnessing the unique features of\nRaw data to facilitate the Raw-to-HDR mapping. Specifically, we learn exposure\nmasks to separate the hard and easy regions of a high dynamic scene. Then, we\nintroduce two important guidances, dual intensity guidance, which guides less\ninformative channels with more informative ones, and global spatial guidance,\nwhich extrapolates scene specifics over an extended spatial domain. To verify\nour Raw-to-HDR approach, we collect a large Raw/HDR paired dataset for both\ntraining and testing. Our empirical evaluations validate the superiority of the\nproposed Raw-to-HDR reconstruction model, as well as our newly captured dataset\nin the experiments.",
        "authors": [
            "Yunhao Zou",
            "Chenggang Yan",
            "Ying Fu"
        ]
    },
    {
        "title": "Denoising Diffusion Autoencoders are Unified Self-supervised Learners",
        "url": "http://arxiv.org/abs/2303.09769",
        "abstract": "Inspired by recent advances in diffusion models, which are reminiscent of\ndenoising autoencoders, we investigate whether they can acquire discriminative\nrepresentations for classification via generative pre-training. This paper\nshows that the networks in diffusion models, namely denoising diffusion\nautoencoders (DDAE), are unified self-supervised learners: by pre-training on\nunconditional image generation, DDAE has already learned strongly\nlinear-separable representations within its intermediate layers without\nauxiliary encoders, thus making diffusion pre-training emerge as a general\napproach for generative-and-discriminative dual learning. To validate this, we\nconduct linear probe and fine-tuning evaluations. Our diffusion-based approach\nachieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and\nTiny-ImageNet, respectively, and is comparable to contrastive learning and\nmasked autoencoders for the first time. Transfer learning from ImageNet also\nconfirms the suitability of DDAE for Vision Transformers, suggesting the\npotential to scale DDAEs as unified foundation models. Code is available at\ngithub.com/FutureXiang/ddae.",
        "authors": [
            "Weilai Xiang",
            "Hongyu Yang",
            "Di Huang",
            "Yunhong Wang"
        ]
    },
    {
        "title": "Robust Object Modeling for Visual Tracking",
        "url": "http://arxiv.org/abs/2308.05140",
        "abstract": "Object modeling has become a core part of recent tracking frameworks. Current\npopular tackers use Transformer attention to extract the template feature\nseparately or interactively with the search region. However, separate template\nlearning lacks communication between the template and search regions, which\nbrings difficulty in extracting discriminative target-oriented features. On the\nother hand, interactive template learning produces hybrid template features,\nwhich may introduce potential distractors to the template via the cluttered\nsearch regions. To enjoy the merits of both methods, we propose a robust object\nmodeling framework for visual tracking (ROMTrack), which simultaneously models\nthe inherent template and the hybrid template features. As a result, harmful\ndistractors can be suppressed by combining the inherent features of target\nobjects with search regions' guidance. Target-related features can also be\nextracted using the hybrid template, thus resulting in a more robust object\nmodeling framework. To further enhance robustness, we present novel variation\ntokens to depict the ever-changing appearance of target objects. Variation\ntokens are adaptable to object deformation and appearance variations, which can\nboost overall performance with negligible computation. Experiments show that\nour ROMTrack sets a new state-of-the-art on multiple benchmarks.",
        "authors": [
            "Yidong Cai",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu"
        ]
    },
    {
        "title": "Temporal Collection and Distribution for Referring Video Object Segmentation",
        "url": "http://arxiv.org/abs/2309.03473",
        "abstract": "Referring video object segmentation aims to segment a referent throughout a\nvideo sequence according to a natural language expression. It requires aligning\nthe natural language expression with the objects' motions and their dynamic\nassociations at the global video level but segmenting objects at the frame\nlevel. To achieve this goal, we propose to simultaneously maintain a global\nreferent token and a sequence of object queries, where the former is\nresponsible for capturing video-level referent according to the language\nexpression, while the latter serves to better locate and segment objects with\neach frame. Furthermore, to explicitly capture object motions and\nspatial-temporal cross-modal reasoning over objects, we propose a novel\ntemporal collection-distribution mechanism for interacting between the global\nreferent token and object queries. Specifically, the temporal collection\nmechanism collects global information for the referent token from object\nqueries to the temporal motions to the language expression. In turn, the\ntemporal distribution first distributes the referent token to the referent\nsequence across all frames and then performs efficient cross-frame reasoning\nbetween the referent sequence and object queries in every frame. Experimental\nresults show that our method outperforms state-of-the-art methods on all\nbenchmarks consistently and significantly.",
        "authors": [
            "Jiajin Tang",
            "Ge Zheng",
            "Sibei Yang"
        ]
    },
    {
        "title": "Global Knowledge Calibration for Fast Open-Vocabulary Segmentation",
        "url": "http://arxiv.org/abs/2303.09181",
        "abstract": "Recent advancements in pre-trained vision-language models, such as CLIP, have\nenabled the segmentation of arbitrary concepts solely from textual inputs, a\nprocess commonly referred to as open-vocabulary semantic segmentation (OVS).\nHowever, existing OVS techniques confront a fundamental challenge: the trained\nclassifier tends to overfit on the base classes observed during training,\nresulting in suboptimal generalization performance to unseen classes. To\nmitigate this issue, recent studies have proposed the use of an additional\nfrozen pre-trained CLIP for classification. Nonetheless, this approach incurs\nheavy computational overheads as the CLIP vision encoder must be repeatedly\nforward-passed for each mask, rendering it impractical for real-world\napplications. To address this challenge, our objective is to develop a fast OVS\nmodel that can perform comparably or better without the extra computational\nburden of the CLIP image encoder during inference. To this end, we propose a\ncore idea of preserving the generalizable representation when fine-tuning on\nknown classes. Specifically, we introduce a text diversification strategy that\ngenerates a set of synonyms for each training category, which prevents the\nlearned representation from collapsing onto specific known category names.\nAdditionally, we employ a text-guided knowledge distillation method to preserve\nthe generalizable knowledge of CLIP. Extensive experiments demonstrate that our\nproposed model achieves robust generalization performance across various\ndatasets. Furthermore, we perform a preliminary exploration of open-vocabulary\nvideo segmentation and present a benchmark that can facilitate future\nopen-vocabulary research in the video domain.",
        "authors": [
            "Kunyang Han",
            "Yong Liu",
            "Jun Hao Liew",
            "Henghui Ding",
            "Yunchao Wei",
            "Jiajun Liu",
            "Yitong Wang",
            "Yansong Tang",
            "Yujiu Yang",
            "Jiashi Feng",
            "Yao Zhao"
        ]
    },
    {
        "title": "MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge",
        "url": "http://arxiv.org/abs/2303.08914",
        "abstract": "Large scale Vision-Language (VL) models have shown tremendous success in\naligning representations between visual and text modalities. This enables\nremarkable progress in zero-shot recognition, image generation & editing, and\nmany other exciting tasks. However, VL models tend to over-represent objects\nwhile paying much less attention to verbs, and require additional tuning on\nvideo data for best zero-shot action recognition performance. While previous\nwork relied on large-scale, fully-annotated data, in this work we propose an\nunsupervised approach. We adapt a VL model for zero-shot and few-shot action\nrecognition using a collection of unlabeled videos and an unpaired action\ndictionary. Based on that, we leverage Large Language Models and VL models to\nbuild a text bag for each unlabeled video via matching, text expansion and\ncaptioning. We use those bags in a Multiple Instance Learning setup to adapt an\nimage-text backbone to video data. Although finetuned on unlabeled video data,\nour resulting models demonstrate high transferability to numerous unseen\nzero-shot downstream tasks, improving the base VL model performance by up to\n14\\%, and even comparing favorably to fully-supervised baselines in both\nzero-shot and few-shot video recognition transfer. The code will be released\nlater at \\url{https://github.com/wlin-at/MAXI}.",
        "authors": [
            "Wei Lin",
            "Leonid Karlinsky",
            "Nina Shvetsova",
            "Horst Possegger",
            "Mateusz Kozinski",
            "Rameswar Panda",
            "Rogerio Feris",
            "Hilde Kuehne",
            "Horst Bischof"
        ]
    },
    {
        "title": "Space Engage: Collaborative Space Supervision for Contrastive-Based Semi-Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.09755",
        "abstract": "Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model\nwith limited labeled images and a substantial volume of unlabeled images. To\nimprove the robustness of representations, powerful methods introduce a\npixel-wise contrastive learning approach in latent space (i.e., representation\nspace) that aggregates the representations to their prototypes in a fully\nsupervised manner. However, previous contrastive-based S4 methods merely rely\non the supervision from the model's output (logits) in logit space during\nunlabeled training. In contrast, we utilize the outputs in both logit space and\nrepresentation space to obtain supervision in a collaborative way. The\nsupervision from two spaces plays two roles: 1) reduces the risk of\nover-fitting to incorrect semantic information in logits with the help of\nrepresentations; 2) enhances the knowledge exchange between the two spaces.\nFurthermore, unlike previous approaches, we use the similarity between\nrepresentations and prototypes as a new indicator to tilt training those\nunder-performing representations and achieve a more efficient contrastive\nlearning process. Results on two public benchmarks demonstrate the competitive\nperformance of our method compared with state-of-the-art methods.",
        "authors": [
            "Changqi Wang",
            "Haoyu Xie",
            "Yuhui Yuan",
            "Chong Fu",
            "Xiangyu Yue"
        ]
    },
    {
        "title": "Delving into Motion-Aware Matching for Monocular 3D Object Tracking",
        "url": "http://arxiv.org/abs/2308.11607",
        "abstract": "Recent advances of monocular 3D object detection facilitate the 3D\nmulti-object tracking task based on low-cost camera sensors. In this paper, we\nfind that the motion cue of objects along different time frames is critical in\n3D multi-object tracking, which is less explored in existing monocular-based\napproaches. In this paper, we propose a motion-aware framework for monocular 3D\nMOT. To this end, we propose MoMA-M3T, a framework that mainly consists of\nthree motion-aware components. First, we represent the possible movement of an\nobject related to all object tracklets in the feature space as its motion\nfeatures. Then, we further model the historical object tracklet along the time\nframe in a spatial-temporal perspective via a motion transformer. Finally, we\npropose a motion-aware matching module to associate historical object tracklets\nand current observations as final tracking results. We conduct extensive\nexperiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T\nachieves competitive performance against state-of-the-art methods. Moreover,\nthe proposed tracker is flexible and can be easily plugged into existing\nimage-based 3D object detectors without re-training. Code and models are\navailable at https://github.com/kuanchihhuang/MoMA-M3T.",
        "authors": [
            "Kuan-Chih Huang",
            "Ming-Hsuan Yang",
            "Yi-Hsuan Tsai"
        ]
    },
    {
        "title": "SoDaCam: Software-defined Cameras via Single-Photon Imaging",
        "url": "http://arxiv.org/abs/2309.00066",
        "abstract": "Reinterpretable cameras are defined by their post-processing capabilities\nthat exceed traditional imaging. We present \"SoDaCam\" that provides\nreinterpretable cameras at the granularity of photons, from photon-cubes\nacquired by single-photon devices. Photon-cubes represent the spatio-temporal\ndetections of photons as a sequence of binary frames, at frame-rates as high as\n100 kHz. We show that simple transformations of the photon-cube, or photon-cube\nprojections, provide the functionality of numerous imaging systems including:\nexposure bracketing, flutter shutter cameras, video compressive systems, event\ncameras, and even cameras that move during exposure. Our photon-cube\nprojections offer the flexibility of being software-defined constructs that are\nonly limited by what is computable, and shot-noise. We exploit this flexibility\nto provide new capabilities for the emulated cameras. As an added benefit, our\nprojections provide camera-dependent compression of photon-cubes, which we\ndemonstrate using an implementation of our projections on a novel compute\narchitecture that is designed for single-photon imaging.",
        "authors": [
            "Varun Sundar",
            "Andrei Ardelean",
            "Tristan Swedish",
            "Claudio Bruschini",
            "Edoardo Charbon",
            "Mohit Gupta"
        ]
    },
    {
        "title": "Reference-guided Controllable Inpainting of Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2304.09677",
        "abstract": "The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led\nto a desire for NeRF editing tools. Here, we focus on inpainting regions in a\nview-consistent and controllable manner. In addition to the typical NeRF inputs\nand masks delineating the unwanted region in each view, we require only a\nsingle inpainted view of the scene, i.e., a reference view. We use monocular\ndepth estimators to back-project the inpainted view to the correct 3D\npositions. Then, via a novel rendering technique, a bilateral solver can\nconstruct view-dependent effects in non-reference views, making the inpainted\nregion appear consistent from any view. For non-reference disoccluded regions,\nwhich cannot be supervised by the single reference view, we devise a method\nbased on image inpainters to guide both the geometry and appearance. Our\napproach shows superior performance to NeRF inpainting baselines, with the\nadditional advantage that a user can control the generated scene via a single\ninpainted image. Project page: https://ashmrz.github.io/reference-guided-3d",
        "authors": [
            "Ashkan Mirzaei",
            "Tristan Aumentado-Armstrong",
            "Marcus A. Brubaker",
            "Jonathan Kelly",
            "Alex Levinshtein",
            "Konstantinos G. Derpanis",
            "Igor Gilitschenski"
        ]
    },
    {
        "title": "Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips",
        "url": "http://arxiv.org/abs/2309.05663",
        "abstract": "We tackle the task of reconstructing hand-object interactions from short\nvideo clips. Given an input video, our approach casts 3D inference as a\nper-video optimization and recovers a neural 3D representation of the object\nshape, as well as the time-varying motion and hand articulation. While the\ninput video naturally provides some multi-view cues to guide 3D inference,\nthese are insufficient on their own due to occlusions and limited viewpoint\nvariations. To obtain accurate 3D, we augment the multi-view signals with\ngeneric data-driven priors to guide reconstruction. Specifically, we learn a\ndiffusion network to model the conditional distribution of (geometric)\nrenderings of objects conditioned on hand configuration and category label, and\nleverage it as a prior to guide the novel-view renderings of the reconstructed\nscene. We empirically evaluate our approach on egocentric videos across 6\nobject categories, and observe significant improvements over prior single-view\nand multi-view methods. Finally, we demonstrate our system's ability to\nreconstruct arbitrary clips from YouTube, showing both 1st and 3rd person\ninteractions.",
        "authors": [
            "Yufei Ye",
            "Poorvi Hebbar",
            "Abhinav Gupta",
            "Shubham Tulsiani"
        ]
    },
    {
        "title": "Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image",
        "url": "http://arxiv.org/abs/2302.02410",
        "abstract": "Reconstructing interacting hands from a single RGB image is a very\nchallenging task. On the one hand, severe mutual occlusion and similar local\nappearance between two hands confuse the extraction of visual features,\nresulting in the misalignment of estimated hand meshes and the image. On the\nother hand, there are complex spatial relationship between interacting hands,\nwhich significantly increases the solution space of hand poses and increases\nthe difficulty of network learning. In this paper, we propose a decoupled\niterative refinement framework to achieve pixel-alignment hand reconstruction\nwhile efficiently modeling the spatial relationship between hands.\nSpecifically, we define two feature spaces with different characteristics,\nnamely 2D visual feature space and 3D joint feature space. First, we obtain\njoint-wise features from the visual feature map and utilize a graph convolution\nnetwork and a transformer to perform intra- and inter-hand information\ninteraction in the 3D joint feature space, respectively. Then, we project the\njoint features with global information back into the 2D visual feature space in\nan obfuscation-free manner and utilize the 2D convolution for pixel-wise\nenhancement. By performing multiple alternate enhancements in the two feature\nspaces, our method can achieve an accurate and robust reconstruction of\ninteracting hands. Our method outperforms all existing two-hand reconstruction\nmethods by a large margin on the InterHand2.6M dataset.",
        "authors": [
            "Pengfei Ren",
            "Chao Wen",
            "Xiaozheng Zheng",
            "Zhou Xue",
            "Haifeng Sun",
            "Qi Qi",
            "Jingyu Wang",
            "Jianxin Liao"
        ]
    },
    {
        "title": "Fast Adversarial Training with Smooth Convergence",
        "url": "http://arxiv.org/abs/2308.12857",
        "abstract": "Fast adversarial training (FAT) is beneficial for improving the adversarial\nrobustness of neural networks. However, previous FAT work has encountered a\nsignificant issue known as catastrophic overfitting when dealing with large\nperturbation budgets, \\ie the adversarial robustness of models declines to near\nzero during training.\n  To address this, we analyze the training process of prior FAT work and\nobserve that catastrophic overfitting is accompanied by the appearance of loss\nconvergence outliers.\n  Therefore, we argue a moderately smooth loss convergence process will be a\nstable FAT process that solves catastrophic overfitting.\n  To obtain a smooth loss convergence process, we propose a novel oscillatory\nconstraint (dubbed ConvergeSmooth) to limit the loss difference between\nadjacent epochs. The convergence stride of ConvergeSmooth is introduced to\nbalance convergence and smoothing. Likewise, we design weight centralization\nwithout introducing additional hyperparameters other than the loss balance\ncoefficient.\n  Our proposed methods are attack-agnostic and thus can improve the training\nstability of various FAT techniques.\n  Extensive experiments on popular datasets show that the proposed methods\nefficiently avoid catastrophic overfitting and outperform all previous FAT\nmethods. Code is available at \\url{https://github.com/FAT-CS/ConvergeSmooth}.",
        "authors": [
            "Mengnan Zhao",
            "Lihe Zhang",
            "Yuqiu Kong",
            "Baocai Yin"
        ]
    },
    {
        "title": "Who Are You Referring To? Coreference Resolution In Image Narrations",
        "url": "http://arxiv.org/abs/2211.14563",
        "abstract": "Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.",
        "authors": [
            "Arushi Goel",
            "Basura Fernando",
            "Frank Keller",
            "Hakan Bilen"
        ]
    },
    {
        "title": "DVGaze: Dual-View Gaze Estimation",
        "url": "http://arxiv.org/abs/2308.10310",
        "abstract": "Gaze estimation methods estimate gaze from facial appearance with a single\ncamera. However, due to the limited view of a single camera, the captured\nfacial appearance cannot provide complete facial information and thus\ncomplicate the gaze estimation problem. Recently, camera devices are rapidly\nupdated. Dual cameras are affordable for users and have been integrated in many\ndevices. This development suggests that we can further improve gaze estimation\nperformance with dual-view gaze estimation. In this paper, we propose a\ndual-view gaze estimation network (DV-Gaze). DV-Gaze estimates dual-view gaze\ndirections from a pair of images. We first propose a dual-view interactive\nconvolution (DIC) block in DV-Gaze. DIC blocks exchange dual-view information\nduring convolution in multiple feature scales. It fuses dual-view features\nalong epipolar lines and compensates for the original feature with the fused\nfeature. We further propose a dual-view transformer to estimate gaze from\ndual-view features. Camera poses are encoded to indicate the position\ninformation in the transformer. We also consider the geometric relation between\ndual-view gaze directions and propose a dual-view gaze consistency loss for\nDV-Gaze. DV-Gaze achieves state-of-the-art performance on ETH-XGaze and EVE\ndatasets. Our experiments also prove the potential of dual-view gaze\nestimation. We release codes in https://github.com/yihuacheng/DVGaze.",
        "authors": [
            "Yihua Cheng",
            "Feng Lu"
        ]
    },
    {
        "title": "Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction",
        "url": "http://arxiv.org/abs/2309.02965",
        "abstract": "Reconstructing both objects and hands in 3D from a single RGB image is\ncomplex. Existing methods rely on manually defined hand-object constraints in\nEuclidean space, leading to suboptimal feature learning. Compared with\nEuclidean space, hyperbolic space better preserves the geometric properties of\nmeshes thanks to its exponentially-growing space distance, which amplifies the\ndifferences between the features based on similarity. In this work, we propose\nthe first precise hand-object reconstruction method in hyperbolic space, namely\nDynamic Hyperbolic Attention Network (DHANet), which leverages intrinsic\nproperties of hyperbolic space to learn representative features. Our method\nthat projects mesh and image features into a unified hyperbolic space includes\ntwo modules, ie. dynamic hyperbolic graph convolution and image-attention\nhyperbolic graph convolution. With these two modules, our method learns mesh\nfeatures with rich geometry-image multi-modal information and models better\nhand-object interaction. Our method provides a promising alternative for fine\nhand-object reconstruction in hyperbolic space. Extensive experiments on three\npublic datasets demonstrate that our method outperforms most state-of-the-art\nmethods.",
        "authors": [
            "Zhiying Leng",
            "Shun-Cheng Wu",
            "Mahdi Saleh",
            "Antonio Montanaro",
            "Hao Yu",
            "Yin Wang",
            "Nassir Navab",
            "Xiaohui Liang",
            "Federico Tombari"
        ]
    },
    {
        "title": "LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses",
        "url": "http://arxiv.org/abs/2304.00054",
        "abstract": "Dense 3D reconstruction from RGB images traditionally assumes static camera\npose estimates. This assumption has endured, even as recent works have\nincreasingly focused on real-time methods for mobile devices. However, the\nassumption of a fixed pose for each image does not hold for online execution:\nposes from real-time SLAM are dynamic and may be updated following events such\nas bundle adjustment and loop closure. This has been addressed in the RGB-D\nsetting, by de-integrating past views and re-integrating them with updated\nposes, but it remains largely untreated in the RGB-only setting. We formalize\nthis problem to define the new task of dense online reconstruction from\ndynamically-posed images. To support further research, we introduce a dataset\ncalled LivePose containing the dynamic poses from a SLAM system running on\nScanNet. We select three recent reconstruction systems and apply a framework\nbased on de-integration to adapt each one to the dynamic-pose setting. In\naddition, we propose a novel, non-linear de-integration module that learns to\nremove stale scene content. We show that responding to pose updates is critical\nfor high-quality reconstruction, and that our de-integration framework is an\neffective solution.",
        "authors": [
            "Noah Stier",
            "Baptiste Angles",
            "Liang Yang",
            "Yajie Yan",
            "Alex Colburn",
            "Ming Chuang"
        ]
    },
    {
        "title": "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks",
        "url": "http://arxiv.org/abs/2308.10438",
        "abstract": "In this paper, we propose a novel layer-adaptive weight-pruning approach for\nDeep Neural Networks (DNNs) that addresses the challenge of optimizing the\noutput distortion minimization while adhering to a target pruning ratio\nconstraint. Our approach takes into account the collective influence of all\nlayers to design a layer-adaptive pruning scheme. We discover and utilize a\nvery important additivity property of output distortion caused by pruning\nweights on multiple layers. This property enables us to formulate the pruning\nas a combinatorial optimization problem and efficiently solve it through\ndynamic programming. By decomposing the problem into sub-problems, we achieve\nlinear time complexity, making our optimization algorithm fast and feasible to\nrun on CPUs. Our extensive experiments demonstrate the superiority of our\napproach over existing methods on the ImageNet and CIFAR-10 datasets. On\nCIFAR-10, our method achieves remarkable improvements, outperforming others by\nup to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms\nof top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1\naccuracy compared to other methods for VGG-16 and ResNet-50, respectively.\nThese results highlight the effectiveness and practicality of our approach for\nenhancing DNN performance through layer-adaptive weight pruning. Code will be\navailable on https://github.com/Akimoto-Cris/RD_VIT_PRUNE.",
        "authors": [
            "Kaixin Xu",
            "Zhe Wang",
            "Xue Geng",
            "Jie Lin",
            "Min Wu",
            "Xiaoli Li",
            "Weisi Lin"
        ]
    },
    {
        "title": "Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution",
        "url": "http://arxiv.org/abs/2308.05022",
        "abstract": "Transformer-based methods have exhibited remarkable potential in single image\nsuper-resolution (SISR) by effectively extracting long-range dependencies.\nHowever, most of the current research in this area has prioritized the design\nof transformer blocks to capture global information, while overlooking the\nimportance of incorporating high-frequency priors, which we believe could be\nbeneficial. In our study, we conducted a series of experiments and found that\ntransformer structures are more adept at capturing low-frequency information,\nbut have limited capacity in constructing high-frequency representations when\ncompared to their convolutional counterparts. Our proposed solution, the\ncross-refinement adaptive feature modulation transformer (CRAFT), integrates\nthe strengths of both convolutional and transformer structures. It comprises\nthree key components: the high-frequency enhancement residual block (HFERB) for\nextracting high-frequency information, the shift rectangle window attention\nblock (SRWAB) for capturing global information, and the hybrid fusion block\n(HFB) for refining the global representation. Our experiments on multiple\ndatasets demonstrate that CRAFT outperforms state-of-the-art methods by up to\n0.29dB while using fewer parameters. The source code will be made available at:\nhttps://github.com/AVC2-UESTC/CRAFT-SR.git.",
        "authors": [
            "Ao Li",
            "Le Zhang",
            "Yun Liu",
            "Ce Zhu"
        ]
    },
    {
        "title": "FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2307.11418",
        "abstract": "As recent advances in Neural Radiance Fields (NeRF) have enabled\nhigh-fidelity 3D face reconstruction and novel view synthesis, its manipulation\nalso became an essential task in 3D vision. However, existing manipulation\nmethods require extensive human labor, such as a user-provided semantic mask\nand manual attribute search unsuitable for non-expert users. Instead, our\napproach is designed to require a single text to manipulate a face\nreconstructed with NeRF. To do so, we first train a scene manipulator, a latent\ncode-conditional deformable NeRF, over a dynamic scene to control a face\ndeformation using the latent code. However, representing a scene deformation\nwith a single latent code is unfavorable for compositing local deformations\nobserved in different instances. As so, our proposed Position-conditional\nAnchor Compositor (PAC) learns to represent a manipulated scene with spatially\nvarying latent codes. Their renderings with the scene manipulator are then\noptimized to yield high cosine similarity to a target text in CLIP embedding\nspace for text-driven manipulation. To the best of our knowledge, our approach\nis the first to address the text-driven manipulation of a face reconstructed\nwith NeRF. Extensive results, comparisons, and ablation studies demonstrate the\neffectiveness of our approach.",
        "authors": [
            "Sungwon Hwang",
            "Junha Hyung",
            "Daejin Kim",
            "Min-Jung Kim",
            "Jaegul Choo"
        ]
    },
    {
        "title": "MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos",
        "url": "http://arxiv.org/abs/2212.13056",
        "abstract": "In this paper, we target at the problem of learning a generalizable dynamic\nradiance field from monocular videos. Different from most existing NeRF methods\nthat are based on multiple views, monocular videos only contain one view at\neach timestamp, thereby suffering from ambiguity along the view direction in\nestimating point features and scene flows. Previous studies such as DynNeRF\ndisambiguate point features by positional encoding, which is not transferable\nand severely limits the generalization ability. As a result, these methods have\nto train one independent model for each scene and suffer from heavy\ncomputational costs when applying to increasing monocular videos in real-world\napplications. To address this, We propose MonoNeRF to simultaneously learn\npoint features and scene flows with point trajectory and feature correspondence\nconstraints across frames. More specifically, we learn an implicit velocity\nfield to estimate point trajectory from temporal features with Neural ODE,\nwhich is followed by a flow-based feature aggregation module to obtain spatial\nfeatures along the point trajectory. We jointly optimize temporal and spatial\nfeatures in an end-to-end manner. Experiments show that our MonoNeRF is able to\nlearn from multiple scenes and support new applications such as scene editing,\nunseen frame synthesis, and fast novel scene adaptation. Codes are available at\nhttps://github.com/tianfr/MonoNeRF.",
        "authors": [
            "Fengrui Tian",
            "Shaoyi Du",
            "Yueqi Duan"
        ]
    },
    {
        "title": "Learning Depth Estimation for Transparent and Mirror Surfaces",
        "url": "http://arxiv.org/abs/2307.15052",
        "abstract": "Inferring the depth of transparent or mirror (ToM) surfaces represents a hard\nchallenge for either sensors, algorithms, or deep networks. We propose a simple\npipeline for learning to estimate depth properly for such surfaces with neural\nnetworks, without requiring any ground-truth annotation. We unveil how to\nobtain reliable pseudo labels by in-painting ToM objects in images and\nprocessing them with a monocular depth estimation model. These labels can be\nused to fine-tune existing monocular or stereo networks, to let them learn how\nto deal with ToM surfaces. Experimental results on the Booster dataset show the\ndramatic improvements enabled by our remarkably simple proposal.",
        "authors": [
            "Alex Costanzino",
            "Pierluigi Zama Ramirez",
            "Matteo Poggi",
            "Fabio Tosi",
            "Stefano Mattoccia",
            "Luigi Di Stefano"
        ]
    },
    {
        "title": "Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2304.02841",
        "abstract": "Unsupervised semantic segmentation is a long-standing challenge in computer\nvision with great significance. Spectral clustering is a theoretically grounded\nsolution to it where the spectral embeddings for pixels are computed to\nconstruct distinct clusters. Despite recent progress in enhancing spectral\nclustering with powerful pre-trained models, current approaches still suffer\nfrom inefficiencies in spectral decomposition and inflexibility in applying\nthem to the test data. This work addresses these issues by casting spectral\nclustering as a parametric approach that employs neural network-based\neigenfunctions to produce spectral embeddings. The outputs of the neural\neigenfunctions are further restricted to discrete vectors that indicate\nclustering assignments directly. As a result, an end-to-end NN-based paradigm\nof spectral clustering emerges. In practice, the neural eigenfunctions are\nlightweight and take the features from pre-trained models as inputs, improving\ntraining efficiency and unleashing the potential of pre-trained models for\ndense prediction. We conduct extensive empirical studies to validate the\neffectiveness of our approach and observe significant performance gains over\ncompetitive baselines on Pascal Context, Cityscapes, and ADE20K benchmarks.",
        "authors": [
            "Zhijie Deng",
            "Yucen Luo"
        ]
    },
    {
        "title": "Representation Uncertainty in Self-Supervised Learning as Variational Inference",
        "url": "http://arxiv.org/abs/2203.11437",
        "abstract": "In this study, a novel self-supervised learning (SSL) method is proposed,\nwhich considers SSL in terms of variational inference to learn not only\nrepresentation but also representation uncertainties. SSL is a method of\nlearning representations without labels by maximizing the similarity between\nimage representations of different augmented views of an image. Meanwhile,\nvariational autoencoder (VAE) is an unsupervised representation learning method\nthat trains a probabilistic generative model with variational inference. Both\nVAE and SSL can learn representations without labels, but their relationship\nhas not been investigated in the past. Herein, the theoretical relationship\nbetween SSL and variational inference has been clarified. Furthermore, a novel\nmethod, namely variational inference SimSiam (VI-SimSiam), has been proposed.\nVI-SimSiam can predict the representation uncertainty by interpreting SimSiam\nwith variational inference and defining the latent space distribution. The\npresent experiments qualitatively show that VI- SimSiam could learn uncertainty\nby comparing input images and predicted uncertainties. Additionally, we\ndescribed a relationship between estimated uncertainty and classification\naccuracy.",
        "authors": [
            "Hiroki Nakamura",
            "Masashi Okada",
            "Tadahiro Taniguchi"
        ]
    },
    {
        "title": "Efficient Diffusion Training via Min-SNR Weighting Strategy",
        "url": "http://arxiv.org/abs/2303.09556",
        "abstract": "Denoising diffusion models have been a mainstream approach for image\ngeneration, however, training these models often suffers from slow convergence.\nIn this paper, we discovered that the slow convergence is partly due to\nconflicting optimization directions between timesteps. To address this issue,\nwe treat the diffusion training as a multi-task learning problem, and introduce\na simple yet effective approach referred to as Min-SNR-$\\gamma$. This method\nadapts loss weights of timesteps based on clamped signal-to-noise ratios, which\neffectively balances the conflicts among timesteps. Our results demonstrate a\nsignificant improvement in converging speed, 3.4$\\times$ faster than previous\nweighting strategies. It is also more effective, achieving a new record FID\nscore of 2.06 on the ImageNet $256\\times256$ benchmark using smaller\narchitectures than that employed in previous state-of-the-art. The code is\navailable at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.",
        "authors": [
            "Tiankai Hang",
            "Shuyang Gu",
            "Chen Li",
            "Jianmin Bao",
            "Dong Chen",
            "Han Hu",
            "Xin Geng",
            "Baining Guo"
        ]
    },
    {
        "title": "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation",
        "url": "http://arxiv.org/abs/2307.11545",
        "abstract": "Parameter Efficient Tuning (PET) has gained attention for reducing the number\nof parameters while maintaining performance and providing better hardware\nresource savings, but few studies investigate dense prediction tasks and\ninteraction between modalities. In this paper, we do an investigation of\nefficient tuning problems on referring image segmentation. We propose a novel\nadapter called Bridger to facilitate cross-modal information exchange and\ninject task-specific information into the pre-trained model. We also design a\nlightweight decoder for image segmentation. Our approach achieves comparable or\nsuperior performance with only 1.61\\% to 3.38\\% backbone parameter updates,\nevaluated on challenging benchmarks. The code is available at\n\\url{https://github.com/kkakkkka/ETRIS}.",
        "authors": [
            "Zunnan Xu",
            "Zhihong Chen",
            "Yong Zhang",
            "Yibing Song",
            "Xiang Wan",
            "Guanbin Li"
        ]
    },
    {
        "title": "Towards Zero-Shot Scale-Aware Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2306.17253",
        "abstract": "Monocular depth estimation is scale-ambiguous, and thus requires scale\nsupervision to produce metric predictions. Even so, the resulting models will\nbe geometry-specific, with learned scales that cannot be directly transferred\nacross domains. Because of that, recent works focus instead on relative depth,\neschewing scale in favor of improved up-to-scale zero-shot transfer. In this\nwork we introduce ZeroDepth, a novel monocular depth estimation framework\ncapable of predicting metric scale for arbitrary test images from different\ndomains and camera parameters. This is achieved by (i) the use of input-level\ngeometric embeddings that enable the network to learn a scale prior over\nobjects; and (ii) decoupling the encoder and decoder stages, via a variational\nlatent representation that is conditioned on single frame information. We\nevaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor\n(NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using\nthe same pre-trained model, outperforming methods that train on in-domain data\nand require test-time scaling to produce metric estimates.",
        "authors": [
            "Vitor Guizilini",
            "Igor Vasiljevic",
            "Dian Chen",
            "Rares Ambrus",
            "Adrien Gaidon"
        ]
    },
    {
        "title": "ATT3D: Amortized Text-to-3D Object Synthesis",
        "url": "http://arxiv.org/abs/2306.07349",
        "abstract": "Text-to-3D modelling has seen exciting progress by combining generative\ntext-to-image models with image-to-3D methods like Neural Radiance Fields.\nDreamFusion recently achieved high-quality results but requires a lengthy,\nper-prompt optimization to create 3D objects. To address this, we amortize\noptimization over text prompts by training on many prompts simultaneously with\na unified model, instead of separately. With this, we share computation across\na prompt set, training in less time than per-prompt optimization. Our framework\n- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to\ngeneralize to unseen setups and smooth interpolations between text for novel\nassets and simple animations.",
        "authors": [
            "Jonathan Lorraine",
            "Kevin Xie",
            "Xiaohui Zeng",
            "Chen-Hsuan Lin",
            "Towaki Takikawa",
            "Nicholas Sharp",
            "Tsung-Yi Lin",
            "Ming-Yu Liu",
            "Sanja Fidler",
            "James Lucas"
        ]
    },
    {
        "title": "Learning by Sorting: Self-supervised Learning with Group Ordering Constraints",
        "url": "http://arxiv.org/abs/2301.02009",
        "abstract": "Contrastive learning has become an important tool in learning representations\nfrom unlabeled data mainly relying on the idea of minimizing distance between\npositive data pairs, e.g., views from the same images, and maximizing distance\nbetween negative data pairs, e.g., views from different images. This paper\nproposes a new variation of the contrastive learning objective, Group Ordering\nConstraints (GroCo), that leverages the idea of sorting the distances of\npositive and negative pairs and computing the respective loss based on how many\npositive pairs have a larger distance than the negative pairs, and thus are not\nordered correctly. To this end, the GroCo loss is based on differentiable\nsorting networks, which enable training with sorting supervision by matching a\ndifferentiable permutation matrix, which is produced by sorting a given set of\nscores, to a respective ground truth permutation matrix. Applying this idea to\ngroupwise pre-ordered inputs of multiple positive and negative pairs allows\nintroducing the GroCo loss with implicit emphasis on strong positives and\nnegatives, leading to better optimization of the local neighborhood. We\nevaluate the proposed formulation on various self-supervised learning\nbenchmarks and show that it not only leads to improved results compared to\nvanilla contrastive learning but also shows competitive performance to\ncomparable methods in linear probing and outperforms current methods in k-NN\nperformance.",
        "authors": [
            "Nina Shvetsova",
            "Felix Petersen",
            "Anna Kukleva",
            "Bernt Schiele",
            "Hilde Kuehne"
        ]
    },
    {
        "title": "Cross Modal Transformer: Towards Fast and Robust 3D Object Detection",
        "url": "http://arxiv.org/abs/2301.01283",
        "abstract": "In this paper, we propose a robust 3D detector, named Cross Modal Transformer\n(CMT), for end-to-end 3D multi-modal detection. Without explicit view\ntransformation, CMT takes the image and point clouds tokens as inputs and\ndirectly outputs accurate 3D bounding boxes. The spatial alignment of\nmulti-modal tokens is performed by encoding the 3D points into multi-modal\nfeatures. The core design of CMT is quite simple while its performance is\nimpressive. It achieves 74.1\\% NDS (state-of-the-art with single model) on\nnuScenes test set while maintaining fast inference speed. Moreover, CMT has a\nstrong robustness even if the LiDAR is missing. Code is released at\nhttps://github.com/junjie18/CMT.",
        "authors": [
            "Junjie Yan",
            "Yingfei Liu",
            "Jianjian Sun",
            "Fan Jia",
            "Shuailin Li",
            "Tiancai Wang",
            "Xiangyu Zhang"
        ]
    },
    {
        "title": "Perceptual Grouping in Contrastive Vision-Language Models",
        "url": "http://arxiv.org/abs/2210.09996",
        "abstract": "Recent advances in zero-shot image recognition suggest that vision-language\nmodels learn generic visual representations with a high degree of semantic\ninformation that may be arbitrarily probed with natural language phrases.\nUnderstanding an image, however, is not just about understanding what content\nresides within an image, but importantly, where that content resides. In this\nwork we examine how well vision-language models are able to understand where\nobjects reside within an image and group together visually related parts of the\nimagery. We demonstrate how contemporary vision and language representation\nlearning models based on contrastive losses and large web-based data capture\nlimited object localization information. We propose a minimal set of\nmodifications that results in models that uniquely learn both semantic and\nspatial information. We measure this performance in terms of zero-shot image\nrecognition, unsupervised bottom-up and top-down semantic segmentations, as\nwell as robustness analyses. We find that the resulting model achieves\nstate-of-the-art results in terms of unsupervised segmentation, and demonstrate\nthat the learned representations are uniquely robust to spurious correlations\nin datasets designed to probe the causal behavior of vision models.",
        "authors": [
            "Kanchana Ranasinghe",
            "Brandon McKinzie",
            "Sachin Ravi",
            "Yinfei Yang",
            "Alexander Toshev",
            "Jonathon Shlens"
        ]
    },
    {
        "title": "Dynamic Perceiver for Efficient Visual Recognition",
        "url": "http://arxiv.org/abs/2306.11248",
        "abstract": "Early exiting has become a promising approach to improving the inference\nefficiency of deep networks. By structuring models with multiple classifiers\n(exits), predictions for ``easy'' samples can be generated at earlier exits,\nnegating the need for executing deeper layers. Current multi-exit networks\ntypically implement linear classifiers at intermediate layers, compelling\nlow-level features to encapsulate high-level semantics. This sub-optimal design\ninvariably undermines the performance of later exits. In this paper, we propose\nDynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedure\nand the early classification task with a novel dual-branch architecture. A\nfeature branch serves to extract image features, while a classification branch\nprocesses a latent code assigned for classification tasks. Bi-directional\ncross-attention layers are established to progressively fuse the information of\nboth branches. Early exits are placed exclusively within the classification\nbranch, thus eliminating the need for linear separability in low-level\nfeatures. Dyn-Perceiver constitutes a versatile and adaptable framework that\ncan be built upon various architectures. Experiments on image classification,\naction recognition, and object detection demonstrate that our method\nsignificantly improves the inference efficiency of different backbones,\noutperforming numerous competitive approaches across a broad range of\ncomputational budgets. Evaluation on both CPU and GPU platforms substantiate\nthe superior practical efficiency of Dyn-Perceiver. Code is available at\nhttps://www.github.com/LeapLabTHU/Dynamic_Perceiver.",
        "authors": [
            "Yizeng Han",
            "Dongchen Han",
            "Zeyu Liu",
            "Yulin Wang",
            "Xuran Pan",
            "Yifan Pu",
            "Chao Deng",
            "Junlan Feng",
            "Shiji Song",
            "Gao Huang"
        ]
    },
    {
        "title": "MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution",
        "url": "http://arxiv.org/abs/2307.07988",
        "abstract": "This work addresses continuous space-time video super-resolution (C-STVSR)\nthat aims to up-scale an input video both spatially and temporally by any\nscaling factors. One key challenge of C-STVSR is to propagate information\ntemporally among the input video frames. To this end, we introduce a space-time\nlocal implicit neural function. It has the striking feature of learning forward\nmotion for a continuum of pixels. We motivate the use of forward motion from\nthe perspective of learning individual motion trajectories, as opposed to\nlearning a mixture of motion trajectories with backward motion. To ease motion\ninterpolation, we encode sparsely sampled forward motion extracted from the\ninput video as the contextual input. Along with a reliability-aware splatting\nand decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art\nperformance on C-STVSR. The source code of MoTIF is available at\nhttps://github.com/sichun233746/MoTIF.",
        "authors": [
            "Yi-Hsin Chen",
            "Si-Cun Chen",
            "Yi-Hsin Chen",
            "Yen-Yu Lin",
            "Wen-Hsiao Peng"
        ]
    },
    {
        "title": "CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception",
        "url": "http://arxiv.org/abs/2304.00670",
        "abstract": "Autonomous driving requires an accurate and fast 3D perception system that\nincludes 3D object detection, tracking, and segmentation. Although recent\nlow-cost camera-based approaches have shown promising results, they are\nsusceptible to poor illumination or bad weather conditions and have a large\nlocalization error. Hence, fusing camera with low-cost radar, which provides\nprecise long-range measurement and operates reliably in all environments, is\npromising but has not yet been thoroughly investigated. In this paper, we\npropose Camera Radar Net (CRN), a novel camera-radar fusion framework that\ngenerates a semantically rich and spatially accurate bird's-eye-view (BEV)\nfeature map for various tasks. To overcome the lack of spatial information in\nan image, we transform perspective view image features to BEV with the help of\nsparse but accurate radar points. We further aggregate image and radar feature\nmaps in BEV using multi-modal deformable attention designed to tackle the\nspatial misalignment between inputs. CRN with real-time setting operates at 20\nFPS while achieving comparable performance to LiDAR detectors on nuScenes, and\neven outperforms at a far distance on 100m setting. Moreover, CRN with offline\nsetting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among\nall camera and camera-radar 3D object detectors.",
        "authors": [
            "Youngseok Kim",
            "Juyeb Shin",
            "Sanmin Kim",
            "In-Jae Lee",
            "Jun Won Choi",
            "Dongsuk Kum"
        ]
    },
    {
        "title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization",
        "url": "http://arxiv.org/abs/2307.15199",
        "abstract": "In a joint vision-language space, a text feature (e.g., from \"a photo of a\ndog\") could effectively represent its relevant image features (e.g., from dog\nphotos). Also, a recent study has demonstrated the cross-modal transferability\nphenomenon of this joint space. From these observations, we propose\nPromptStyler which simulates various distribution shifts in the joint space by\nsynthesizing diverse styles via prompts without using any images to deal with\nsource-free domain generalization. The proposed method learns to generate a\nvariety of style features (from \"a S* style of a\") via learnable style word\nvectors for pseudo-words S*. To ensure that learned styles do not distort\ncontent information, we force style-content features (from \"a S* style of a\n[class]\") to be located nearby their corresponding content features (from\n\"[class]\") in the joint vision-language space. After learning style word\nvectors, we train a linear classifier using synthesized style-content features.\nPromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and\nDomainNet, even though it does not require any images for training.",
        "authors": [
            "Junhyeong Cho",
            "Gilhyun Nam",
            "Sungyeon Kim",
            "Hunmin Yang",
            "Suha Kwak"
        ]
    },
    {
        "title": "Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption",
        "url": "http://arxiv.org/abs/2309.03729",
        "abstract": "Training a generative model with limited number of samples is a challenging\ntask. Current methods primarily rely on few-shot model adaption to train the\nnetwork. However, in scenarios where data is extremely limited (less than 10),\nthe generative network tends to overfit and suffers from content degradation.\nTo address these problems, we propose a novel phasic content fusing few-shot\ndiffusion model with directional distribution consistency loss, which targets\ndifferent learning objectives at distinct training stages of the diffusion\nmodel. Specifically, we design a phasic training strategy with phasic content\nfusion to help our model learn content and style information when t is large,\nand learn local details of target domain when t is small, leading to an\nimprovement in the capture of content, style and local details. Furthermore, we\nintroduce a novel directional distribution consistency loss that ensures the\nconsistency between the generated and source distributions more efficiently and\nstably than the prior methods, preventing our model from overfitting. Finally,\nwe propose a cross-domain structure guidance strategy that enhances structure\nconsistency during domain adaptation. Theoretical analysis, qualitative and\nquantitative experiments demonstrate the superiority of our approach in\nfew-shot generative model adaption tasks compared to state-of-the-art methods.\nThe source code is available at:\nhttps://github.com/sjtuplayer/few-shot-diffusion.",
        "authors": [
            "Teng Hu",
            "Jiangning Zhang",
            "Liang Liu",
            "Ran Yi",
            "Siqi Kou",
            "Haokun Zhu",
            "Xu Chen",
            "Yabiao Wang",
            "Chengjie Wang",
            "Lizhuang Ma"
        ]
    },
    {
        "title": "SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.13323",
        "abstract": "LiDAR-based semantic perception tasks are critical yet challenging for\nautonomous driving. Due to the motion of objects and static/dynamic occlusion,\ntemporal information plays an essential role in reinforcing perception by\nenhancing and completing single-frame knowledge. Previous approaches either\ndirectly stack historical frames to the current frame or build a 4D\nspatio-temporal neighborhood using KNN, which duplicates computation and\nhinders realtime performance. Based on our observation that stacking all the\nhistorical points would damage performance due to a large amount of redundant\nand misleading information, we propose the Sparse Voxel-Adjacent Query Network\n(SVQNet) for 4D LiDAR semantic segmentation. To take full advantage of the\nhistorical frames high-efficiently, we shunt the historical points into two\ngroups with reference to the current points. One is the Voxel-Adjacent\nNeighborhood carrying local enhancing knowledge. The other is the Historical\nContext completing the global knowledge. Then we propose new modules to select\nand extract the instructive features from the two groups. Our SVQNet achieves\nstate-of-the-art performance in LiDAR semantic segmentation of the\nSemanticKITTI benchmark and the nuScenes dataset.",
        "authors": [
            "Xuechao Chen",
            "Shuangjie Xu",
            "Xiaoyi Zou",
            "Tongyi Cao",
            "Dit-Yan Yeung",
            "Lu Fang"
        ]
    },
    {
        "title": "HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling",
        "url": "http://arxiv.org/abs/2301.10460",
        "abstract": "We present the first active learning tool for fine-grained 3D part labeling,\na problem which challenges even the most advanced deep learning (DL) methods\ndue to the significant structural variations among the small and intricate\nparts. For the same reason, the necessary data annotation effort is tremendous,\nmotivating approaches to minimize human involvement. Our labeling tool\niteratively verifies or modifies part labels predicted by a deep neural\nnetwork, with human feedback continually improving the network prediction. To\neffectively reduce human efforts, we develop two novel features in our tool,\nhierarchical and symmetry-aware active labeling. Our human-in-the-loop\napproach, coined HAL3D, achieves 100% accuracy (barring human errors) on any\ntest set with pre-defined hierarchical part labels, with 80% time-saving over\nmanual effort.",
        "authors": [
            "Fenggen Yu",
            "Yiming Qian",
            "Francisca Gil-Ureta",
            "Brian Jackson",
            "Eric Bennett",
            "Hao Zhang"
        ]
    },
    {
        "title": "MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion",
        "url": "http://arxiv.org/abs/2309.11847",
        "abstract": "In this paper, we introduce a new approach for high-quality multi-exposure\nimage fusion (MEF). We show that the fusion weights of an exposure can be\nencoded into a 1D lookup table (LUT), which takes pixel intensity value as\ninput and produces fusion weight as output. We learn one 1D LUT for each\nexposure, then all the pixels from different exposures can query 1D LUT of that\nexposure independently for high-quality and efficient fusion. Specifically, to\nlearn these 1D LUTs, we involve attention mechanism in various dimensions\nincluding frame, channel and spatial ones into the MEF task so as to bring us\nsignificant quality improvement over the state-of-the-art (SOTA). In addition,\nwe collect a new MEF dataset consisting of 960 samples, 155 of which are\nmanually tuned by professionals as ground-truth for evaluation. Our network is\ntrained by this dataset in an unsupervised manner. Extensive experiments are\nconducted to demonstrate the effectiveness of all the newly proposed\ncomponents, and results show that our approach outperforms the SOTA in our and\nanother representative dataset SICE, both qualitatively and quantitatively.\nMoreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC\nGPU. Given its high quality, efficiency and robustness, our method has been\nshipped into millions of Android mobiles across multiple brands world-wide.\nCode is available at: https://github.com/Hedlen/MEFLUT.",
        "authors": [
            "Ting Jiang",
            "Chuan Wang",
            "Xinpeng Li",
            "Ru Li",
            "Haoqiang Fan",
            "Shuaicheng Liu"
        ]
    },
    {
        "title": "FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning",
        "url": "http://arxiv.org/abs/2308.09160",
        "abstract": "Personalized Federated Learning (PFL) represents a promising solution for\ndecentralized learning in heterogeneous data environments. Partial model\npersonalization has been proposed to improve the efficiency of PFL by\nselectively updating local model parameters instead of aggregating all of them.\nHowever, previous work on partial model personalization has mainly focused on\nConvolutional Neural Networks (CNNs), leaving a gap in understanding how it can\nbe applied to other popular models such as Vision Transformers (ViTs). In this\nwork, we investigate where and how to partially personalize a ViT model.\nSpecifically, we empirically evaluate the sensitivity to data distribution of\neach type of layer. Based on the insights that the self-attention layer and the\nclassification head are the most sensitive parts of a ViT, we propose a novel\napproach called FedPerfix, which leverages plugins to transfer information from\nthe aggregated model to the local client as a personalization. Finally, we\nevaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home\ndatasets and demonstrate its effectiveness in improving the model's performance\ncompared to several advanced PFL methods.",
        "authors": [
            "Guangyu Sun",
            "Matias Mendieta",
            "Jun Luo",
            "Shandong Wu",
            "Chen Chen"
        ]
    },
    {
        "title": "Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration",
        "url": "http://arxiv.org/abs/2307.09621",
        "abstract": "In this paper, we address the problem of conditional scene decoration for\n360-degree images. Our method takes a 360-degree background photograph of an\nindoor scene and generates decorated images of the same scene in the panorama\nview. To do this, we develop a 360-aware object layout generator that learns\nlatent object vectors in the 360-degree view to enable a variety of furniture\narrangements for an input 360-degree background image. We use this object\nlayout to condition a generative adversarial network to synthesize images of an\ninput scene. To further reinforce the generation capability of our model, we\ndevelop a simple yet effective scene emptier that removes the generated\nfurniture and produces an emptied scene for our model to learn a cyclic\nconstraint. We train the model on the Structure3D dataset and show that our\nmodel can generate diverse decorations with controllable object layout. Our\nmethod achieves state-of-the-art performance on the Structure3D dataset and\ngeneralizes well to the Zillow indoor scene dataset. Our user study confirms\nthe immersive experiences provided by the realistic image quality and furniture\nlayout in our generation results. Our implementation will be made available.",
        "authors": [
            "Ka Chun Shum",
            "Hong-Wing Pang",
            "Binh-Son Hua",
            "Duc Thanh Nguyen",
            "Sai-Kit Yeung"
        ]
    },
    {
        "title": "The Unreasonable Effectiveness of Large Language-Vision Models for Source-Free Video Domain Adaptation",
        "url": "http://arxiv.org/abs/2308.09139",
        "abstract": "Source-Free Video Unsupervised Domain Adaptation (SFVUDA) task consists in\nadapting an action recognition model, trained on a labelled source dataset, to\nan unlabelled target dataset, without accessing the actual source data. The\nprevious approaches have attempted to address SFVUDA by leveraging\nself-supervision (e.g., enforcing temporal consistency) derived from the target\ndata itself. In this work, we take an orthogonal approach by exploiting\n\"web-supervision\" from Large Language-Vision Models (LLVMs), driven by the\nrationale that LLVMs contain a rich world prior surprisingly robust to\ndomain-shift. We showcase the unreasonable effectiveness of integrating LLVMs\nfor SFVUDA by devising an intuitive and parameter-efficient method, which we\nname Domain Adaptation with Large Language-Vision models (DALL-V), that\ndistills the world prior and complementary source model information into a\nstudent network tailored for the target. Despite the simplicity, DALL-V\nachieves significant improvement over state-of-the-art SFVUDA methods.",
        "authors": [
            "Giacomo Zara",
            "Alessandro Conti",
            "Subhankar Roy",
            "St\u00e9phane Lathuili\u00e8re",
            "Paolo Rota",
            "Elisa Ricci"
        ]
    },
    {
        "title": "GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers",
        "url": "http://arxiv.org/abs/2307.13251",
        "abstract": "Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge\nin computer vision, where state-of-the-art methods are mainly based on full\nsupervision. As annotating ground truth dense instance masks is tedious and\nexpensive, solving 3DIS with weak supervision has become more practical. In\nthis paper, we propose GaPro, a new instance segmentation for 3D point clouds\nusing axis-aligned 3D bounding box supervision. Our two-step approach involves\ngenerating pseudo labels from box annotations and training a 3DIS network with\nthe resulting labels. Additionally, we employ the self-training strategy to\nimprove the performance of our method further. We devise an effective Gaussian\nProcess to generate pseudo instance masks from the bounding boxes and resolve\nambiguities when they overlap, resulting in pseudo instance masks with their\nuncertainty values. Our experiments show that GaPro outperforms previous weakly\nsupervised 3D instance segmentation methods and has competitive performance\ncompared to state-of-the-art fully supervised ones. Furthermore, we demonstrate\nthe robustness of our approach, where we can adapt various state-of-the-art\nfully supervised methods to the weak supervision task by using our pseudo\nlabels for training. The source code and trained models are available at\nhttps://github.com/VinAIResearch/GaPro.",
        "authors": [
            "Tuan Duc Ngo",
            "Binh-Son Hua",
            "Khoi Nguyen"
        ]
    },
    {
        "title": "STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition",
        "url": "http://arxiv.org/abs/2301.03046",
        "abstract": "Existing methods of privacy-preserving action recognition (PPAR) mainly focus\non frame-level (spatial) privacy removal through 2D CNNs. Unfortunately, they\nhave two major drawbacks. First, they may compromise temporal dynamics in input\nvideos, which are critical for accurate action recognition. Second, they are\nvulnerable to practical attacking scenarios where attackers probe for privacy\nfrom an entire video rather than individual frames. To address these issues, we\npropose a novel framework STPrivacy to perform video-level PPAR. For the first\ntime, we introduce vision Transformers into PPAR by treating a video as a\ntubelet sequence, and accordingly design two complementary mechanisms, i.e.,\nsparsification and anonymization, to remove privacy from a spatio-temporal\nperspective. In specific, our privacy sparsification mechanism applies adaptive\ntoken selection to abandon action-irrelevant tubelets. Then, our anonymization\nmechanism implicitly manipulates the remaining action-tubelets to erase privacy\nin the embedding space through adversarial learning. These mechanisms provide\nsignificant advantages in terms of privacy preservation for human eyes and\naction-privacy trade-off adjustment during deployment. We additionally\ncontribute the first two large-scale PPAR benchmarks, VP-HMDB51 and VP-UCF101,\nto the community. Extensive evaluations on them, as well as two other tasks,\nvalidate the effectiveness and generalization capability of our framework.",
        "authors": [
            "Ming Li",
            "Xiangyu Xu",
            "Hehe Fan",
            "Pan Zhou",
            "Jun Liu",
            "Jia-Wei Liu",
            "Jiahe Li",
            "Jussi Keppo",
            "Mike Zheng Shou",
            "Shuicheng Yan"
        ]
    },
    {
        "title": "Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation",
        "url": "http://arxiv.org/abs/2308.01547",
        "abstract": "We generalize the class vectors found in neural networks to linear subspaces\n(i.e.~points in the Grassmann manifold) and show that the Grassmann Class\nRepresentation (GCR) enables the simultaneous improvement in accuracy and\nfeature transferability. In GCR, each class is a subspace and the logit is\ndefined as the norm of the projection of a feature onto the class subspace. We\nintegrate Riemannian SGD into deep learning frameworks such that class\nsubspaces in a Grassmannian are jointly optimized with the rest model\nparameters. Compared to the vector form, the representative capability of\nsubspaces is more powerful. We show that on ImageNet-1K, the top-1 error of\nResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced by 5.6%, 4.5%, 3.0% and\n3.5%, respectively. Subspaces also provide freedom for features to vary and we\nobserved that the intra-class feature variability grows when the subspace\ndimension increases. Consequently, we found the quality of GCR features is\nbetter for downstream tasks. For ResNet50-D, the average linear transfer\naccuracy across 6 datasets improves from 77.98% to 79.70% compared to the\nstrong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4%\nand for Deit3, it improves from 73.8% to 81.4%. With these encouraging results,\nwe believe that more applications could benefit from the Grassmann class\nrepresentation. Code is released at https://github.com/innerlee/GCR.",
        "authors": [
            "Haoqi Wang",
            "Zhizhong Li",
            "Wayne Zhang"
        ]
    },
    {
        "title": "Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence",
        "url": "http://arxiv.org/abs/2301.01218",
        "abstract": "Deep neural networks are vulnerable to adversarial attacks. In this paper, we\ntake the role of investigators who want to trace the attack and identify the\nsource, that is, the particular model which the adversarial examples are\ngenerated from. Techniques derived would aid forensic investigation of attack\nincidents and serve as deterrence to potential attacks. We consider the\nbuyers-seller setting where a machine learning model is to be distributed to\nvarious buyers and each buyer receives a slightly different copy with same\nfunctionality. A malicious buyer generates adversarial examples from a\nparticular copy $\\mathcal{M}_i$ and uses them to attack other copies. From\nthese adversarial examples, the investigator wants to identify the source\n$\\mathcal{M}_i$. To address this problem, we propose a two-stage\nseparate-and-trace framework. The model separation stage generates multiple\ncopies of a model for a same classification task. This process injects unique\ncharacteristics into each copy so that adversarial examples generated have\ndistinct and traceable features. We give a parallel structure which embeds a\n``tracer'' in each copy, and a noise-sensitive training loss to achieve this\ngoal. The tracing stage takes in adversarial examples and a few candidate\nmodels, and identifies the likely source. Based on the unique features induced\nby the noise-sensitive loss function, we could effectively trace the potential\nadversarial copy by considering the output logits from each tracer. Empirical\nresults show that it is possible to trace the origin of the adversarial example\nand the mechanism can be applied to a wide range of architectures and datasets.",
        "authors": [
            "Han Fang",
            "Jiyi Zhang",
            "Yupeng Qiu",
            "Ke Xu",
            "Chengfang Fang",
            "Ee-Chien Chang"
        ]
    },
    {
        "title": "Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation",
        "url": "http://arxiv.org/abs/2308.02874",
        "abstract": "Diffusion probabilistic models have achieved remarkable success in text\nguided image generation. However, generating 3D shapes is still challenging due\nto the lack of sufficient data containing 3D models along with their\ndescriptions. Moreover, text based descriptions of 3D shapes are inherently\nambiguous and lack details. In this paper, we propose a sketch and text guided\nprobabilistic diffusion model for colored point cloud generation that\nconditions the denoising process jointly with a hand drawn sketch of the object\nand its textual description. We incrementally diffuse the point coordinates and\ncolor values in a joint diffusion process to reach a Gaussian distribution.\nColored point cloud generation thus amounts to learning the reverse diffusion\nprocess, conditioned by the sketch and text, to iteratively recover the desired\nshape and color. Specifically, to learn effective sketch-text embedding, our\nmodel adaptively aggregates the joint embedding of text prompt and the sketch\nbased on a capsule attention network. Our model uses staged diffusion to\ngenerate the shape and then assign colors to different parts conditioned on the\nappearance prompt while preserving precise shapes from the first stage. This\ngives our model the flexibility to extend to multiple tasks, such as appearance\nre-editing and part segmentation. Experimental results demonstrate that our\nmodel outperforms recent state-of-the-art in point cloud generation.",
        "authors": [
            "Zijie Wu",
            "Yaonan Wang",
            "Mingtao Feng",
            "He Xie",
            "Ajmal Mian"
        ]
    },
    {
        "title": "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2308.12968",
        "abstract": "Automatic high-quality rendering of anime scenes from complex real-world\nimages is of significant practical value. The challenges of this task lie in\nthe complexity of the scenes, the unique features of anime style, and the lack\nof high-quality datasets to bridge the domain gap. Despite promising attempts,\nprevious efforts are still incompetent in achieving satisfactory results with\nconsistent semantic preservation, evident stylization, and fine details. In\nthis study, we propose Scenimefy, a novel semi-supervised image-to-image\ntranslation framework that addresses these challenges. Our approach guides the\nlearning with structure-consistent pseudo paired data, simplifying the pure\nunsupervised setting. The pseudo data are derived uniquely from a\nsemantic-constrained StyleGAN leveraging rich model priors like CLIP. We\nfurther apply segmentation-guided data selection to obtain high-quality pseudo\nsupervision. A patch-wise contrastive style loss is introduced to improve\nstylization and fine details. Besides, we contribute a high-resolution anime\nscene dataset to facilitate future research. Our extensive experiments\ndemonstrate the superiority of our method over state-of-the-art baselines in\nterms of both perceptual quality and quantitative performance.",
        "authors": [
            "Yuxin Jiang",
            "Liming Jiang",
            "Shuai Yang",
            "Chen Change Loy"
        ]
    },
    {
        "title": "General Image-to-Image Translation with One-Shot Image Guidance",
        "url": "http://arxiv.org/abs/2307.14352",
        "abstract": "Large-scale text-to-image models pre-trained on massive text-image pairs show\nexcellent performance in image synthesis recently. However, image can provide\nmore intuitive visual concepts than plain text. People may ask: how can we\nintegrate the desired visual concept into an existing image, such as our\nportrait? Current methods are inadequate in meeting this demand as they lack\nthe ability to preserve content or translate visual concepts effectively.\nInspired by this, we propose a novel framework named visual concept translator\n(VCT) with the ability to preserve content in the source image and translate\nthe visual concepts guided by a single reference image. The proposed VCT\ncontains a content-concept inversion (CCI) process to extract contents and\nconcepts, and a content-concept fusion (CCF) process to gather the extracted\ninformation to obtain the target image. Given only one reference image, the\nproposed VCT can complete a wide range of general image-to-image translation\ntasks with excellent results. Extensive experiments are conducted to prove the\nsuperiority and effectiveness of the proposed methods. Codes are available at\nhttps://github.com/CrystalNeuro/visual-concept-translator.",
        "authors": [
            "Bin Cheng",
            "Zuhao Liu",
            "Yunbo Peng",
            "Yue Lin"
        ]
    },
    {
        "title": "Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation",
        "url": "http://arxiv.org/abs/2309.11081",
        "abstract": "Sound can convey significant information for spatial reasoning in our daily\nlives. To endow deep networks with such ability, we address the challenge of\ndense indoor prediction with sound in both 2D and 3D via cross-modal knowledge\ndistillation. In this work, we propose a Spatial Alignment via Matching (SAM)\ndistillation framework that elicits local correspondence between the two\nmodalities in vision-to-audio knowledge transfer. SAM integrates audio features\nwith visually coherent learnable spatial embeddings to resolve inconsistencies\nin multiple layers of a student model. Our approach does not rely on a specific\ninput representation, allowing for flexibility in the input shapes or\ndimensions without performance degradation. With a newly curated benchmark\nnamed Dense Auditory Prediction of Surroundings (DAPS), we are the first to\ntackle dense indoor prediction of omnidirectional surroundings in both 2D and\n3D with audio observations. Specifically, for audio-based depth estimation,\nsemantic segmentation, and challenging 3D scene reconstruction, the proposed\ndistillation framework consistently achieves state-of-the-art performance\nacross various metrics and backbone architectures.",
        "authors": [
            "Heeseung Yun",
            "Joonil Na",
            "Gunhee Kim"
        ]
    },
    {
        "title": "Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly",
        "url": "http://arxiv.org/abs/2309.06810",
        "abstract": "Shape assembly aims to reassemble parts (or fragments) into a complete\nobject, which is a common task in our daily life. Different from the semantic\npart assembly (e.g., assembling a chair's semantic parts like legs into a whole\nchair), geometric part assembly (e.g., assembling bowl fragments into a\ncomplete bowl) is an emerging task in computer vision and robotics. Instead of\nsemantic information, this task focuses on geometric information of parts. As\nthe both geometric and pose space of fractured parts are exceptionally large,\nshape pose disentanglement of part representations is beneficial to geometric\nshape assembly. In our paper, we propose to leverage SE(3) equivariance for\nsuch shape pose disentanglement. Moreover, while previous works in vision and\nrobotics only consider SE(3) equivariance for the representations of single\nobjects, we move a step forward and propose leveraging SE(3) equivariance for\nrepresentations considering multi-part correlations, which further boosts the\nperformance of the multi-part assembly. Experiments demonstrate the\nsignificance of SE(3) equivariance and our proposed method for geometric shape\nassembly. Project page: https://crtie.github.io/SE-3-part-assembly/",
        "authors": [
            "Ruihai Wu",
            "Chenrui Tie",
            "Yushi Du",
            "Yan Zhao",
            "Hao Dong"
        ]
    },
    {
        "title": "Adversarial Bayesian Augmentation for Single-Source Domain Generalization",
        "url": "http://arxiv.org/abs/2307.09520",
        "abstract": "Generalizing to unseen image domains is a challenging problem primarily due\nto the lack of diverse training data, inaccessible target data, and the large\ndomain shift that may exist in many real-world settings. As such data\naugmentation is a critical component of domain generalization methods that seek\nto address this problem. We present Adversarial Bayesian Augmentation (ABA), a\nnovel algorithm that learns to generate image augmentations in the challenging\nsingle-source domain generalization setting. ABA draws on the strengths of\nadversarial learning and Bayesian neural networks to guide the generation of\ndiverse data augmentations -- these synthesized image domains aid the\nclassifier in generalizing to unseen domains. We demonstrate the strength of\nABA on several types of domain shift including style shift, subpopulation\nshift, and shift in the medical imaging setting. ABA outperforms all previous\nstate-of-the-art methods, including pre-specified augmentations, pixel-based\nand convolutional-based augmentations.",
        "authors": [
            "Sheng Cheng",
            "Tejas Gokhale",
            "Yezhou Yang"
        ]
    },
    {
        "title": "Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering",
        "url": "http://arxiv.org/abs/2309.09724",
        "abstract": "In this study, we address the challenge of 3D scene structure recovery from\nmonocular depth estimation. While traditional depth estimation methods leverage\nlabeled datasets to directly predict absolute depth, recent advancements\nadvocate for mix-dataset training, enhancing generalization across diverse\nscenes. However, such mixed dataset training yields depth predictions only up\nto an unknown scale and shift, hindering accurate 3D reconstructions. Existing\nsolutions necessitate extra 3D datasets or geometry-complete depth annotations,\nconstraints that limit their versatility. In this paper, we propose a learning\nframework that trains models to predict geometry-preserving depth without\nrequiring extra data or annotations. To produce realistic 3D structures, we\nrender novel views of the reconstructed scenes and design loss functions to\npromote depth estimation consistency across different views. Comprehensive\nexperiments underscore our framework's superior generalization capabilities,\nsurpassing existing state-of-the-art methods on several benchmark datasets\nwithout leveraging extra training information. Moreover, our innovative loss\nfunctions empower the model to autonomously recover domain-specific\nscale-and-shift coefficients using solely unlabeled images.",
        "authors": [
            "Chi Zhang",
            "Wei Yin",
            "Gang Yu",
            "Zhibin Wang",
            "Tao Chen",
            "Bin Fu",
            "Joey Tianyi Zhou",
            "Chunhua Shen"
        ]
    },
    {
        "title": "Self-regulating Prompts: Foundational Model Adaptation without Forgetting",
        "url": "http://arxiv.org/abs/2307.06948",
        "abstract": "Prompt learning has emerged as an efficient alternative for fine-tuning\nfoundational models, such as CLIP, for various downstream tasks. Conventionally\ntrained using the task-specific objective, i.e., cross-entropy loss, prompts\ntend to overfit downstream data distributions and find it challenging to\ncapture task-agnostic general features from the frozen CLIP. This leads to the\nloss of the model's original generalization capability. To address this issue,\nour work introduces a self-regularization framework for prompting called\nPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the\nprompts to optimize for both task-specific and task-agnostic general\nrepresentations using a three-pronged approach by: (a) regulating prompted\nrepresentations via mutual agreement maximization with the frozen model, (b)\nregulating with self-ensemble of prompts over the training trajectory to encode\ntheir complementary strengths, and (c) regulating with textual diversity to\nmitigate sample diversity imbalance with the visual branch. To the best of our\nknowledge, this is the first regularization framework for prompt learning that\navoids overfitting by jointly attending to pre-trained model features, the\ntraining trajectory during prompting, and the textual diversity. PromptSRC\nexplicitly steers the prompts to learn a representation space that maximizes\nperformance on downstream tasks without compromising CLIP generalization. We\nperform extensive experiments on 4 benchmarks where PromptSRC overall performs\nfavorably well compared to the existing methods. Our code and pre-trained\nmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.",
        "authors": [
            "Muhammad Uzair Khattak",
            "Syed Talal Wasim",
            "Muzammal Naseer",
            "Salman Khan",
            "Ming-Hsuan Yang",
            "Fahad Shahbaz Khan"
        ]
    },
    {
        "title": "ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling",
        "url": "http://arxiv.org/abs/2304.09423",
        "abstract": "The research fields of parametric face model and 3D face reconstruction have\nbeen extensively studied. However, a critical question remains unanswered: how\nto tailor the face model for specific reconstruction settings. We argue that\nreconstruction with multi-view uncalibrated images demands a new model with\nstronger capacity. Our study shifts attention from data-dependent 3D Morphable\nModels (3DMM) to an understudied human-designed skinning model. We propose\nAdaptive Skinning Model (ASM), which redefines the skinning model with more\ncompact and fully tunable parameters. With extensive experiments, we\ndemonstrate that ASM achieves significantly improved capacity than 3DMM, with\nthe additional advantage of model size and easy implementation for new\ntopology. We achieve state-of-the-art performance with ASM for multi-view\nreconstruction on the Florence MICC Coop benchmark. Our quantitative analysis\ndemonstrates the importance of a high-capacity model for fully exploiting\nabundant information from multi-view input in reconstruction. Furthermore, our\nmodel with physical-semantic parameters can be directly utilized for real-world\napplications, such as in-game avatar creation. As a result, our work opens up\nnew research direction for parametric face model and facilitates future\nresearch on multi-view reconstruction.",
        "authors": [
            "Kai Yang",
            "Hong Shang",
            "Tianyang Shi",
            "Xinghan Chen",
            "Jingkai Zhou",
            "Zhongqian Sun",
            "Wei Yang"
        ]
    },
    {
        "title": "EverLight: Indoor-Outdoor Editable HDR Lighting Estimation",
        "url": "http://arxiv.org/abs/2304.13207",
        "abstract": "Because of the diversity in lighting environments, existing illumination\nestimation techniques have been designed explicitly on indoor or outdoor\nenvironments. Methods have focused specifically on capturing accurate energy\n(e.g., through parametric lighting models), which emphasizes shading and strong\ncast shadows; or producing plausible texture (e.g., with GANs), which\nprioritizes plausible reflections. Approaches which provide editable lighting\ncapabilities have been proposed, but these tend to be with simplified lighting\nmodels, offering limited realism. In this work, we propose to bridge the gap\nbetween these recent trends in the literature, and propose a method which\ncombines a parametric light model with 360{\\deg} panoramas, ready to use as\nHDRI in rendering engines. We leverage recent advances in GAN-based LDR\npanorama extrapolation from a regular image, which we extend to HDR using\nparametric spherical gaussians. To achieve this, we introduce a novel lighting\nco-modulation method that injects lighting-related features throughout the\ngenerator, tightly coupling the original or edited scene illumination within\nthe panorama generation process. In our representation, users can easily edit\nlight direction, intensity, number, etc. to impact shading while providing\nrich, complex reflections while seamlessly blending with the edits.\nFurthermore, our method encompasses indoor and outdoor environments,\ndemonstrating state-of-the-art results even when compared to domain-specific\nmethods.",
        "authors": [
            "Mohammad Reza Karimi Dastjerdi",
            "Jonathan Eisenmann",
            "Yannick Hold-Geoffroy",
            "Jean-Fran\u00e7ois Lalonde"
        ]
    },
    {
        "title": "MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2304.09913",
        "abstract": "Weakly-supervised semantic segmentation aims to reduce labeling costs by\ntraining semantic segmentation models using weak supervision, such as\nimage-level class labels. However, most approaches struggle to produce accurate\nlocalization maps and suffer from false predictions in class-related\nbackgrounds (i.e., biased objects), such as detecting a railroad with the train\nclass. Recent methods that remove biased objects require additional supervision\nfor manually identifying biased objects for each problematic class and\ncollecting their datasets by reviewing predictions, limiting their\napplicability to the real-world dataset with multiple labels and complex\nrelationships for biasing. Following the first observation that biased features\ncan be separated and eliminated by matching biased objects with backgrounds in\nthe same dataset, we propose a fully-automatic/model-agnostic biased removal\nframework called MARS (Model-Agnostic biased object Removal without additional\nSupervision), which utilizes semantically consistent features of an\nunsupervised technique to eliminate biased objects in pseudo labels.\nSurprisingly, we show that MARS achieves new state-of-the-art results on two\npopular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014\n(val: 49.4%), by consistently improving the performance of various WSSS models\nby at least 30% without additional supervision.",
        "authors": [
            "Sanghyun Jo",
            "In-Jae Yu",
            "Kyungsu Kim"
        ]
    },
    {
        "title": "CAFA: Class-Aware Feature Alignment for Test-Time Adaptation",
        "url": "http://arxiv.org/abs/2206.00205",
        "abstract": "Despite recent advancements in deep learning, deep neural networks continue\nto suffer from performance degradation when applied to new data that differs\nfrom training data. Test-time adaptation (TTA) aims to address this challenge\nby adapting a model to unlabeled data at test time. TTA can be applied to\npretrained networks without modifying their training procedures, enabling them\nto utilize a well-formed source distribution for adaptation. One possible\napproach is to align the representation space of test samples to the source\ndistribution (\\textit{i.e.,} feature alignment). However, performing feature\nalignment in TTA is especially challenging in that access to labeled source\ndata is restricted during adaptation. That is, a model does not have a chance\nto learn test data in a class-discriminative manner, which was feasible in\nother adaptation tasks (\\textit{e.g.,} unsupervised domain adaptation) via\nsupervised losses on the source data. Based on this observation, we propose a\nsimple yet effective feature alignment loss, termed as Class-Aware Feature\nAlignment (CAFA), which simultaneously 1) encourages a model to learn target\nrepresentations in a class-discriminative manner and 2) effectively mitigates\nthe distribution shifts at test time. Our method does not require any\nhyper-parameters or additional losses, which are required in previous\napproaches. We conduct extensive experiments on 6 different datasets and show\nour proposed method consistently outperforms existing baselines.",
        "authors": [
            "Sanghun Jung",
            "Jungsoo Lee",
            "Nanhee Kim",
            "Amirreza Shaban",
            "Byron Boots",
            "Jaegul Choo"
        ]
    },
    {
        "title": "Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification",
        "url": "http://arxiv.org/abs/2308.10658",
        "abstract": "Long-Term Person Re-Identification (LT-ReID) has become increasingly crucial\nin computer vision and biometrics. In this work, we aim to extend LT-ReID\nbeyond pedestrian recognition to include a wider range of real-world human\nactivities while still accounting for cloth-changing scenarios over large time\ngaps. This setting poses additional challenges due to the geometric\nmisalignment and appearance ambiguity caused by the diversity of human pose and\nclothing. To address these challenges, we propose a new approach 3DInvarReID\nfor (i) disentangling identity from non-identity components (pose, clothing\nshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3D\nclothed body shapes and learning discriminative features of naked body shapes\nfor person ReID in a joint manner. To better evaluate our study of LT-ReID, we\ncollect a real-world dataset called CCDA, which contains a wide variety of\nhuman activities and clothing changes. Experimentally, we show the superior\nperformance of our approach for person ReID.",
        "authors": [
            "Feng Liu",
            "Minchul Kim",
            "ZiAng Gu",
            "Anil Jain",
            "Xiaoming Liu"
        ]
    },
    {
        "title": "Agile Modeling: From Concept to Classifier in Minutes",
        "url": "http://arxiv.org/abs/2302.12948",
        "abstract": "The application of computer vision to nuanced subjective use cases is\ngrowing. While crowdsourcing has served the vision community well for most\nobjective tasks (such as labeling a \"zebra\"), it now falters on tasks where\nthere is substantial subjectivity in the concept (such as identifying \"gourmet\ntuna\"). However, empowering any user to develop a classifier for their concept\nis technically difficult: users are neither machine learning experts, nor have\nthe patience to label thousands of examples. In reaction, we introduce the\nproblem of Agile Modeling: the process of turning any subjective visual concept\ninto a computer vision model through a real-time user-in-the-loop interactions.\nWe instantiate an Agile Modeling prototype for image classification and show\nthrough a user study (N=14) that users can create classifiers with minimal\neffort under 30 minutes. We compare this user driven process with the\ntraditional crowdsourcing paradigm and find that the crowd's notion often\ndiffers from that of the user's, especially as the concepts become more\nsubjective. Finally, we scale our experiments with simulations of users\ntraining classifiers for ImageNet21k categories to further demonstrate the\nefficacy.",
        "authors": [
            "Otilia Stretcu",
            "Edward Vendrow",
            "Kenji Hata",
            "Krishnamurthy Viswanathan",
            "Vittorio Ferrari",
            "Sasan Tavakkol",
            "Wenlei Zhou",
            "Aditya Avinash",
            "Enming Luo",
            "Neil Gordon Alldrin",
            "MohammadHossein Bateni",
            "Gabriel Berger",
            "Andrew Bunner",
            "Chun-Ta Lu",
            "Javier A Rey",
            "Giulia DeSalvo",
            "Ranjay Krishna",
            "Ariel Fuxman"
        ]
    },
    {
        "title": "Improving Lens Flare Removal with General-Purpose Pipeline and Multiple Light Sources Recovery",
        "url": "http://arxiv.org/abs/2308.16460",
        "abstract": "When taking images against strong light sources, the resulting images often\ncontain heterogeneous flare artifacts. These artifacts can importantly affect\nimage visual quality and downstream computer vision tasks. While collecting\nreal data pairs of flare-corrupted/flare-free images for training flare removal\nmodels is challenging, current methods utilize the direct-add approach to\nsynthesize data. However, these methods do not consider automatic exposure and\ntone mapping in image signal processing pipeline (ISP), leading to the limited\ngeneralization capability of deep models training using such data. Besides,\nexisting methods struggle to handle multiple light sources due to the different\nsizes, shapes and illuminance of various light sources. In this paper, we\npropose a solution to improve the performance of lens flare removal by\nrevisiting the ISP and remodeling the principle of automatic exposure in the\nsynthesis pipeline and design a more reliable light sources recovery strategy.\nThe new pipeline approaches realistic imaging by discriminating the local and\nglobal illumination through convex combination, avoiding global illumination\nshifting and local over-saturation. Our strategy for recovering multiple light\nsources convexly averages the input and output of the neural network based on\nilluminance levels, thereby avoiding the need for a hard threshold in\nidentifying light sources. We also contribute a new flare removal testing\ndataset containing the flare-corrupted images captured by ten types of consumer\nelectronics. The dataset facilitates the verification of the generalization\ncapability of flare removal methods. Extensive experiments show that our\nsolution can effectively improve the performance of lens flare removal and push\nthe frontier toward more general situations.",
        "authors": [
            "Yuyan Zhou",
            "Dong Liang",
            "Songcan Chen",
            "Sheng-Jun Huang",
            "Shuo Yang",
            "Chongyi Li"
        ]
    },
    {
        "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
        "url": "http://arxiv.org/abs/2309.00035",
        "abstract": "Computer vision models have known performance disparities across attributes\nsuch as gender and skin tone. This means during tasks such as classification\nand detection, model performance differs for certain classes based on the\ndemographics of the people in the image. These disparities have been shown to\nexist, but until now there has not been a unified approach to measure these\ndifferences for common use-cases of computer vision models. We present a new\nbenchmark named FACET (FAirness in Computer Vision EvaluaTion), a large,\npublicly available evaluation set of 32k images for some of the most common\nvision tasks - image classification, object detection and segmentation. For\nevery image in FACET, we hired expert reviewers to manually annotate\nperson-related attributes such as perceived skin tone and hair type, manually\ndraw bounding boxes and label fine-grained person-related classes such as disk\njockey or guitarist. In addition, we use FACET to benchmark state-of-the-art\nvision models and present a deeper understanding of potential performance\ndisparities and challenges across sensitive demographic attributes. With the\nexhaustive annotations collected, we probe models using single demographics\nattributes as well as multiple attributes using an intersectional approach\n(e.g. hair color and perceived skin tone). Our results show that\nclassification, detection, segmentation, and visual grounding models exhibit\nperformance disparities across demographic attributes and intersections of\nattributes. These harms suggest that not all people represented in datasets\nreceive fair and equitable treatment in these vision tasks. We hope current and\nfuture results using our benchmark will contribute to fairer, more robust\nvision models. FACET is available publicly at https://facet.metademolab.com/",
        "authors": [
            "Laura Gustafson",
            "Chloe Rolland",
            "Nikhila Ravi",
            "Quentin Duval",
            "Aaron Adcock",
            "Cheng-Yang Fu",
            "Melissa Hall",
            "Candace Ross"
        ]
    },
    {
        "title": "Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation",
        "url": "http://arxiv.org/abs/2308.10898",
        "abstract": "We study the problem of few-shot physically-aware articulated mesh\ngeneration. By observing an articulated object dataset containing only a few\nexamples, we wish to learn a model that can generate diverse meshes with high\nvisual fidelity and physical validity. Previous mesh generative models either\nhave difficulties in depicting a diverse data space from only a few examples or\nfail to ensure physical validity of their samples. Regarding the above\nchallenges, we propose two key innovations, including 1) a hierarchical mesh\ndeformation-based generative model based upon the divide-and-conquer philosophy\nto alleviate the few-shot challenge by borrowing transferrable deformation\npatterns from large scale rigid meshes and 2) a physics-aware deformation\ncorrection scheme to encourage physically plausible generations. We conduct\nextensive experiments on 6 articulated categories to demonstrate the\nsuperiority of our method in generating articulated meshes with better\ndiversity, higher visual fidelity, and better physical validity over previous\nmethods in the few-shot setting. Further, we validate solid contributions of\nour two innovations in the ablation study. Project page with code is available\nat https://meowuu7.github.io/few-arti-obj-gen.",
        "authors": [
            "Xueyi Liu",
            "Bin Wang",
            "He Wang",
            "Li Yi"
        ]
    },
    {
        "title": "Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction",
        "url": "http://arxiv.org/abs/2304.06714",
        "abstract": "3D-aware image synthesis encompasses a variety of tasks, such as scene\ngeneration and novel view synthesis from images. Despite numerous task-specific\nmethods, developing a comprehensive model remains challenging. In this paper,\nwe present SSDNeRF, a unified approach that employs an expressive diffusion\nmodel to learn a generalizable prior of neural radiance fields (NeRF) from\nmulti-view images of diverse objects. Previous studies have used two-stage\napproaches that rely on pretrained NeRFs as real data to train diffusion\nmodels. In contrast, we propose a new single-stage training paradigm with an\nend-to-end objective that jointly optimizes a NeRF auto-decoder and a latent\ndiffusion model, enabling simultaneous 3D reconstruction and prior learning,\neven from sparsely available views. At test time, we can directly sample the\ndiffusion prior for unconditional generation, or combine it with arbitrary\nobservations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates\nrobust results comparable to or better than leading task-specific methods in\nunconditional generation and single/sparse-view 3D reconstruction.",
        "authors": [
            "Hansheng Chen",
            "Jiatao Gu",
            "Anpei Chen",
            "Wei Tian",
            "Zhuowen Tu",
            "Lingjie Liu",
            "Hao Su"
        ]
    },
    {
        "title": "Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation",
        "url": "http://arxiv.org/abs/2308.13266",
        "abstract": "Tracking any given object(s) spatially and temporally is a common purpose in\nVisual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint\ntracking and segmentation have been attempted in some studies but they often\nlack full compatibility of both box and mask in initialization and prediction,\nand mainly focus on single-object scenarios. To address these limitations, this\npaper proposes a Multi-object Mask-box Integrated framework for unified\nTracking and Segmentation, dubbed MITS. Firstly, the unified identification\nmodule is proposed to support both box and mask reference for initialization,\nwhere detailed object information is inferred from boxes or directly retained\nfrom masks. Additionally, a novel pinpoint box predictor is proposed for\naccurate multi-object box prediction, facilitating target-oriented\nrepresentation learning. All target objects are processed simultaneously from\nencoding to propagation and decoding, as a unified pipeline for VOT and VOS.\nExperimental results show MITS achieves state-of-the-art performance on both\nVOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor\nby around 6% on the GOT-10k test set, and significantly improves the\nperformance of box initialization on VOS benchmarks. The code is available at\nhttps://github.com/yoxu515/MITS.",
        "authors": [
            "Yuanyou Xu",
            "Zongxin Yang",
            "Yi Yang"
        ]
    },
    {
        "title": "One-Shot Generative Domain Adaptation",
        "url": "http://arxiv.org/abs/2111.09876",
        "abstract": "This work aims at transferring a Generative Adversarial Network (GAN)\npre-trained on one image domain to a new domain referring to as few as just one\ntarget image. The main challenge is that, under limited supervision, it is\nextremely difficult to synthesize photo-realistic and highly diverse images,\nwhile acquiring representative characters of the target. Different from\nexisting approaches that adopt the vanilla fine-tuning strategy, we import two\nlightweight modules to the generator and the discriminator respectively.\nConcretely, we introduce an attribute adaptor into the generator yet freeze its\noriginal parameters, through which it can reuse the prior knowledge to the most\nextent and hence maintain the synthesis quality and diversity. We then equip\nthe well-learned discriminator backbone with an attribute classifier to ensure\nthat the generator captures the appropriate characters from the reference.\nFurthermore, considering the poor diversity of the training data (i.e., as few\nas only one image), we propose to also constrain the diversity of the\ngenerative domain in the training process, alleviating the optimization\ndifficulty. Our approach brings appealing results under various settings,\nsubstantially surpassing state-of-the-art alternatives, especially in terms of\nsynthesis diversity. Noticeably, our method works well even with large domain\ngaps, and robustly converges within a few minutes for each experiment.",
        "authors": [
            "Ceyuan Yang",
            "Yujun Shen",
            "Zhiyi Zhang",
            "Yinghao Xu",
            "Jiapeng Zhu",
            "Zhirong Wu",
            "Bolei Zhou"
        ]
    },
    {
        "title": "Prototypes-oriented Transductive Few-shot Learning with Conditional Transport",
        "url": "http://arxiv.org/abs/2308.03047",
        "abstract": "Transductive Few-Shot Learning (TFSL) has recently attracted increasing\nattention since it typically outperforms its inductive peer by leveraging\nstatistics of query samples. However, previous TFSL methods usually encode\nuniform prior that all the classes within query samples are equally likely,\nwhich is biased in imbalanced TFSL and causes severe performance degradation.\n  Given this pivotal issue, in this work, we propose a novel Conditional\nTransport (CT) based imbalanced TFSL model called {\\textbf P}rototypes-oriented\n{\\textbf U}nbiased {\\textbf T}ransfer {\\textbf M}odel (PUTM) to fully exploit\nunbiased statistics of imbalanced query samples, which employs forward and\nbackward navigators as transport matrices to balance the prior of query samples\nper class between uniform and adaptive data-driven distributions. For\nefficiently transferring statistics learned by CT, we further derive a closed\nform solution to refine prototypes based on MAP given the learned navigators.\nThe above two steps of discovering and transferring unbiased statistics follow\nan iterative manner, formulating our EM-based solver.\n  Experimental results on four standard benchmarks including miniImageNet,\ntieredImageNet, CUB, and CIFAR-FS demonstrate superiority of our model in\nclass-imbalanced generalization.",
        "authors": [
            "Long Tian",
            "Jingyi Feng",
            "Wenchao Chen",
            "Xiaoqiang Chai",
            "Liming Wang",
            "Xiyang Liu",
            "Bo Chen"
        ]
    },
    {
        "title": "SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection",
        "url": "http://arxiv.org/abs/2304.14340",
        "abstract": "By identifying four important components of existing LiDAR-camera 3D object\ndetection methods (LiDAR and camera candidates, transformation, and fusion\noutputs), we observe that all existing methods either find dense candidates or\nyield dense representations of scenes. However, given that objects occupy only\na small part of a scene, finding dense candidates and generating dense\nrepresentations is noisy and inefficient. We propose SparseFusion, a novel\nmulti-sensor 3D detection method that exclusively uses sparse candidates and\nsparse representations. Specifically, SparseFusion utilizes the outputs of\nparallel detectors in the LiDAR and camera modalities as sparse candidates for\nfusion. We transform the camera candidates into the LiDAR coordinate space by\ndisentangling the object representations. Then, we can fuse the multi-modality\ncandidates in a unified 3D space by a lightweight self-attention module. To\nmitigate negative transfer between modalities, we propose novel semantic and\ngeometric cross-modality transfer modules that are applied prior to the\nmodality-specific detectors. SparseFusion achieves state-of-the-art performance\non the nuScenes benchmark while also running at the fastest speed, even\noutperforming methods with stronger backbones. We perform extensive experiments\nto demonstrate the effectiveness and efficiency of our modules and overall\nmethod pipeline. Our code will be made publicly available at\nhttps://github.com/yichen928/SparseFusion.",
        "authors": [
            "Yichen Xie",
            "Chenfeng Xu",
            "Marie-Julie Rakotosaona",
            "Patrick Rim",
            "Federico Tombari",
            "Kurt Keutzer",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ]
    },
    {
        "title": "DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners",
        "url": "http://arxiv.org/abs/2309.03483",
        "abstract": "State-of-the-art visual grounding models can achieve high detection accuracy,\nbut they are not designed to distinguish between all objects versus only\ncertain objects of interest. In natural language, in order to specify a\nparticular object or set of objects of interest, humans use determiners such as\n\"my\", \"either\" and \"those\". Determiners, as an important word class, are a type\nof schema in natural language about the reference or quantity of the noun.\nExisting grounded referencing datasets place much less emphasis on determiners,\ncompared to other word classes such as nouns, verbs and adjectives. This makes\nit difficult to develop models that understand the full variety and complexity\nof object referencing. Thus, we have developed and released the DetermiNet\ndataset , which comprises 250,000 synthetically generated images and captions\nbased on 25 determiners. The task is to predict bounding boxes to identify\nobjects of interest, constrained by the semantics of the given determiner. We\nfind that current state-of-the-art visual grounding models do not perform well\non the dataset, highlighting the limitations of existing models on reference\nand quantification tasks.",
        "authors": [
            "Clarence Lee",
            "M Ganesh Kumar",
            "Cheston Tan"
        ]
    },
    {
        "title": "3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2308.06635",
        "abstract": "Tracking 3D objects accurately and consistently is crucial for autonomous\nvehicles, enabling more reliable downstream tasks such as trajectory prediction\nand motion planning. Based on the substantial progress in object detection in\nrecent years, the tracking-by-detection paradigm has become a popular choice\ndue to its simplicity and efficiency. State-of-the-art 3D multi-object tracking\n(MOT) approaches typically rely on non-learned model-based algorithms such as\nKalman Filter but require many manually tuned parameters. On the other hand,\nlearning-based approaches face the problem of adapting the training to the\nonline setting, leading to inevitable distribution mismatch between training\nand inference as well as suboptimal performance. In this work, we propose\n3DMOTFormer, a learned geometry-based 3D MOT framework building upon the\ntransformer architecture. We use an Edge-Augmented Graph Transformer to reason\non the track-detection bipartite graph frame-by-frame and conduct data\nassociation via edge classification. To reduce the distribution mismatch\nbetween training and inference, we propose a novel online training strategy\nwith an autoregressive and recurrent forward pass as well as sequential batch\noptimization. Using CenterPoint detections, our approach achieves 71.2% and\n68.2% AMOTA on the nuScenes validation and test split, respectively. In\naddition, a trained 3DMOTFormer model generalizes well across different object\ndetectors. Code is available at: https://github.com/dsx0511/3DMOTFormer.",
        "authors": [
            "Shuxiao Ding",
            "Eike Rehder",
            "Lukas Schneider",
            "Marius Cordts",
            "Juergen Gall"
        ]
    },
    {
        "title": "Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning",
        "url": "http://arxiv.org/abs/2303.15833",
        "abstract": "Continual domain shift poses a significant challenge in real-world\napplications, particularly in situations where labeled data is not available\nfor new domains. The challenge of acquiring knowledge in this problem setting\nis referred to as unsupervised continual domain shift learning. Existing\nmethods for domain adaptation and generalization have limitations in addressing\nthis issue, as they focus either on adapting to a specific domain or\ngeneralizing to unseen domains, but not both. In this paper, we propose\nComplementary Domain Adaptation and Generalization (CoDAG), a simple yet\neffective learning framework that combines domain adaptation and generalization\nin a complementary manner to achieve three major goals of unsupervised\ncontinual domain shift learning: adapting to a current domain, generalizing to\nunseen domains, and preventing forgetting of previously seen domains. Our\napproach is model-agnostic, meaning that it is compatible with any existing\ndomain adaptation and generalization algorithms. We evaluate CoDAG on several\nbenchmark datasets and demonstrate that our model outperforms state-of-the-art\nmodels in all datasets and evaluation metrics, highlighting its effectiveness\nand robustness in handling unsupervised continual domain shift learning.",
        "authors": [
            "Wonguk Cho",
            "Jinha Park",
            "Taesup Kim"
        ]
    },
    {
        "title": "RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction",
        "url": "http://arxiv.org/abs/2303.08605",
        "abstract": "Recently, neural implicit surfaces have become popular for multi-view\nreconstruction. To facilitate practical applications like scene editing and\nmanipulation, some works extend the framework with semantic masks input for the\nobject-compositional reconstruction rather than the holistic perspective.\nThough achieving plausible disentanglement, the performance drops significantly\nwhen processing the indoor scenes where objects are usually partially observed.\nWe propose RICO to address this by regularizing the unobservable regions for\nindoor compositional reconstruction. Our key idea is to first regularize the\nsmoothness of the occluded background, which then in turn guides the foreground\nobject reconstruction in unobservable regions based on the object-background\nrelationship. Particularly, we regularize the geometry smoothness of occluded\nbackground patches. With the improved background surface, the signed distance\nfunction and the reversedly rendered depth of objects can be optimized to bound\nthem within the background range. Extensive experiments show our method\noutperforms other methods on synthetic and real-world indoor scenes and prove\nthe effectiveness of proposed regularizations. The code is available at\nhttps://github.com/kyleleey/RICO.",
        "authors": [
            "Zizhang Li",
            "Xiaoyang Lyu",
            "Yuanyuan Ding",
            "Mengmeng Wang",
            "Yiyi Liao",
            "Yong Liu"
        ]
    },
    {
        "title": "CLR: Channel-wise Lightweight Reprogramming for Continual Learning",
        "url": "http://arxiv.org/abs/2307.11386",
        "abstract": "Continual learning aims to emulate the human ability to continually\naccumulate knowledge over sequential tasks. The main challenge is to maintain\nperformance on previously learned tasks after learning new tasks, i.e., to\navoid catastrophic forgetting. We propose a Channel-wise Lightweight\nReprogramming (CLR) approach that helps convolutional neural networks (CNNs)\novercome catastrophic forgetting during continual learning. We show that a CNN\nmodel trained on an old task (or self-supervised proxy task) could be\n``reprogrammed\" to solve a new task by using our proposed lightweight (very\ncheap) reprogramming parameter. With the help of CLR, we have a better\nstability-plasticity trade-off to solve continual learning problems: To\nmaintain stability and retain previous task ability, we use a common\ntask-agnostic immutable part as the shared ``anchor\" parameter set. We then add\ntask-specific lightweight reprogramming parameters to reinterpret the outputs\nof the immutable parts, to enable plasticity and integrate new knowledge. To\nlearn sequential tasks, we only train the lightweight reprogramming parameters\nto learn each new task. Reprogramming parameters are task-specific and\nexclusive to each task, which makes our method immune to catastrophic\nforgetting. To minimize the parameter requirement of reprogramming to learn new\ntasks, we make reprogramming lightweight by only adjusting essential kernels\nand learning channel-wise linear mappings from anchor parameters to\ntask-specific domain knowledge. We show that, for general CNNs, the CLR\nparameter increase is less than 0.6\\% for any new task. Our method outperforms\n13 state-of-the-art continual learning baselines on a new challenging sequence\nof 53 image classification datasets. Code and data are available at\nhttps://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming",
        "authors": [
            "Yunhao Ge",
            "Yuecheng Li",
            "Shuo Ni",
            "Jiaping Zhao",
            "Ming-Hsuan Yang",
            "Laurent Itti"
        ]
    },
    {
        "title": "IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization",
        "url": "http://arxiv.org/abs/2308.13168",
        "abstract": "Semi-supervised learning (SSL) aims to leverage massive unlabeled data when\nlabels are expensive to obtain. Unfortunately, in many real-world applications,\nthe collected unlabeled data will inevitably contain unseen-class outliers not\nbelonging to any of the labeled classes. To deal with the challenging open-set\nSSL task, the mainstream methods tend to first detect outliers and then filter\nthem out. However, we observe a surprising fact that such approach could result\nin more severe performance degradation when labels are extremely scarce, as the\nunreliable outlier detector may wrongly exclude a considerable portion of\nvaluable inliers. To tackle with this issue, we introduce a novel open-set SSL\nframework, IOMatch, which can jointly utilize inliers and outliers, even when\nit is difficult to distinguish exactly between them. Specifically, we propose\nto employ a multi-binary classifier in combination with the standard closed-set\nclassifier for producing unified open-set classification targets, which regard\nall outliers as a single new class. By adopting these targets as open-set\npseudo-labels, we optimize an open-set classifier with all unlabeled samples\nincluding both inliers and outliers. Extensive experiments have shown that\nIOMatch significantly outperforms the baseline methods across different\nbenchmark datasets and different settings despite its remarkable simplicity.\nOur code and models are available at https://github.com/nukezil/IOMatch.",
        "authors": [
            "Zekun Li",
            "Lei Qi",
            "Yinghuan Shi",
            "Yang Gao"
        ]
    },
    {
        "title": "Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.11166",
        "abstract": "Impressive performance on point cloud semantic segmentation has been achieved\nby fully-supervised methods with large amounts of labelled data. As it is\nlabour-intensive to acquire large-scale point cloud data with point-wise\nlabels, many attempts have been made to explore learning 3D point cloud\nsegmentation with limited annotations. Active learning is one of the effective\nstrategies to achieve this purpose but is still under-explored. The most recent\nmethods of this kind measure the uncertainty of each pre-divided region for\nmanual labelling but they suffer from redundant information and require\nadditional efforts for region division. This paper aims at addressing this\nissue by developing a hierarchical point-based active learning strategy.\nSpecifically, we measure the uncertainty for each point by a hierarchical\nminimum margin uncertainty module which considers the contextual information at\nmultiple levels. Then, a feature-distance suppression strategy is designed to\nselect important and representative points for manual labelling. Besides, to\nbetter exploit the unlabelled data, we build a semi-supervised segmentation\nframework based on our active strategy. Extensive experiments on the S3DIS and\nScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and\n100% performance of fully-supervised baseline with only 0.07% and 0.1% training\ndata, respectively, outperforming the state-of-the-art weakly-supervised and\nactive learning methods. The code will be available at\nhttps://github.com/SmiletoE/HPAL.",
        "authors": [
            "Zongyi Xu",
            "Bo Yuan",
            "Shanshan Zhao",
            "Qianni Zhang",
            "Xinbo Gao"
        ]
    },
    {
        "title": "Doppelgangers: Learning to Disambiguate Images of Similar Structures",
        "url": "http://arxiv.org/abs/2309.02420",
        "abstract": "We consider the visual disambiguation task of determining whether a pair of\nvisually similar images depict the same or distinct 3D surfaces (e.g., the same\nor opposite sides of a symmetric building). Illusory image matches, where two\nimages observe distinct but visually similar 3D surfaces, can be challenging\nfor humans to differentiate, and can also lead 3D reconstruction algorithms to\nproduce erroneous results. We propose a learning-based approach to visual\ndisambiguation, formulating it as a binary classification task on image pairs.\nTo that end, we introduce a new dataset for this problem, Doppelgangers, which\nincludes image pairs of similar structures with ground truth labels. We also\ndesign a network architecture that takes the spatial distribution of local\nkeypoints and matches as input, allowing for better reasoning about both local\nand global cues. Our evaluation shows that our method can distinguish illusory\nmatches in difficult cases, and can be integrated into SfM pipelines to produce\ncorrect, disambiguated 3D reconstructions. See our project page for our code,\ndatasets, and more results: http://doppelgangers-3d.github.io/.",
        "authors": [
            "Ruojin Cai",
            "Joseph Tung",
            "Qianqian Wang",
            "Hadar Averbuch-Elor",
            "Bharath Hariharan",
            "Noah Snavely"
        ]
    },
    {
        "title": "Grounded Entity-Landmark Adaptive Pre-Training for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2308.12587",
        "abstract": "Cross-modal alignment is one key challenge for Vision-and-Language Navigation\n(VLN). Most existing studies concentrate on mapping the global instruction or\nsingle sub-instruction to the corresponding trajectory. However, another\ncritical problem of achieving fine-grained alignment at the entity level is\nseldom considered. To address this problem, we propose a novel Grounded\nEntity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve\nthe adaptive pre-training paradigm, we first introduce grounded entity-landmark\nhuman annotations into the Room-to-Room (R2R) dataset, named GEL-R2R.\nAdditionally, we adopt three grounded entity-landmark adaptive pre-training\nobjectives: 1) entity phrase prediction, 2) landmark bounding box prediction,\nand 3) entity-landmark semantic alignment, which explicitly supervise the\nlearning of fine-grained cross-modal alignment between entity phrases and\nenvironment landmarks. Finally, we validate our model on two downstream\nbenchmarks: VLN with descriptive instructions (R2R) and dialogue instructions\n(CVDN). The comprehensive experiments show that our GELA model achieves\nstate-of-the-art results on both tasks, demonstrating its effectiveness and\ngeneralizability.",
        "authors": [
            "Yibo Cui",
            "Liang Xie",
            "Yakun Zhang",
            "Meishan Zhang",
            "Ye Yan",
            "Erwei Yin"
        ]
    },
    {
        "title": "Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge",
        "url": "http://arxiv.org/abs/2308.09311",
        "abstract": "This paper proposes a novel lip reading framework, especially for\nlow-resource languages, which has not been well addressed in the previous\nliterature. Since low-resource languages do not have enough video-text paired\ndata to train the model to have sufficient power to model lip movements and\nlanguage, it is regarded as challenging to develop lip reading models for\nlow-resource languages. In order to mitigate the challenge, we try to learn\ngeneral speech knowledge, the ability to model lip movements, from a\nhigh-resource language through the prediction of speech units. It is known that\ndifferent languages partially share common phonemes, thus general speech\nknowledge learned from one language can be extended to other languages. Then,\nwe try to learn language-specific knowledge, the ability to model language, by\nproposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder\nsaves language-specific audio features into memory banks and can be trained on\naudio-text paired data which is more easily accessible than video-text paired\ndata. Therefore, with LMDecoder, we can transform the input speech units into\nlanguage-specific audio features and translate them into texts by utilizing the\nlearned rich language knowledge. Finally, by combining general speech knowledge\nand language-specific knowledge, we can efficiently develop lip reading models\neven for low-resource languages. Through extensive experiments using five\nlanguages, English, Spanish, French, Italian, and Portuguese, the effectiveness\nof the proposed method is evaluated.",
        "authors": [
            "Minsu Kim",
            "Jeong Hun Yeo",
            "Jeongsoo Choi",
            "Yong Man Ro"
        ]
    },
    {
        "title": "Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning",
        "url": "http://arxiv.org/abs/2309.05911",
        "abstract": "Deepfake has recently raised a plethora of societal concerns over its\npossible security threats and dissemination of fake information. Much research\non deepfake detection has been undertaken. However, detecting low quality as\nwell as simultaneously detecting different qualities of deepfakes still remains\na grave challenge. Most SOTA approaches are limited by using a single specific\nmodel for detecting certain deepfake video quality type. When constructing\nmultiple models with prior information about video quality, this kind of\nstrategy incurs significant computational cost, as well as model and training\ndata overhead. Further, it cannot be scalable and practical to deploy in\nreal-world settings. In this work, we propose a universal intra-model\ncollaborative learning framework to enable the effective and simultaneous\ndetection of different quality of deepfakes. That is, our approach is the\nquality-agnostic deepfake detection method, dubbed QAD . In particular, by\nobserving the upper bound of general error expectation, we maximize the\ndependency between intermediate representations of images from different\nquality levels via Hilbert-Schmidt Independence Criterion. In addition, an\nAdversarial Weight Perturbation module is carefully devised to enable the model\nto be more robust against image corruption while boosting the overall model's\nperformance. Extensive experiments over seven popular deepfake datasets\ndemonstrate the superiority of our QAD model over prior SOTA benchmarks.",
        "authors": [
            "Binh M. Le",
            "Simon S. Woo"
        ]
    },
    {
        "title": "Object-Centric Multiple Object Tracking",
        "url": "http://arxiv.org/abs/2309.00233",
        "abstract": "Unsupervised object-centric learning methods allow the partitioning of scenes\ninto entities without additional localization information and are excellent\ncandidates for reducing the annotation burden of multiple-object tracking (MOT)\npipelines. Unfortunately, they lack two key properties: objects are often split\ninto parts and are not consistently tracked over time. In fact,\nstate-of-the-art models achieve pixel-level accuracy and temporal consistency\nby relying on supervised object detection with additional ID labels for the\nassociation through time. This paper proposes a video object-centric model for\nMOT. It consists of an index-merge module that adapts the object-centric slots\ninto detection outputs and an object memory module that builds complete object\nprototypes to handle occlusions. Benefited from object-centric learning, we\nonly require sparse detection labels (0%-6.25%) for object localization and\nfeature binding. Relying on our self-supervised\nExpectation-Maximization-inspired loss for object association, our approach\nrequires no ID labels. Our experiments significantly narrow the gap between the\nexisting object-centric model and the fully supervised state-of-the-art and\noutperform several unsupervised trackers.",
        "authors": [
            "Zixu Zhao",
            "Jiaze Wang",
            "Max Horn",
            "Yizhuo Ding",
            "Tong He",
            "Zechen Bai",
            "Dominik Zietlow",
            "Carl-Johann Simon-Gabriel",
            "Bing Shuai",
            "Zhuowen Tu",
            "Thomas Brox",
            "Bernt Schiele",
            "Yanwei Fu",
            "Francesco Locatello",
            "Zheng Zhang",
            "Tianjun Xiao"
        ]
    },
    {
        "title": "HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation",
        "url": "http://arxiv.org/abs/2302.14581",
        "abstract": "2D-to-3D human pose lifting is fundamental for 3D human pose estimation\n(HPE), for which graph convolutional networks (GCNs) have proven inherently\nsuitable for modeling the human skeletal topology. However, the current\nGCN-based 3D HPE methods update the node features by aggregating their\nneighbors' information without considering the interaction of joints in\ndifferent joint synergies. Although some studies have proposed importing limb\ninformation to learn the movement patterns, the latent synergies among joints,\nsuch as maintaining balance are seldom investigated. We propose the Hop-wise\nGraphFormer with Intragroup Joint Refinement (HopFIR) architecture to tackle\nthe 3D HPE problem. HopFIR mainly consists of a novel hop-wise GraphFormer\n(HGF) module and an intragroup joint refinement (IJR) module. The HGF module\ngroups the joints by k-hop neighbors and applies a hopwise transformer-like\nattention mechanism to these groups to discover latent joint synergies. The IJR\nmodule leverages the prior limb information for peripheral joint refinement.\nExtensive experimental results show that HopFIR outperforms the SOTA methods by\na large margin, with a mean per-joint position error (MPJPE) on the Human3.6M\ndataset of 32.67 mm. We also demonstrate that the state-of-the-art GCN-based\nmethods can benefit from the proposed hop-wise attention mechanism with a\nsignificant improvement in performance: SemGCN and MGCN are improved by 8.9%\nand 4.5%, respectively.",
        "authors": [
            "Kai Zhai",
            "Qiang Nie",
            "Bo Ouyang",
            "Xiang Li",
            "Shanlin Yang"
        ]
    },
    {
        "title": "Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning",
        "url": "http://arxiv.org/abs/2308.02533",
        "abstract": "Deep neural networks are susceptible to adversarial examples, posing a\nsignificant security risk in critical applications. Adversarial Training (AT)\nis a well-established technique to enhance adversarial robustness, but it often\ncomes at the cost of decreased generalization ability. This paper proposes\nRobustness Critical Fine-Tuning (RiFT), a novel approach to enhance\ngeneralization without compromising adversarial robustness. The core idea of\nRiFT is to exploit the redundant capacity for robustness by fine-tuning the\nadversarially trained model on its non-robust-critical module. To do so, we\nintroduce module robust criticality (MRC), a measure that evaluates the\nsignificance of a given module to model robustness under worst-case weight\nperturbations. Using this measure, we identify the module with the lowest MRC\nvalue as the non-robust-critical module and fine-tune its weights to obtain\nfine-tuned weights. Subsequently, we linearly interpolate between the\nadversarially trained weights and fine-tuned weights to derive the optimal\nfine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18,\nResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, and\nTiny-ImageNet datasets. Our experiments show that \\method can significantly\nimprove both generalization and out-of-distribution robustness by around 1.5%\nwhile maintaining or even slightly enhancing adversarial robustness. Code is\navailable at https://github.com/microsoft/robustlearn.",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Xixu Hu",
            "Xing Xie",
            "Ge Yang"
        ]
    },
    {
        "title": "LightGlue: Local Feature Matching at Light Speed",
        "url": "http://arxiv.org/abs/2306.13643",
        "abstract": "We introduce LightGlue, a deep neural network that learns to match local\nfeatures across images. We revisit multiple design decisions of SuperGlue, the\nstate of the art in sparse matching, and derive simple but effective\nimprovements. Cumulatively, they make LightGlue more efficient - in terms of\nboth memory and computation, more accurate, and much easier to train. One key\nproperty is that LightGlue is adaptive to the difficulty of the problem: the\ninference is much faster on image pairs that are intuitively easy to match, for\nexample because of a larger visual overlap or limited appearance change. This\nopens up exciting prospects for deploying deep matchers in latency-sensitive\napplications like 3D reconstruction. The code and trained models are publicly\navailable at https://github.com/cvg/LightGlue.",
        "authors": [
            "Philipp Lindenberger",
            "Paul-Edouard Sarlin",
            "Marc Pollefeys"
        ]
    },
    {
        "title": "Masked Autoencoders are Efficient Class Incremental Learners",
        "url": "http://arxiv.org/abs/2308.12510",
        "abstract": "Class Incremental Learning (CIL) aims to sequentially learn new classes while\navoiding catastrophic forgetting of previous knowledge. We propose to use\nMasked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally\ndesigned to learn useful representations through reconstructive unsupervised\nlearning, and they can be easily integrated with a supervised loss for\nclassification. Moreover, MAEs can reliably reconstruct original input images\nfrom randomly selected patches, which we use to store exemplars from past tasks\nmore efficiently for CIL. We also propose a bilateral MAE framework to learn\nfrom image-level and embedding-level fusion, which produces better-quality\nreconstructed images and more stable representations. Our experiments confirm\nthat our approach performs better than the state-of-the-art on CIFAR-100,\nImageNet-Subset, and ImageNet-Full. The code is available at\nhttps://github.com/scok30/MAE-CIL .",
        "authors": [
            "Jiang-Tian Zhai",
            "Xialei Liu",
            "Andrew D. Bagdanov",
            "Ke Li",
            "Ming-Ming Cheng"
        ]
    },
    {
        "title": "Towards Semi-supervised Learning with Non-random Missing Labels",
        "url": "http://arxiv.org/abs/2308.08872",
        "abstract": "Semi-supervised learning (SSL) tackles the label missing problem by enabling\nthe effective usage of unlabeled data. While existing SSL methods focus on the\ntraditional setting, a practical and challenging scenario called label Missing\nNot At Random (MNAR) is usually ignored. In MNAR, the labeled and unlabeled\ndata fall into different class distributions resulting in biased label\nimputation, which deteriorates the performance of SSL models. In this work,\nclass transition tracking based Pseudo-Rectifying Guidance (PRG) is devised for\nMNAR. We explore the class-level guidance information obtained by the Markov\nrandom walk, which is modeled on a dynamically created graph built over the\nclass tracking matrix. PRG unifies the historical information of class\ndistribution and class transitions caused by the pseudo-rectifying procedure to\nmaintain the model's unbiased enthusiasm towards assigning pseudo-labels to all\nclasses, so as the quality of pseudo-labels on both popular classes and rare\nclasses in MNAR could be improved. Finally, we show the superior performance of\nPRG across a variety of MNAR scenarios, outperforming the latest SSL approaches\ncombining bias removal solutions by a large margin. Code and model weights are\navailable at https://github.com/NJUyued/PRG4SSL-MNAR.",
        "authors": [
            "Yue Duan",
            "Zhen Zhao",
            "Lei Qi",
            "Luping Zhou",
            "Lei Wang",
            "Yinghuan Shi"
        ]
    },
    {
        "title": "DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds",
        "url": "http://arxiv.org/abs/2306.06023",
        "abstract": "Existing offboard 3D detectors always follow a modular pipeline design to\ntake advantage of unlimited sequential point clouds. We have found that the\nfull potential of offboard 3D detectors is not explored mainly due to two\nreasons: (1) the onboard multi-object tracker cannot generate sufficient\ncomplete object trajectories, and (2) the motion state of objects poses an\ninevitable challenge for the object-centric refining stage in leveraging the\nlong-term temporal context representation. To tackle these problems, we propose\na novel paradigm of offboard 3D object detection, named DetZero. Concretely, an\noffline tracker coupled with a multi-frame detector is proposed to focus on the\ncompleteness of generated object tracks. An attention-mechanism refining module\nis proposed to strengthen contextual information interaction across long-term\nsequential point clouds for object refining with decomposed regression methods.\nExtensive experiments on Waymo Open Dataset show our DetZero outperforms all\nstate-of-the-art onboard and offboard 3D detection methods. Notably, DetZero\nranks 1st place on Waymo 3D object detection leaderboard with 85.15 mAPH (L2)\ndetection performance. Further experiments validate the application of taking\nthe place of human labels with such high-quality results. Our empirical study\nleads to rethinking conventions and interesting findings that can guide future\nresearch on offboard 3D object detection.",
        "authors": [
            "Tao Ma",
            "Xuemeng Yang",
            "Hongbin Zhou",
            "Xin Li",
            "Botian Shi",
            "Junjie Liu",
            "Yuchen Yang",
            "Zhizheng Liu",
            "Liang He",
            "Yu Qiao",
            "Yikang Li",
            "Hongsheng Li"
        ]
    },
    {
        "title": "ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition",
        "url": "http://arxiv.org/abs/2308.07815",
        "abstract": "Class imbalance is a common challenge in real-world recognition tasks, where\nthe majority of classes have few samples, also known as tail classes. We\naddress this challenge with the perspective of generalization and empirically\nfind that the promising Sharpness-Aware Minimization (SAM) fails to address\ngeneralization issues under the class-imbalanced setting. Through investigating\nthis specific type of task, we identify that its generalization bottleneck\nprimarily lies in the severe overfitting for tail classes with limited training\ndata. To overcome this bottleneck, we leverage class priors to restrict the\ngeneralization scope of the class-agnostic SAM and propose a class-aware\nsmoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the\nguidance of class priors, our ImbSAM specifically improves generalization\ntargeting tail classes. We also verify the efficacy of ImbSAM on two\nprototypical applications of class-imbalanced recognition: long-tailed\nclassification and semi-supervised anomaly detection, where our ImbSAM\ndemonstrates remarkable performance improvements for tail classes and anomaly.\nOur code implementation is available at\nhttps://github.com/cool-xuan/Imbalanced_SAM.",
        "authors": [
            "Yixuan Zhou",
            "Yi Qu",
            "Xing Xu",
            "Hengtao Shen"
        ]
    },
    {
        "title": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection",
        "url": "http://arxiv.org/abs/2203.13310",
        "abstract": "Monocular 3D object detection has long been a challenging task in autonomous\ndriving. Most existing methods follow conventional 2D detectors to first\nlocalize object centers, and then predict 3D attributes by neighboring\nfeatures. However, only using local visual features is insufficient to\nunderstand the scene-level 3D spatial structures and ignores the long-range\ninter-object depth relations. In this paper, we introduce the first DETR\nframework for Monocular DEtection with a depth-guided TRansformer, named\nMonoDETR. We modify the vanilla transformer to be depth-aware and guide the\nwhole detection process by contextual depth cues. Specifically, concurrent to\nthe visual encoder that captures object appearances, we introduce to predict a\nforeground depth map, and specialize a depth encoder to extract non-local depth\nembeddings. Then, we formulate 3D object candidates as learnable queries and\npropose a depth-guided decoder to conduct object-scene depth interactions. In\nthis way, each object query estimates its 3D attributes adaptively from the\ndepth-guided regions on the image and is no longer constrained to local visual\nfeatures. On KITTI benchmark with monocular images as input, MonoDETR achieves\nstate-of-the-art performance and requires no extra dense depth annotations.\nBesides, our depth-guided modules can also be plug-and-play to enhance\nmulti-view 3D object detectors on nuScenes dataset, demonstrating our superior\ngeneralization capacity. Code is available at\nhttps://github.com/ZrrSkywalker/MonoDETR.",
        "authors": [
            "Renrui Zhang",
            "Han Qiu",
            "Tai Wang",
            "Ziyu Guo",
            "Xuanzhuo Xu",
            "Ziteng Cui",
            "Yu Qiao",
            "Peng Gao",
            "Hongsheng Li"
        ]
    },
    {
        "title": "Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond",
        "url": "http://arxiv.org/abs/2307.08996",
        "abstract": "An authentic face restoration system is becoming increasingly demanding in\nmany computer vision applications, e.g., image enhancement, video\ncommunication, and taking portrait. Most of the advanced face restoration\nmodels can recover high-quality faces from low-quality ones but usually fail to\nfaithfully generate realistic and high-frequency details that are favored by\nusers. To achieve authentic restoration, we propose $\\textbf{IDM}$, an\n$\\textbf{I}$teratively learned face restoration system based on denoising\n$\\textbf{D}$iffusion $\\textbf{M}$odels (DDMs). We define the criterion of an\nauthentic face restoration system, and argue that denoising diffusion models\nare naturally endowed with this property from two aspects: intrinsic iterative\nrefinement and extrinsic iterative enhancement. Intrinsic learning can preserve\nthe content well and gradually refine the high-quality details, while extrinsic\nenhancement helps clean the data and improve the restoration task one step\nfurther. We demonstrate superior performance on blind face restoration tasks.\nBeyond restoration, we find the authentically cleaned data by the proposed\nrestoration system is also helpful to image generation tasks in terms of\ntraining stabilization and sample quality. Without modifying the models, we\nachieve better quality than state-of-the-art on FFHQ and ImageNet generation\nusing either GANs or diffusion models.",
        "authors": [
            "Yang Zhao",
            "Tingbo Hou",
            "Yu-Chuan Su",
            "Xuhui Jia. Yandong Li",
            "Matthias Grundmann"
        ]
    },
    {
        "title": "LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation",
        "url": "http://arxiv.org/abs/2309.09294",
        "abstract": "Gestures are non-verbal but important behaviors accompanying people's speech.\nWhile previous methods are able to generate speech rhythm-synchronized\ngestures, the semantic context of the speech is generally lacking in the\ngesticulations. Although semantic gestures do not occur very regularly in human\nspeech, they are indeed the key for the audience to understand the speech\ncontext in a more immersive environment. Hence, we introduce LivelySpeaker, a\nframework that realizes semantics-aware co-speech gesture generation and offers\nseveral control handles. In particular, our method decouples the task into two\nstages: script-based gesture generation and audio-guided rhythm refinement.\nSpecifically, the script-based gesture generation leverages the pre-trained\nCLIP text embeddings as the guidance for generating gestures that are highly\nsemantically aligned with the script. Then, we devise a simple but effective\ndiffusion-based gesture generation backbone simply using pure MLPs, that is\nconditioned on only audio signals and learns to gesticulate with realistic\nmotions. We utilize such powerful prior to rhyme the script-guided gestures\nwith the audio signals, notably in a zero-shot setting. Our novel two-stage\ngeneration framework also enables several applications, such as changing the\ngesticulation style, editing the co-speech gestures via textual prompting, and\ncontrolling the semantic awareness and rhythm alignment with guided diffusion.\nExtensive experiments demonstrate the advantages of the proposed framework over\ncompeting methods. In addition, our core diffusion-based generative model also\nachieves state-of-the-art performance on two benchmarks. The code and model\nwill be released to facilitate future research.",
        "authors": [
            "Yihao Zhi",
            "Xiaodong Cun",
            "Xuelin Chen",
            "Xi Shen",
            "Wen Guo",
            "Shaoli Huang",
            "Shenghua Gao"
        ]
    },
    {
        "title": "Contrastive Feature Masking Open-Vocabulary Vision Transformer",
        "url": "http://arxiv.org/abs/2309.00775",
        "abstract": "We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an\nimage-text pretraining methodology that achieves simultaneous learning of\nimage- and region-level representation for open-vocabulary object detection\n(OVD). Our approach combines the masked autoencoder (MAE) objective into the\ncontrastive learning objective to improve the representation for localization\ntasks. Unlike standard MAE, we perform reconstruction in the joint image-text\nembedding space, rather than the pixel space as is customary with the classical\nMAE method, which causes the model to better learn region-level semantics.\nMoreover, we introduce Positional Embedding Dropout (PED) to address scale\nvariation between image-text pretraining and detection finetuning by randomly\ndropping out the positional embeddings during pretraining. PED improves\ndetection performance and enables the use of a frozen ViT backbone as a region\nclassifier, preventing the forgetting of open-vocabulary knowledge during\ndetection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT\nachieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6\npoints and achieves better zero-shot detection transfer. Finally, CFM-ViT\nacquires strong image-level representation, outperforming the state of the art\non 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.",
        "authors": [
            "Dahun Kim",
            "Anelia Angelova",
            "Weicheng Kuo"
        ]
    },
    {
        "title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment",
        "url": "http://arxiv.org/abs/2207.13085",
        "abstract": "Detection transformer (DETR) relies on one-to-one assignment, assigning one\nground-truth object to one prediction, for end-to-end detection without NMS\npost-processing. It is known that one-to-many assignment, assigning one\nground-truth object to multiple predictions, succeeds in detection methods such\nas Faster R-CNN and FCOS. While the naive one-to-many assignment does not work\nfor DETR, and it remains challenging to apply one-to-many assignment for DETR\ntraining. In this paper, we introduce Group DETR, a simple yet efficient DETR\ntraining approach that introduces a group-wise way for one-to-many assignment.\nThis approach involves using multiple groups of object queries, conducting\none-to-one assignment within each group, and performing decoder self-attention\nseparately. It resembles data augmentation with automatically-learned object\nquery augmentation. It is also equivalent to simultaneously training\nparameter-sharing networks of the same architecture, introducing more\nsupervision and thus improving DETR training. The inference process is the same\nas DETR trained normally and only needs one group of queries without any\narchitecture modification. Group DETR is versatile and is applicable to various\nDETR variants. The experiments show that Group DETR significantly speeds up the\ntraining convergence and improves the performance of various DETR-based models.\nCode will be available at \\url{https://github.com/Atten4Vis/GroupDETR}.",
        "authors": [
            "Qiang Chen",
            "Xiaokang Chen",
            "Jian Wang",
            "Shan Zhang",
            "Kun Yao",
            "Haocheng Feng",
            "Junyu Han",
            "Errui Ding",
            "Gang Zeng",
            "Jingdong Wang"
        ]
    },
    {
        "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models",
        "url": "http://arxiv.org/abs/2303.06628",
        "abstract": "Continual learning (CL) can help pre-trained vision-language models\nefficiently adapt to new or under-trained data distributions without\nre-training. Nevertheless, during the continual training of the Contrastive\nLanguage-Image Pre-training (CLIP) model, we observe that the model's zero-shot\ntransfer ability significantly degrades due to catastrophic forgetting.\nExisting CL methods can mitigate forgetting by replaying previous data.\nHowever, since the CLIP dataset is private, replay methods cannot access the\npre-training dataset. In addition, replaying data of previously learned\ndownstream tasks can enhance their performance but comes at the cost of\nsacrificing zero-shot performance. To address this challenge, we propose a\nnovel method ZSCL to prevent zero-shot transfer degradation in the continual\nlearning of vision-language models in both feature and parameter space. In the\nfeature space, a reference dataset is introduced for distillation between the\ncurrent and initial models. The reference dataset should have semantic\ndiversity but no need to be labeled, seen in pre-training, or matched\nimage-text pairs. In parameter space, we prevent a large parameter shift by\naveraging weights during the training. We propose a more challenging\nMulti-domain Task Incremental Learning (MTIL) benchmark to evaluate different\nmethods, where tasks are from various domains instead of class-separated in a\nsingle dataset. Our method outperforms other methods in the traditional\nclass-incremental learning setting and the MTIL by 9.7% average score. Our code\nlocates at https://github.com/Thunderbeee/ZSCL.",
        "authors": [
            "Zangwei Zheng",
            "Mingyuan Ma",
            "Kai Wang",
            "Ziheng Qin",
            "Xiangyu Yue",
            "Yang You"
        ]
    },
    {
        "title": "EGC: Image Generation and Classification via a Diffusion Energy-Based Model",
        "url": "http://arxiv.org/abs/2304.02012",
        "abstract": "Learning image classification and image generation using the same set of\nnetwork parameters is a challenging problem. Recent advanced approaches perform\nwell in one task often exhibit poor performance in the other. This work\nintroduces an energy-based classifier and generator, namely EGC, which can\nachieve superior performance in both tasks using a single neural network.\nUnlike a conventional classifier that outputs a label given an image (i.e., a\nconditional distribution $p(y|\\mathbf{x})$), the forward pass in EGC is a\nclassifier that outputs a joint distribution $p(\\mathbf{x},y)$, enabling an\nimage generator in its backward pass by marginalizing out the label $y$. This\nis done by estimating the energy and classification probability given a noisy\nimage in the forward pass, while denoising it using the score function\nestimated in the backward pass. EGC achieves competitive generation results\ncompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN\nChurch, while achieving superior classification accuracy and robustness against\nadversarial attacks on CIFAR-10. This work represents the first successful\nattempt to simultaneously excel in both tasks using a single set of network\nparameters. We believe that EGC bridges the gap between discriminative and\ngenerative learning.",
        "authors": [
            "Qiushan Guo",
            "Chuofan Ma",
            "Yi Jiang",
            "Zehuan Yuan",
            "Yizhou Yu",
            "Ping Luo"
        ]
    },
    {
        "title": "OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction",
        "url": "http://arxiv.org/abs/2304.05316",
        "abstract": "The vision-based perception for autonomous driving has undergone a\ntransformation from the bird-eye-view (BEV) representations to the 3D semantic\noccupancy. Compared with the BEV planes, the 3D semantic occupancy further\nprovides structural information along the vertical direction. This paper\npresents OccFormer, a dual-path transformer network to effectively process the\n3D volume for semantic occupancy prediction. OccFormer achieves a long-range,\ndynamic, and efficient encoding of the camera-generated 3D voxel features. It\nis obtained by decomposing the heavy 3D processing into the local and global\ntransformer pathways along the horizontal plane. For the occupancy decoder, we\nadapt the vanilla Mask2Former for 3D semantic occupancy by proposing\npreserve-pooling and class-guided sampling, which notably mitigate the sparsity\nand class imbalance. Experimental results demonstrate that OccFormer\nsignificantly outperforms existing methods for semantic scene completion on\nSemanticKITTI dataset and for LiDAR semantic segmentation on nuScenes dataset.\nCode is available at \\url{https://github.com/zhangyp15/OccFormer}.",
        "authors": [
            "Yunpeng Zhang",
            "Zheng Zhu",
            "Dalong Du"
        ]
    },
    {
        "title": "Probabilistic Triangulation for Uncalibrated Multi-View 3D Human Pose Estimation",
        "url": "http://arxiv.org/abs/2309.04756",
        "abstract": "3D human pose estimation has been a long-standing challenge in computer\nvision and graphics, where multi-view methods have significantly progressed but\nare limited by the tedious calibration processes. Existing multi-view methods\nare restricted to fixed camera pose and therefore lack generalization ability.\nThis paper presents a novel Probabilistic Triangulation module that can be\nembedded in a calibrated 3D human pose estimation method, generalizing it to\nuncalibration scenes. The key idea is to use a probability distribution to\nmodel the camera pose and iteratively update the distribution from 2D features\ninstead of using camera pose. Specifically, We maintain a camera pose\ndistribution and then iteratively update this distribution by computing the\nposterior probability of the camera pose through Monte Carlo sampling. This\nway, the gradients can be directly back-propagated from the 3D pose estimation\nto the 2D heatmap, enabling end-to-end training. Extensive experiments on\nHuman3.6M and CMU Panoptic demonstrate that our method outperforms other\nuncalibration methods and achieves comparable results with state-of-the-art\ncalibration methods. Thus, our method achieves a trade-off between estimation\naccuracy and generalizability. Our code is in\nhttps://github.com/bymaths/probabilistic_triangulation",
        "authors": [
            "Boyuan Jiang",
            "Lei Hu",
            "Shihong Xia"
        ]
    },
    {
        "title": "Joint Metrics Matter: A Better Standard for Trajectory Forecasting",
        "url": "http://arxiv.org/abs/2305.06292",
        "abstract": "Multi-modal trajectory forecasting methods commonly evaluate using\nsingle-agent metrics (marginal metrics), such as minimum Average Displacement\nError (ADE) and Final Displacement Error (FDE), which fail to capture joint\nperformance of multiple interacting agents. Only focusing on marginal metrics\ncan lead to unnatural predictions, such as colliding trajectories or diverging\ntrajectories for people who are clearly walking together as a group.\nConsequently, methods optimized for marginal metrics lead to overly-optimistic\nestimations of performance, which is detrimental to progress in trajectory\nforecasting research. In response to the limitations of marginal metrics, we\npresent the first comprehensive evaluation of state-of-the-art (SOTA)\ntrajectory forecasting methods with respect to multi-agent metrics (joint\nmetrics): JADE, JFDE, and collision rate. We demonstrate the importance of\njoint metrics as opposed to marginal metrics with quantitative evidence and\nqualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We\nintroduce a new loss function incorporating joint metrics that, when applied to\na SOTA trajectory forecasting method, achieves a 7\\% improvement in JADE / JFDE\non the ETH / UCY datasets with respect to the previous SOTA. Our results also\nindicate that optimizing for joint metrics naturally leads to an improvement in\ninteraction modeling, as evidenced by a 16\\% decrease in mean collision rate on\nthe ETH / UCY datasets with respect to the previous SOTA. Code is available at\n\\texttt{\\hyperlink{https://github.com/ericaweng/joint-metrics-matter}{github.com/ericaweng/joint-metrics-matter}}.",
        "authors": [
            "Erica Weng",
            "Hana Hoshino",
            "Deva Ramanan",
            "Kris Kitani"
        ]
    },
    {
        "title": "TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer",
        "url": "http://arxiv.org/abs/2211.10705",
        "abstract": "In this paper, we introduce a set of simple yet effective TOken REduction\n(TORE) strategies for Transformer-based Human Mesh Recovery from monocular\nimages. Current SOTA performance is achieved by Transformer-based structures.\nHowever, they suffer from high model complexity and computation cost caused by\nredundant tokens. We propose token reduction strategies based on two important\naspects, i.e., the 3D geometry structure and 2D image feature, where we\nhierarchically recover the mesh geometry with priors from body structure and\nconduct token clustering to pass fewer but more discriminative image feature\ntokens to the Transformer. Our method massively reduces the number of tokens\ninvolved in high-complexity interactions in the Transformer. This leads to a\nsignificantly reduced computational cost while still achieving competitive or\neven higher accuracy in shape recovery. Extensive experiments across a wide\nrange of benchmarks validate the superior effectiveness of the proposed method.\nWe further demonstrate the generalizability of our method on hand mesh\nrecovery. Visit our project page at\nhttps://frank-zy-dou.github.io/projects/Tore/index.html.",
        "authors": [
            "Zhiyang Dou",
            "Qingxuan Wu",
            "Cheng Lin",
            "Zeyu Cao",
            "Qiangqiang Wu",
            "Weilin Wan",
            "Taku Komura",
            "Wenping Wang"
        ]
    },
    {
        "title": "Test Time Adaptation for Blind Image Quality Assessment",
        "url": "http://arxiv.org/abs/2307.14735",
        "abstract": "While the design of blind image quality assessment (IQA) algorithms has\nimproved significantly, the distribution shift between the training and testing\nscenarios often leads to a poor performance of these methods at inference time.\nThis motivates the study of test time adaptation (TTA) techniques to improve\ntheir performance at inference time. Existing auxiliary tasks and loss\nfunctions used for TTA may not be relevant for quality-aware adaptation of the\npre-trained model. In this work, we introduce two novel quality-relevant\nauxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In\nparticular, we introduce a group contrastive loss at the batch level and a\nrelative rank loss at the sample level to make the model quality aware and\nadapt to the target data. Our experiments reveal that even using a small batch\nof images from the test distribution helps achieve significant improvement in\nperformance by updating the batch normalization statistics of the source model.",
        "authors": [
            "Subhadeep Roy",
            "Shankhanil Mitra",
            "Soma Biswas",
            "Rajiv Soundararajan"
        ]
    },
    {
        "title": "GeT: Generative Target Structure Debiasing for Domain Adaptation",
        "url": "http://arxiv.org/abs/2308.10205",
        "abstract": "Domain adaptation (DA) aims to transfer knowledge from a fully labeled source\nto a scarcely labeled or totally unlabeled target under domain shift. Recently,\nsemi-supervised learning-based (SSL) techniques that leverage pseudo labeling\nhave been increasingly used in DA. Despite the competitive performance, these\npseudo labeling methods rely heavily on the source domain to generate pseudo\nlabels for the target domain and therefore still suffer considerably from\nsource data bias. Moreover, class distribution bias in the target domain is\nalso often ignored in the pseudo label generation and thus leading to further\ndeterioration of performance. In this paper, we propose GeT that learns a\nnon-bias target embedding distribution with high quality pseudo labels.\nSpecifically, we formulate an online target generative classifier to induce the\ntarget distribution into distinctive Gaussian components weighted by their\nclass priors to mitigate source data bias and enhance target class\ndiscriminability. We further propose a structure similarity regularization\nframework to alleviate target class distribution bias and further improve\ntarget class discriminability. Experimental results show that our proposed GeT\nis effective and achieves consistent improvements under various DA settings\nwith and without class distribution bias. Our code is available at:\nhttps://lulusindazc.github.io/getproject/.",
        "authors": [
            "Can Zhang",
            "Gim Hee Lee"
        ]
    },
    {
        "title": "D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation",
        "url": "http://arxiv.org/abs/2308.04197",
        "abstract": "Temporal sentence grounding (TSG) aims to locate a specific moment from an\nuntrimmed video with a given natural language query. Recently, weakly\nsupervised methods still have a large performance gap compared to fully\nsupervised ones, while the latter requires laborious timestamp annotations. In\nthis study, we aim to reduce the annotation cost yet keep competitive\nperformance for TSG task compared to fully supervised ones. To achieve this\ngoal, we investigate a recently proposed glance-supervised temporal sentence\ngrounding task, which requires only single frame annotation (referred to as\nglance annotation) for each query. Under this setup, we propose a Dynamic\nGaussian prior based Grounding framework with Glance annotation (D3G), which\nconsists of a Semantic Alignment Group Contrastive Learning module (SA-GCL) and\na Dynamic Gaussian prior Adjustment module (DGA). Specifically, SA-GCL samples\nreliable positive moments from a 2D temporal map via jointly leveraging\nGaussian prior and semantic consistency, which contributes to aligning the\npositive sentence-moment pairs in the joint embedding space. Moreover, to\nalleviate the annotation bias resulting from glance annotation and model\ncomplex queries consisting of multiple events, we propose the DGA module, which\nadjusts the distribution dynamically to approximate the ground truth of target\nmoments. Extensive experiments on three challenging benchmarks verify the\neffectiveness of the proposed D3G. It outperforms the state-of-the-art weakly\nsupervised methods by a large margin and narrows the performance gap compared\nto fully supervised methods. Code is available at\nhttps://github.com/solicucu/D3G.",
        "authors": [
            "Hanjun Li",
            "Xiujun Shu",
            "Sunan He",
            "Ruizhi Qiao",
            "Wei Wen",
            "Taian Guo",
            "Bei Gan",
            "Xing Sun"
        ]
    },
    {
        "title": "GEDepth: Ground Embedding for Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2309.09975",
        "abstract": "Monocular depth estimation is an ill-posed problem as the same 2D image can\nbe projected from infinite 3D scenes. Although the leading algorithms in this\nfield have reported significant improvement, they are essentially geared to the\nparticular compound of pictorial observations and camera parameters (i.e.,\nintrinsics and extrinsics), strongly limiting their generalizability in\nreal-world scenarios. To cope with this challenge, this paper proposes a novel\nground embedding module to decouple camera parameters from pictorial cues, thus\npromoting the generalization capability. Given camera parameters, the proposed\nmodule generates the ground depth, which is stacked with the input image and\nreferenced in the final depth prediction. A ground attention is designed in the\nmodule to optimally combine ground depth with residual depth. Our ground\nembedding is highly flexible and lightweight, leading to a plug-in module that\nis amenable to be integrated into various depth estimation networks.\nExperiments reveal that our approach achieves the state-of-the-art results on\npopular benchmarks, and more importantly, renders significant generalization\nimprovement on a wide range of cross-domain tests.",
        "authors": [
            "Xiaodong Yang",
            "Zhuang Ma",
            "Zhiyu Ji",
            "Zhe Ren"
        ]
    },
    {
        "title": "DETRs with Collaborative Hybrid Assignments Training",
        "url": "http://arxiv.org/abs/2211.12860",
        "abstract": "In this paper, we provide the observation that too few queries assigned as\npositive samples in DETR with one-to-one set matching leads to sparse\nsupervision on the encoder's output which considerably hurt the discriminative\nfeature learning of the encoder and vice visa for attention learning in the\ndecoder. To alleviate this, we present a novel collaborative hybrid assignments\ntraining scheme, namely $\\mathcal{C}$o-DETR, to learn more efficient and\neffective DETR-based detectors from versatile label assignment manners. This\nnew training scheme can easily enhance the encoder's learning ability in\nend-to-end detectors by training the multiple parallel auxiliary heads\nsupervised by one-to-many label assignments such as ATSS and Faster RCNN. In\naddition, we conduct extra customized positive queries by extracting the\npositive coordinates from these auxiliary heads to improve the training\nefficiency of positive samples in the decoder. In inference, these auxiliary\nheads are discarded and thus our method introduces no additional parameters and\ncomputational cost to the original detector while requiring no hand-crafted\nnon-maximum suppression (NMS). We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach on DETR variants, including DAB-DETR,\nDeformable-DETR, and DINO-Deformable-DETR. The state-of-the-art\nDINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO\nval. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on\nCOCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear\nmargins with much fewer model sizes. Codes are available at\n\\url{https://github.com/Sense-X/Co-DETR}.",
        "authors": [
            "Zhuofan Zong",
            "Guanglu Song",
            "Yu Liu"
        ]
    },
    {
        "title": "Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape",
        "url": "http://arxiv.org/abs/2308.11737",
        "abstract": "Accurately estimating the 3D pose and shape is an essential step towards\nunderstanding animal behavior, and can potentially benefit many downstream\napplications, such as wildlife conservation. However, research in this area is\nheld back by the lack of a comprehensive and diverse dataset with high-quality\n3D pose and shape annotations. In this paper, we propose Animal3D, the first\ncomprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D\nconsists of 3379 images collected from 40 mammal species, high-quality\nannotations of 26 keypoints, and importantly the pose and shape parameters of\nthe SMAL model. All annotations were labeled and checked manually in a\nmulti-stage process to ensure highest quality results. Based on the Animal3D\ndataset, we benchmark representative shape and pose estimation models at: (1)\nsupervised learning from only the Animal3D data, (2) synthetic to real transfer\nfrom synthetically generated images, and (3) fine-tuning human pose and shape\nestimation models. Our experimental results demonstrate that predicting the 3D\nshape and pose of animals across species remains a very challenging task,\ndespite significant advances in human pose estimation. Our results further\ndemonstrate that synthetic pre-training is a viable strategy to boost the model\nperformance. Overall, Animal3D opens new directions for facilitating future\nresearch in animal 3D pose and shape estimation, and is publicly available.",
        "authors": [
            "Jiacong Xu",
            "Yi Zhang",
            "Jiawei Peng",
            "Wufei Ma",
            "Artur Jesslen",
            "Pengliang Ji",
            "Qixin Hu",
            "Jiehua Zhang",
            "Qihao Liu",
            "Jiahao Wang",
            "Wei Ji",
            "Chen Wang",
            "Xiaoding Yuan",
            "Prakhar Kaushik",
            "Guofeng Zhang",
            "Jie Liu",
            "Yushan Xie",
            "Yawen Cui",
            "Alan Yuille",
            "Adam Kortylewski"
        ]
    },
    {
        "title": "Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor",
        "url": "http://arxiv.org/abs/2308.14383",
        "abstract": "Light-weight time-of-flight (ToF) depth sensors are compact and\ncost-efficient, and thus widely used on mobile devices for tasks such as\nautofocus and obstacle detection. However, due to the sparse and noisy depth\nmeasurements, these sensors have rarely been considered for dense geometry\nreconstruction. In this work, we present the first dense SLAM system with a\nmonocular camera and a light-weight ToF sensor. Specifically, we propose a\nmulti-modal implicit scene representation that supports rendering both the\nsignals from the RGB camera and light-weight ToF sensor which drives the\noptimization by comparing with the raw sensor inputs. Moreover, in order to\nguarantee successful pose tracking and reconstruction, we exploit a predicted\ndepth as an intermediate supervision and develop a coarse-to-fine optimization\nstrategy for efficient learning of the implicit representation. At last, the\ntemporal information is explicitly exploited to deal with the noisy signals\nfrom light-weight ToF sensors to improve the accuracy and robustness of the\nsystem. Experiments demonstrate that our system well exploits the signals of\nlight-weight ToF sensors and achieves competitive results both on camera\ntracking and dense scene reconstruction. Project page:\n\\url{https://zju3dv.github.io/tof_slam/}.",
        "authors": [
            "Xinyang Liu",
            "Yijin Li",
            "Yanbin Teng",
            "Hujun Bao",
            "Guofeng Zhang",
            "Yinda Zhang",
            "Zhaopeng Cui"
        ]
    },
    {
        "title": "MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.09421",
        "abstract": "In the field of monocular 3D detection, it is common practice to utilize\nscene geometric clues to enhance the detector's performance. However, many\nexisting works adopt these clues explicitly such as estimating a depth map and\nback-projecting it into 3D space. This explicit methodology induces sparsity in\n3D representations due to the increased dimensionality from 2D to 3D, and leads\nto substantial information loss, especially for distant and occluded objects.\nTo alleviate this issue, we propose MonoNeRD, a novel detection framework that\ncan infer dense 3D geometry and occupancy. Specifically, we model scenes with\nSigned Distance Functions (SDF), facilitating the production of dense 3D\nrepresentations. We treat these representations as Neural Radiance Fields\n(NeRF) and then employ volume rendering to recover RGB images and depth maps.\nTo the best of our knowledge, this work is the first to introduce volume\nrendering for M3D, and demonstrates the potential of implicit reconstruction\nfor image-based 3D perception. Extensive experiments conducted on the KITTI-3D\nbenchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.\nCodes are available at https://github.com/cskkxjk/MonoNeRD.",
        "authors": [
            "Junkai Xu",
            "Liang Peng",
            "Haoran Cheng",
            "Hao Li",
            "Wei Qian",
            "Ke Li",
            "Wenxiao Wang",
            "Deng Cai"
        ]
    },
    {
        "title": "Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver",
        "url": "http://arxiv.org/abs/2304.01289",
        "abstract": "The main challenge of monocular 3D object detection is the accurate\nlocalization of 3D center. Motivated by a new and strong observation that this\nchallenge can be remedied by a 3D-space local-grid search scheme in an ideal\ncase, we propose a stage-wise approach, which combines the information flow\nfrom 2D-to-3D (3D bounding box proposal generation with a single 2D image) and\n3D-to-2D (proposal verification by denoising with 3D-to-2D contexts) in a\ntop-down manner. Specifically, we first obtain initial proposals from\noff-the-shelf backbone monocular 3D detectors. Then, we generate a 3D anchor\nspace by local-grid sampling from the initial proposals. Finally, we perform 3D\nbounding box denoising at the 3D-to-2D proposal verification stage. To\neffectively learn discriminative features for denoising highly overlapped\nproposals, this paper presents a method of using the Perceiver I/O model to\nfuse the 3D-to-2D geometric information and the 2D appearance information. With\nthe encoded latent representation of a proposal, the verification head is\nimplemented with a self-attention module. Our method, named as MonoXiver, is\ngeneric and can be easily adapted to any backbone monocular 3D detectors.\nExperimental results on the well-established KITTI dataset and the challenging\nlarge-scale Waymo dataset show that MonoXiver consistently achieves improvement\nwith limited computation overhead.",
        "authors": [
            "Xianpeng Liu",
            "Ce Zheng",
            "Kelvin Cheng",
            "Nan Xue",
            "Guo-Jun Qi",
            "Tianfu Wu"
        ]
    },
    {
        "title": "TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses",
        "url": "http://arxiv.org/abs/2306.05888",
        "abstract": "3D multi-object tracking (MOT) is vital for many applications including\nautonomous driving vehicles and service robots. With the commonly used\ntracking-by-detection paradigm, 3D MOT has made important progress in recent\nyears. However, these methods only use the detection boxes of the current frame\nto obtain trajectory-box association results, which makes it impossible for the\ntracker to recover objects missed by the detector. In this paper, we present\nTrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover the\nmissed object by detector, we generates multiple trajectory hypotheses with\nhybrid candidate boxes, including temporally predicted boxes and current-frame\ndetection boxes, for trajectory-box association. The predicted boxes can\npropagate object's history trajectory information to the current frame and thus\nthe network can tolerate short-term miss detection of the tracked objects. We\ncombine long-term object motion feature and short-term object appearance\nfeature to create per-hypothesis feature embedding, which reduces the\ncomputational overhead for spatial-temporal encoding. Additionally, we\nintroduce a Global-Local Interaction Module to conduct information interaction\namong all hypotheses and models their spatial relations, leading to accurate\nestimation of hypotheses. Our TrajectoryFormer achieves state-of-the-art\nperformance on the Waymo 3D MOT benchmarks. Code is available at\nhttps://github.com/poodarchu/EFG .",
        "authors": [
            "Xuesong Chen",
            "Shaoshuai Shi",
            "Chao Zhang",
            "Benjin Zhu",
            "Qiang Wang",
            "Ka Chun Cheung",
            "Simon See",
            "Hongsheng Li"
        ]
    },
    {
        "title": "See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data",
        "url": "http://arxiv.org/abs/2307.10782",
        "abstract": "Zero-shot point cloud segmentation aims to make deep models capable of\nrecognizing novel objects in point cloud that are unseen in the training phase.\nRecent trends favor the pipeline which transfers knowledge from seen classes\nwith labels to unseen classes without labels. They typically align visual\nfeatures with semantic features obtained from word embedding by the supervision\nof seen classes' annotations. However, point cloud contains limited information\nto fully match with semantic features. In fact, the rich appearance information\nof images is a natural complement to the textureless point cloud, which is not\nwell explored in previous literature. Motivated by this, we propose a novel\nmulti-modal zero-shot learning method to better utilize the complementary\ninformation of point clouds and images for more accurate visual-semantic\nalignment. Extensive experiments are performed in two popular benchmarks, i.e.,\nSemanticKITTI and nuScenes, and our method outperforms current SOTA methods\nwith 52% and 49% improvement on average for unseen class mIoU, respectively.",
        "authors": [
            "Yuhang Lu",
            "Qi Jiang",
            "Runnan Chen",
            "Yuenan Hou",
            "Xinge Zhu",
            "Yuexin Ma"
        ]
    },
    {
        "title": "SKED: Sketch-guided Text-based 3D Editing",
        "url": "http://arxiv.org/abs/2303.10735",
        "abstract": "Text-to-image diffusion models are gradually introduced into computer\ngraphics, recently enabling the development of Text-to-3D pipelines in an open\ndomain. However, for interactive editing purposes, local manipulations of\ncontent through a simplistic textual interface can be arduous. Incorporating\nuser guided sketches with Text-to-image pipelines offers users more intuitive\ncontrol. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing\nNeural Radiance Fields (NeRF) through gradients from arbitrary rendering views,\nconditioning on sketches is not straightforward. In this paper, we present\nSKED, a technique for editing 3D shapes represented by NeRFs. Our technique\nutilizes as few as two guiding sketches from different views to alter an\nexisting neural field. The edited region respects the prompt semantics through\na pre-trained diffusion model. To ensure the generated output adheres to the\nprovided sketches, we propose novel loss functions to generate the desired\nedits while preserving the density and radiance of the base instance. We\ndemonstrate the effectiveness of our proposed method through several\nqualitative and quantitative experiments. https://sked-paper.github.io/",
        "authors": [
            "Aryan Mikaeili",
            "Or Perel",
            "Mehdi Safaee",
            "Daniel Cohen-Or",
            "Ali Mahdavi-Amiri"
        ]
    },
    {
        "title": "MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors",
        "url": "http://arxiv.org/abs/2303.05071",
        "abstract": "3D single object tracking has been a crucial problem for decades with\nnumerous applications such as autonomous driving. Despite its wide-ranging use,\nthis task remains challenging due to the significant appearance variation\ncaused by occlusion and size differences among tracked targets. To address\nthese issues, we present MBPTrack, which adopts a Memory mechanism to utilize\npast information and formulates localization in a coarse-to-fine scheme using\nBox Priors given in the first frame. Specifically, past frames with targetness\nmasks serve as an external memory, and a transformer-based module propagates\ntracked target cues from the memory to the current frame. To precisely localize\nobjects of all sizes, MBPTrack first predicts the target center via Hough\nvoting. By leveraging box priors given in the first frame, we adaptively sample\nreference points around the target center that roughly cover the target of\ndifferent sizes. Then, we obtain dense feature maps by aggregating point\nfeatures into the reference points, where localization can be performed more\neffectively. Extensive experiments demonstrate that MBPTrack achieves\nstate-of-the-art performance on KITTI, nuScenes and Waymo Open Dataset, while\nrunning at 50 FPS on a single RTX3090 GPU.",
        "authors": [
            "Tian-Xing Xu",
            "Yuan-Chen Guo",
            "Yu-Kun Lai",
            "Song-Hai Zhang"
        ]
    },
    {
        "title": "Novel-View Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views",
        "url": "http://arxiv.org/abs/2308.11198",
        "abstract": "Hand-object interaction understanding and the barely addressed novel view\nsynthesis are highly desired in the immersive communication, whereas it is\nchallenging due to the high deformation of hand and heavy occlusions between\nhand and object. In this paper, we propose a neural rendering and pose\nestimation system for hand-object interaction from sparse views, which can also\nenable 3D hand-object interaction editing. We share the inspiration from recent\nscene understanding work that shows a scene specific model built beforehand can\nsignificantly improve and unblock vision tasks especially when inputs are\nsparse, and extend it to the dynamic hand-object interaction scenario and\npropose to solve the problem in two stages. We first learn the shape and\nappearance prior knowledge of hands and objects separately with the neural\nrepresentation at the offline stage. During the online stage, we design a\nrendering-based joint model fitting framework to understand the dynamic\nhand-object interaction with the pre-built hand and object models as well as\ninteraction priors, which thereby overcomes penetration and separation issues\nbetween hand and object and also enables novel view synthesis. In order to get\nstable contact during the hand-object interaction process in a sequence, we\npropose a stable contact loss to make the contact region to be consistent.\nExperiments demonstrate that our method outperforms the state-of-the-art\nmethods. Code and dataset are available in project webpage\nhttps://iscas3dv.github.io/HO-NeRF.",
        "authors": [
            "Wentian Qu",
            "Zhaopeng Cui",
            "Yinda Zhang",
            "Chenyu Meng",
            "Cuixia Ma",
            "Xiaoming Deng",
            "Hongan Wang"
        ]
    },
    {
        "title": "EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes",
        "url": "http://arxiv.org/abs/2307.07961",
        "abstract": "Visual Emotion Analysis (VEA) aims at predicting people's emotional responses\nto visual stimuli. This is a promising, yet challenging, task in affective\ncomputing, which has drawn increasing attention in recent years. Most of the\nexisting work in this area focuses on feature design, while little attention\nhas been paid to dataset construction. In this work, we introduce EmoSet, the\nfirst large-scale visual emotion dataset annotated with rich attributes, which\nis superior to existing datasets in four aspects: scale, annotation richness,\ndiversity, and data balance. EmoSet comprises 3.3 million images in total, with\n118,102 of these images carefully labeled by human annotators, making it five\ntimes larger than the largest existing dataset. EmoSet includes images from\nsocial networks, as well as artistic images, and it is well balanced between\ndifferent emotion categories. Motivated by psychological studies, in addition\nto emotion category, each image is also annotated with a set of describable\nemotion attributes: brightness, colorfulness, scene type, object class, facial\nexpression, and human action, which can help understand visual emotions in a\nprecise and interpretable way. The relevance of these emotion attributes is\nvalidated by analyzing the correlations between them and visual emotion, as\nwell as by designing an attribute module to help visual emotion recognition. We\nbelieve EmoSet will bring some key insights and encourage further research in\nvisual emotion analysis and understanding. Project page:\nhttps://vcc.tech/EmoSet.",
        "authors": [
            "Jingyuan Yang",
            "Qirui Huang",
            "Tingting Ding",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ]
    },
    {
        "title": "Distilling from Similar Tasks for Transfer Learning on a Budget",
        "url": "http://arxiv.org/abs/2304.12314",
        "abstract": "We address the challenge of getting efficient yet accurate recognition\nsystems with limited labels. While recognition models improve with model size\nand amount of data, many specialized applications of computer vision have\nsevere resource constraints both during training and inference. Transfer\nlearning is an effective solution for training with few labels, however often\nat the expense of a computationally costly fine-tuning of large base models. We\npropose to mitigate this unpleasant trade-off between compute and accuracy via\nsemi-supervised cross-domain distillation from a set of diverse source models.\nInitially, we show how to use task similarity metrics to select a single\nsuitable source model to distill from, and that a good selection process is\nimperative for good downstream performance of a target model. We dub this\napproach DistillNearest. Though effective, DistillNearest assumes a single\nsource model matches the target task, which is not always the case. To\nalleviate this, we propose a weighted multi-source distillation method to\ndistill multiple source models trained on different domains weighted by their\nrelevance for the target task into a single efficient model (named\nDistillWeighted). Our methods need no access to source data, and merely need\nfeatures and pseudo-labels of the source models. When the goal is accurate\nrecognition under computational constraints, both DistillNearest and\nDistillWeighted approaches outperform both transfer learning from strong\nImageNet initializations as well as state-of-the-art semi-supervised techniques\nsuch as FixMatch. Averaged over 8 diverse target tasks our multi-source method\noutperforms the baselines by 5.6%-points and 4.5%-points, respectively.",
        "authors": [
            "Kenneth Borup",
            "Cheng Perng Phoo",
            "Bharath Hariharan"
        ]
    },
    {
        "title": "Class-relation Knowledge Distillation for Novel Class Discovery",
        "url": "http://arxiv.org/abs/2307.09158",
        "abstract": "We tackle the problem of novel class discovery, which aims to learn novel\nclasses without supervision based on labeled data from known classes. A key\nchallenge lies in transferring the knowledge in the known-class data to the\nlearning of novel classes. Previous methods mainly focus on building a shared\nrepresentation space for knowledge transfer and often ignore modeling class\nrelations. To address this, we introduce a class relation representation for\nthe novel classes based on the predicted class distribution of a model trained\non known classes. Empirically, we find that such class relation becomes less\ninformative during typical discovery training. To prevent such information\nloss, we propose a novel knowledge distillation framework, which utilizes our\nclass-relation representation to regularize the learning of novel classes. In\naddition, to enable a flexible knowledge distillation scheme for each data\npoint in novel classes, we develop a learnable weighting function for the\nregularization, which adaptively promotes knowledge transfer based on the\nsemantic similarity between the novel and known classes. To validate the\neffectiveness and generalization of our method, we conduct extensive\nexperiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, and\nFGVC-Aircraft datasets. Our results demonstrate that the proposed method\noutperforms the previous state-of-the-art methods by a significant margin on\nalmost all benchmarks. Code is available at\n\\href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.",
        "authors": [
            "Peiyan Gu",
            "Chuyu Zhang",
            "Ruijie Xu",
            "Xuming He"
        ]
    },
    {
        "title": "PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.03982",
        "abstract": "Recently, polar-based representation has shown promising properties in\nperceptual tasks. In addition to Cartesian-based approaches, which separate\npoint clouds unevenly, representing point clouds as polar grids has been\nrecognized as an alternative due to (1) its advantage in robust performance\nunder different resolutions and (2) its superiority in streaming-based\napproaches. However, state-of-the-art polar-based detection methods inevitably\nsuffer from the feature distortion problem because of the non-uniform division\nof polar representation, resulting in a non-negligible performance gap compared\nto Cartesian-based approaches. To tackle this issue, we present PARTNER, a\nnovel 3D object detector in the polar coordinate. PARTNER alleviates the\ndilemma of feature distortion with global representation re-alignment and\nfacilitates the regression by introducing instance-level geometric information\ninto the detection head. Extensive experiments show overwhelming advantages in\nstreaming-based detection and different resolutions. Furthermore, our method\noutperforms the previous polar-based works with remarkable margins of 3.68% and\n9.15% on Waymo and ONCE validation set, thus achieving competitive results over\nthe state-of-the-art methods.",
        "authors": [
            "Ming Nie",
            "Yujing Xue",
            "Chunwei Wang",
            "Chaoqiang Ye",
            "Hang Xu",
            "Xinge Zhu",
            "Qingqiu Huang",
            "Michael Bi Mi",
            "Xinchao Wang",
            "Li Zhang"
        ]
    },
    {
        "title": "PNI : Industrial Anomaly Detection using Position and Neighborhood Information",
        "url": "http://arxiv.org/abs/2211.12634",
        "abstract": "Because anomalous samples cannot be used for training, many anomaly detection\nand localization methods use pre-trained networks and non-parametric modeling\nto estimate encoded feature distribution. However, these methods neglect the\nimpact of position and neighborhood information on the distribution of normal\nfeatures. To overcome this, we propose a new algorithm, \\textbf{PNI}, which\nestimates the normal distribution using conditional probability given\nneighborhood features, modeled with a multi-layer perceptron network. Moreover,\nposition information is utilized by creating a histogram of representative\nfeatures at each position. Instead of simply resizing the anomaly map, the\nproposed method employs an additional refine network trained on synthetic\nanomaly images to better interpolate and account for the shape and edge of the\ninput image. We conducted experiments on the MVTec AD benchmark dataset and\nachieved state-of-the-art performance, with \\textbf{99.56\\%} and\n\\textbf{98.98\\%} AUROC scores in anomaly detection and localization,\nrespectively.",
        "authors": [
            "Jaehyeok Bae",
            "Jae-Han Lee",
            "Seyun Kim"
        ]
    },
    {
        "title": "Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction",
        "url": "http://arxiv.org/abs/2308.06554",
        "abstract": "Despite recent advances in 3D human mesh reconstruction, domain gap between\ntraining and test data is still a major challenge. Several prior works tackle\nthe domain gap problem via test-time adaptation that fine-tunes a network\nrelying on 2D evidence (e.g., 2D human keypoints) from test images. However,\nthe high reliance on 2D evidence during adaptation causes two major issues.\nFirst, 2D evidence induces depth ambiguity, preventing the learning of accurate\n3D human geometry. Second, 2D evidence is noisy or partially non-existent\nduring test time, and such imperfect 2D evidence leads to erroneous adaptation.\nTo overcome the above issues, we introduce CycleAdapt, which cyclically adapts\ntwo networks: a human mesh reconstruction network (HMRNet) and a human motion\ndenoising network (MDNet), given a test video. In our framework, to alleviate\nhigh reliance on 2D evidence, we fully supervise HMRNet with generated 3D\nsupervision targets by MDNet. Our cyclic adaptation scheme progressively\nelaborates the 3D supervision targets, which compensate for imperfect 2D\nevidence. As a result, our CycleAdapt achieves state-of-the-art performance\ncompared to previous test-time adaptation methods. The codes are available at\nhttps://github.com/hygenie1228/CycleAdapt_RELEASE.",
        "authors": [
            "Hyeongjin Nam",
            "Daniel Sungho Jung",
            "Yeonguk Oh",
            "Kyoung Mu Lee"
        ]
    },
    {
        "title": "Mixed Neural Voxels for Fast Multi-view Video Synthesis",
        "url": "http://arxiv.org/abs/2212.00190",
        "abstract": "Synthesizing high-fidelity videos from real-world multi-view input is\nchallenging because of the complexities of real-world environments and highly\ndynamic motions. Previous works based on neural radiance fields have\ndemonstrated high-quality reconstructions of dynamic scenes. However, training\nsuch models on real-world scenes is time-consuming, usually taking days or\nweeks. In this paper, we present a novel method named MixVoxels to better\nrepresent the dynamic scenes with fast training speed and competitive rendering\nqualities. The proposed MixVoxels represents the 4D dynamic scenes as a mixture\nof static and dynamic voxels and processes them with different networks. In\nthis way, the computation of the required modalities for static voxels can be\nprocessed by a lightweight model, which essentially reduces the amount of\ncomputation, especially for many daily dynamic scenes dominated by the static\nbackground. To separate the two kinds of voxels, we propose a novel variation\nfield to estimate the temporal variance of each voxel. For the dynamic voxels,\nwe design an inner-product time query method to efficiently query multiple time\nsteps, which is essential to recover the high-dynamic motions. As a result,\nwith 15 minutes of training for dynamic scenes with inputs of 300-frame videos,\nMixVoxels achieves better PSNR than previous methods. Codes and trained models\nare available at https://github.com/fengres/mixvoxels",
        "authors": [
            "Feng Wang",
            "Sinan Tan",
            "Xinghang Li",
            "Zeyue Tian",
            "Yafei Song",
            "Huaping Liu"
        ]
    },
    {
        "title": "Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer",
        "url": "http://arxiv.org/abs/2307.07754",
        "abstract": "Video-based human pose transfer is a video-to-video generation task that\nanimates a plain source human image based on a series of target human poses.\nConsidering the difficulties in transferring highly structural patterns on the\ngarments and discontinuous poses, existing methods often generate\nunsatisfactory results such as distorted textures and flickering artifacts. To\naddress these issues, we propose a novel Deformable Motion Modulation (DMM)\nthat utilizes geometric kernel offset with adaptive weight modulation to\nsimultaneously perform feature alignment and style transfer. Different from\nnormal style modulation used in style transfer, the proposed modulation\nmechanism adaptively reconstructs smoothed frames from style codes according to\nthe object shape through an irregular receptive field of view. To enhance the\nspatio-temporal consistency, we leverage bidirectional propagation to extract\nthe hidden motion information from a warped image sequence generated by noisy\nposes. The proposed feature propagation significantly enhances the motion\nprediction ability by forward and backward propagation. Both quantitative and\nqualitative experimental results demonstrate superiority over the\nstate-of-the-arts in terms of image fidelity and visual continuity. The source\ncode is publicly available at github.com/rocketappslab/bdmm.",
        "authors": [
            "Wing-Yin Yu",
            "Lai-Man Po",
            "Ray C. C. Cheung",
            "Yuzhi Zhao",
            "Yu Xue",
            "Kun Li"
        ]
    },
    {
        "title": "Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2308.13411",
        "abstract": "Glaucoma is the number one cause of irreversible blindness globally. A major\nchallenge for accurate glaucoma detection and progression forecasting is the\nbottleneck of limited labeled patients with the state-of-the-art (SOTA) 3D\nretinal imaging data of optical coherence tomography (OCT). To address the data\nscarcity issue, this paper proposes two solutions. First, we develop a novel\ngeneralization-reinforced semi-supervised learning (SSL) model called pseudo\nsupervisor to optimally utilize unlabeled data. Compared with SOTA models, the\nproposed pseudo supervisor optimizes the policy of predicting pseudo labels\nwith unlabeled samples to improve empirical generalization. Our pseudo\nsupervisor model is evaluated with two clinical tasks consisting of glaucoma\ndetection and progression forecasting. The progression forecasting task is\nevaluated both unimodally and multimodally. Our pseudo supervisor model\ndemonstrates superior performance than SOTA SSL comparison models. Moreover,\nour model also achieves the best results on the publicly available LAG fundus\ndataset. Second, we introduce the Harvard Glaucoma Detection and Progression\n(Harvard-GDP) Dataset, a multimodal multitask dataset that includes data from\n1,000 patients with OCT imaging data, as well as labels for glaucoma detection\nand progression. This is the largest glaucoma detection dataset with 3D OCT\nimaging data and the first glaucoma progression forecasting dataset that is\npublicly available. Detailed sex and racial analysis are provided, which can be\nused by interested researchers for fairness learning studies. Our released\ndataset is benchmarked with several SOTA supervised CNN and transformer deep\nlearning models. The dataset and code are made publicly available via\n\\url{https://ophai.hms.harvard.edu/datasets/harvard-gdp1000}.",
        "authors": [
            "Yan Luo",
            "Min Shi",
            "Yu Tian",
            "Tobias Elze",
            "Mengyu Wang"
        ]
    },
    {
        "title": "Tracking Everything Everywhere All at Once",
        "url": "http://arxiv.org/abs/2306.05422",
        "abstract": "We present a new test-time optimization method for estimating dense and\nlong-range motion from a video sequence. Prior optical flow or particle video\ntracking algorithms typically operate within limited temporal windows,\nstruggling to track through occlusions and maintain global consistency of\nestimated motion trajectories. We propose a complete and globally consistent\nmotion representation, dubbed OmniMotion, that allows for accurate, full-length\nmotion estimation of every pixel in a video. OmniMotion represents a video\nusing a quasi-3D canonical volume and performs pixel-wise tracking via\nbijections between local and canonical space. This representation allows us to\nensure global consistency, track through occlusions, and model any combination\nof camera and object motion. Extensive evaluations on the TAP-Vid benchmark and\nreal-world footage show that our approach outperforms prior state-of-the-art\nmethods by a large margin both quantitatively and qualitatively. See our\nproject page for more results: http://omnimotion.github.io/",
        "authors": [
            "Qianqian Wang",
            "Yen-Yu Chang",
            "Ruojin Cai",
            "Zhengqi Li",
            "Bharath Hariharan",
            "Aleksander Holynski",
            "Noah Snavely"
        ]
    },
    {
        "title": "Group Pose: A Simple Baseline for End-to-End Multi-Person Pose Estimation",
        "url": "http://arxiv.org/abs/2308.07313",
        "abstract": "In this paper, we study the problem of end-to-end multi-person pose\nestimation. State-of-the-art solutions adopt the DETR-like framework, and\nmainly develop the complex decoder, e.g., regarding pose estimation as keypoint\nbox detection and combining with human detection in ED-Pose, hierarchically\npredicting with pose decoder and joint (keypoint) decoder in PETR. We present a\nsimple yet effective transformer approach, named Group Pose. We simply regard\n$K$-keypoint pose estimation as predicting a set of $N\\times K$ keypoint\npositions, each from a keypoint query, as well as representing each pose with\nan instance query for scoring $N$ pose predictions. Motivated by the intuition\nthat the interaction, among across-instance queries of different types, is not\ndirectly helpful, we make a simple modification to decoder self-attention. We\nreplace single self-attention over all the $N\\times(K+1)$ queries with two\nsubsequent group self-attentions: (i) $N$ within-instance self-attention, with\neach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$\nsame-type across-instance self-attention, each over $N$ queries of the same\ntype. The resulting decoder removes the interaction among across-instance\ntype-different queries, easing the optimization and thus improving the\nperformance. Experimental results on MS COCO and CrowdPose show that our\napproach without human box supervision is superior to previous methods with\ncomplex decoders, and even is slightly better than ED-Pose that uses human box\nsupervision. $\\href{https://github.com/Michel-liu/GroupPose-Paddle}{\\rm\nPaddle}$ and $\\href{https://github.com/Michel-liu/GroupPose}{\\rm PyTorch}$ code\nare available.",
        "authors": [
            "Huan Liu",
            "Qiang Chen",
            "Zichang Tan",
            "Jiang-Jiang Liu",
            "Jian Wang",
            "Xiangbo Su",
            "Xiaolong Li",
            "Kun Yao",
            "Junyu Han",
            "Errui Ding",
            "Yao Zhao",
            "Jingdong Wang"
        ]
    },
    {
        "title": "Objects Do Not Disappear: Video Object Detection by Single-Frame Object Location Anticipation",
        "url": "http://arxiv.org/abs/2308.04770",
        "abstract": "Objects in videos are typically characterized by continuous smooth motion. We\nexploit continuous smooth motion in three ways. 1) Improved accuracy by using\nobject motion as an additional source of supervision, which we obtain by\nanticipating object locations from a static keyframe. 2) Improved efficiency by\nonly doing the expensive feature computations on a small subset of all frames.\nBecause neighboring video frames are often redundant, we only compute features\nfor a single static keyframe and predict object locations in subsequent frames.\n3) Reduced annotation cost, where we only annotate the keyframe and use smooth\npseudo-motion between keyframes. We demonstrate computational efficiency,\nannotation efficiency, and improved mean average precision compared to the\nstate-of-the-art on four datasets: ImageNet VID, EPIC KITCHENS-55,\nYouTube-BoundingBoxes, and Waymo Open dataset. Our source code is available at\nhttps://github.com/L-KID/Videoobject-detection-by-location-anticipation.",
        "authors": [
            "Xin Liu",
            "Fatemeh Karimi Nejadasl",
            "Jan C. van Gemert",
            "Olaf Booij",
            "Silvia L. Pintea"
        ]
    },
    {
        "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
        "url": "http://arxiv.org/abs/2304.02173",
        "abstract": "Charts are a powerful tool for visually conveying complex data, but their\ncomprehension poses a challenge due to the diverse chart types and intricate\ncomponents. Existing chart comprehension methods suffer from either heuristic\nrules or an over-reliance on OCR systems, resulting in suboptimal performance.\nTo address these issues, we present ChartReader, a unified framework that\nseamlessly integrates chart derendering and comprehension tasks. Our approach\nincludes a transformer-based chart component detection module and an extended\npre-trained vision-language model for chart-to-X tasks. By learning the rules\nof charts automatically from annotated datasets, our approach eliminates the\nneed for manual rule-making, reducing effort and enhancing accuracy.~We also\nintroduce a data variable replacement technique and extend the input and\nposition embeddings of the pre-trained model for cross-task training. We\nevaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks,\ndemonstrating its superiority over existing methods. Our proposed framework can\nsignificantly reduce the manual effort involved in chart analysis, providing a\nstep towards a universal chart understanding model. Moreover, our approach\noffers opportunities for plug-and-play integration with mainstream LLMs such as\nT5 and TaPas, extending their capability to chart comprehension tasks. The code\nis available at https://github.com/zhiqic/ChartReader.",
        "authors": [
            "Zhi-Qi Cheng",
            "Qi Dai",
            "Siyao Li",
            "Jingdong Sun",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ]
    },
    {
        "title": "Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition",
        "url": "http://arxiv.org/abs/2308.11489",
        "abstract": "We are concerned with a challenging scenario in unpaired multiview video\nlearning. In this case, the model aims to learn comprehensive multiview\nrepresentations while the cross-view semantic information exhibits variations.\nWe propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this\nunpaired multiview learning problem. The key idea is to build cross-view\npseudo-pairs and do view-invariant alignment by leveraging the semantic\ninformation of videos. To facilitate the data efficiency of multiview learning,\nwe further perform video-text alignment for first-person and third-person\nvideos, to fully leverage the semantic knowledge to improve video\nrepresentations. Extensive experiments on multiple benchmark datasets verify\nthe effectiveness of our framework. Our method also outperforms multiple\nexisting view-alignment methods, under the more challenging scenario than\ntypical paired or unpaired multimodal or multiview learning. Our code is\navailable at https://github.com/wqtwjt1996/SUM-L.",
        "authors": [
            "Qitong Wang",
            "Long Zhao",
            "Liangzhe Yuan",
            "Ting Liu",
            "Xi Peng"
        ]
    },
    {
        "title": "Neural LiDAR Fields for Novel View Synthesis",
        "url": "http://arxiv.org/abs/2305.01643",
        "abstract": "We present Neural Fields for LiDAR (NFL), a method to optimise a neural field\nscene representation from LiDAR measurements, with the goal of synthesizing\nrealistic LiDAR scans from novel viewpoints. NFL combines the rendering power\nof neural fields with a detailed, physically motivated model of the LiDAR\nsensing process, thus enabling it to accurately reproduce key sensor behaviors\nlike beam divergence, secondary returns, and ray dropping. We evaluate NFL on\nsynthetic and real LiDAR scans and show that it outperforms explicit\nreconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR\nnovel view synthesis task. Moreover, we show that the improved realism of the\nsynthesized views narrows the domain gap to real scans and translates to better\nregistration and semantic segmentation performance.",
        "authors": [
            "Shengyu Huang",
            "Zan Gojcic",
            "Zian Wang",
            "Francis Williams",
            "Yoni Kasten",
            "Sanja Fidler",
            "Konrad Schindler",
            "Or Litany"
        ]
    },
    {
        "title": "Source-free Depth for Object Pop-out",
        "url": "http://arxiv.org/abs/2212.05370",
        "abstract": "Depth cues are known to be useful for visual perception. However, direct\nmeasurement of depth is often impracticable. Fortunately, though, modern\nlearning-based methods offer promising depth maps by inference in the wild. In\nthis work, we adapt such depth inference models for object segmentation using\nthe objects' \"pop-out\" prior in 3D. The \"pop-out\" is a simple composition prior\nthat assumes objects reside on the background surface. Such compositional prior\nallows us to reason about objects in the 3D space. More specifically, we adapt\nthe inferred depth maps such that objects can be localized using only 3D\ninformation. Such separation, however, requires knowledge about contact surface\nwhich we learn using the weak supervision of the segmentation mask. Our\nintermediate representation of contact surface, and thereby reasoning about\nobjects purely in 3D, allows us to better transfer the depth knowledge into\nsemantics. The proposed adaptation method uses only the depth model without\nneeding the source data used for training, making the learning process\nefficient and practical. Our experiments on eight datasets of two challenging\ntasks, namely camouflaged object detection and salient object detection,\nconsistently demonstrate the benefit of our method in terms of both performance\nand generalizability.",
        "authors": [
            "Zongwei Wu",
            "Danda Pani Paudel",
            "Deng-Ping Fan",
            "Jingjing Wang",
            "Shuo Wang",
            "C\u00e9dric Demonceaux",
            "Radu Timofte",
            "Luc Van Gool"
        ]
    },
    {
        "title": "Token-Label Alignment for Vision Transformers",
        "url": "http://arxiv.org/abs/2210.06455",
        "abstract": "Data mixing strategies (e.g., CutMix) have shown the ability to greatly\nimprove the performance of convolutional neural networks (CNNs). They mix two\nimages as inputs for training and assign them with a mixed label with the same\nratio. While they are shown effective for vision transformers (ViTs), we\nidentify a token fluctuation phenomenon that has suppressed the potential of\ndata mixing strategies. We empirically observe that the contributions of input\ntokens fluctuate as forward propagating, which might induce a different mixing\nratio in the output tokens. The training target computed by the original data\nmixing strategy can thus be inaccurate, resulting in less effective training.\nTo address this, we propose a token-label alignment (TL-Align) method to trace\nthe correspondence between transformed tokens and the original tokens to\nmaintain a label for each token. We reuse the computed attention at each layer\nfor efficient token-label alignment, introducing only negligible additional\ntraining costs. Extensive experiments demonstrate that our method improves the\nperformance of ViTs on image classification, semantic segmentation, objective\ndetection, and transfer learning tasks. Code is available at:\nhttps://github.com/Euphoria16/TL-Align.",
        "authors": [
            "Han Xiao",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Jie Zhou",
            "Jiwen Lu"
        ]
    },
    {
        "title": "Understanding 3D Object Interaction from a Single Image",
        "url": "http://arxiv.org/abs/2305.09664",
        "abstract": "Humans can easily understand a single image as depicting multiple potential\nobjects permitting interaction. We use this skill to plan our interactions with\nthe world and accelerate understanding new objects without engaging in\ninteraction. In this paper, we would like to endow machines with the similar\nability, so that intelligent agents can better explore the 3D scene or\nmanipulate objects. Our approach is a transformer-based model that predicts the\n3D location, physical properties and affordance of objects. To power this\nmodel, we collect a dataset with Internet videos, egocentric videos and indoor\nimages to train and validate our approach. Our model yields strong performance\non our data, and generalizes well to robotics data. Project site:\nhttps://jasonqsy.github.io/3DOI/",
        "authors": [
            "Shengyi Qian",
            "David F. Fouhey"
        ]
    },
    {
        "title": "Learning Gabor Texture Features for Fine-Grained Recognition",
        "url": "http://arxiv.org/abs/2308.05396",
        "abstract": "Extracting and using class-discriminative features is critical for\nfine-grained recognition. Existing works have demonstrated the possibility of\napplying deep CNNs to exploit features that distinguish similar classes.\nHowever, CNNs suffer from problems including frequency bias and loss of\ndetailed local information, which restricts the performance of recognizing\nfine-grained categories. To address the challenge, we propose a novel texture\nbranch as complimentary to the CNN branch for feature extraction. We\ninnovatively utilize Gabor filters as a powerful extractor to exploit texture\nfeatures, motivated by the capability of Gabor filters in effectively capturing\nmulti-frequency features and detailed local information. We implement several\ndesigns to enhance the effectiveness of Gabor filters, including imposing\nconstraints on parameter values and developing a learning method to determine\nthe optimal parameters. Moreover, we introduce a statistical feature extractor\nto utilize informative statistical information from the signals captured by\nGabor filters, and a gate selection mechanism to enable efficient computation\nby only considering qualified regions as input for texture extraction. Through\nthe integration of features from the Gabor-filter-based texture branch and\nCNN-based semantic branch, we achieve comprehensive information extraction. We\ndemonstrate the efficacy of our method on multiple datasets, including\nCUB-200-2011, NA-bird, Stanford Dogs, and GTOS-mobile. State-of-the-art\nperformance is achieved using our approach.",
        "authors": [
            "Lanyun Zhu",
            "Tianrun Chen",
            "Jianxiong Yin",
            "Simon See",
            "Jun Liu"
        ]
    },
    {
        "title": "Weakly-Supervised Action Localization by Hierarchically-Structured Latent Attention Modeling",
        "url": "http://arxiv.org/abs/2308.09946",
        "abstract": "Weakly-supervised action localization aims to recognize and localize action\ninstancese in untrimmed videos with only video-level labels. Most existing\nmodels rely on multiple instance learning(MIL), where the predictions of\nunlabeled instances are supervised by classifying labeled bags. The MIL-based\nmethods are relatively well studied with cogent performance achieved on\nclassification but not on localization. Generally, they locate temporal regions\nby the video-level classification but overlook the temporal variations of\nfeature semantics. To address this problem, we propose a novel attention-based\nhierarchically-structured latent model to learn the temporal variations of\nfeature semantics. Specifically, our model entails two components, the first is\nan unsupervised change-points detection module that detects change-points by\nlearning the latent representations of video features in a temporal hierarchy\nbased on their rates of change, and the second is an attention-based\nclassification model that selects the change-points of the foreground as the\nboundaries. To evaluate the effectiveness of our model, we conduct extensive\nexperiments on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3. The\nexperiments show that our method outperforms current state-of-the-art methods,\nand even achieves comparable performance with fully-supervised methods.",
        "authors": [
            "Guiqin Wang",
            "Peng Zhao",
            "Cong Zhao",
            "Shusen Yang",
            "Jie Cheng",
            "Luziwei Leng",
            "Jianxing Liao",
            "Qinghai Guo"
        ]
    },
    {
        "title": "Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model Using Pixel-Aligned Reconstruction Priors",
        "url": "http://arxiv.org/abs/2302.01162",
        "abstract": "Fast generation of high-quality 3D digital humans is important to a vast\nnumber of applications ranging from entertainment to professional concerns.\nRecent advances in differentiable rendering have enabled the training of 3D\ngenerative models without requiring 3D ground truths. However, the quality of\nthe generated 3D humans still has much room to improve in terms of both\nfidelity and diversity. In this paper, we present Get3DHuman, a novel 3D human\nframework that can significantly boost the realism and diversity of the\ngenerated outcomes by only using a limited budget of 3D ground-truth data. Our\nkey observation is that the 3D generator can profit from human-related priors\nlearned through 2D human generators and 3D reconstructors. Specifically, we\nbridge the latent space of Get3DHuman with that of StyleGAN-Human via a\nspecially-designed prior network, where the input latent code is mapped to the\nshape and texture feature volumes spanned by the pixel-aligned 3D\nreconstructor. The outcomes of the prior network are then leveraged as the\nsupervisory signals for the main generator network. To ensure effective\ntraining, we further propose three tailored losses applied to the generated\nfeature volumes and the intermediate feature maps. Extensive experiments\ndemonstrate that Get3DHuman greatly outperforms the other state-of-the-art\napproaches and can support a wide range of applications including shape\ninterpolation, shape re-texturing, and single-view reconstruction through\nlatent inversion.",
        "authors": [
            "Zhangyang Xiong",
            "Di Kang",
            "Derong Jin",
            "Weikai Chen",
            "Linchao Bao",
            "Shuguang Cui",
            "Xiaoguang Han"
        ]
    },
    {
        "title": "Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data",
        "url": "http://arxiv.org/abs/2309.06302",
        "abstract": "This paper aims to remove specular highlights from a single object-level\nimage. Although previous methods have made some progresses, their performance\nremains somewhat limited, particularly for real images with complex specular\nhighlights. To this end, we propose a three-stage network to address them.\nSpecifically, given an input image, we first decompose it into the albedo,\nshading, and specular residue components to estimate a coarse specular-free\nimage. Then, we further refine the coarse result to alleviate its visual\nartifacts such as color distortion. Finally, we adjust the tone of the refined\nresult to match that of the input as closely as possible. In addition, to\nfacilitate network training and quantitative evaluation, we present a\nlarge-scale synthetic dataset of object-level images, covering diverse objects\nand illumination conditions. Extensive experiments illustrate that our network\nis able to generalize well to unseen real object-level images, and even produce\ngood results for scene-level images with multiple background objects and\ncomplex lighting.",
        "authors": [
            "Gang Fu",
            "Qing Zhang",
            "Lei Zhu",
            "Chunxia Xiao",
            "Ping Li"
        ]
    },
    {
        "title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning",
        "url": "http://arxiv.org/abs/2210.07346",
        "abstract": "As a new paradigm in machine learning, self-supervised learning (SSL) is\ncapable of learning high-quality representations of complex data without\nrelying on labels. In addition to eliminating the need for labeled data,\nresearch has found that SSL improves the adversarial robustness over supervised\nlearning since lacking labels makes it more challenging for adversaries to\nmanipulate model predictions. However, the extent to which this robustness\nsuperiority generalizes to other types of attacks remains an open question.\n  We explore this question in the context of backdoor attacks. Specifically, we\ndesign and evaluate CTRL, an embarrassingly simple yet highly effective\nself-supervised backdoor attack. By only polluting a tiny fraction of training\ndata (<= 1%) with indistinguishable poisoning samples, CTRL causes any\ntrigger-embedded input to be misclassified to the adversary's designated class\nwith a high probability (>= 99%) at inference time. Our findings suggest that\nSSL and supervised learning are comparably vulnerable to backdoor attacks. More\nimportantly, through the lens of CTRL, we study the inherent vulnerability of\nSSL to backdoor attacks. With both empirical and analytical evidence, we reveal\nthat the representation invariance property of SSL, which benefits adversarial\nrobustness, may also be the very reason making \\ssl highly susceptible to\nbackdoor attacks. Our findings also imply that the existing defenses against\nsupervised backdoor attacks are not easily retrofitted to the unique\nvulnerability of SSL.",
        "authors": [
            "Changjiang Li",
            "Ren Pang",
            "Zhaohan Xi",
            "Tianyu Du",
            "Shouling Ji",
            "Yuan Yao",
            "Ting Wang"
        ]
    },
    {
        "title": "Cross-Modal Translation and Alignment for Survival Analysis",
        "url": "http://arxiv.org/abs/2309.12855",
        "abstract": "With the rapid advances in high-throughput sequencing technologies, the focus\nof survival analysis has shifted from examining clinical indicators to\nincorporating genomic profiles with pathological images. However, existing\nmethods either directly adopt a straightforward fusion of pathological features\nand genomic profiles for survival prediction, or take genomic profiles as\nguidance to integrate the features of pathological images. The former would\noverlook intrinsic cross-modal correlations. The latter would discard\npathological information irrelevant to gene expression. To address these\nissues, we present a Cross-Modal Translation and Alignment (CMTA) framework to\nexplore the intrinsic cross-modal correlations and transfer potential\ncomplementary information. Specifically, we construct two parallel\nencoder-decoder structures for multi-modal data to integrate intra-modal\ninformation and generate cross-modal representation. Taking the generated\ncross-modal representation to enhance and recalibrate intra-modal\nrepresentation can significantly improve its discrimination for comprehensive\nsurvival analysis. To explore the intrinsic crossmodal correlations, we further\ndesign a cross-modal attention module as the information bridge between\ndifferent modalities to perform cross-modal interactions and transfer\ncomplementary information. Our extensive experiments on five public TCGA\ndatasets demonstrate that our proposed framework outperforms the\nstate-of-the-art methods.",
        "authors": [
            "Fengtao Zhou",
            "Hao Chen"
        ]
    },
    {
        "title": "Active Stereo Without Pattern Projector",
        "url": "http://arxiv.org/abs/2309.12315",
        "abstract": "This paper proposes a novel framework integrating the principles of active\nstereo in standard passive camera systems without a physical pattern projector.\nWe virtually project a pattern over the left and right images according to the\nsparse measurements obtained from a depth sensor. Any such devices can be\nseamlessly plugged into our framework, allowing for the deployment of a virtual\nactive stereo setup in any possible environment, overcoming the limitation of\npattern projectors, such as limited working range or environmental conditions.\nExperiments on indoor/outdoor datasets, featuring both long and close-range,\nsupport the seamless effectiveness of our approach, boosting the accuracy of\nboth stereo algorithms and deep networks.",
        "authors": [
            "Luca Bartolomei",
            "Matteo Poggi",
            "Fabio Tosi",
            "Andrea Conti",
            "Stefano Mattoccia"
        ]
    },
    {
        "title": "Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching",
        "url": "http://arxiv.org/abs/2307.14071",
        "abstract": "Correlation based stereo matching has achieved outstanding performance, which\npursues cost volume between two feature maps. Unfortunately, current methods\nwith a fixed model do not work uniformly well across various datasets, greatly\nlimiting their real-world applicability. To tackle this issue, this paper\nproposes a new perspective to dynamically calculate correlation for robust\nstereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module\nis introduced to robustly adapt the same model for different scenarios.\nSpecifically, a variance-based uncertainty estimation is employed to adaptively\nadjust the sampling area during warping operation. Additionally, we improve the\ntraditional non-parametric warping with learnable parameters, such that the\nposition-specific weights can be learned. We show that by empowering the\nrecurrent network with the UGAC module, stereo matching can be exploited more\nrobustly and effectively. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury\ndatasets when employing the same fixed model over these datasets without any\nretraining procedure. To target real-time applications, we further design a\nlightweight model based on UGAC, which also outperforms other methods over\nKITTI benchmarks with only 0.6 M parameters.",
        "authors": [
            "Junpeng Jing",
            "Jiankun Li",
            "Pengfei Xiong",
            "Jiangyu Liu",
            "Shuaicheng Liu",
            "Yichen Guo",
            "Xin Deng",
            "Mai Xu",
            "Lai Jiang",
            "Leonid Sigal"
        ]
    },
    {
        "title": "ReFit: Recurrent Fitting Network for 3D Human Recovery",
        "url": "http://arxiv.org/abs/2308.11184",
        "abstract": "We present Recurrent Fitting (ReFit), a neural network architecture for\nsingle-image, parametric 3D human reconstruction. ReFit learns a\nfeedback-update loop that mirrors the strategy of solving an inverse problem\nthrough optimization. At each iterative step, it reprojects keypoints from the\nhuman model to feature maps to query feedback, and uses a recurrent-based\nupdater to adjust the model to fit the image better. Because ReFit encodes\nstrong knowledge of the inverse problem, it is faster to train than previous\nregression models. At the same time, ReFit improves state-of-the-art\nperformance on standard benchmarks. Moreover, ReFit applies to other\noptimization settings, such as multi-view fitting and single-view shape\nfitting. Project website: https://yufu-wang.github.io/refit_humans/",
        "authors": [
            "Yufu Wang",
            "Kostas Daniilidis"
        ]
    },
    {
        "title": "Towards Instance-adaptive Inference for Federated Learning",
        "url": "http://arxiv.org/abs/2308.06051",
        "abstract": "Federated learning (FL) is a distributed learning paradigm that enables\nmultiple clients to learn a powerful global model by aggregating local\ntraining. However, the performance of the global model is often hampered by\nnon-i.i.d. distribution among the clients, requiring extensive efforts to\nmitigate inter-client data heterogeneity. Going beyond inter-client data\nheterogeneity, we note that intra-client heterogeneity can also be observed on\ncomplex real-world data and seriously deteriorate FL performance. In this\npaper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client\ndata heterogeneity by enabling instance-adaptive inference in the FL framework.\nInstead of huge instance-adaptive models, we resort to a parameter-efficient\nfine-tuning method, i.e., scale and shift deep features (SSF), upon a\npre-trained model. Specifically, we first train an SSF pool for each client,\nand aggregate these SSF pools on the server side, thus still maintaining a low\ncommunication cost. To enable instance-adaptive inference, for a given\ninstance, we dynamically find the best-matched SSF subsets from the pool and\naggregate them to generate an adaptive SSF specified for the instance, thereby\nreducing the intra-client as well as the inter-client heterogeneity. Extensive\nexperiments show that our FedIns outperforms state-of-the-art FL algorithms,\ne.g., a 6.64\\% improvement against the top-performing method with less than\n15\\% communication cost on Tiny-ImageNet. Our code and models will be publicly\nreleased.",
        "authors": [
            "Chun-Mei Feng",
            "Kai Yu",
            "Nian Liu",
            "Xinxing Xu",
            "Salman Khan",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "CGBA: Curvature-aware Geometric Black-box Attack",
        "url": "http://arxiv.org/abs/2308.03163",
        "abstract": "Decision-based black-box attacks often necessitate a large number of queries\nto craft an adversarial example. Moreover, decision-based attacks based on\nquerying boundary points in the estimated normal vector direction often suffer\nfrom inefficiency and convergence issues. In this paper, we propose a novel\nquery-efficient curvature-aware geometric decision-based black-box attack\n(CGBA) that conducts boundary search along a semicircular path on a restricted\n2D plane to ensure finding a boundary point successfully irrespective of the\nboundary curvature. While the proposed CGBA attack can work effectively for an\narbitrary decision boundary, it is particularly efficient in exploiting the low\ncurvature to craft high-quality adversarial examples, which is widely seen and\nexperimentally verified in commonly used classifiers under non-targeted\nattacks. In contrast, the decision boundaries often exhibit higher curvature\nunder targeted attacks. Thus, we develop a new query-efficient variant, CGBA-H,\nthat is adapted for the targeted attack. In addition, we further design an\nalgorithm to obtain a better initial boundary point at the expense of some\nextra queries, which considerably enhances the performance of the targeted\nattack. Extensive experiments are conducted to evaluate the performance of our\nproposed methods against some well-known classifiers on the ImageNet and\nCIFAR10 datasets, demonstrating the superiority of CGBA and CGBA-H over\nstate-of-the-art non-targeted and targeted attacks, respectively. The source\ncode is available at https://github.com/Farhamdur/CGBA.",
        "authors": [
            "Md Farhamdur Reza",
            "Ali Rahmati",
            "Tianfu Wu",
            "Huaiyu Dai"
        ]
    },
    {
        "title": "Online Clustered Codebook",
        "url": "http://arxiv.org/abs/2307.15139",
        "abstract": "Vector Quantisation (VQ) is experiencing a comeback in machine learning,\nwhere it is increasingly used in representation learning. However, optimizing\nthe codevectors in existing VQ-VAE is not entirely trivial. A problem is\ncodebook collapse, where only a small subset of codevectors receive gradients\nuseful for their optimisation, whereas a majority of them simply ``dies off''\nand is never updated or used. This limits the effectiveness of VQ for learning\nlarger codebooks in complex computer vision tasks that require high-capacity\nrepresentations. In this paper, we present a simple alternative method for\nonline codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects\nencoded features as anchors to update the ``dead'' codevectors, while\noptimising the codebooks which are alive via the original loss. This strategy\nbrings unused codevectors closer in distribution to the encoded features,\nincreasing the likelihood of being chosen and optimized. We extensively\nvalidate the generalization capability of our quantiser on various datasets,\ntasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE,\nVQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with\njust a few lines of code.",
        "authors": [
            "Chuanxia Zheng",
            "Andrea Vedaldi"
        ]
    },
    {
        "title": "A Multidimensional Analysis of Social Biases in Vision Transformers",
        "url": "http://arxiv.org/abs/2308.01948",
        "abstract": "The embedding spaces of image models have been shown to encode a range of\nsocial biases such as racism and sexism. Here, we investigate specific factors\nthat contribute to the emergence of these biases in Vision Transformers (ViT).\nTherefore, we measure the impact of training data, model architecture, and\ntraining objectives on social biases in the learned representations of ViTs.\nOur findings indicate that counterfactual augmentation training using\ndiffusion-based image editing can mitigate biases, but does not eliminate them.\nMoreover, we find that larger models are less biased than smaller models, and\nthat models trained using discriminative objectives are less biased than those\ntrained using generative objectives. In addition, we observe inconsistencies in\nthe learned social biases. To our surprise, ViTs can exhibit opposite biases\nwhen trained on the same data set using different self-supervised objectives.\nOur findings give insights into the factors that contribute to the emergence of\nsocial biases and suggests that we could achieve substantial fairness\nimprovements based on model design choices.",
        "authors": [
            "Jannik Brinkmann",
            "Paul Swoboda",
            "Christian Bartelt"
        ]
    },
    {
        "title": "Verbs in Action: Improving Verb Understanding in Video-Language Models",
        "url": "http://arxiv.org/abs/2304.06708",
        "abstract": "Understanding verbs is crucial to modelling how people and objects interact\nwith each other and the environment through space and time. Recently,\nstate-of-the-art video-language models based on CLIP have been shown to have\nlimited verb understanding and to rely extensively on nouns, restricting their\nperformance in real-world video applications that require action and temporal\nunderstanding. In this work, we improve verb understanding for CLIP-based\nvideo-language models by proposing a new Verb-Focused Contrastive (VFC)\nframework. This consists of two main components: (1) leveraging pretrained\nlarge language models (LLMs) to create hard negatives for cross-modal\ncontrastive learning, together with a calibration strategy to balance the\noccurrence of concepts in positive and negative pairs; and (2) enforcing a\nfine-grained, verb phrase alignment loss. Our method achieves state-of-the-art\nresults for zero-shot performance on three downstream tasks that focus on verb\nunderstanding: video-text matching, video question-answering and video\nclassification. To the best of our knowledge, this is the first work which\nproposes a method to alleviate the verb understanding problem, and does not\nsimply highlight it.",
        "authors": [
            "Liliane Momeni",
            "Mathilde Caron",
            "Arsha Nagrani",
            "Andrew Zisserman",
            "Cordelia Schmid"
        ]
    },
    {
        "title": "Exploring Predicate Visual Context in Detecting of Human-Object Interactions",
        "url": "http://arxiv.org/abs/2308.06202",
        "abstract": "Recently, the DETR framework has emerged as the dominant approach for\nhuman--object interaction (HOI) research. In particular, two-stage\ntransformer-based HOI detectors are amongst the most performant and\ntraining-efficient approaches. However, these often condition HOI\nclassification on object features that lack fine-grained contextual\ninformation, eschewing pose and orientation information in favour of visual\ncues about object identity and box extremities. This naturally hinders the\nrecognition of complex or ambiguous interactions. In this work, we study these\nissues through visualisations and carefully designed experiments. Accordingly,\nwe investigate how best to re-introduce image features via cross-attention.\nWith an improved query design, extensive exploration of keys and values, and\nbox pair positional embeddings as spatial guidance, our model with enhanced\npredicate visual context (PViC) outperforms state-of-the-art methods on the\nHICO-DET and V-COCO benchmarks, while maintaining low training cost.",
        "authors": [
            "Frederic Z. Zhang",
            "Yuhui Yuan",
            "Dylan Campbell",
            "Zhuoyao Zhong",
            "Stephen Gould"
        ]
    },
    {
        "title": "Robo3D: Towards Robust and Reliable 3D Perception against Corruptions",
        "url": "http://arxiv.org/abs/2303.17597",
        "abstract": "The robustness of 3D perception systems under natural corruptions from\nenvironments and sensors is pivotal for safety-critical applications. Existing\nlarge-scale 3D perception datasets often contain data that are meticulously\ncleaned. Such configurations, however, cannot reflect the reliability of\nperception models during the deployment stage. In this work, we present Robo3D,\nthe first comprehensive benchmark heading toward probing the robustness of 3D\ndetectors and segmentors under out-of-distribution scenarios against natural\ncorruptions that occur in real-world environments. Specifically, we consider\neight corruption types stemming from severe weather conditions, external\ndisturbances, and internal sensor failure. We uncover that, although promising\nresults have been progressively achieved on standard benchmarks,\nstate-of-the-art 3D perception models are at risk of being vulnerable to\ncorruptions. We draw key observations on the use of data representations,\naugmentation schemes, and training strategies, that could severely affect the\nmodel's performance. To pursue better robustness, we propose a\ndensity-insensitive training framework along with a simple flexible\nvoxelization strategy to enhance the model resiliency. We hope our benchmark\nand approach could inspire future research in designing more robust and\nreliable 3D perception models. Our robustness benchmark suite is publicly\navailable.",
        "authors": [
            "Lingdong Kong",
            "Youquan Liu",
            "Xin Li",
            "Runnan Chen",
            "Wenwei Zhang",
            "Jiawei Ren",
            "Liang Pan",
            "Kai Chen",
            "Ziwei Liu"
        ]
    },
    {
        "title": "Towards Saner Deep Image Registration",
        "url": "http://arxiv.org/abs/2307.09696",
        "abstract": "With recent advances in computing hardware and surges of deep-learning\narchitectures, learning-based deep image registration methods have surpassed\ntheir traditional counterparts, in terms of metric performance and inference\ntime. However, these methods focus on improving performance measurements such\nas Dice, resulting in less attention given to model behaviors that are equally\ndesirable for registrations, especially for medical imaging. This paper\ninvestigates these behaviors for popular learning-based deep registrations\nunder a sanity-checking microscope. We find that most existing registrations\nsuffer from low inverse consistency and nondiscrimination of identical pairs\ndue to overly optimized image similarities. To rectify these behaviors, we\npropose a novel regularization-based sanity-enforcer method that imposes two\nsanity checks on the deep model to reduce its inverse consistency errors and\nincrease its discriminative power simultaneously. Moreover, we derive a set of\ntheoretical guarantees for our sanity-checked image registration method, with\nexperimental results supporting our theoretical findings and their\neffectiveness in increasing the sanity of models without sacrificing any\nperformance. Our code and models are available at\nhttps://github.com/tuffr5/Saner-deep-registration.",
        "authors": [
            "Bin Duan",
            "Ming Zhong",
            "Yan Yan"
        ]
    },
    {
        "title": "Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning",
        "url": "http://arxiv.org/abs/2308.06038",
        "abstract": "Benefiting from prompt tuning, recent years have witnessed the promising\nperformance of pre-trained vision-language models, e.g., CLIP, on versatile\ndownstream tasks. In this paper, we focus on a particular setting of learning\nadaptive prompts on the fly for each test sample from an unseen new domain,\nwhich is known as test-time prompt tuning (TPT). Existing TPT methods typically\nrely on data augmentation and confidence selection. However, conventional data\naugmentation techniques, e.g., random resized crops, suffers from the lack of\ndata diversity, while entropy-based confidence selection alone is not\nsufficient to guarantee prediction fidelity. To address these issues, we\npropose a novel TPT method, named DiffTPT, which leverages pre-trained\ndiffusion models to generate diverse and informative new data. Specifically, we\nincorporate augmented data by both conventional method and pre-trained stable\ndiffusion to exploit their respective merits, improving the models ability to\nadapt to unknown new test data. Moreover, to ensure the prediction fidelity of\ngenerated data, we introduce a cosine similarity-based filtration technique to\nselect the generated data with higher similarity to the single test sample. Our\nexperiments on test datasets with distribution shifts and unseen categories\ndemonstrate that DiffTPT improves the zero-shot accuracy by an average of\n5.13\\% compared to the state-of-the-art TPT method. Our code and models will be\npublicly released.",
        "authors": [
            "Chun-Mei Feng",
            "Kai Yu",
            "Yong Liu",
            "Salman Khan",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "Interaction-aware Joint Attention Estimation Using People Attributes",
        "url": "http://arxiv.org/abs/2308.05382",
        "abstract": "This paper proposes joint attention estimation in a single image. Different\nfrom related work in which only the gaze-related attributes of people are\nindependently employed, (I) their locations and actions are also employed as\ncontextual cues for weighting their attributes, and (ii) interactions among all\nof these attributes are explicitly modeled in our method. For the interaction\nmodeling, we propose a novel Transformer-based attention network to encode\njoint attention as low-dimensional features. We introduce a specialized MLP\nhead with positional embedding to the Transformer so that it predicts pixelwise\nconfidence of joint attention for generating the confidence heatmap. This\npixelwise prediction improves the heatmap accuracy by avoiding the ill-posed\nproblem in which the high-dimensional heatmap is predicted from the\nlow-dimensional features. The estimated joint attention is further improved by\nbeing integrated with general image-based attention estimation. Our method\noutperforms SOTA methods quantitatively in comparative experiments. Code:\nhttps://anonymous.4open.science/r/anonymized_codes-ECA4.",
        "authors": [
            "Chihiro Nakatani",
            "Hiroaki Kawashima",
            "Norimichi Ukita"
        ]
    },
    {
        "title": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model",
        "url": "http://arxiv.org/abs/2303.05118",
        "abstract": "The goal of continual learning is to improve the performance of recognition\nmodels in learning sequentially arrived data. Although most existing works are\nestablished on the premise of learning from scratch, growing efforts have been\ndevoted to incorporating the benefits of pre-training. However, how to\nadaptively exploit the pre-trained knowledge for each incremental task while\nmaintaining its generalizability remains an open question. In this work, we\npresent an extensive analysis for continual learning on a pre-trained model\n(CLPM), and attribute the key challenge to a progressive overfitting problem.\nObserving that selectively reducing the learning rate can almost resolve this\nissue in the representation layer, we propose a simple but extremely effective\napproach named Slow Learner with Classifier Alignment (SLCA), which further\nimproves the classification layer by modeling the class-wise distributions and\naligning the classification layers in a post-hoc fashion. Across a variety of\nscenarios, our proposal provides substantial improvements for CLPM (e.g., up to\n49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split\nCUB-200 and Split Cars-196, respectively), and thus outperforms\nstate-of-the-art approaches by a large margin. Based on such a strong baseline,\ncritical factors and promising directions are analyzed in-depth to facilitate\nsubsequent research. Code has been made available at:\nhttps://github.com/GengDavid/SLCA.",
        "authors": [
            "Gengwei Zhang",
            "Liyuan Wang",
            "Guoliang Kang",
            "Ling Chen",
            "Yunchao Wei"
        ]
    },
    {
        "title": "Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
        "url": "http://arxiv.org/abs/2304.10465",
        "abstract": "Contrastive language-image pretraining (CLIP) has demonstrated remarkable\nsuccess in various image tasks. However, how to extend CLIP with effective\ntemporal modeling is still an open and crucial problem. Existing factorized or\njoint spatial-temporal modeling trades off between the efficiency and\nperformance. While modeling temporal information within straight through tube\nis widely adopted in literature, we find that simple frame alignment already\nprovides enough essence without temporal attention. To this end, in this paper,\nwe proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes\nthe temporal modeling effort while achieving incredibly high performance.\nSpecifically, for a frame pair, an interactive point is predicted in each\nframe, serving as a mutual information rich region. By enhancing the features\naround the interactive point, two frames are implicitly aligned. The aligned\nfeatures are then pooled into a single token, which is leveraged in the\nsubsequent spatial self-attention. Our method allows eliminating the costly or\ninsufficient temporal self-attention in video. Extensive experiments on\nbenchmarks demonstrate the superiority and generality of our module.\nParticularly, the proposed ILA achieves a top-1 accuracy of 88.7% on\nKinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is\nreleased at https://github.com/Francis-Rings/ILA .",
        "authors": [
            "Shuyuan Tu",
            "Qi Dai",
            "Zuxuan Wu",
            "Zhi-Qi Cheng",
            "Han Hu",
            "Yu-Gang Jiang"
        ]
    },
    {
        "title": "AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration",
        "url": "http://arxiv.org/abs/2309.10438",
        "abstract": "Diffusion models are emerging expressive generative models, in which a large\nnumber of time steps (inference steps) are required for a single image\ngeneration. To accelerate such tedious process, reducing steps uniformly is\nconsidered as an undisputed principle of diffusion models. We consider that\nsuch a uniform assumption is not the optimal solution in practice; i.e., we can\nfind different optimal time steps for different models. Therefore, we propose\nto search the optimal time steps sequence and compressed model architecture in\na unified framework to achieve effective image generation for diffusion models\nwithout any further training. Specifically, we first design a unified search\nspace that consists of all possible time steps and various architectures. Then,\na two stage evolutionary algorithm is introduced to find the optimal solution\nin the designed search space. To further accelerate the search process, we\nemploy FID score between generated and real samples to estimate the performance\nof the sampled examples. As a result, the proposed method is (i).training-free,\nobtaining the optimal time steps and model architecture without any training\nprocess; (ii). orthogonal to most advanced diffusion samplers and can be\nintegrated to gain better sample quality. (iii). generalized, where the\nsearched time steps and architectures can be directly applied on different\ndiffusion models with the same guidance scale. Experimental results show that\nour method achieves excellent performance by using only a few time steps, e.g.\n17.86 FID score on ImageNet 64 $\\times$ 64 with only four steps, compared to\n138.66 with DDIM. The code is available at\nhttps://github.com/lilijiangg/AutoDiffusion.",
        "authors": [
            "Lijiang Li",
            "Huixia Li",
            "Xiawu Zheng",
            "Jie Wu",
            "Xuefeng Xiao",
            "Rui Wang",
            "Min Zheng",
            "Xin Pan",
            "Fei Chao",
            "Rongrong Ji"
        ]
    },
    {
        "title": "SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes",
        "url": "http://arxiv.org/abs/2304.05170",
        "abstract": "Multi-object tracking in sports scenes plays a critical role in gathering\nplayers statistics, supporting further analysis, such as automatic tactical\nanalysis. Yet existing MOT benchmarks cast little attention on the domain,\nlimiting its development. In this work, we present a new large-scale\nmulti-object tracking dataset in diverse sports scenes, coined as\n\\emph{SportsMOT}, where all players on the court are supposed to be tracked. It\nconsists of 240 video sequences, over 150K frames (almost 15\\times MOT17) and\nover 1.6M bounding boxes (3\\times MOT17) collected from 3 sports categories,\nincluding basketball, volleyball and football. Our dataset is characterized\nwith two key properties: 1) fast and variable-speed motion and 2) similar yet\ndistinguishable appearance. We expect SportsMOT to encourage the MOT trackers\nto promote in both motion-based association and appearance-based association.\nWe benchmark several state-of-the-art trackers and reveal the key challenge of\nSportsMOT lies in object association. To alleviate the issue, we further\npropose a new multi-object tracking framework, termed as \\emph{MixSort},\nintroducing a MixFormer-like structure as an auxiliary association model to\nprevailing tracking-by-detection trackers. By integrating the customized\nappearance-based association with the original motion-based association,\nMixSort achieves state-of-the-art performance on SportsMOT and MOT17. Based on\nMixSort, we give an in-depth analysis and provide some profound insights into\nSportsMOT. The dataset and code will be available at\nhttps://deeperaction.github.io/datasets/sportsmot.html.",
        "authors": [
            "Yutao Cui",
            "Chenkai Zeng",
            "Xiaoyu Zhao",
            "Yichun Yang",
            "Gangshan Wu",
            "Limin Wang"
        ]
    },
    {
        "title": "Localizing Moments in Long Video Via Multimodal Guidance",
        "url": "http://arxiv.org/abs/2302.13372",
        "abstract": "The recent introduction of the large-scale, long-form MAD and Ego4D datasets\nhas enabled researchers to investigate the performance of current\nstate-of-the-art methods for video grounding in the long-form setup, with\ninteresting findings: current grounding methods alone fail at tackling this\nchallenging task and setup due to their inability to process long video\nsequences. In this paper, we propose a method for improving the performance of\nnatural language grounding in long videos by identifying and pruning out\nnon-describable windows. We design a guided grounding framework consisting of a\nGuidance Model and a base grounding model. The Guidance Model emphasizes\ndescribable windows, while the base grounding model analyzes short temporal\nwindows to determine which segments accurately match a given language query. We\noffer two designs for the Guidance Model: Query-Agnostic and Query-Dependent,\nwhich balance efficiency and accuracy. Experiments demonstrate that our\nproposed method outperforms state-of-the-art models by 4.1% in MAD and 4.52% in\nEgo4D (NLQ), respectively. Code, data and MAD's audio features necessary to\nreproduce our experiments are available at:\nhttps://github.com/waybarrios/guidance-based-video-grounding.",
        "authors": [
            "Wayner Barrios",
            "Mattia Soldan",
            "Alberto Mario Ceballos-Arroyo",
            "Fabian Caba Heilbron",
            "Bernard Ghanem"
        ]
    },
    {
        "title": "Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations",
        "url": "http://arxiv.org/abs/2309.01858",
        "abstract": "Fine-grained and instance-level recognition methods are commonly trained and\nevaluated on specific domains, in a model per domain scenario. Such an\napproach, however, is impractical in real large-scale applications. In this\nwork, we address the problem of universal image embedding, where a single\nuniversal model is trained and used in multiple domains. First, we leverage\nexisting domain-specific datasets to carefully construct a new large-scale\npublic benchmark for the evaluation of universal image embeddings, with 241k\nquery images, 1.4M index images and 2.8M training images across 8 different\ndomains and 349k classes. We define suitable metrics, training and evaluation\nprotocols to foster future research in this area. Second, we provide a\ncomprehensive experimental evaluation on the new dataset, demonstrating that\nexisting approaches and simplistic extensions lead to worse performance than an\nassembly of models trained for each domain separately. Finally, we conducted a\npublic research competition on this topic, leveraging industrial datasets,\nwhich attracted the participation of more than 1k teams worldwide. This\nexercise generated many interesting research ideas and findings which we\npresent in detail. Project webpage: https://cmp.felk.cvut.cz/univ_emb/",
        "authors": [
            "Nikolaos-Antonios Ypsilantis",
            "Kaifeng Chen",
            "Bingyi Cao",
            "M\u00e1rio Lipovsk\u00fd",
            "Pelin Dogan-Sch\u00f6nberger",
            "Grzegorz Makosa",
            "Boris Bluntschli",
            "Mojtaba Seyedhosseini",
            "Ond\u0159ej Chum",
            "Andr\u00e9 Araujo"
        ]
    },
    {
        "title": "SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving",
        "url": "http://arxiv.org/abs/2303.06209",
        "abstract": "Unsupervised optical flow estimation is especially hard near occlusions and\nmotion boundaries and in low-texture regions. We show that additional\ninformation such as semantics and domain knowledge can help better constrain\nthis problem. We introduce SemARFlow, an unsupervised optical flow network\ndesigned for autonomous driving data that takes estimated semantic segmentation\nmasks as additional inputs. This additional information is injected into the\nencoder and into a learned upsampler that refines the flow output. In addition,\na simple yet effective semantic augmentation module provides self-supervision\nwhen learning flow and its boundaries for vehicles, poles, and sky. Together,\nthese injections of semantic information improve the KITTI-2015 optical flow\ntest error rate from 11.80% to 8.38%. We also show visible improvements around\nobject boundaries as well as a greater ability to generalize across datasets.\nCode is available at\nhttps://github.com/duke-vision/semantic-unsup-flow-release.",
        "authors": [
            "Shuai Yuan",
            "Shuzhi Yu",
            "Hannah Kim",
            "Carlo Tomasi"
        ]
    },
    {
        "title": "TiDAL: Learning Training Dynamics for Active Learning",
        "url": "http://arxiv.org/abs/2210.06788",
        "abstract": "Active learning (AL) aims to select the most useful data samples from an\nunlabeled data pool and annotate them to expand the labeled dataset under a\nlimited budget. Especially, uncertainty-based methods choose the most uncertain\nsamples, which are known to be effective in improving model performance.\nHowever, AL literature often overlooks training dynamics (TD), defined as the\never-changing model behavior during optimization via stochastic gradient\ndescent, even though other areas of literature have empirically shown that TD\nprovides important clues for measuring the sample uncertainty. In this paper,\nwe propose a novel AL method, Training Dynamics for Active Learning (TiDAL),\nwhich leverages the TD to quantify uncertainties of unlabeled data. Since\ntracking the TD of all the large-scale unlabeled data is impractical, TiDAL\nutilizes an additional prediction module that learns the TD of labeled data. To\nfurther justify the design of TiDAL, we provide theoretical and empirical\nevidence to argue the usefulness of leveraging TD for AL. Experimental results\nshow that our TiDAL achieves better or comparable performance on both balanced\nand imbalanced benchmark datasets compared to state-of-the-art AL methods,\nwhich estimate data uncertainty using only static information after model\ntraining.",
        "authors": [
            "Seong Min Kye",
            "Kwanghee Choi",
            "Hyeongmin Byun",
            "Buru Chang"
        ]
    },
    {
        "title": "Uncertainty-aware Unsupervised Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2307.15409",
        "abstract": "Without manually annotated identities, unsupervised multi-object trackers are\ninferior to learning reliable feature embeddings. It causes the\nsimilarity-based inter-frame association stage also be error-prone, where an\nuncertainty problem arises. The frame-by-frame accumulated uncertainty prevents\ntrackers from learning the consistent feature embedding against time variation.\nTo avoid this uncertainty problem, recent self-supervised techniques are\nadopted, whereas they failed to capture temporal relations. The interframe\nuncertainty still exists. In fact, this paper argues that though the\nuncertainty problem is inevitable, it is possible to leverage the uncertainty\nitself to improve the learned consistency in turn. Specifically, an\nuncertainty-based metric is developed to verify and rectify the risky\nassociations. The resulting accurate pseudo-tracklets boost learning the\nfeature consistency. And accurate tracklets can incorporate temporal\ninformation into spatial transformation. This paper proposes a tracklet-guided\naugmentation strategy to simulate tracklets' motion, which adopts a\nhierarchical uncertainty-based sampling mechanism for hard sample mining. The\nultimate unsupervised MOT framework, namely U2MOT, is proven effective on\nMOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performance\namong the published supervised and unsupervised trackers.",
        "authors": [
            "Kai Liu",
            "Sheng Jin",
            "Zhihang Fu",
            "Ze Chen",
            "Rongxin Jiang",
            "Jieping Ye"
        ]
    },
    {
        "title": "Can Language Models Learn to Listen?",
        "url": "http://arxiv.org/abs/2308.10897",
        "abstract": "We present a framework for generating appropriate facial responses from a\nlistener in dyadic social interactions based on the speaker's words. Given an\ninput transcription of the speaker's words with their timestamps, our approach\nautoregressively predicts a response of a listener: a sequence of listener\nfacial gestures, quantized using a VQ-VAE. Since gesture is a language\ncomponent, we propose treating the quantized atomic motion elements as\nadditional language token inputs to a transformer-based large language model.\nInitializing our transformer with the weights of a language model pre-trained\nonly on text results in significantly higher quality listener responses than\ntraining a transformer from scratch. We show that our generated listener motion\nis fluent and reflective of language semantics through quantitative metrics and\na qualitative user study. In our evaluation, we analyze the model's ability to\nutilize temporal and semantic aspects of spoken text. Project page:\nhttps://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/",
        "authors": [
            "Evonne Ng",
            "Sanjay Subramanian",
            "Dan Klein",
            "Angjoo Kanazawa",
            "Trevor Darrell",
            "Shiry Ginosar"
        ]
    },
    {
        "title": "SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference",
        "url": "http://arxiv.org/abs/2303.08308",
        "abstract": "The combination of Neural Architecture Search (NAS) and quantization has\nproven successful in automatically designing low-FLOPs INT8 quantized neural\nnetworks (QNN). However, directly applying NAS to design accurate QNN models\nthat achieve low latency on real-world devices leads to inferior performance.\nIn this work, we find that the poor INT8 latency is due to the\nquantization-unfriendly issue: the operator and configuration (e.g., channel\nwidth) choices in prior art search spaces lead to diverse quantization\nefficiency and can slow down the INT8 inference speed. To address this\nchallenge, we propose SpaceEvo, an automatic method for designing a dedicated,\nquantization-friendly search space for each target hardware. The key idea of\nSpaceEvo is to automatically search hardware-preferred operators and\nconfigurations to construct the search space, guided by a metric called Q-T\nscore to quantify how quantization-friendly a candidate search space is. We\nfurther train a quantized-for-all supernet over our discovered search space,\nenabling the searched models to be directly deployed without extra retraining\nor quantization. Our discovered models establish new SOTA INT8 quantized\naccuracy under various latency constraints, achieving up to 10.1% accuracy\nimprovement on ImageNet than prior art CNNs under the same latency. Extensive\nexperiments on diverse edge devices demonstrate that SpaceEvo consistently\noutperforms existing manually-designed search spaces with up to 2.5x faster\nspeed while achieving the same accuracy.",
        "authors": [
            "Li Lyna Zhang",
            "Xudong Wang",
            "Jiahang Xu",
            "Quanlu Zhang",
            "Yujing Wang",
            "Yuqing Yang",
            "Ningxin Zheng",
            "Ting Cao",
            "Mao Yang"
        ]
    },
    {
        "title": "SurfsUP: Learning Fluid Simulation for Novel Surfaces",
        "url": "http://arxiv.org/abs/2304.06197",
        "abstract": "Modeling the mechanics of fluid in complex scenes is vital to applications in\ndesign, graphics, and robotics. Learning-based methods provide fast and\ndifferentiable fluid simulators, however most prior work is unable to\naccurately model how fluids interact with genuinely novel surfaces not seen\nduring training. We introduce SURFSUP, a framework that represents objects\nimplicitly using signed distance functions (SDFs), rather than an explicit\nrepresentation of meshes or particles. This continuous representation of\ngeometry enables more accurate simulation of fluid-object interactions over\nlong time periods while simultaneously making computation more efficient.\nMoreover, SURFSUP trained on simple shape primitives generalizes considerably\nout-of-distribution, even to complex real-world scenes and objects. Finally, we\nshow we can invert our model to design simple objects to manipulate fluid flow.",
        "authors": [
            "Arjun Mani",
            "Ishaan Preetam Chandratreya",
            "Elliot Creager",
            "Carl Vondrick",
            "Richard Zemel"
        ]
    },
    {
        "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-Trained Vision-Language Models",
        "url": "http://arxiv.org/abs/2307.15049",
        "abstract": "Prompt tuning and adapter tuning have shown great potential in transferring\npre-trained vision-language models (VLMs) to various downstream tasks. In this\nwork, we design a new type of tuning method, termed as regularized mask tuning,\nwhich masks the network parameters through a learnable selection. Inspired by\nneural pathways, we argue that the knowledge required by a downstream task\nalready exists in the pre-trained weights but just gets concealed in the\nupstream pre-training stage. To bring the useful knowledge back into light, we\nfirst identify a set of parameters that are important to a given downstream\ntask, then attach a binary mask to each parameter, and finally optimize these\nmasks on the downstream data with the parameters frozen. When updating the\nmask, we introduce a novel gradient dropout strategy to regularize the\nparameter selection, in order to prevent the model from forgetting old\nknowledge and overfitting the downstream data. Experimental results on 11\ndatasets demonstrate the consistent superiority of our method over previous\nalternatives. It is noteworthy that we manage to deliver 18.73% performance\nimprovement compared to the zero-shot CLIP via masking an average of only 2.56%\nparameters. Furthermore, our method is synergistic with most existing\nparameter-efficient tuning methods and can boost the performance on top of\nthem. Project page can be found here (https://wuw2019.github.io/R-AMT/).",
        "authors": [
            "Kecheng Zheng",
            "Wei Wu",
            "Ruili Feng",
            "Kai Zhu",
            "Jiawei Liu",
            "Deli Zhao",
            "Zheng-Jun Zha",
            "Wei Chen",
            "Yujun Shen"
        ]
    },
    {
        "title": "Skill Transformer: A Monolithic Policy for Mobile Manipulation",
        "url": "http://arxiv.org/abs/2308.09873",
        "abstract": "We present Skill Transformer, an approach for solving long-horizon robotic\ntasks by combining conditional sequence modeling and skill modularity.\nConditioned on egocentric and proprioceptive observations of a robot, Skill\nTransformer is trained end-to-end to predict both a high-level skill (e.g.,\nnavigation, picking, placing), and a whole-body low-level action (e.g., base\nand arm motion), using a transformer architecture and demonstration\ntrajectories that solve the full task. It retains the composability and\nmodularity of the overall task through a skill predictor module while reasoning\nabout low-level actions and avoiding hand-off errors, common in modular\napproaches. We test Skill Transformer on an embodied rearrangement benchmark\nand find it performs robust task planning and low-level control in new\nscenarios, achieving a 2.5x higher success rate than baselines in hard\nrearrangement problems.",
        "authors": [
            "Xiaoyu Huang",
            "Dhruv Batra",
            "Akshara Rai",
            "Andrew Szot"
        ]
    },
    {
        "title": "Improving Pixel-based MIM by Reducing Wasted Modeling Capability",
        "url": "http://arxiv.org/abs/2308.00261",
        "abstract": "There has been significant progress in Masked Image Modeling (MIM). Existing\nMIM methods can be broadly categorized into two groups based on the\nreconstruction target: pixel-based and tokenizer-based approaches. The former\noffers a simpler pipeline and lower computational cost, but it is known to be\nbiased toward high-frequency details. In this paper, we provide a set of\nempirical studies to confirm this limitation of pixel-based MIM and propose a\nnew method that explicitly utilizes low-level features from shallow layers to\naid pixel reconstruction. By incorporating this design into our base method,\nMAE, we reduce the wasted modeling capability of pixel-based MIM, improving its\nconvergence and achieving non-trivial improvements across various downstream\ntasks. To the best of our knowledge, we are the first to systematically\ninvestigate multi-level feature fusion for isotropic architectures like the\nstandard Vision Transformer (ViT). Notably, when applied to a smaller model\n(e.g., ViT-S), our method yields significant performance gains, such as 1.2\\%\non fine-tuning, 2.8\\% on linear probing, and 2.6\\% on semantic segmentation.\nCode and models are available at https://github.com/open-mmlab/mmpretrain.",
        "authors": [
            "Yuan Liu",
            "Songyang Zhang",
            "Jiacheng Chen",
            "Zhaohui Yu",
            "Kai Chen",
            "Dahua Lin"
        ]
    },
    {
        "title": "Persistent-Transient Duality: A Multi-Mechanism Approach for Modeling Human-Object Interaction",
        "url": "http://arxiv.org/abs/2307.12729",
        "abstract": "Humans are highly adaptable, swiftly switching between different modes to\nprogressively handle different tasks, situations and contexts. In Human-object\ninteraction (HOI) activities, these modes can be attributed to two mechanisms:\n(1) the large-scale consistent plan for the whole activity and (2) the\nsmall-scale children interactive actions that start and end along the timeline.\nWhile neuroscience and cognitive science have confirmed this multi-mechanism\nnature of human behavior, machine modeling approaches for human motion are\ntrailing behind. While attempted to use gradually morphing structures (e.g.,\ngraph attention networks) to model the dynamic HOI patterns, they miss the\nexpeditious and discrete mode-switching nature of the human motion. To bridge\nthat gap, this work proposes to model two concurrent mechanisms that jointly\ncontrol human motion: the Persistent process that runs continually on the\nglobal scale, and the Transient sub-processes that operate intermittently on\nthe local context of the human while interacting with objects. These two\nmechanisms form an interactive Persistent-Transient Duality that\nsynergistically governs the activity sequences. We model this conceptual\nduality by a parent-child neural network of Persistent and Transient channels\nwith a dedicated neural module for dynamic mechanism switching. The framework\nis trialed on HOI motion forecasting. On two rich datasets and a wide variety\nof settings, the model consistently delivers superior performances, proving its\nsuitability for the challenge.",
        "authors": [
            "Hung Tran",
            "Vuong Le",
            "Svetha Venkatesh",
            "Truyen Tran"
        ]
    },
    {
        "title": "When to Learn What: Model-Adaptive Data Augmentation Curriculum",
        "url": "http://arxiv.org/abs/2309.04747",
        "abstract": "Data augmentation (DA) is widely used to improve the generalization of neural\nnetworks by enforcing the invariances and symmetries to pre-defined\ntransformations applied to input data. However, a fixed augmentation policy may\nhave different effects on each sample in different training stages but existing\napproaches cannot adjust the policy to be adaptive to each sample and the\ntraining model. In this paper, we propose Model Adaptive Data Augmentation\n(MADAug) that jointly trains an augmentation policy network to teach the model\nwhen to learn what. Unlike previous work, MADAug selects augmentation operators\nfor each input image by a model-adaptive policy varying between training\nstages, producing a data augmentation curriculum optimized for better\ngeneralization. In MADAug, we train the policy through a bi-level optimization\nscheme, which aims to minimize a validation-set loss of a model trained using\nthe policy-produced data augmentations. We conduct an extensive evaluation of\nMADAug on multiple image classification tasks and network architectures with\nthorough comparisons to existing DA approaches. MADAug outperforms or is on par\nwith other baselines and exhibits better fairness: it brings improvement to all\nclasses and more to the difficult ones. Moreover, MADAug learned policy shows\nbetter performance when transferred to fine-grained datasets. In addition, the\nauto-optimized policy in MADAug gradually introduces increasing perturbations\nand naturally forms an easy-to-hard curriculum.",
        "authors": [
            "Chengkai Hou",
            "Jieyu Zhang",
            "Tianyi Zhou"
        ]
    },
    {
        "title": "DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models",
        "url": "http://arxiv.org/abs/2211.16487",
        "abstract": "Traditionally, monocular 3D human pose estimation employs a machine learning\nmodel to predict the most likely 3D pose for a given input image. However, a\nsingle image can be highly ambiguous and induces multiple plausible solutions\nfor the 2D-3D lifting step which results in overly confident 3D pose\npredictors. To this end, we propose \\emph{DiffPose}, a conditional diffusion\nmodel, that predicts multiple hypotheses for a given input image. In comparison\nto similar approaches, our diffusion model is straightforward and avoids\nintensive hyperparameter tuning, complex network structures, mode collapse, and\nunstable training. Moreover, we tackle a problem of the common two-step\napproach that first estimates a distribution of 2D joint locations via\njoint-wise heatmaps and consecutively approximates them based on first- or\nsecond-moment statistics. Since such a simplification of the heatmaps removes\nvalid information about possibly correct, though labeled unlikely, joint\nlocations, we propose to represent the heatmaps as a set of 2D joint candidate\nsamples. To extract information about the original distribution from these\nsamples we introduce our \\emph{embedding transformer} that conditions the\ndiffusion model. Experimentally, we show that DiffPose slightly improves upon\nthe state of the art for multi-hypothesis pose estimation for simple poses and\noutperforms it by a large margin for highly ambiguous poses.",
        "authors": [
            "Karl Holmquist",
            "Bastian Wandt"
        ]
    },
    {
        "title": "COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos",
        "url": "http://arxiv.org/abs/2210.01781",
        "abstract": "The ability to forecast human-environment collisions from egocentric\nobservations is vital to enable collision avoidance in applications such as VR,\nAR, and wearable assistive robotics. In this work, we introduce the challenging\nproblem of predicting collisions in diverse environments from multi-view\negocentric videos captured from body-mounted cameras. Solving this problem\nrequires a generalizable perception system that can classify which human body\njoints will collide and estimate a collision region heatmap to localize\ncollisions in the environment. To achieve this, we propose a transformer-based\nmodel called COPILOT to perform collision prediction and localization\nsimultaneously, which accumulates information across multi-view inputs through\na novel 4D space-time-viewpoint attention mechanism. To train our model and\nenable future research on this task, we develop a synthetic data generation\nframework that produces egocentric videos of virtual humans moving and\ncolliding within diverse 3D environments. This framework is then used to\nestablish a large-scale dataset consisting of 8.6M egocentric RGBD frames.\nExtensive experiments show that COPILOT generalizes to unseen synthetic as well\nas real-world scenes. We further demonstrate COPILOT outputs are useful for\ndownstream collision avoidance through simple closed-loop control. Please visit\nour project webpage at https://sites.google.com/stanford.edu/copilot.",
        "authors": [
            "Boxiao Pan",
            "Bokui Shen",
            "Davis Rempe",
            "Despoina Paschalidou",
            "Kaichun Mo",
            "Yanchao Yang",
            "Leonidas J. Guibas"
        ]
    },
    {
        "title": "EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation",
        "url": "http://arxiv.org/abs/2304.07803",
        "abstract": "Estimating the depths of equirectangular (i.e., 360) images (EIs) is\nchallenging given the distorted 180 x 360 field-of-view, which is hard to be\naddressed via convolutional neural network (CNN). Although a transformer with\nglobal attention achieves significant improvements over CNN for EI depth\nestimation task, it is computationally inefficient, which raises the need for\ntransformer with local attention. However, to apply local attention\nsuccessfully for EIs, a specific strategy, which addresses distorted\nequirectangular geometry and limited receptive field simultaneously, is\nrequired. Prior works have only cared either of them, resulting in\nunsatisfactory depths occasionally. In this paper, we propose an\nequirectangular geometry-biased transformer termed EGformer. While limiting the\ncomputational cost and the number of network parameters, EGformer enables the\nextraction of the equirectangular geometry-aware local attention with a large\nreceptive field. To achieve this, we actively utilize the equirectangular\ngeometry as the bias for the local attention instead of struggling to reduce\nthe distortion of EIs. As compared to the most recent EI depth estimation\nstudies, the proposed approach yields the best depth outcomes overall with the\nlowest computational cost and the fewest parameters, demonstrating the\neffectiveness of the proposed methods.",
        "authors": [
            "Ilwi Yun",
            "Chanyong Shin",
            "Hyunku Lee",
            "Hyuk-Jae Lee",
            "Chae Eun Rhee"
        ]
    },
    {
        "title": "Generating Realistic Images from In-the-wild Sounds",
        "url": "http://arxiv.org/abs/2309.02405",
        "abstract": "Representing wild sounds as images is an important but challenging task due\nto the lack of paired datasets between sound and images and the significant\ndifferences in the characteristics of these two modalities. Previous studies\nhave focused on generating images from sound in limited categories or music. In\nthis paper, we propose a novel approach to generate images from in-the-wild\nsounds. First, we convert sound into text using audio captioning. Second, we\npropose audio attention and sentence attention to represent the rich\ncharacteristics of sound and visualize the sound. Lastly, we propose a direct\nsound optimization with CLIPscore and AudioCLIP and generate images with a\ndiffusion-based model. In experiments, it shows that our model is able to\ngenerate high quality images from wild sounds and outperforms baselines in both\nquantitative and qualitative evaluations on wild audio datasets.",
        "authors": [
            "Taegyeong Lee",
            "Jeonghun Kang",
            "Hyeonyu Kim",
            "Taehwan Kim"
        ]
    },
    {
        "title": "DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration",
        "url": "http://arxiv.org/abs/2303.06682",
        "abstract": "Diffusion models have recently received a surge of interest due to their\nimpressive performance for image restoration, especially in terms of noise\nrobustness. However, existing diffusion-based methods are trained on a large\namount of training data and perform very well in-distribution, but can be quite\nsusceptible to distribution shift. This is especially inappropriate for\ndata-starved hyperspectral image (HSI) restoration. To tackle this problem,\nthis work puts forth a self-supervised diffusion model for HSI restoration,\nnamely Denoising Diffusion Spatio-Spectral Model (\\texttt{DDS2M}), which works\nby inferring the parameters of the proposed Variational Spatio-Spectral Module\n(VS2M) during the reverse diffusion process, solely using the degraded HSI\nwithout any extra training data. In VS2M, a variational inference-based loss\nfunction is customized to enable the untrained spatial and spectral networks to\nlearn the posterior distribution, which serves as the transitions of the\nsampling chain to help reverse the diffusion process. Benefiting from its\nself-supervised nature and the diffusion process, \\texttt{DDS2M} enjoys\nstronger generalization ability to various HSIs compared to existing\ndiffusion-based methods and superior robustness to noise compared to existing\nHSI restoration methods. Extensive experiments on HSI denoising, noisy HSI\ncompletion and super-resolution on a variety of HSIs demonstrate\n\\texttt{DDS2M}'s superiority over the existing task-specific state-of-the-arts.",
        "authors": [
            "Yuchun Miao",
            "Lefei Zhang",
            "Liangpei Zhang",
            "Dacheng Tao"
        ]
    },
    {
        "title": "Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
        "url": "http://arxiv.org/abs/2308.09363",
        "abstract": "Video Question Answering (VideoQA) is a challenging task that entails complex\nmulti-modal reasoning. In contrast to multiple-choice VideoQA which aims to\npredict the answer given several options, the goal of open-ended VideoQA is to\nanswer questions without restricting candidate answers. However, the majority\nof previous VideoQA models formulate open-ended VideoQA as a classification\ntask to classify the video-question pairs into a fixed answer set, i.e.,\nclosed-vocabulary, which contains only frequent answers (e.g., top-1000\nanswers). This leads the model to be biased toward only frequent answers and\nfail to generalize on out-of-vocabulary answers. We hence propose a new\nbenchmark, Open-vocabulary Video Question Answering (OVQA), to measure the\ngeneralizability of VideoQA models by considering rare and unseen answers. In\naddition, in order to improve the model's generalization power, we introduce a\nnovel GNN-based soft verbalizer that enhances the prediction on rare and unseen\nanswers by aggregating the information from their similar words. For\nevaluation, we introduce new baselines by modifying the existing\n(closed-vocabulary) open-ended VideoQA models and improve their performances by\nfurther taking into account rare and unseen answers. Our ablation studies and\nqualitative analyses demonstrate that our GNN-based soft verbalizer further\nimproves the model performance, especially on rare and unseen answers. We hope\nthat our benchmark OVQA can serve as a guide for evaluating the\ngeneralizability of VideoQA models and inspire future research. Code is\navailable at https://github.com/mlvlab/OVQA.",
        "authors": [
            "Dohwan Ko",
            "Ji Soo Lee",
            "Miso Choi",
            "Jaewon Chu",
            "Jihwan Park",
            "Hyunwoo J. Kim"
        ]
    },
    {
        "title": "Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation",
        "url": "http://arxiv.org/abs/2301.10100",
        "abstract": "Semantic segmentation of point clouds in autonomous driving datasets requires\ntechniques that can process large numbers of points efficiently. Sparse 3D\nconvolutions have become the de-facto tools to construct deep neural networks\nfor this task: they exploit point cloud sparsity to reduce the memory and\ncomputational loads and are at the core of today's best methods. In this paper,\nwe propose an alternative method that reaches the level of state-of-the-art\nmethods without requiring sparse convolutions. We actually show that such level\nof performance is achievable by relying on tools a priori unfit for large scale\nand high-performing 3D perception. In particular, we propose a novel 3D\nbackbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions\nand present how to train it to reach high performance on SemanticKITTI and\nnuScenes. We believe that WaffleIron is a compelling alternative to backbones\nusing sparse 3D convolutions, especially in frameworks and on hardware where\nthose convolutions are not readily available.",
        "authors": [
            "Gilles Puy",
            "Alexandre Boulch",
            "Renaud Marlet"
        ]
    },
    {
        "title": "AutoReP: Automatic ReLU Replacement for Fast Private Network Inference",
        "url": "http://arxiv.org/abs/2308.10134",
        "abstract": "The growth of the Machine-Learning-As-A-Service (MLaaS) market has\nhighlighted clients' data privacy and security issues. Private inference (PI)\ntechniques using cryptographic primitives offer a solution but often have high\ncomputation and communication costs, particularly with non-linear operators\nlike ReLU. Many attempts to reduce ReLU operations exist, but they may need\nheuristic threshold selection or cause substantial accuracy loss. This work\nintroduces AutoReP, a gradient-based approach to lessen non-linear operators\nand alleviate these issues. It automates the selection of ReLU and polynomial\nfunctions to speed up PI applications and introduces distribution-aware\npolynomial approximation (DaPa) to maintain model expressivity while accurately\napproximating ReLUs. Our experimental results demonstrate significant accuracy\nimprovements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%,\n12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget,\nTiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever,\nAutoReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55%\naccuracy with 176.1 times ReLU budget reduction.",
        "authors": [
            "Hongwu Peng",
            "Shaoyi Huang",
            "Tong Zhou",
            "Yukui Luo",
            "Chenghong Wang",
            "Zigeng Wang",
            "Jiahui Zhao",
            "Xi Xie",
            "Ang Li",
            "Tony Geng",
            "Kaleel Mahmood",
            "Wujie Wen",
            "Xiaolin Xu",
            "Caiwen Ding"
        ]
    },
    {
        "title": "Self-Ordering Point Clouds",
        "url": "http://arxiv.org/abs/2304.00961",
        "abstract": "In this paper we address the task of finding representative subsets of points\nin a 3D point cloud by means of a point-wise ordering. Only a few works have\ntried to address this challenging vision problem, all with the help of hard to\nobtain point and cloud labels. Different from these works, we introduce the\ntask of point-wise ordering in 3D point clouds through self-supervision, which\nwe call self-ordering. We further contribute the first end-to-end trainable\nnetwork that learns a point-wise ordering in a self-supervised fashion. It\nutilizes a novel differentiable point scoring-sorting strategy and it\nconstructs an hierarchical contrastive scheme to obtain self-supervision\nsignals. We extensively ablate the method and show its scalability and superior\nperformance even compared to supervised ordering methods on multiple datasets\nand tasks including zero-shot ordering of point clouds from unseen categories.",
        "authors": [
            "Pengwan Yang",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ]
    },
    {
        "title": "Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation",
        "url": "http://arxiv.org/abs/2302.04308",
        "abstract": "In medical vision, different imaging modalities provide complementary\ninformation. However, in practice, not all modalities may be available during\ninference or even training. Previous approaches, e.g., knowledge distillation\nor image synthesis, often assume the availability of full modalities for all\npatients during training; this is unrealistic and impractical due to the\nvariability in data collection across sites. We propose a novel approach to\nlearn enhanced modality-agnostic representations by employing a meta-learning\nstrategy in training, even when only limited full modality samples are\navailable. Meta-learning enhances partial modality representations to full\nmodality representations by meta-training on partial modality data and\nmeta-testing on limited full modality samples. Additionally, we co-supervise\nthis feature enrichment by introducing an auxiliary adversarial learning\nbranch. More specifically, a missing modality detector is used as a\ndiscriminator to mimic the full modality setting. Our segmentation framework\nsignificantly outperforms state-of-the-art brain tumor segmentation techniques\nin missing modality scenarios.",
        "authors": [
            "Aishik Konwer",
            "Xiaoling Hu",
            "Joseph Bae",
            "Xuan Xu",
            "Chao Chen",
            "Prateek Prasanna"
        ]
    },
    {
        "title": "Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition",
        "url": "http://arxiv.org/abs/2208.10741",
        "abstract": "Graph convolutional networks (GCNs) are the most commonly used methods for\nskeleton-based action recognition and have achieved remarkable performance.\nGenerating adjacency matrices with semantically meaningful edges is\nparticularly important for this task, but extracting such edges is challenging\nproblem. To solve this, we propose a hierarchically decomposed graph\nconvolutional network (HD-GCN) architecture with a novel hierarchically\ndecomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every\njoint node into several sets to extract major structurally adjacent and distant\nedges, and uses them to construct an HD-Graph containing those edges in the\nsame semantic spaces of a human skeleton. In addition, we introduce an\nattention-guided hierarchy aggregation (A-HA) module to highlight the dominant\nhierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way\nensemble method, which uses only joint and bone stream without any motion\nstream. The proposed model is evaluated and achieves state-of-the-art\nperformance on four large, popular datasets. Finally, we demonstrate the\neffectiveness of our model with various comparative experiments.",
        "authors": [
            "Jungho Lee",
            "Minhyeok Lee",
            "Dogyoon Lee",
            "Sangyoun Lee"
        ]
    },
    {
        "title": "LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction",
        "url": "http://arxiv.org/abs/2307.12194",
        "abstract": "Accurate reconstruction of both the geometric and topological details of a 3D\nobject from a single 2D image embodies a fundamental challenge in computer\nvision. Existing explicit/implicit solutions to this problem struggle to\nrecover self-occluded geometry and/or faithfully reconstruct topological shape\nstructures. To resolve this dilemma, we introduce LIST, a novel neural\narchitecture that leverages local and global image features to accurately\nreconstruct the geometric and topological structure of a 3D object from a\nsingle image. We utilize global 2D features to predict a coarse shape of the\ntarget object and then use it as a base for higher-resolution reconstruction.\nBy leveraging both local 2D features from the image and 3D features from the\ncoarse prediction, we can predict the signed distance between an arbitrary\npoint and the target surface via an implicit predictor with great accuracy.\nFurthermore, our model does not require camera estimation or pixel alignment.\nIt provides an uninfluenced reconstruction from the input-view direction.\nThrough qualitative and quantitative analysis, we show the superiority of our\nmodel in reconstructing 3D objects from both synthetic and real-world images\nagainst the state of the art.",
        "authors": [
            "Mohammad Samiul Arshad",
            "William J. Beksi"
        ]
    },
    {
        "title": "Rethinking Mobile Block for Efficient Attention-based Models",
        "url": "http://arxiv.org/abs/2301.01146",
        "abstract": "This paper focuses on developing modern, efficient, lightweight models for\ndense predictions while trading off parameters, FLOPs, and performance.\nInverted Residual Block (IRB) serves as the infrastructure for lightweight\nCNNs, but no counterpart has been recognized by attention-based studies. This\nwork rethinks lightweight infrastructure from efficient IRB and effective\ncomponents of Transformer from a unified perspective, extending CNN-based IRB\nto attention-based models and abstracting a one-residual Meta Mobile Block\n(MMB) for lightweight model design. Following simple but effective design\ncriterion, we deduce a modern Inverted Residual Mobile Block (iRMB) and build a\nResNet-like Efficient MOdel (EMO) with only iRMB for down-stream tasks.\nExtensive experiments on ImageNet-1K, COCO2017, and ADE20K benchmarks\ndemonstrate the superiority of our EMO over state-of-the-art methods, e.g.,\nEMO-1M/2M/5M achieve 71.5, 75.1, and 78.4 Top-1 that surpass equal-order\nCNN-/Attention-based models, while trading-off the parameter, efficiency, and\naccuracy well: running 2.8-4.0x faster than EdgeNeXt on iPhone14.",
        "authors": [
            "Jiangning Zhang",
            "Xiangtai Li",
            "Jian Li",
            "Liang Liu",
            "Zhucun Xue",
            "Boshen Zhang",
            "Zhengkai Jiang",
            "Tianxin Huang",
            "Yabiao Wang",
            "Chengjie Wang"
        ]
    },
    {
        "title": "REAP: A Large-Scale Realistic Adversarial Patch Benchmark",
        "url": "http://arxiv.org/abs/2212.05680",
        "abstract": "Machine learning models are known to be susceptible to adversarial\nperturbation. One famous attack is the adversarial patch, a sticker with a\nparticularly crafted pattern that makes the model incorrectly predict the\nobject it is placed on. This attack presents a critical threat to\ncyber-physical systems that rely on cameras such as autonomous cars. Despite\nthe significance of the problem, conducting research in this setting has been\ndifficult; evaluating attacks and defenses in the real world is exceptionally\ncostly while synthetic data are unrealistic. In this work, we propose the REAP\n(REalistic Adversarial Patch) benchmark, a digital benchmark that allows the\nuser to evaluate patch attacks on real images, and under real-world conditions.\nBuilt on top of the Mapillary Vistas dataset, our benchmark contains over\n14,000 traffic signs. Each sign is augmented with a pair of geometric and\nlighting transformations, which can be used to apply a digitally generated\npatch realistically onto the sign. Using our benchmark, we perform the first\nlarge-scale assessments of adversarial patch attacks under realistic\nconditions. Our experiments suggest that adversarial patch attacks may present\na smaller threat than previously believed and that the success rate of an\nattack on simpler digital simulations is not predictive of its actual\neffectiveness in practice. We release our benchmark publicly at\nhttps://github.com/wagner-group/reap-benchmark.",
        "authors": [
            "Nabeel Hingun",
            "Chawin Sitawarin",
            "Jerry Li",
            "David Wagner"
        ]
    },
    {
        "title": "StegaNeRF: Embedding Invisible Information within Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2212.01602",
        "abstract": "Recent advances in neural rendering imply a future of widespread visual data\ndistributions through sharing NeRF model weights. However, while common visual\ndata (images and videos) have standard approaches to embed ownership or\ncopyright information explicitly or subtly, the problem remains unexplored for\nthe emerging NeRF format. We present StegaNeRF, a method for steganographic\ninformation embedding in NeRF renderings. We design an optimization framework\nallowing accurate hidden information extractions from images rendered by NeRF,\nwhile preserving its original visual quality. We perform experimental\nevaluations of our method under several potential deployment scenarios, and we\nfurther discuss the insights discovered through our analysis. StegaNeRF\nsignifies an initial exploration into the novel problem of instilling\ncustomizable, imperceptible, and recoverable information to NeRF renderings,\nwith minimal impact to rendered images. Project page:\nhttps://xggnet.github.io/StegaNeRF/.",
        "authors": [
            "Chenxin Li",
            "Brandon Y. Feng",
            "Zhiwen Fan",
            "Panwang Pan",
            "Zhangyang Wang"
        ]
    },
    {
        "title": "DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition",
        "url": "http://arxiv.org/abs/2211.01146",
        "abstract": "Image Signal Processors (ISPs) play important roles in image recognition\ntasks as well as in the perceptual quality of captured images. In most cases,\nexperts make a lot of effort to manually tune many parameters of ISPs, but the\nparameters are sub-optimal. In the literature, two types of techniques have\nbeen actively studied: a machine learning-based parameter tuning technique and\na DNN-based ISP technique. The former is lightweight but lacks expressive\npower. The latter has expressive power, but the computational cost is too heavy\non edge devices. To solve these problems, we propose \"DynamicISP,\" which\nconsists of multiple classical ISP functions and dynamically controls the\nparameters of each frame according to the recognition result of the previous\nframe. We show our method successfully controls the parameters of multiple ISP\nfunctions and achieves state-of-the-art accuracy with low computational cost in\nsingle and multi-category object detection tasks.",
        "authors": [
            "Masakazu Yoshimura",
            "Junji Otsuka",
            "Atsushi Irie",
            "Takeshi Ohashi"
        ]
    },
    {
        "title": "A step towards understanding why classification helps regression",
        "url": "http://arxiv.org/abs/2308.10603",
        "abstract": "A number of computer vision deep regression approaches report improved\nresults when adding a classification loss to the regression loss. Here, we\nexplore why this is useful in practice and when it is beneficial. To do so, we\nstart from precisely controlled dataset variations and data samplings and find\nthat the effect of adding a classification loss is the most pronounced for\nregression with imbalanced data. We explain these empirical findings by\nformalizing the relation between the balanced and imbalanced regression losses.\nFinally, we show that our findings hold on two real imbalanced image datasets\nfor depth estimation (NYUD2-DIR), and age estimation (IMDB-WIKI-DIR), and on\nthe problem of imbalanced video progress prediction (Breakfast). Our main\ntakeaway is: for a regression task, if the data sampling is imbalanced, then\nadd a classification loss.",
        "authors": [
            "Silvia L. Pintea",
            "Yancong Lin",
            "Jouke Dijkstra",
            "Jan C. van Gemert"
        ]
    },
    {
        "title": "Robust Evaluation of Diffusion-Based Adversarial Purification",
        "url": "http://arxiv.org/abs/2303.09051",
        "abstract": "We question the current evaluation practice on diffusion-based purification\nmethods. Diffusion-based purification methods aim to remove adversarial effects\nfrom an input data point at test time. The approach gains increasing attention\nas an alternative to adversarial training due to the disentangling between\ntraining and testing. Well-known white-box attacks are often employed to\nmeasure the robustness of the purification. However, it is unknown whether\nthese attacks are the most effective for the diffusion-based purification since\nthe attacks are often tailored for adversarial training. We analyze the current\npractices and provide a new guideline for measuring the robustness of\npurification methods against adversarial attacks. Based on our analysis, we\nfurther propose a new purification strategy improving robustness compared to\nthe current diffusion-based purification methods.",
        "authors": [
            "Minjong Lee",
            "Dongwoo Kim"
        ]
    },
    {
        "title": "Hyperbolic Audio-visual Zero-shot Learning",
        "url": "http://arxiv.org/abs/2308.12558",
        "abstract": "Audio-visual zero-shot learning aims to classify samples consisting of a pair\nof corresponding audio and video sequences from classes that are not present\nduring training. An analysis of the audio-visual data reveals a large degree of\nhyperbolicity, indicating the potential benefit of using a hyperbolic\ntransformation to achieve curvature-aware geometric learning, with the aim of\nexploring more complex hierarchical data structures for this task. The proposed\napproach employs a novel loss function that incorporates cross-modality\nalignment between video and audio features in the hyperbolic space.\nAdditionally, we explore the use of multiple adaptive curvatures for hyperbolic\nprojections. The experimental results on this very challenging task demonstrate\nthat our proposed hyperbolic approach for zero-shot learning outperforms the\nSOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL\nachieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,\nrespectively.",
        "authors": [
            "Jie Hong",
            "Zeeshan Hayder",
            "Junlin Han",
            "Pengfei Fang",
            "Mehrtash Harandi",
            "Lars Petersson"
        ]
    },
    {
        "title": "CTP:Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation",
        "url": "http://arxiv.org/abs/2308.07146",
        "abstract": "Vision-Language Pretraining (VLP) has shown impressive results on diverse\ndownstream tasks by offline training on large-scale datasets. Regarding the\ngrowing nature of real-world data, such an offline training paradigm on\never-expanding data is unsustainable, because models lack the continual\nlearning ability to accumulate knowledge constantly. However, most continual\nlearning studies are limited to uni-modal classification and existing\nmulti-modal datasets cannot simulate continual non-stationary data stream\nscenarios. To support the study of Vision-Language Continual Pretraining\n(VLCP), we first contribute a comprehensive and unified benchmark dataset P9D\nwhich contains over one million product image-text pairs from 9 industries. The\ndata from each industry as an independent task supports continual learning and\nconforms to the real-world long-tail nature to simulate pretraining on web\ndata. We comprehensively study the characteristics and challenges of VLCP, and\npropose a new algorithm: Compatible momentum contrast with Topology\nPreservation, dubbed CTP. The compatible momentum model absorbs the knowledge\nof the current and previous-task models to flexibly update the modal feature.\nMoreover, Topology Preservation transfers the knowledge of embedding across\ntasks while preserving the flexibility of feature adjustment. The experimental\nresults demonstrate our method not only achieves superior performance compared\nwith other baselines but also does not bring an expensive training burden.\nDataset and codes are available at https://github.com/KevinLight831/CTP.",
        "authors": [
            "Hongguang Zhu",
            "Yunchao Wei",
            "Xiaodan Liang",
            "Chunjie Zhang",
            "Yao Zhao"
        ]
    },
    {
        "title": "Distribution Shift Matters for Knowledge Distillation with Webly Collected Images",
        "url": "http://arxiv.org/abs/2307.11469",
        "abstract": "Knowledge distillation aims to learn a lightweight student network from a\npre-trained teacher network. In practice, existing knowledge distillation\nmethods are usually infeasible when the original training data is unavailable\ndue to some privacy issues and data management considerations. Therefore,\ndata-free knowledge distillation approaches proposed to collect training\ninstances from the Internet. However, most of them have ignored the common\ndistribution shift between the instances from original training data and webly\ncollected data, affecting the reliability of the trained student network. To\nsolve this problem, we propose a novel method dubbed ``Knowledge Distillation\nbetween Different Distributions\" (KD$^{3}$), which consists of three\ncomponents. Specifically, we first dynamically select useful training instances\nfrom the webly collected data according to the combined predictions of teacher\nnetwork and student network. Subsequently, we align both the weighted features\nand classifier parameters of the two networks for knowledge memorization.\nMeanwhile, we also build a new contrastive learning block called\nMixDistribution to generate perturbed data with a new distribution for instance\nalignment, so that the student network can further learn a\ndistribution-invariant representation. Intensive experiments on various\nbenchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the\nstate-of-the-art data-free knowledge distillation approaches.",
        "authors": [
            "Jialiang Tang",
            "Shuo Chen",
            "Gang Niu",
            "Masashi Sugiyama",
            "Chen Gong"
        ]
    },
    {
        "title": "Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution",
        "url": "http://arxiv.org/abs/2307.08544",
        "abstract": "Look-up table(LUT)-based methods have shown the great efficacy in single\nimage super-resolution (SR) task. However, previous methods ignore the\nessential reason of restricted receptive field (RF) size in LUT, which is\ncaused by the interaction of space and channel features in vanilla convolution.\nThey can only increase the RF at the cost of linearly increasing LUT size. To\nenlarge RF with contained LUT sizes, we propose a novel Reconstructed\nConvolution(RC) module, which decouples channel-wise and spatial calculation.\nIt can be formulated as $n^2$ 1D LUTs to maintain $n\\times n$ receptive field,\nwhich is obviously smaller than $n\\times n$D LUT formulated before. The LUT\ngenerated by our RC module reaches less than 1/10000 storage compared with\nSR-LUT baseline. The proposed Reconstructed Convolution module based LUT\nmethod, termed as RCLUT, can enlarge the RF size by 9 times than the\nstate-of-the-art LUT-based SR method and achieve superior performance on five\npopular benchmark dataset. Moreover, the efficient and robust RC module can be\nused as a plugin to improve other LUT-based SR methods. The code is available\nat https://github.com/liuguandu/RC-LUT.",
        "authors": [
            "Guandu Liu",
            "Yukang Ding",
            "Mading Li",
            "Ming Sun",
            "Xing Wen",
            "Bin Wang"
        ]
    },
    {
        "title": "Action Sensitivity Learning for Temporal Action Localization",
        "url": "http://arxiv.org/abs/2305.15701",
        "abstract": "Temporal action localization (TAL), which involves recognizing and locating\naction instances, is a challenging task in video understanding. Most existing\napproaches directly predict action classes and regress offsets to boundaries,\nwhile overlooking the discrepant importance of each frame. In this paper, we\npropose an Action Sensitivity Learning framework (ASL) to tackle this task,\nwhich aims to assess the value of each frame and then leverage the generated\naction sensitivity to recalibrate the training procedure. We first introduce a\nlightweight Action Sensitivity Evaluator to learn the action sensitivity at the\nclass level and instance level, respectively. The outputs of the two branches\nare combined to reweight the gradient of the two sub-tasks. Moreover, based on\nthe action sensitivity of each frame, we design an Action Sensitive Contrastive\nLoss to enhance features, where the action-aware frames are sampled as positive\npairs to push away the action-irrelevant frames. The extensive studies on\nvarious action localization benchmarks (i.e., MultiThumos, Charades,\nEgo4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) show\nthat ASL surpasses the state-of-the-art in terms of average-mAP under multiple\ntypes of scenarios, e.g., single-labeled, densely-labeled and egocentric.",
        "authors": [
            "Jiayi Shao",
            "Xiaohan Wang",
            "Ruijie Quan",
            "Junjun Zheng",
            "Jiang Yang",
            "Yi Yang"
        ]
    },
    {
        "title": "PEANUT: Predicting and Navigating to Unseen Targets",
        "url": "http://arxiv.org/abs/2212.02497",
        "abstract": "Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an\nunderstanding of the spatial and semantic regularities in environment layouts.\nIn this work, we present a straightforward method for learning these\nregularities by predicting the locations of unobserved objects from incomplete\nsemantic maps. Our method differs from previous prediction-based navigation\nmethods, such as frontier potential prediction or egocentric map completion, by\ndirectly predicting unseen targets while leveraging the global context from all\npreviously explored areas. Our prediction model is lightweight and can be\ntrained in a supervised manner using a relatively small amount of passively\ncollected data. Once trained, the model can be incorporated into a modular\npipeline for ObjectNav without the need for any reinforcement learning. We\nvalidate the effectiveness of our method on the HM3D and MP3D ObjectNav\ndatasets. We find that it achieves the state-of-the-art on both datasets,\ndespite not using any additional data for training.",
        "authors": [
            "Albert J. Zhai",
            "Shenlong Wang"
        ]
    },
    {
        "title": "Pluralistic Aging Diffusion Autoencoder",
        "url": "http://arxiv.org/abs/2303.11086",
        "abstract": "Face aging is an ill-posed problem because multiple plausible aging patterns\nmay correspond to a given input. Most existing methods often produce one\ndeterministic estimation. This paper proposes a novel CLIP-driven Pluralistic\nAging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.\nFirst, we employ diffusion models to generate diverse low-level aging details\nvia a sequential denoising reverse process. Second, we present Probabilistic\nAging Embedding (PAE) to capture diverse high-level aging patterns, which\nrepresents age information as probabilistic distributions in the common CLIP\nlatent space. A text-guided KL-divergence loss is designed to guide this\nlearning. Our method can achieve pluralistic face aging conditioned on\nopen-world aging texts and arbitrary unseen face images. Qualitative and\nquantitative experiments demonstrate that our method can generate more diverse\nand high-quality plausible aging results.",
        "authors": [
            "Peipei Li",
            "Rui Wang",
            "Huaibo Huang",
            "Ran He",
            "Zhaofeng He"
        ]
    },
    {
        "title": "ModelGiF: Gradient Fields for Model Functional Distance",
        "url": "http://arxiv.org/abs/2309.11013",
        "abstract": "The last decade has witnessed the success of deep learning and the surge of\npublicly released trained models, which necessitates the quantification of the\nmodel functional distance for various purposes. However, quantifying the model\nfunctional distance is always challenging due to the opacity in inner workings\nand the heterogeneity in architectures or tasks. Inspired by the concept of\n\"field\" in physics, in this work we introduce Model Gradient Field (abbr.\nModelGiF) to extract homogeneous representations from the heterogeneous\npre-trained models. Our main assumption underlying ModelGiF is that each\npre-trained deep model uniquely determines a ModelGiF over the input space. The\ndistance between models can thus be measured by the similarity between their\nModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite\nof testbeds, including task relatedness estimation, intellectual property\nprotection, and model unlearning verification. Experimental results demonstrate\nthe versatility of the proposed ModelGiF on these tasks, with significantly\nsuperiority performance to state-of-the-art competitors. Codes are available at\nhttps://github.com/zju-vipa/modelgif.",
        "authors": [
            "Jie Song",
            "Zhengqi Xu",
            "Sai Wu",
            "Gang Chen",
            "Mingli Song"
        ]
    },
    {
        "title": "PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment",
        "url": "http://arxiv.org/abs/2306.15667",
        "abstract": "Camera pose estimation is a long-standing computer vision problem that to\ndate often relies on classical methods, such as handcrafted keypoint matching,\nRANSAC and bundle adjustment. In this paper, we propose to formulate the\nStructure from Motion (SfM) problem inside a probabilistic diffusion framework,\nmodelling the conditional distribution of camera poses given input images. This\nnovel view of an old problem has several advantages. (i) The nature of the\ndiffusion framework mirrors the iterative procedure of bundle adjustment. (ii)\nThe formulation allows a seamless integration of geometric constraints from\nepipolar geometry. (iii) It excels in typically difficult scenarios such as\nsparse views with wide baselines. (iv) The method can predict intrinsics and\nextrinsics for an arbitrary amount of images. We demonstrate that our method\nPoseDiffusion significantly improves over the classic SfM pipelines and the\nlearned approaches on two real-world datasets. Finally, it is observed that our\nmethod can generalize across datasets without further training. Project page:\nhttps://posediffusion.github.io/",
        "authors": [
            "Jianyuan Wang",
            "Christian Rupprecht",
            "David Novotny"
        ]
    },
    {
        "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
        "url": "http://arxiv.org/abs/2303.11897",
        "abstract": "Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.",
        "authors": [
            "Yushi Hu",
            "Benlin Liu",
            "Jungo Kasai",
            "Yizhong Wang",
            "Mari Ostendorf",
            "Ranjay Krishna",
            "Noah A Smith"
        ]
    },
    {
        "title": "SIGMA: Scale-Invariant Global Sparse Shape Matching",
        "url": "http://arxiv.org/abs/2308.08393",
        "abstract": "We propose a novel mixed-integer programming (MIP) formulation for generating\nprecise sparse correspondences for highly non-rigid shapes. To this end, we\nintroduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsic\nand extrinsic geometric information to measure the deformation quality induced\nby predicted correspondences. We integrate the PLBO, together with an\norientation-aware regulariser, into a novel MIP formulation that can be solved\nto global optimality for many practical problems. In contrast to previous\nmethods, our approach is provably invariant to rigid transformations and global\nscaling, initialisation-free, has optimality guarantees, and scales to high\nresolution meshes with (empirically observed) linear time. We show\nstate-of-the-art results for sparse non-rigid matching on several challenging\n3D datasets, including data with inconsistent meshing, as well as applications\nin mesh-to-point-cloud matching.",
        "authors": [
            "Maolin Gao",
            "Paul Roetzer",
            "Marvin Eisenberger",
            "Zorah L\u00e4hner",
            "Michael Moeller",
            "Daniel Cremers",
            "Florian Bernard"
        ]
    },
    {
        "title": "CORE: Cooperative Reconstruction for Multi-Agent Perception",
        "url": "http://arxiv.org/abs/2307.11514",
        "abstract": "This paper presents CORE, a conceptually simple, effective and\ncommunication-efficient model for multi-agent cooperative perception. It\naddresses the task from a novel perspective of cooperative reconstruction,\nbased on two key insights: 1) cooperating agents together provide a more\nholistic observation of the environment, and 2) the holistic observation can\nserve as valuable supervision to explicitly guide the model learning how to\nreconstruct the ideal observation based on collaboration. CORE instantiates the\nidea with three major components: a compressor for each agent to create more\ncompact feature representation for efficient broadcasting, a lightweight\nattentive collaboration component for cross-agent message aggregation, and a\nreconstruction module to reconstruct the observation based on aggregated\nfeature representations. This learning-to-reconstruct idea is task-agnostic,\nand offers clear and reasonable supervision to inspire more effective\ncollaboration, eventually promoting perception tasks. We validate CORE on\nOPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3D\nobject detection and semantic segmentation. Results demonstrate that the model\nachieves state-of-the-art performance on both tasks, and is more\ncommunication-efficient.",
        "authors": [
            "Binglu Wang",
            "Lei Zhang",
            "Zhaozhong Wang",
            "Yongqiang Zhao",
            "Tianfei Zhou"
        ]
    },
    {
        "title": "VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs",
        "url": "http://arxiv.org/abs/2304.06020",
        "abstract": "We propose $\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled\n$\\textbf{Vid}$eo representation based upon $\\textbf{Style}$GAN and\nNeural-$\\textbf{ODE}$s. Effective traversal of the latent space learned by\nGenerative Adversarial Networks (GANs) has been the basis for recent\nbreakthroughs in image editing. However, the applicability of such advancements\nto the video domain has been hindered by the difficulty of representing and\ncontrolling videos in the latent space of GANs. In particular, videos are\ncomposed of content (i.e., appearance) and complex motion components that\nrequire a special mechanism to disentangle and control. To achieve this,\nVidStyleODE encodes the video content in a pre-trained StyleGAN $\\mathcal{W}_+$\nspace and benefits from a latent ODE component to summarize the spatiotemporal\ndynamics of the input video. Our novel continuous video generation process then\ncombines the two to generate high-quality and temporally consistent videos with\nvarying frame rates. We show that our proposed method enables a variety of\napplications on real videos: text-guided appearance manipulation, motion\nmanipulation, image animation, and video interpolation and extrapolation.\nProject website: https://cyberiada.github.io/VidStyleODE",
        "authors": [
            "Moayed Haji Ali",
            "Andrew Bond",
            "Tolga Birdal",
            "Duygu Ceylan",
            "Levent Karacan",
            "Erkut Erdem",
            "Aykut Erdem"
        ]
    },
    {
        "title": "CiT: Curation in Training for Effective Vision-Language Data",
        "url": "http://arxiv.org/abs/2301.02241",
        "abstract": "Large vision-language models are generally applicable to many downstream\ntasks, but come at an exorbitant training cost that only large institutions can\nafford. This paper trades generality for efficiency and presents Curation in\nTraining (CiT), a simple and efficient vision-text learning algorithm that\ncouples a data objective into training. CiT automatically yields quality data\nto speed-up contrastive image-text training and alleviates the need for an\noffline data filtering pipeline, allowing broad data sources (including raw\nimage-text pairs from the web). CiT contains two loops: an outer loop curating\nthe training data and an inner loop consuming the curated training data. The\ntext encoder connects the two loops. Given metadata for tasks of interest,\ne.g., class names, and a large pool of image-text pairs, CiT alternatively\nselects relevant training data from the pool by measuring the similarity of\ntheir text embeddings and embeddings of the metadata. In our experiments, we\nobserve that CiT can speed up training by over an order of magnitude,\nespecially if the raw data size is large.",
        "authors": [
            "Hu Xu",
            "Saining Xie",
            "Po-Yao Huang",
            "Licheng Yu",
            "Russell Howes",
            "Gargi Ghosh",
            "Luke Zettlemoyer",
            "Christoph Feichtenhofer"
        ]
    },
    {
        "title": "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis",
        "url": "http://arxiv.org/abs/2303.16196",
        "abstract": "Neural Radiance Field (NeRF) significantly degrades when only a limited\nnumber of views are available. To complement the lack of 3D information,\ndepth-based models, such as DSNeRF and MonoSDF, explicitly assume the\navailability of accurate depth maps of multiple views. They linearly scale the\naccurate depth maps as supervision to guide the predicted depth of few-shot\nNeRFs. However, accurate depth maps are difficult and expensive to capture due\nto wide-range depth distances in the wild.\n  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that\nexploits depth priors from real-world inaccurate observations. The inaccurate\ndepth observations are either from pre-trained depth models or coarse depth\nmaps of consumer-level depth sensors. Since coarse depth maps are not strictly\nscaled to the ground-truth depth maps, we propose a simple yet effective\nconstraint, a local depth ranking method, on NeRFs such that the expected depth\nranking of the NeRF is consistent with that of the coarse depth maps in local\npatches. To preserve the spatial continuity of the estimated depth of NeRF, we\nfurther propose a spatial continuity constraint to encourage the consistency of\nthe expected depth continuity of NeRF with coarse depth maps. Surprisingly,\nwith simple depth ranking constraints, SparseNeRF outperforms all\nstate-of-the-art few-shot NeRF methods (including depth-based models) on\nstandard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD\nthat contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13\nPro. Extensive experiments on NVS-RGBD dataset also validate the superiority\nand generalizability of SparseNeRF. Code and dataset are available at\nhttps://sparsenerf.github.io/.",
        "authors": [
            "Guangcong Wang",
            "Zhaoxi Chen",
            "Chen Change Loy",
            "Ziwei Liu"
        ]
    },
    {
        "title": "Towards Models that Can See and Read",
        "url": "http://arxiv.org/abs/2301.07389",
        "abstract": "Visual Question Answering (VQA) and Image Captioning (CAP), which are among\nthe most popular vision-language tasks, have analogous scene-text versions that\nrequire reasoning from the text in the image. Despite their obvious\nresemblance, the two are treated independently and, as we show, yield\ntask-specific methods that can either see or read, but not both. In this work,\nwe conduct an in-depth analysis of this phenomenon and propose UniTNT, a\nUnified Text-Non-Text approach, which grants existing multimodal architectures\nscene-text understanding capabilities. Specifically, we treat scene-text\ninformation as an additional modality, fusing it with any pretrained\nencoder-decoder-based architecture via designated modules. Thorough experiments\nreveal that UniTNT leads to the first single model that successfully handles\nboth task types. Moreover, we show that scene-text understanding capabilities\ncan boost vision-language models' performance on general VQA and CAP by up to\n2.69% and 0.6 CIDEr, respectively.",
        "authors": [
            "Roy Ganz",
            "Oren Nuriel",
            "Aviad Aberdam",
            "Yair Kittenplon",
            "Shai Mazor",
            "Ron Litman"
        ]
    },
    {
        "title": "ProPainter: Improving Propagation and Transformer for Video Inpainting",
        "url": "http://arxiv.org/abs/2309.03897",
        "abstract": "Flow-based propagation and spatiotemporal Transformer are two mainstream\nmechanisms in video inpainting (VI). Despite the effectiveness of these\ncomponents, they still suffer from some limitations that affect their\nperformance. Previous propagation-based approaches are performed separately\neither in the image or feature domain. Global image propagation isolated from\nlearning may cause spatial misalignment due to inaccurate optical flow.\nMoreover, memory or computational constraints limit the temporal range of\nfeature propagation and video Transformer, preventing exploration of\ncorrespondence information from distant frames. To address these issues, we\npropose an improved framework, called ProPainter, which involves enhanced\nProPagation and an efficient Transformer. Specifically, we introduce\ndual-domain propagation that combines the advantages of image and feature\nwarping, exploiting global correspondences reliably. We also propose a\nmask-guided sparse video Transformer, which achieves high efficiency by\ndiscarding unnecessary and redundant tokens. With these components, ProPainter\noutperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining\nappealing efficiency.",
        "authors": [
            "Shangchen Zhou",
            "Chongyi Li",
            "Kelvin C. K. Chan",
            "Chen Change Loy"
        ]
    },
    {
        "title": "Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos",
        "url": "http://arxiv.org/abs/2308.10089",
        "abstract": "This work focuses on the 3D reconstruction of non-rigid objects based on\nmonocular RGB video sequences. Concretely, we aim at building high-fidelity\nmodels for generic object categories and casually captured scenes. To this end,\nwe do not assume known root poses of objects, and do not utilize\ncategory-specific templates or dense pose priors. The key idea of our method,\nRoot Pose Decomposition (RPD), is to maintain a per-frame root pose\ntransformation, meanwhile building a dense field with local transformations to\nrectify the root pose. The optimization of local transformations is performed\nby point registration to the canonical space. We also adapt RPD to multi-object\nscenarios with object occlusions and individual differences. As a result, RPD\nallows non-rigid 3D reconstruction for complicated scenarios containing objects\nwith large deformations, complex motion patterns, occlusions, and scale\ndiversities of different individuals. Such a pipeline potentially scales to\ndiverse sets of objects in the wild. We experimentally show that RPD surpasses\nstate-of-the-art methods on the challenging DAVIS, OVIS, and AMA datasets.",
        "authors": [
            "Yikai Wang",
            "Yinpeng Dong",
            "Fuchun Sun",
            "Xiao Yang"
        ]
    },
    {
        "title": "3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping",
        "url": "http://arxiv.org/abs/2212.07378",
        "abstract": "We present 3DHumanGAN, a 3D-aware generative adversarial network that\nsynthesizes photorealistic images of full-body humans with consistent\nappearances under different view-angles and body-poses. To tackle the\nrepresentational and computational challenges in synthesizing the articulated\nstructure of human bodies, we propose a novel generator architecture in which a\n2D convolutional backbone is modulated by a 3D pose mapping network. The 3D\npose mapping network is formulated as a renderable implicit function\nconditioned on a posed 3D human mesh. This design has several merits: i) it\nleverages the strength of 2D GANs to produce high-quality images; ii) it\ngenerates consistent images under varying view-angles and poses; iii) the model\ncan incorporate the 3D human prior and enable pose conditioning. Project page:\nhttps://3dhumangan.github.io/.",
        "authors": [
            "Zhuoqian Yang",
            "Shikai Li",
            "Wayne Wu",
            "Bo Dai"
        ]
    },
    {
        "title": "Priority-Centric Human Motion Generation in Discrete Latent Space",
        "url": "http://arxiv.org/abs/2308.14480",
        "abstract": "Text-to-motion generation is a formidable task, aiming to produce human\nmotions that align with the input text while also adhering to human\ncapabilities and physical laws. While there have been advancements in diffusion\nmodels, their application in discrete spaces remains underexplored. Current\nmethods often overlook the varying significance of different motions, treating\nthem uniformly. It is essential to recognize that not all motions hold the same\nrelevance to a particular textual description. Some motions, being more salient\nand informative, should be given precedence during generation. In response, we\nintroduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which\nutilizes a Transformer-based VQ-VAE to derive a concise, discrete motion\nrepresentation, incorporating a global self-attention mechanism and a\nregularization term to counteract code collapse. We also present a motion\ndiscrete diffusion model that employs an innovative noise schedule, determined\nby the significance of each motion token within the entire motion sequence.\nThis approach retains the most salient motions during the reverse diffusion\nprocess, leading to more semantically rich and varied motions. Additionally, we\nformulate two strategies to gauge the importance of motion tokens, drawing from\nboth textual and visual indicators. Comprehensive experiments on the HumanML3D\nand KIT-ML datasets confirm that our model surpasses existing techniques in\nfidelity and diversity, particularly for intricate textual descriptions.",
        "authors": [
            "Hanyang Kong",
            "Kehong Gong",
            "Dongze Lian",
            "Michael Bi Mi",
            "Xinchao Wang"
        ]
    },
    {
        "title": "Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation",
        "url": "http://arxiv.org/abs/2308.14023",
        "abstract": "Conventional Domain Adaptation (DA) methods aim to learn domain-invariant\nfeature representations to improve the target adaptation performance. However,\nwe motivate that domain-specificity is equally important since in-domain\ntrained models hold crucial domain-specific properties that are beneficial for\nadaptation. Hence, we propose to build a framework that supports\ndisentanglement and learning of domain-specific factors and task-specific\nfactors in a unified model. Motivated by the success of vision transformers in\nseveral multi-modal vision problems, we find that queries could be leveraged to\nextract the domain-specific factors. Hence, we propose a novel\nDomain-specificity-inducing Transformer (DSiT) framework for disentangling and\nlearning both domain-specific and task-specific factors. To achieve\ndisentanglement, we propose to construct novel Domain-Representative Inputs\n(DRI) with domain-specific information to train a domain classifier with a\nnovel domain token. We are the first to utilize vision transformers for domain\nadaptation in a privacy-oriented source-free setting, and our approach achieves\nstate-of-the-art performance on single-source, multi-source, and multi-target\nbenchmarks",
        "authors": [
            "Sunandini Sanyal",
            "Ashish Ramayee Asokan",
            "Suvaansh Bhambri",
            "Akshay Kulkarni",
            "Jogendra Nath Kundu",
            "R. Venkatesh Babu"
        ]
    },
    {
        "title": "Towards Improved Input Masking for Convolutional Neural Networks",
        "url": "http://arxiv.org/abs/2211.14646",
        "abstract": "The ability to remove features from the input of machine learning models is\nvery important to understand and interpret model predictions. However, this is\nnon-trivial for vision models since masking out parts of the input image\ntypically causes large distribution shifts. This is because the baseline color\nused for masking (typically grey or black) is out of distribution. Furthermore,\nthe shape of the mask itself can contain unwanted signals which can be used by\nthe model for its predictions. Recently, there has been some progress in\nmitigating this issue (called missingness bias) in image masking for vision\ntransformers. In this work, we propose a new masking method for CNNs we call\nlayer masking in which the missingness bias caused by masking is reduced to a\nlarge extent. Intuitively, layer masking applies a mask to intermediate\nactivation maps so that the model only processes the unmasked input. We show\nthat our method (i) is able to eliminate or minimize the influence of the mask\nshape or color on the output of the model, and (ii) is much better than\nreplacing the masked region by black or grey for input perturbation based\ninterpretability techniques like LIME. Thus, layer masking is much less\naffected by missingness bias than other masking strategies. We also demonstrate\nhow the shape of the mask may leak information about the class, thus affecting\nestimates of model reliance on class-relevant features derived from input\nmasking. Furthermore, we discuss the role of data augmentation techniques for\ntackling this problem, and argue that they are not sufficient for preventing\nmodel reliance on mask shape. The code for this project is publicly available\nat https://github.com/SriramB-98/layer_masking",
        "authors": [
            "Sriram Balasubramanian",
            "Soheil Feizi"
        ]
    },
    {
        "title": "3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack",
        "url": "http://arxiv.org/abs/2308.07546",
        "abstract": "With the maturity of depth sensors, the vulnerability of 3D point cloud\nmodels has received increasing attention in various applications such as\nautonomous driving and robot navigation. Previous 3D adversarial attackers\neither follow the white-box setting to iteratively update the coordinate\nperturbations based on gradients, or utilize the output model logits to\nestimate noisy gradients in the black-box setting. However, these attack\nmethods are hard to be deployed in real-world scenarios since realistic 3D\napplications will not share any model details to users. Therefore, we explore a\nmore challenging yet practical 3D attack setting, \\textit{i.e.}, attacking\npoint clouds with black-box hard labels, in which the attacker can only have\naccess to the prediction label of the input. To tackle this setting, we propose\na novel 3D attack method, termed \\textbf{3D} \\textbf{H}ard-label\natt\\textbf{acker} (\\textbf{3DHacker}), based on the developed decision boundary\nalgorithm to generate adversarial samples solely with the knowledge of class\nlabels. Specifically, to construct the class-aware model decision boundary,\n3DHacker first randomly fuses two point clouds of different classes in the\nspectral domain to craft their intermediate sample with high imperceptibility,\nthen projects it onto the decision boundary via binary search. To restrict the\nfinal perturbation size, 3DHacker further introduces an iterative optimization\nstrategy to move the intermediate sample along the decision boundary for\ngenerating adversarial point clouds with smallest trivial perturbations.\nExtensive evaluations show that, even in the challenging hard-label setting,\n3DHacker still competitively outperforms existing 3D attacks regarding the\nattack performance as well as adversary quality.",
        "authors": [
            "Yunbo Tao",
            "Daizong Liu",
            "Pan Zhou",
            "Yulai Xie",
            "Wei Du",
            "Wei Hu"
        ]
    },
    {
        "title": "Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking",
        "url": "http://arxiv.org/abs/2308.06904",
        "abstract": "Transformer-based visual trackers have demonstrated significant progress\nowing to their superior modeling capabilities. However, existing trackers are\nhampered by low speed, limiting their applicability on devices with limited\ncomputational power. To alleviate this problem, we propose HiT, a new family of\nefficient tracking models that can run at high speed on different devices while\nretaining high performance. The central idea of HiT is the Bridge Module, which\nbridges the gap between modern lightweight transformers and the tracking\nframework. The Bridge Module incorporates the high-level information of deep\nfeatures into the shallow large-resolution features. In this way, it produces\nbetter features for the tracking head. We also propose a novel dual-image\nposition encoding technique that simultaneously encodes the position\ninformation of both the search region and template images. The HiT model\nachieves promising speed with competitive performance. For instance, it runs at\n61 frames per second (fps) on the Nvidia Jetson AGX edge device. Furthermore,\nHiT attains 64.6% AUC on the LaSOT benchmark, surpassing all previous efficient\ntrackers.",
        "authors": [
            "Ben Kang",
            "Xin Chen",
            "Dong Wang",
            "Houwen Peng",
            "Huchuan Lu"
        ]
    },
    {
        "title": "Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation",
        "url": "http://arxiv.org/abs/2309.04946",
        "abstract": "Audio-driven talking-head synthesis is a popular research topic for virtual\nhuman-related applications. However, the inflexibility and inefficiency of\nexisting methods, which necessitate expensive end-to-end training to transfer\nemotions from guidance videos to talking-head predictions, are significant\nlimitations. In this work, we propose the Emotional Adaptation for Audio-driven\nTalking-head (EAT) method, which transforms emotion-agnostic talking-head\nmodels into emotion-controllable ones in a cost-effective and efficient manner\nthrough parameter-efficient adaptations. Our approach utilizes a pretrained\nemotion-agnostic talking-head transformer and introduces three lightweight\nadaptations (the Deep Emotional Prompts, Emotional Deformation Network, and\nEmotional Adaptation Module) from different perspectives to enable precise and\nrealistic emotion controls. Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including LRW\nand MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable\ngeneralization ability, even in scenarios where emotional training videos are\nscarce or nonexistent. Project website: https://yuangan.github.io/eat/",
        "authors": [
            "Yuan Gan",
            "Zongxin Yang",
            "Xihang Yue",
            "Lingyun Sun",
            "Yi Yang"
        ]
    },
    {
        "title": "Object-aware Gaze Target Detection",
        "url": "http://arxiv.org/abs/2307.09662",
        "abstract": "Gaze target detection aims to predict the image location where the person is\nlooking and the probability that a gaze is out of the scene. Several works have\ntackled this task by regressing a gaze heatmap centered on the gaze location,\nhowever, they overlooked decoding the relationship between the people and the\ngazed objects. This paper proposes a Transformer-based architecture that\nautomatically detects objects (including heads) in the scene to build\nassociations between every head and the gazed-head/object, resulting in a\ncomprehensive, explainable gaze analysis composed of: gaze target area, gaze\npixel point, the class and the image location of the gazed-object. Upon\nevaluation of the in-the-wild benchmarks, our method achieves state-of-the-art\nresults on all metrics (up to 2.91% gain in AUC, 50% reduction in gaze\ndistance, and 9% gain in out-of-frame average precision) for gaze target\ndetection and 11-13% improvement in average precision for the classification\nand the localization of the gazed-objects. The code of the proposed method is\npublicly available.",
        "authors": [
            "Francesco Tonini",
            "Nicola Dall'Asen",
            "Cigdem Beyan",
            "Elisa Ricci"
        ]
    },
    {
        "title": "VADER: Video Alignment Differencing and Retrieval",
        "url": "http://arxiv.org/abs/2303.13193",
        "abstract": "We propose VADER, a spatio-temporal matching, alignment, and change\nsummarization method to help fight misinformation spread via manipulated\nvideos. VADER matches and coarsely aligns partial video fragments to candidate\nvideos using a robust visual descriptor and scalable search over adaptively\nchunked video content. A transformer-based alignment module then refines the\ntemporal localization of the query fragment within the matched video. A\nspace-time comparator module identifies regions of manipulation between aligned\ncontent, invariant to any changes due to any residual temporal misalignments or\nartifacts arising from non-editorial changes of the content. Robustly matching\nvideo to a trusted source enables conclusions to be drawn on video provenance,\nenabling informed trust decisions on content encountered.",
        "authors": [
            "Alexander Black",
            "Simon Jenni",
            "Tu Bui",
            "Md. Mehrab Tanjim",
            "Stefano Petrangeli",
            "Ritwik Sinha",
            "Viswanathan Swaminathan",
            "John Collomosse"
        ]
    },
    {
        "title": "HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation",
        "url": "http://arxiv.org/abs/2303.15994",
        "abstract": "Panoptic Scene Graph generation (PSG) is a recently proposed task in image\nscene understanding that aims to segment the image and extract triplets of\nsubjects, objects and their relations to build a scene graph. This task is\nparticularly challenging for two reasons. First, it suffers from a long-tail\nproblem in its relation categories, making naive biased methods more inclined\nto high-frequency relations. Existing unbiased methods tackle the long-tail\nproblem by data/loss rebalancing to favor low-frequency relations. Second, a\nsubject-object pair can have two or more semantically overlapping relations.\nWhile existing methods favor one over the other, our proposed HiLo framework\nlets different network branches specialize on low and high frequency relations,\nenforce their consistency and fuse the results. To the best of our knowledge we\nare the first to propose an explicitly unbiased PSG method. In extensive\nexperiments we show that our HiLo framework achieves state-of-the-art results\non the PSG task. We also apply our method to the Scene Graph Generation task\nthat predicts boxes instead of masks and see improvements over all baseline\nmethods. Code is available at https://github.com/franciszzj/HiLo.",
        "authors": [
            "Zijian Zhou",
            "Miaojing Shi",
            "Holger Caesar"
        ]
    },
    {
        "title": "Chop & Learn: Recognizing and Generating Object-State Compositions",
        "url": "http://arxiv.org/abs/2309.14339",
        "abstract": "Recognizing and generating object-state compositions has been a challenging\ntask, especially when generalizing to unseen compositions. In this paper, we\nstudy the task of cutting objects in different styles and the resulting object\nstate changes. We propose a new benchmark suite Chop & Learn, to accommodate\nthe needs of learning objects and different cut styles using multiple\nviewpoints. We also propose a new task of Compositional Image Generation, which\ncan transfer learned cut styles to different objects, by generating novel\nobject-state images. Moreover, we also use the videos for Compositional Action\nRecognition, and show valuable uses of this dataset for multiple video tasks.\nProject website: https://chopnlearn.github.io.",
        "authors": [
            "Nirat Saini",
            "Hanyu Wang",
            "Archana Swaminathan",
            "Vinoj Jayasundara",
            "Bo He",
            "Kamal Gupta",
            "Abhinav Shrivastava"
        ]
    },
    {
        "title": "Automatic Animation of Hair Blowing in Still Portrait Photos",
        "url": "http://arxiv.org/abs/2309.14207",
        "abstract": "We propose a novel approach to animate human hair in a still portrait photo.\nExisting work has largely studied the animation of fluid elements such as water\nand fire. However, hair animation for a real image remains underexplored, which\nis a challenging problem, due to the high complexity of hair structure and\ndynamics. Considering the complexity of hair structure, we innovatively treat\nhair wisp extraction as an instance segmentation problem, where a hair wisp is\nreferred to as an instance. With advanced instance segmentation networks, our\nmethod extracts meaningful and natural hair wisps. Furthermore, we propose a\nwisp-aware animation module that animates hair wisps with pleasing motions\nwithout noticeable artifacts. The extensive experiments show the superiority of\nour method. Our method provides the most pleasing and compelling viewing\nexperience in the qualitative experiments and outperforms state-of-the-art\nstill-image animation methods by a large margin in the quantitative evaluation.\nProject url: \\url{https://nevergiveu.github.io/AutomaticHairBlowing/}",
        "authors": [
            "Wenpeng Xiao",
            "Wentao Liu",
            "Yitong Wang",
            "Bernard Ghanem",
            "Bing Li"
        ]
    },
    {
        "title": "A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction",
        "url": "http://arxiv.org/abs/2301.06782",
        "abstract": "Neural Radiance Fields (NeRF) has achieved impressive results in single\nobject scene reconstruction and novel view synthesis, which have been\ndemonstrated on many single modality and single object focused indoor scene\ndatasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on\nlarge-scale outdoor scene reconstruction is still limited, as there is no\nunified outdoor scene dataset for large-scale NeRF evaluation due to expensive\ndata acquisition and calibration costs. In this paper, we propose a large-scale\noutdoor multi-modal dataset, OMMO dataset, containing complex land objects and\nscenes with calibrated images, point clouds and prompt annotations. Meanwhile,\na new benchmark for several outdoor NeRF-based tasks is established, such as\nnovel view synthesis, surface reconstruction, and multi-modal NeRF. To create\nthe dataset, we capture and collect a large number of real fly-view videos and\nselect high-quality and high-resolution clips from them. Then we design a\nquality review module to refine images, remove low-quality frames and\nfail-to-calibrate scenes through a learning-based automatic evaluation plus\nmanual review. Finally, a number of volunteers are employed to add the text\ndescriptions for each scene and key-frame to meet the potential multi-modal\nrequirements in the future. Compared with existing NeRF datasets, our dataset\ncontains abundant real-world urban and natural scenes with various scales,\ncamera trajectories, and lighting conditions. Experiments show that our dataset\ncan benchmark most state-of-the-art NeRF methods on different tasks. We will\nrelease the dataset and model weights very soon.",
        "authors": [
            "Chongshan Lu",
            "Fukun Yin",
            "Xin Chen",
            "Tao Chen",
            "Gang YU",
            "Jiayuan Fan"
        ]
    },
    {
        "title": "4D Panoptic Segmentation as Invariant and Equivariant Field Prediction",
        "url": "http://arxiv.org/abs/2303.15651",
        "abstract": "In this paper, we develop rotation-equivariant neural networks for 4D\npanoptic segmentation. 4D panoptic segmentation is a benchmark task for\nautonomous driving that requires recognizing semantic classes and object\ninstances on the road based on LiDAR scans, as well as assigning temporally\nconsistent IDs to instances across time. We observe that the driving scenario\nis symmetric to rotations on the ground plane. Therefore, rotation-equivariance\ncould provide better generalization and more robust feature learning.\nSpecifically, we review the object instance clustering strategies and restate\nthe centerness-based approach and the offset-based approach as the prediction\nof invariant scalar fields and equivariant vector fields. Other sub-tasks are\nalso unified from this perspective, and different invariant and equivariant\nlayers are designed to facilitate their predictions. Through evaluation on the\nstandard 4D panoptic segmentation benchmark of SemanticKITTI, we show that our\nequivariant models achieve higher accuracy with lower computational costs\ncompared to their non-equivariant counterparts. Moreover, our method sets the\nnew state-of-the-art performance and achieves 1st place on the SemanticKITTI 4D\nPanoptic Segmentation leaderboard.",
        "authors": [
            "Minghan Zhu",
            "Shizhong Han",
            "Hong Cai",
            "Shubhankar Borse",
            "Maani Ghaffari",
            "Fatih Porikli"
        ]
    },
    {
        "title": "Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection",
        "url": "http://arxiv.org/abs/2204.02964",
        "abstract": "We present an approach to efficiently and effectively adapt a masked image\nmodeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object\ndetection, which is based on our two novel observations: (i) A MIM pre-trained\nvanilla ViT encoder can work surprisingly well in the challenging object-level\nrecognition scenario even with randomly sampled partial observations, e.g.,\nonly 25% $\\sim$ 50% of the input embeddings. (ii) In order to construct\nmulti-scale representations for object detection from single-scale ViT, a\nrandomly initialized compact convolutional stem supplants the pre-trained large\nkernel patchify stem, and its intermediate features can naturally serve as the\nhigher resolution inputs of a feature pyramid network without further\nupsampling or other manipulations. While the pre-trained ViT is only regarded\nas the 3$^{rd}$-stage of our detector's backbone instead of the whole feature\nextractor. This results in a ConvNet-ViT hybrid feature extractor. The proposed\ndetector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform\nhierarchical Swin Transformer by 2.5 box AP and 2.6 mask AP on COCO, and\nachieves better results compared with the previous best adapted vanilla ViT\ndetector using a more modest fine-tuning recipe while converging 2.8$\\times$\nfaster. Code and pre-trained models are available at\nhttps://github.com/hustvl/MIMDet.",
        "authors": [
            "Yuxin Fang",
            "Shusheng Yang",
            "Shijie Wang",
            "Yixiao Ge",
            "Ying Shan",
            "Xinggang Wang"
        ]
    },
    {
        "title": "Spatio-Temporal Crop Aggregation for Video Representation Learning",
        "url": "http://arxiv.org/abs/2211.17042",
        "abstract": "We propose Spatio-temporal Crop Aggregation for video representation LEarning\n(SCALE), a novel method that enjoys high scalability at both training and\ninference time. Our model builds long-range video features by learning from\nsets of video clip-level features extracted with a pre-trained backbone. To\ntrain the model, we propose a self-supervised objective consisting of masked\nclip feature prediction. We apply sparsity to both the input, by extracting a\nrandom set of video clips, and to the loss function, by only reconstructing the\nsparse inputs. Moreover, we use dimensionality reduction by working in the\nlatent space of a pre-trained backbone applied to single video clips. These\ntechniques make our method not only extremely efficient to train but also\nhighly effective in transfer learning. We demonstrate that our video\nrepresentation yields state-of-the-art performance with linear, non-linear, and\nKNN probing on common action classification and video understanding datasets.",
        "authors": [
            "Sepehr Sameni",
            "Simon Jenni",
            "Paolo Favaro"
        ]
    },
    {
        "title": "Neural-PBIR Reconstruction of Shape, Material, and Illumination",
        "url": "http://arxiv.org/abs/2304.13445",
        "abstract": "Reconstructing the shape and spatially varying surface appearances of a\nphysical-world object as well as its surrounding illumination based on 2D\nimages (e.g., photographs) of the object has been a long-standing problem in\ncomputer vision and graphics. In this paper, we introduce an accurate and\nhighly efficient object reconstruction pipeline combining neural based object\nreconstruction and physics-based inverse rendering (PBIR). Our pipeline firstly\nleverages a neural SDF based shape reconstruction to produce high-quality but\npotentially imperfect object shape. Then, we introduce a neural material and\nlighting distillation stage to achieve high-quality predictions for material\nand illumination. In the last stage, initialized by the neural predictions, we\nperform PBIR to refine the initial results and obtain the final high-quality\nreconstruction of object shape, material, and illumination. Experimental\nresults demonstrate our pipeline significantly outperforms existing methods\nquality-wise and performance-wise.",
        "authors": [
            "Cheng Sun",
            "Guangyan Cai",
            "Zhengqin Li",
            "Kai Yan",
            "Cheng Zhang",
            "Carl Marshall",
            "Jia-Bin Huang",
            "Shuang Zhao",
            "Zhao Dong"
        ]
    },
    {
        "title": "BlindHarmony: \"Blind\" Harmonization for MR Images via Flow Model",
        "url": "http://arxiv.org/abs/2305.10732",
        "abstract": "In MRI, images of the same contrast (e.g., T$_1$) from the same subject can\nexhibit noticeable differences when acquired using different hardware,\nsequences, or scan parameters. These differences in images create a domain gap\nthat needs to be bridged by a step called image harmonization, to process the\nimages successfully using conventional or deep learning-based image analysis\n(e.g., segmentation). Several methods, including deep learning-based\napproaches, have been proposed to achieve image harmonization. However, they\noften require datasets from multiple domains for deep learning training and may\nstill be unsuccessful when applied to images from unseen domains. To address\nthis limitation, we propose a novel concept called `Blind Harmonization', which\nutilizes only target domain data for training but still has the capability to\nharmonize images from unseen domains. For the implementation of blind\nharmonization, we developed BlindHarmony using an unconditional flow model\ntrained on target domain data. The harmonized image is optimized to have a\ncorrelation with the input source domain image while ensuring that the latent\nvector of the flow model is close to the center of the Gaussian distribution.\nBlindHarmony was evaluated on both simulated and real datasets and compared to\nconventional methods. BlindHarmony demonstrated noticeable performance on both\ndatasets, highlighting its potential for future use in clinical settings. The\nsource code is available at: https://github.com/SNU-LIST/BlindHarmony",
        "authors": [
            "Hwihun Jeong",
            "Heejoon Byun",
            "Dong Un Kang",
            "Jongho Lee"
        ]
    },
    {
        "title": "Zero-guidance Segmentation Using Zero Segment Labels",
        "url": "http://arxiv.org/abs/2303.13396",
        "abstract": "CLIP has enabled new and exciting joint vision-language applications, one of\nwhich is open-vocabulary segmentation, which can locate any segment given an\narbitrary text query. In our research, we ask whether it is possible to\ndiscover semantic segments without any user guidance in the form of text\nqueries or predefined classes, and label them using natural language\nautomatically? We propose a novel problem zero-guidance segmentation and the\nfirst baseline that leverages two pre-trained generalist models, DINO and CLIP,\nto solve this problem without any fine-tuning or segmentation dataset. The\ngeneral idea is to first segment an image into small over-segments, encode them\ninto CLIP's visual-language space, translate them into text labels, and merge\nsemantically similar segments together. The key challenge, however, is how to\nencode a visual segment into a segment-specific embedding that balances global\nand local context information, both useful for recognition. Our main\ncontribution is a novel attention-masking technique that balances the two\ncontexts by analyzing the attention layers inside CLIP. We also introduce\nseveral metrics for the evaluation of this new task. With CLIP's innate\nknowledge, our method can precisely locate the Mona Lisa painting among a\nmuseum crowd. Project page: https://zero-guide-seg.github.io/.",
        "authors": [
            "Pitchaporn Rewatbowornwong",
            "Nattanat Chatthee",
            "Ekapol Chuangsuwanich",
            "Supasorn Suwajanakorn"
        ]
    },
    {
        "title": "Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence",
        "url": "http://arxiv.org/abs/2302.13562",
        "abstract": "Reducing communication overhead in federated learning (FL) is challenging but\ncrucial for large-scale distributed privacy-preserving machine learning. While\nmethods utilizing sparsification or others can largely lower the communication\noverhead, the convergence rate is also greatly compromised. In this paper, we\npropose a novel method, named single-step synthetic features compressor (3SFC),\nto achieve communication-efficient FL by directly constructing a tiny synthetic\ndataset based on raw gradients. Thus, 3SFC can achieve an extremely low\ncompression rate when the constructed dataset contains only one data sample.\nMoreover, 3SFC's compressing phase utilizes a similarity-based objective\nfunction so that it can be optimized with just one step, thereby considerably\nimproving its performance and robustness. In addition, to minimize the\ncompressing error, error feedback (EF) is also incorporated into 3SFC.\nExperiments on multiple datasets and models suggest that 3SFC owns\nsignificantly better convergence rates compared to competing methods with lower\ncompression rates (up to 0.02%). Furthermore, ablation studies and\nvisualizations show that 3SFC can carry more information than competing methods\nfor every communication round, further validating its effectiveness.",
        "authors": [
            "Yuhao Zhou",
            "Mingjia Shi",
            "Yuanxi Li",
            "Qing Ye",
            "Yanan Sun",
            "Jiancheng Lv"
        ]
    },
    {
        "title": "SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator",
        "url": "http://arxiv.org/abs/2307.08492",
        "abstract": "In this paper, we propose a novel network, SVDFormer, to tackle two specific\nchallenges in point cloud completion: understanding faithful global shapes from\nincomplete point clouds and generating high-accuracy local structures. Current\nmethods either perceive shape patterns using only 3D coordinates or import\nextra images with well-calibrated intrinsic parameters to guide the geometry\nestimation of the missing parts. However, these approaches do not always fully\nleverage the cross-modal self-structures available for accurate and\nhigh-quality point cloud completion. To this end, we first design a Self-view\nFusion Network that leverages multiple-view depth image information to observe\nincomplete self-shape and generate a compact global shape. To reveal highly\ndetailed structures, we then introduce a refinement module, called\nSelf-structure Dual-generator, in which we incorporate learned shape priors and\ngeometric self-similarities for producing new points. By perceiving the\nincompleteness of each point, the dual-path design disentangles refinement\nstrategies conditioned on the structural type of each point. SVDFormer absorbs\nthe wisdom of self-structures, avoiding any additional paired information such\nas color images with precisely calibrated camera intrinsic parameters.\nComprehensive experiments indicate that our method achieves state-of-the-art\nperformance on widely-used benchmarks. Code will be available at\nhttps://github.com/czvvd/SVDFormer.",
        "authors": [
            "Zhe Zhu",
            "Honghua Chen",
            "Xing He",
            "Weiming Wang",
            "Jing Qin",
            "Mingqiang Wei"
        ]
    },
    {
        "title": "CTVIS: Consistent Training for Online Video Instance Segmentation",
        "url": "http://arxiv.org/abs/2307.12616",
        "abstract": "The discrimination of instance embeddings plays a vital role in associating\ninstances across time for online video instance segmentation (VIS). Instance\nembedding learning is directly supervised by the contrastive loss computed upon\nthe contrastive items (CIs), which are sets of anchor/positive/negative\nembeddings. Recent online VIS methods leverage CIs sourced from one reference\nframe only, which we argue is insufficient for learning highly discriminative\nembeddings. Intuitively, a possible strategy to enhance CIs is replicating the\ninference phase during training. To this end, we propose a simple yet effective\ntraining strategy, called Consistent Training for Online VIS (CTVIS), which\ndevotes to aligning the training and inference pipelines in terms of building\nCIs. Specifically, CTVIS constructs CIs by referring inference the\nmomentum-averaged embedding and the memory bank storage mechanisms, and adding\nnoise to the relevant embeddings. Such an extension allows a reliable\ncomparison between embeddings of current instances and the stable\nrepresentations of historical instances, thereby conferring an advantage in\nmodeling VIS challenges such as occlusion, re-identification, and deformation.\nEmpirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on three\nVIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS\n(35.5% AP). Furthermore, we find that pseudo-videos transformed from images can\ntrain robust models surpassing fully-supervised ones.",
        "authors": [
            "Kaining Ying",
            "Qing Zhong",
            "Weian Mao",
            "Zhenhua Wang",
            "Hao Chen",
            "Lin Yuanbo Wu",
            "Yifan Liu",
            "Chengxiang Fan",
            "Yunzhi Zhuge",
            "Chunhua Shen"
        ]
    },
    {
        "title": "Hallucination Improves the Performance of Unsupervised Visual Representation Learning",
        "url": "http://arxiv.org/abs/2307.12168",
        "abstract": "Contrastive learning models based on Siamese structure have demonstrated\nremarkable performance in self-supervised learning. Such a success of\ncontrastive learning relies on two conditions, a sufficient number of positive\npairs and adequate variations between them. If the conditions are not met,\nthese frameworks will lack semantic contrast and be fragile on overfitting. To\naddress these two issues, we propose Hallucinator that could efficiently\ngenerate additional positive samples for further contrast. The Hallucinator is\ndifferentiable and creates new data in the feature space. Thus, it is optimized\ndirectly with the pre-training task and introduces nearly negligible\ncomputation. Moreover, we reduce the mutual information of hallucinated pairs\nand smooth them through non-linear operations. This process helps avoid\nover-confident contrastive learning models during the training and achieves\nmore transformation-invariant feature embeddings. Remarkably, we empirically\nprove that the proposed Hallucinator generalizes well to various contrastive\nlearning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear\nclassification protocol, a stable accuracy gain is achieved, ranging from 0.3%\nto 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is\nalso observed in transferring pre-train encoders to the downstream tasks,\nincluding object detection and segmentation.",
        "authors": [
            "Jing Wu",
            "Jennifer Hobbs",
            "Naira Hovakimyan"
        ]
    },
    {
        "title": "S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields",
        "url": "http://arxiv.org/abs/2308.07032",
        "abstract": "Recently, Neural Radiance Field (NeRF) has shown great success in rendering\nnovel-view images of a given scene by learning an implicit representation with\nonly posed RGB images. NeRF and relevant neural field methods (e.g., neural\nsurface representation) typically optimize a point-wise loss and make\npoint-wise predictions, where one data point corresponds to one pixel.\nUnfortunately, this line of research failed to use the collective supervision\nof distant pixels, although it is known that pixels in an image or scene can\nprovide rich structural information. To the best of our knowledge, we are the\nfirst to design a nonlocal multiplex training paradigm for NeRF and relevant\nneural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss\nthat processes multiple data points as a whole set instead of process multiple\ninputs independently. Our extensive experiments demonstrate the unreasonable\neffectiveness of S3IM in improving NeRF and neural surface representation for\nnearly free. The improvements of quality metrics can be particularly\nsignificant for those relatively difficult tasks: e.g., the test MSE loss\nunexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view\nsynthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance\nreduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is\nconsistently robust even with sparse inputs, corrupted images, and dynamic\nscenes.",
        "authors": [
            "Zeke Xie",
            "Xindi Yang",
            "Yujie Yang",
            "Qi Sun",
            "Yixiang Jiang",
            "Haoran Wang",
            "Yunfeng Cai",
            "Mingming Sun"
        ]
    },
    {
        "title": "GlobalMapper: Arbitrary-Shaped Urban Layout Generation",
        "url": "http://arxiv.org/abs/2307.09693",
        "abstract": "Modeling and designing urban building layouts is of significant interest in\ncomputer vision, computer graphics, and urban applications. A building layout\nconsists of a set of buildings in city blocks defined by a network of roads. We\nobserve that building layouts are discrete structures, consisting of multiple\nrows of buildings of various shapes, and are amenable to skeletonization for\nmapping arbitrary city block shapes to a canonical form. Hence, we propose a\nfully automatic approach to building layout generation using graph attention\nnetworks. Our method generates realistic urban layouts given arbitrary road\nnetworks, and enables conditional generation based on learned priors. Our\nresults, including user study, demonstrate superior performance as compared to\nprior layout generation networks, support arbitrary city block and varying\nbuilding shapes as demonstrated by generating layouts for 28 large cities.",
        "authors": [
            "Liu He",
            "Daniel Aliaga"
        ]
    },
    {
        "title": "Membrane Potential Batch Normalization for Spiking Neural Networks",
        "url": "http://arxiv.org/abs/2308.08359",
        "abstract": "As one of the energy-efficient alternatives of conventional neural networks\n(CNNs), spiking neural networks (SNNs) have gained more and more interest\nrecently. To train the deep models, some effective batch normalization (BN)\ntechniques are proposed in SNNs. All these BNs are suggested to be used after\nthe convolution layer as usually doing in CNNs. However, the spiking neuron is\nmuch more complex with the spatio-temporal dynamics. The regulated data flow\nafter the BN layer will be disturbed again by the membrane potential updating\noperation before the firing function, i.e., the nonlinear activation.\nTherefore, we advocate adding another BN layer before the firing function to\nnormalize the membrane potential again, called MPBN. To eliminate the induced\ntime cost of MPBN, we also propose a training-inference-decoupled\nre-parameterization technique to fold the trained MPBN into the firing\nthreshold. With the re-parameterization technique, the MPBN will not introduce\nany extra time burden in the inference. Furthermore, the MPBN can also adopt\nthe element-wised form, while these BNs after the convolution layer can only\nuse the channel-wised form. Experimental results show that the proposed MPBN\nperforms well on both popular non-spiking static and neuromorphic datasets. Our\ncode is open-sourced at \\href{https://github.com/yfguo91/MPBN}{MPBN}.",
        "authors": [
            "Yufei Guo",
            "Yuhan Zhang",
            "Yuanpei Chen",
            "Weihang Peng",
            "Xiaode Liu",
            "Liwen Zhang",
            "Xuhui Huang",
            "Zhe Ma"
        ]
    },
    {
        "title": "Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2309.03598",
        "abstract": "In semi-supervised learning, unlabeled samples can be utilized through\naugmentation and consistency regularization. However, we observed certain\nsamples, even undergoing strong augmentation, are still correctly classified\nwith high confidence, resulting in a loss close to zero. It indicates that\nthese samples have been already learned well and do not provide any additional\noptimization benefits to the model. We refer to these samples as ``naive\nsamples\". Unfortunately, existing SSL models overlook the characteristics of\nnaive samples, and they just apply the same learning strategy to all samples.\nTo further optimize the SSL model, we emphasize the importance of giving\nattention to naive samples and augmenting them in a more diverse manner. Sample\nadaptive augmentation (SAA) is proposed for this stated purpose and consists of\ntwo modules: 1) sample selection module; 2) sample augmentation module.\nSpecifically, the sample selection module picks out {naive samples} based on\nhistorical training information at each epoch, then the naive samples will be\naugmented in a more diverse manner in the sample augmentation module. Thanks to\nthe extreme ease of implementation of the above modules, SAA is advantageous\nfor being simple and lightweight. We add SAA on top of FixMatch and FlexMatch\nrespectively, and experiments demonstrate SAA can significantly improve the\nmodels. For example, SAA helped improve the accuracy of FixMatch from 92.50% to\n94.76% and that of FlexMatch from 95.01% to 95.31% on CIFAR-10 with 40 labels.",
        "authors": [
            "Guan Gui",
            "Zhen Zhao",
            "Lei Qi",
            "Luping Zhou",
            "Lei Wang",
            "Yinghuan Shi"
        ]
    },
    {
        "title": "Imitator: Personalized Speech-driven 3D Facial Animation",
        "url": "http://arxiv.org/abs/2301.00023",
        "abstract": "Speech-driven 3D facial animation has been widely explored, with applications\nin gaming, character animation, virtual reality, and telepresence systems.\nState-of-the-art methods deform the face topology of the target actor to sync\nthe input audio without considering the identity-specific speaking style and\nfacial idiosyncrasies of the target actor, thus, resulting in unrealistic and\ninaccurate lip movements. To address this, we present Imitator, a speech-driven\nfacial expression synthesis method, which learns identity-specific details from\na short input video and produces novel facial expressions matching the\nidentity-specific speaking style and facial idiosyncrasies of the target actor.\nSpecifically, we train a style-agnostic transformer on a large facial\nexpression dataset which we use as a prior for audio-driven facial expressions.\nBased on this prior, we optimize for identity-specific speaking style based on\na short reference video. To train the prior, we introduce a novel loss function\nbased on detected bilabial consonants to ensure plausible lip closures and\nconsequently improve the realism of the generated expressions. Through detailed\nexperiments and a user study, we show that our approach produces temporally\ncoherent facial expressions from input audio while preserving the speaking\nstyle of the target actors.",
        "authors": [
            "Balamurugan Thambiraja",
            "Ikhsanul Habibie",
            "Sadegh Aliakbarian",
            "Darren Cosker",
            "Christian Theobalt",
            "Justus Thies"
        ]
    },
    {
        "title": "Unified Coarse-to-Fine Alignment for Video-Text Retrieval",
        "url": "http://arxiv.org/abs/2309.10091",
        "abstract": "The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.",
        "authors": [
            "Ziyang Wang",
            "Yi-Lin Sung",
            "Feng Cheng",
            "Gedas Bertasius",
            "Mohit Bansal"
        ]
    },
    {
        "title": "Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models",
        "url": "http://arxiv.org/abs/2303.06571",
        "abstract": "Prompt tuning, a recently emerging paradigm, enables the powerful\nvision-language pre-training models to adapt to downstream tasks in a parameter\n-- and data -- efficient way, by learning the ``soft prompts'' to condition\nfrozen pre-training models. Though effective, it is particularly problematic in\nthe few-shot scenario, where prompt tuning performance is sensitive to the\ninitialization and requires a time-consuming process to find a good\ninitialization, thus restricting the fast adaptation ability of the\npre-training models. In addition, prompt tuning could undermine the\ngeneralizability of the pre-training models, because the learnable prompt\ntokens are easy to overfit to the limited training samples. To address these\nissues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM)\nframework that jointly meta-learns an efficient soft prompt initialization for\nbetter adaptation and a lightweight gradient regulating function for strong\ncross-domain generalizability in a meta-learning paradigm using only the\nunlabeled image-text pre-training data. Rather than designing a specific prompt\ntuning method, our GRAM can be easily incorporated into various prompt tuning\nmethods in a model-agnostic way, and comprehensive experiments show that GRAM\nbrings about consistent improvement for them in several settings (i.e.,\nfew-shot learning, cross-domain generalization, cross-dataset generalization,\netc.) over 11 datasets. Further, experiments show that GRAM enables the\northogonal methods of textual and visual prompt tuning to work in a\nmutually-enhanced way, offering better generalizability beyond the uni-modal\nprompt tuning methods.",
        "authors": [
            "Juncheng Li",
            "Minghe Gao",
            "Longhui Wei",
            "Siliang Tang",
            "Wenqiao Zhang",
            "Mengze Li",
            "Wei Ji",
            "Qi Tian",
            "Tat-Seng Chua",
            "Yueting Zhuang"
        ]
    },
    {
        "title": "Zero-Shot Composed Image Retrieval with Textual Inversion",
        "url": "http://arxiv.org/abs/2303.15247",
        "abstract": "Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nquery composed of a reference image and a relative caption that describes the\ndifference between the two images. The high effort and cost required for\nlabeling datasets for CIR hamper the widespread usage of existing methods, as\nthey rely on supervised learning. In this work, we propose a new task,\nZero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled\ntraining dataset. Our approach, named zero-Shot composEd imAge Retrieval with\ntextuaL invErsion (SEARLE), maps the visual features of the reference image\ninto a pseudo-word token in CLIP token embedding space and integrates it with\nthe relative caption. To support research on ZS-CIR, we introduce an\nopen-domain benchmarking dataset named Composed Image Retrieval on Common\nObjects in context (CIRCO), which is the first dataset for CIR containing\nmultiple ground truths for each query. The experiments show that SEARLE\nexhibits better performance than the baselines on the two main datasets for CIR\ntasks, FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and\nthe model are publicly available at https://github.com/miccunifi/SEARLE.",
        "authors": [
            "Alberto Baldrati",
            "Lorenzo Agnolucci",
            "Marco Bertini",
            "Alberto Del Bimbo"
        ]
    },
    {
        "title": "WALDO: Future Video Synthesis Using Object Layer Decomposition and Parametric Flow Prediction",
        "url": "http://arxiv.org/abs/2211.14308",
        "abstract": "This paper presents WALDO (WArping Layer-Decomposed Objects), a novel\napproach to the prediction of future video frames from past ones. Individual\nimages are decomposed into multiple layers combining object masks and a small\nset of control points. The layer structure is shared across all frames in each\nvideo to build dense inter-frame connections. Complex scene motions are modeled\nby combining parametric geometric transformations associated with individual\nlayers, and video synthesis is broken down into discovering the layers\nassociated with past frames, predicting the corresponding transformations for\nupcoming ones and warping the associated object regions accordingly, and\nfilling in the remaining image parts. Extensive experiments on multiple\nbenchmarks including urban videos (Cityscapes and KITTI) and videos featuring\nnonrigid motions (UCF-Sports and H3.6M), show that our method consistently\noutperforms the state of the art by a significant margin in every case. Code,\npretrained models, and video samples synthesized by our approach can be found\nin the project webpage https://16lemoing.github.io/waldo.",
        "authors": [
            "Guillaume Le Moing",
            "Jean Ponce",
            "Cordelia Schmid"
        ]
    },
    {
        "title": "ParCNetV2: Oversized Kernel with Enhanced Attention",
        "url": "http://arxiv.org/abs/2211.07157",
        "abstract": "Transformers have shown great potential in various computer vision tasks. By\nborrowing design concepts from transformers, many studies revolutionized CNNs\nand showed remarkable results. This paper falls in this line of studies.\nSpecifically, we propose a new convolutional neural network, ParCNetV2, that\nextends position-aware circular convolution (ParCNet) with oversized\nconvolutions and bifurcate gate units to enhance attention. The oversized\nconvolution employs a kernel with twice the input size to model long-range\ndependencies through a global receptive field. Simultaneously, it achieves\nimplicit positional encoding by removing the shift-invariant property from\nconvolution kernels, i.e., the effective kernels at different spatial locations\nare different when the kernel size is twice as large as the input size. The\nbifurcate gate unit implements an attention mechanism similar to self-attention\nin transformers. It is applied through element-wise multiplication of the two\nbranches, one serves as feature transformation while the other serves as\nattention weights. Additionally, we introduce a uniform local-global\nconvolution block to unify the design of the early and late stage convolution\nblocks. Extensive experiments demonstrate the superiority of our method over\nother convolutional neural networks and hybrid models that combine CNNs and\ntransformers. Code will be released.",
        "authors": [
            "Ruihan Xu",
            "Haokui Zhang",
            "Wenze Hu",
            "Shiliang Zhang",
            "Xiaoyu Wang"
        ]
    },
    {
        "title": "BiFF: Bi-level Future Fusion with Polyline-based Coordinate for Interactive Trajectory Prediction",
        "url": "http://arxiv.org/abs/2306.14161",
        "abstract": "Predicting future trajectories of surrounding agents is essential for\nsafety-critical autonomous driving. Most existing work focuses on predicting\nmarginal trajectories for each agent independently. However, it has rarely been\nexplored in predicting joint trajectories for interactive agents. In this work,\nwe propose Bi-level Future Fusion (BiFF) to explicitly capture future\ninteractions between interactive agents. Concretely, BiFF fuses the high-level\nfuture intentions followed by low-level future behaviors. Then the\npolyline-based coordinate is specifically designed for multi-agent prediction\nto ensure data efficiency, frame robustness, and prediction accuracy.\nExperiments show that BiFF achieves state-of-the-art performance on the\ninteractive prediction benchmark of Waymo Open Motion Dataset.",
        "authors": [
            "Yiyao Zhu",
            "Di Luan",
            "Shaojie Shen"
        ]
    },
    {
        "title": "Normalizing Flows for Human Pose Anomaly Detection",
        "url": "http://arxiv.org/abs/2211.10946",
        "abstract": "Video anomaly detection is an ill-posed problem because it relies on many\nparameters such as appearance, pose, camera angle, background, and more. We\ndistill the problem to anomaly detection of human pose, thus decreasing the\nrisk of nuisance parameters such as appearance affecting the result. Focusing\non pose alone also has the side benefit of reducing bias against distinct\nminority groups. Our model works directly on human pose graph sequences and is\nexceptionally lightweight (~1K parameters), capable of running on any machine\nable to run the pose estimation with negligible additional resources. We\nleverage the highly compact pose representation in a normalizing flows\nframework, which we extend to tackle the unique characteristics of\nspatio-temporal pose data and show its advantages in this use case. The\nalgorithm is quite general and can handle training data of only normal examples\nas well as a supervised setting that consists of labeled normal and abnormal\nexamples. We report state-of-the-art results on two anomaly detection\nbenchmarks - the unsupervised ShanghaiTech dataset and the recent supervised\nUBnormal dataset.",
        "authors": [
            "Or Hirschorn",
            "Shai Avidan"
        ]
    },
    {
        "title": "Reconstructing Groups of People with Hypergraph Relational Reasoning",
        "url": "http://arxiv.org/abs/2308.15844",
        "abstract": "Due to the mutual occlusion, severe scale variation, and complex spatial\ndistribution, the current multi-person mesh recovery methods cannot produce\naccurate absolute body poses and shapes in large-scale crowded scenes. To\naddress the obstacles, we fully exploit crowd features for reconstructing\ngroups of people from a monocular image. A novel hypergraph relational\nreasoning network is proposed to formulate the complex and high-order relation\ncorrelations among individuals and groups in the crowd. We first extract\ncompact human features and location information from the original\nhigh-resolution image. By conducting the relational reasoning on the extracted\nindividual features, the underlying crowd collectiveness and interaction\nrelationship can provide additional group information for the reconstruction.\nFinally, the updated individual features and the localization information are\nused to regress human meshes in camera coordinates. To facilitate the network\ntraining, we further build pseudo ground-truth on two crowd datasets, which may\nalso promote future research on pose estimation and human behavior\nunderstanding in crowded scenes. The experimental results show that our\napproach outperforms other baseline methods both in crowded and common\nscenarios. The code and datasets are publicly available at\nhttps://github.com/boycehbz/GroupRec.",
        "authors": [
            "Buzhen Huang",
            "Jingyi Ju",
            "Zhihao Li",
            "Yangang Wang"
        ]
    },
    {
        "title": "PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction",
        "url": "http://arxiv.org/abs/2308.16477",
        "abstract": "Vectorized high-definition map online construction has garnered considerable\nattention in the field of autonomous driving research. Most existing approaches\nmodel changeable map elements using a fixed number of points, or predict local\nmaps in a two-stage autoregressive manner, which may miss essential details and\nlead to error accumulation. Towards precise map element learning, we propose a\nsimple yet effective architecture named PivotNet, which adopts unified\npivot-based map representations and is formulated as a direct set prediction\nparadigm. Concretely, we first propose a novel point-to-line mask module to\nencode both the subordinate and geometrical point-line priors in the network.\nThen, a well-designed pivot dynamic matching module is proposed to model the\ntopology in dynamic point sequences by introducing the concept of sequence\nmatching. Furthermore, to supervise the position and topology of the vectorized\npoint predictions, we propose a dynamic vectorized sequence loss. Extensive\nexperiments and ablations show that PivotNet is remarkably superior to other\nSOTAs by 5.9 mAP at least. The code will be available soon.",
        "authors": [
            "Wenjie Ding",
            "Limeng Qiao",
            "Xi Qiu",
            "Chi Zhang"
        ]
    },
    {
        "title": "Universal Domain Adaptation via Compressive Attention Matching",
        "url": "http://arxiv.org/abs/2304.11862",
        "abstract": "Universal domain adaptation (UniDA) aims to transfer knowledge from the\nsource domain to the target domain without any prior knowledge about the label\nset. The challenge lies in how to determine whether the target samples belong\nto common categories. The mainstream methods make judgments based on the sample\nfeatures, which overemphasizes global information while ignoring the most\ncrucial local objects in the image, resulting in limited accuracy. To address\nthis issue, we propose a Universal Attention Matching (UniAM) framework by\nexploiting the self-attention mechanism in vision transformer to capture the\ncrucial object information. The proposed framework introduces a novel\nCompressive Attention Matching (CAM) approach to explore the core information\nby compressively representing attentions. Furthermore, CAM incorporates a\nresidual-based measurement to determine the sample commonness. By utilizing the\nmeasurement, UniAM achieves domain-wise and category-wise Common Feature\nAlignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first\nmethod utilizing the attention in vision transformer directly to perform\nclassification tasks. Extensive experiments show that UniAM outperforms the\ncurrent state-of-the-art methods on various benchmark datasets.",
        "authors": [
            "Didi Zhu",
            "Yincuan Li",
            "Junkun Yuan",
            "Zexi Li",
            "Kun Kuang",
            "Chao Wu"
        ]
    },
    {
        "title": "Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport",
        "url": "http://arxiv.org/abs/2308.01779",
        "abstract": "Weakly-supervised image segmentation has recently attracted increasing\nresearch attentions, aiming to avoid the expensive pixel-wise labeling. In this\npaper, we present an effective method, namely Point2Mask, to achieve\nhigh-quality panoptic prediction using only a single random point annotation\nper target for training. Specifically, we formulate the panoptic pseudo-mask\ngeneration as an Optimal Transport (OT) problem, where each ground-truth (gt)\npoint label and pixel sample are defined as the label supplier and consumer,\nrespectively. The transportation cost is calculated by the introduced\ntask-oriented maps, which focus on the category-wise and instance-wise\ndifferences among the various thing and stuff targets. Furthermore, a\ncentroid-based scheme is proposed to set the accurate unit number for each gt\npoint supplier. Hence, the pseudo-mask generation is converted into finding the\noptimal transport plan at a globally minimal transportation cost, which can be\nsolved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC and\nCOCO demonstrate the promising performance of our proposed Point2Mask approach\nto point-supervised panoptic segmentation. Source code is available at:\nhttps://github.com/LiWentomng/Point2Mask.",
        "authors": [
            "Wentong Li",
            "Yuqian Yuan",
            "Song Wang",
            "Jianke Zhu",
            "Jianshu Li",
            "Jian Liu",
            "Lei Zhang"
        ]
    },
    {
        "title": "What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification",
        "url": "http://arxiv.org/abs/2209.03320",
        "abstract": "Open-vocabulary models are a promising new paradigm for image classification.\nUnlike traditional classification models, open-vocabulary models classify among\nany arbitrary set of categories specified with natural language during\ninference. This natural language, called \"prompts\", typically consists of a set\nof hand-written templates (e.g., \"a photo of a {}\") which are completed with\neach of the category names. This work introduces a simple method to generate\nhigher accuracy prompts, without relying on any explicit knowledge of the task\ndomain and with far fewer hand-constructed sentences. To achieve this, we\ncombine open-vocabulary models with large language models (LLMs) to create\nCustomized Prompts via Language models (CuPL, pronounced \"couple\"). In\nparticular, we leverage the knowledge contained in LLMs in order to generate\nmany descriptive sentences that contain important discriminating\ncharacteristics of the image categories. This allows the model to place a\ngreater importance on these regions in the image when making predictions. We\nfind that this straightforward and general approach improves accuracy on a\nrange of zero-shot image classification benchmarks, including over one\npercentage point gain on ImageNet. Finally, this simple baseline requires no\nadditional training and remains completely zero-shot. Code available at\nhttps://github.com/sarahpratt/CuPL.",
        "authors": [
            "Sarah Pratt",
            "Ian Covert",
            "Rosanne Liu",
            "Ali Farhadi"
        ]
    },
    {
        "title": "Scene as Occupancy",
        "url": "http://arxiv.org/abs/2306.02851",
        "abstract": "Human driver can easily describe the complex traffic scene by visual system.\nSuch an ability of precise perception is essential for driver's planning. To\nachieve this, a geometry-aware representation that quantizes the physical 3D\nscene into structured grid map with semantic labels per cell, termed as 3D\nOccupancy, would be desirable. Compared to the form of bounding box, a key\ninsight behind occupancy is that it could capture the fine-grained details of\ncritical obstacles in the scene, and thereby facilitate subsequent tasks. Prior\nor concurrent literature mainly concentrate on a single scene completion task,\nwhere we might argue that the potential of this occupancy representation might\nobsess broader impact. In this paper, we propose OccNet, a multi-view\nvision-centric pipeline with a cascade and temporal voxel decoder to\nreconstruct 3D occupancy. At the core of OccNet is a general occupancy\nembedding to represent 3D physical world. Such a descriptor could be applied\ntowards a wide span of driving tasks, including detection, segmentation and\nplanning. To validate the effectiveness of this new representation and our\nproposed algorithm, we propose OpenOcc, the first dense high-quality 3D\noccupancy benchmark built on top of nuScenes. Empirical experiments show that\nthere are evident performance gain across multiple tasks, e.g., motion planning\ncould witness a collision rate reduction by 15%-58%, demonstrating the\nsuperiority of our method.",
        "authors": [
            "Chonghao Sima",
            "Wenwen Tong",
            "Tai Wang",
            "Li Chen",
            "Silei Wu",
            "Hanming Deng",
            "Yi Gu",
            "Lewei Lu",
            "Ping Luo",
            "Dahua Lin",
            "Hongyang Li"
        ]
    },
    {
        "title": "RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World",
        "url": "http://arxiv.org/abs/2307.07653",
        "abstract": "Physical adversarial attacks against deep neural networks (DNNs) have\nrecently gained increasing attention. The current mainstream physical attacks\nuse printed adversarial patches or camouflage to alter the appearance of the\ntarget object. However, these approaches generate conspicuous adversarial\npatterns that show poor stealthiness. Another physical deployable attack is the\noptical attack, featuring stealthiness while exhibiting weakly in the daytime\nwith sunlight. In this paper, we propose a novel Reflected Light Attack (RFLA),\nfeaturing effective and stealthy in both the digital and physical world, which\nis implemented by placing the color transparent plastic sheet and a paper cut\nof a specific shape in front of the mirror to create different colored\ngeometries on the target object. To achieve these goals, we devise a general\nframework based on the circle to model the reflected light on the target\nobject. Specifically, we optimize a circle (composed of a coordinate and\nradius) to carry various geometrical shapes determined by the optimized angle.\nThe fill color of the geometry shape and its corresponding transparency are\nalso optimized. We extensively evaluate the effectiveness of RFLA on different\ndatasets and models. Experiment results suggest that the proposed method\nachieves over 99% success rate on different datasets and models in the digital\nworld. Additionally, we verify the effectiveness of the proposed method in\ndifferent physical environments by using sunlight or a flashlight.",
        "authors": [
            "Donghua Wang",
            "Wen Yao",
            "Tingsong Jiang",
            "Chao Li",
            "Xiaoqian Chen"
        ]
    },
    {
        "title": "PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification",
        "url": "http://arxiv.org/abs/2307.09066",
        "abstract": "Multi-label image classification is a prediction task that aims to identify\nmore than one label from a given image. This paper considers the semantic\nconsistency of the latent space between the visual patch and linguistic label\ndomains and introduces the conditional transport (CT) theory to bridge the\nacknowledged gap. While recent cross-modal attention-based studies have\nattempted to align such two representations and achieved impressive\nperformance, they required carefully-designed alignment modules and extra\ncomplex operations in the attention computation. We find that by formulating\nthe multi-label classification as a CT problem, we can exploit the interactions\nbetween the image and label efficiently by minimizing the bidirectional CT\ncost. Specifically, after feeding the images and textual labels into the\nmodality-specific encoders, we view each image as a mixture of patch embeddings\nand a mixture of label embeddings, which capture the local region features and\nthe class prototypes, respectively. CT is then employed to learn and align\nthose two semantic sets by defining the forward and backward navigators.\nImportantly, the defined navigators in CT distance model the similarities\nbetween patches and labels, which provides an interpretable tool to visualize\nthe learned prototypes. Extensive experiments on three public image benchmarks\nshow that the proposed model consistently outperforms the previous methods.",
        "authors": [
            "Miaoge Li",
            "Dongsheng Wang",
            "Xinyang Liu",
            "Zequn Zeng",
            "Ruiying Lu",
            "Bo Chen",
            "Mingyuan Zhou"
        ]
    },
    {
        "title": "Long-range Multimodal Pretraining for Movie Understanding",
        "url": "http://arxiv.org/abs/2308.09775",
        "abstract": "Learning computer vision models from (and for) movies has a long-standing\nhistory. While great progress has been attained, there is still a need for a\npretrained multimodal model that can perform well in the ever-growing set of\nmovie understanding tasks the community has been establishing. In this work, we\nintroduce Long-range Multimodal Pretraining, a strategy, and a model that\nleverages movie data to train transferable multimodal and cross-modal encoders.\nOur key idea is to learn from all modalities in a movie by observing and\nextracting relationships over a long-range. After pretraining, we run ablation\nstudies on the LVU benchmark and validate our modeling choices and the\nimportance of learning from long-range time spans. Our model achieves\nstate-of-the-art on several LVU tasks while being much more data efficient than\nprevious works. Finally, we evaluate our model's transferability by setting a\nnew state-of-the-art in five different benchmarks.",
        "authors": [
            "Dawit Mureja Argaw",
            "Joon-Young Lee",
            "Markus Woodson",
            "In So Kweon",
            "Fabian Caba Heilbron"
        ]
    },
    {
        "title": "Spectrum-guided Multi-granularity Referring Video Object Segmentation",
        "url": "http://arxiv.org/abs/2307.13537",
        "abstract": "Current referring video object segmentation (R-VOS) techniques extract\nconditional kernels from encoded (low-resolution) vision-language features to\nsegment the decoded high-resolution features. We discovered that this causes\nsignificant feature drift, which the segmentation kernels struggle to perceive\nduring the forward computation. This negatively affects the ability of\nsegmentation kernels. To address the drift problem, we propose a\nSpectrum-guided Multi-granularity (SgMg) approach, which performs direct\nsegmentation on the encoded features and employs visual details to further\noptimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion\n(SCF) to perform intra-frame global interactions in the spectral domain for\neffective multimodal representation. Finally, we extend SgMg to perform\nmulti-object R-VOS, a new paradigm that enables simultaneous segmentation of\nmultiple referred objects in a video. This not only makes R-VOS faster, but\nalso more practical. Extensive experiments show that SgMg achieves\nstate-of-the-art performance on four video benchmark datasets, outperforming\nthe nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg\nenables multi-object R-VOS, runs about 3 times faster while maintaining\nsatisfactory performance. Code is available at https://github.com/bo-miao/SgMg.",
        "authors": [
            "Bo Miao",
            "Mohammed Bennamoun",
            "Yongsheng Gao",
            "Ajmal Mian"
        ]
    },
    {
        "title": "Sound Source Localization is All about Cross-Modal Alignment",
        "url": "http://arxiv.org/abs/2309.10724",
        "abstract": "Humans can easily perceive the direction of sound sources in a visual scene,\ntermed sound source localization. Recent studies on learning-based sound source\nlocalization have mainly explored the problem from a localization perspective.\nHowever, prior arts and existing benchmarks do not account for a more important\naspect of the problem, cross-modal semantic understanding, which is essential\nfor genuine sound source localization. Cross-modal semantic understanding is\nimportant in understanding semantically mismatched audio-visual events, e.g.,\nsilent objects, or off-screen sounds. To account for this, we propose a\ncross-modal alignment task as a joint task with sound source localization to\nbetter learn the interaction between audio and visual modalities. Thereby, we\nachieve high localization performance with strong cross-modal semantic\nunderstanding. Our method outperforms the state-of-the-art approaches in both\nsound source localization and cross-modal retrieval. Our work suggests that\njointly tackling both tasks is necessary to conquer genuine sound source\nlocalization.",
        "authors": [
            "Arda Senocak",
            "Hyeonggon Ryu",
            "Junsik Kim",
            "Tae-Hyun Oh",
            "Hanspeter Pfister",
            "Joon Son Chung"
        ]
    },
    {
        "title": "TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts",
        "url": "http://arxiv.org/abs/2307.15324",
        "abstract": "Learning discriminative task-specific features simultaneously for multiple\ndistinct tasks is a fundamental problem in multi-task learning. Recent\nstate-of-the-art models consider directly decoding task-specific features from\none shared task-generic feature (e.g., feature from a backbone layer), and\nutilize carefully designed decoders to produce multi-task features. However, as\nthe input feature is fully shared and each task decoder also shares decoding\nparameters for different input samples, it leads to a static feature decoding\nprocess, producing less discriminative task-specific representations. To tackle\nthis limitation, we propose TaskExpert, a novel multi-task mixture-of-experts\nmodel that enables learning multiple representative task-generic feature spaces\nand decoding task-specific features in a dynamic manner. Specifically,\nTaskExpert introduces a set of expert networks to decompose the backbone\nfeature into several representative task-generic features. Then, the\ntask-specific features are decoded by using dynamic task-specific gating\nnetworks operating on the decomposed task-generic features. Furthermore, to\nestablish long-range modeling of the task-specific representations from\ndifferent layers of TaskExpert, we design a multi-task feature memory that\nupdates at each layer and acts as an additional feature expert for dynamic\ntask-specific feature decoding. Extensive experiments demonstrate that our\nTaskExpert clearly outperforms previous best-performing methods on all 9\nmetrics of two competitive multi-task learning benchmarks for visual scene\nunderstanding (i.e., PASCAL-Context and NYUD-v2). Codes and models will be made\npublicly available at https://github.com/prismformore/Multi-Task-Transformer",
        "authors": [
            "Hanrong Ye",
            "Dan Xu"
        ]
    },
    {
        "title": "Meta OOD Learning For Continuously Adaptive OOD Detection",
        "url": "http://arxiv.org/abs/2309.11705",
        "abstract": "Out-of-distribution (OOD) detection is crucial to modern deep learning\napplications by identifying and alerting about the OOD samples that should not\nbe tested or used for making predictions. Current OOD detection methods have\nmade significant progress when in-distribution (ID) and OOD samples are drawn\nfrom static distributions. However, this can be unrealistic when applied to\nreal-world systems which often undergo continuous variations and shifts in ID\nand OOD distributions over time. Therefore, for an effective application in\nreal-world systems, the development of OOD detection methods that can adapt to\nthese dynamic and evolving distributions is essential. In this paper, we\npropose a novel and more realistic setting called continuously adaptive\nout-of-distribution (CAOOD) detection which targets on developing an OOD\ndetection model that enables dynamic and quick adaptation to a new arriving\ndistribution, with insufficient ID samples during deployment time. To address\nCAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt\ndiagram such that a good initialized OOD detection model is learned during the\ntraining process. In the testing process, MOL ensures OOD detection performance\nover shifting distributions by quickly adapting to new distributions with a few\nadaptations. Extensive experiments on several OOD benchmarks endorse the\neffectiveness of our method in preserving both ID classification accuracy and\nOOD detection performance on continuously shifting distributions.",
        "authors": [
            "Xinheng Wu",
            "Jie Lu",
            "Zhen Fang",
            "Guangquan Zhang"
        ]
    },
    {
        "title": "MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning",
        "url": "http://arxiv.org/abs/2304.13819",
        "abstract": "3D pose transfer is a challenging generation task that aims to transfer the\npose of a source geometry onto a target geometry with the target identity\npreserved. Many prior methods require keypoint annotations to find\ncorrespondence between the source and target. Current pose transfer methods\nallow end-to-end correspondence learning but require the desired final output\nas ground truth for supervision. Unsupervised methods have been proposed for\ngraph convolutional models but they require ground truth correspondence between\nthe source and target inputs. We present a novel self-supervised framework for\n3D pose transfer which can be trained in unsupervised, semi-supervised, or\nfully supervised settings without any correspondence labels. We introduce two\ncontrastive learning constraints in the latent space: a mesh-level loss for\ndisentangling global patterns including pose and identity, and a point-level\nloss for discriminating local semantics. We demonstrate quantitatively and\nqualitatively that our method achieves state-of-the-art results in supervised\n3D pose transfer, with comparable results in unsupervised and semi-supervised\nsettings. Our method is also generalisable to unseen human and animal data with\ncomplex topologies.",
        "authors": [
            "Jiaze Sun",
            "Zhixiang Chen",
            "Tae-Kyun Kim"
        ]
    },
    {
        "title": "BlendFace: Re-designing Identity Encoders for Face-Swapping",
        "url": "http://arxiv.org/abs/2307.10854",
        "abstract": "The great advancements of generative adversarial networks and face\nrecognition models in computer vision have made it possible to swap identities\non images from single sources. Although a lot of studies seems to have proposed\nalmost satisfactory solutions, we notice previous methods still suffer from an\nidentity-attribute entanglement that causes undesired attributes swapping\nbecause widely used identity encoders, eg, ArcFace, have some crucial attribute\nbiases owing to their pretraining on face recognition tasks. To address this\nissue, we design BlendFace, a novel identity encoder for face-swapping. The key\nidea behind BlendFace is training face recognition models on blended images\nwhose attributes are replaced with those of another mitigates inter-personal\nbiases such as hairsyles. BlendFace feeds disentangled identity features into\ngenerators and guides generators properly as an identity loss function.\nExtensive experiments demonstrate that BlendFace improves the\nidentity-attribute disentanglement in face-swapping models, maintaining a\ncomparable quantitative performance to previous methods.",
        "authors": [
            "Kaede Shiohara",
            "Xingchao Yang",
            "Takafumi Taketomi"
        ]
    },
    {
        "title": "A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions",
        "url": "http://arxiv.org/abs/2308.12700",
        "abstract": "Creating layouts is a fundamental step in graphic design. In this work, we\npropose to use text as the guidance to create graphic layouts, i.e.,\nText-to-Layout, aiming to lower the design barriers. Text-to-Layout is a\nchallenging task, because it needs to consider the implicit, combined, and\nincomplete layout constraints from text, each of which has not been studied in\nprevious work. To address this, we present a two-stage approach, named\nparse-then-place. The approach introduces an intermediate representation (IR)\nbetween text and layout to represent diverse layout constraints. With IR,\nText-to-Layout is decomposed into a parse stage and a place stage. The parse\nstage takes a textual description as input and generates an IR, in which the\nimplicit constraints from the text are transformed into explicit ones. The\nplace stage generates layouts based on the IR. To model combined and incomplete\nconstraints, we use a Transformer-based layout generation model and carefully\ndesign a way to represent constraints and layouts as sequences. Besides, we\nadopt the pretrain-then-finetune strategy to boost the performance of the\nlayout generation model with large-scale unlabeled layouts. To evaluate our\napproach, we construct two Text-to-Layout datasets and conduct experiments on\nthem. Quantitative results, qualitative analysis, and user studies demonstrate\nthe effectiveness of our approach.",
        "authors": [
            "Jiawei Lin",
            "Jiaqi Guo",
            "Shizhao Sun",
            "Weijiang Xu",
            "Ting Liu",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ]
    },
    {
        "title": "DreamBooth3D: Subject-Driven Text-to-3D Generation",
        "url": "http://arxiv.org/abs/2303.13508",
        "abstract": "We present DreamBooth3D, an approach to personalize text-to-3D generative\nmodels from as few as 3-6 casually captured images of a subject. Our approach\ncombines recent advances in personalizing text-to-image models (DreamBooth)\nwith text-to-3D generation (DreamFusion). We find that naively combining these\nmethods fails to yield satisfactory subject-specific 3D assets due to\npersonalized text-to-image models overfitting to the input viewpoints of the\nsubject. We overcome this through a 3-stage optimization strategy where we\njointly leverage the 3D consistency of neural radiance fields together with the\npersonalization capability of text-to-image models. Our method can produce\nhigh-quality, subject-specific 3D assets with text-driven modifications such as\nnovel poses, colors and attributes that are not seen in any of the input images\nof the subject.",
        "authors": [
            "Amit Raj",
            "Srinivas Kaza",
            "Ben Poole",
            "Michael Niemeyer",
            "Nataniel Ruiz",
            "Ben Mildenhall",
            "Shiran Zada",
            "Kfir Aberman",
            "Michael Rubinstein",
            "Jonathan Barron",
            "Yuanzhen Li",
            "Varun Jampani"
        ]
    },
    {
        "title": "Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation",
        "url": "http://arxiv.org/abs/2308.02097",
        "abstract": "Multi-modality image fusion and segmentation play a vital role in autonomous\ndriving and robotic operation. Early efforts focus on boosting the performance\nfor only one task, \\emph{e.g.,} fusion or segmentation, making it hard to\nreach~`Best of Both Worlds'. To overcome this issue, in this paper, we propose\na \\textbf{M}ulti-\\textbf{i}nteractive \\textbf{F}eature learning architecture\nfor image fusion and \\textbf{Seg}mentation, namely SegMiF, and exploit\ndual-task correlation to promote the performance of both tasks. The SegMiF is\nof a cascade structure, containing a fusion sub-network and a commonly used\nsegmentation sub-network. By slickly bridging intermediate features between two\ncomponents, the knowledge learned from the segmentation task can effectively\nassist the fusion task. Also, the benefited fusion network supports the\nsegmentation one to perform more pretentiously. Besides, a hierarchical\ninteractive attention block is established to ensure fine-grained mapping of\nall the vital information between two tasks, so that the modality/semantic\nfeatures can be fully mutual-interactive. In addition, a dynamic weight factor\nis introduced to automatically adjust the corresponding weights of each task,\nwhich can balance the interactive feature correspondence and break through the\nlimitation of laborious tuning. Furthermore, we construct a smart multi-wave\nbinocular imaging system and collect a full-time multi-modality benchmark with\n15 annotated pixel-level categories for image fusion and segmentation.\nExtensive experiments on several public datasets and our benchmark demonstrate\nthat the proposed method outputs visually appealing fused images and perform\naveragely $7.66\\%$ higher segmentation mIoU in the real-world scene than the\nstate-of-the-art approaches. The source code and benchmark are available at\n\\url{https://github.com/JinyuanLiu-CV/SegMiF}.",
        "authors": [
            "Jinyuan Liu",
            "Zhu Liu",
            "Guanyao Wu",
            "Long Ma",
            "Risheng Liu",
            "Wei Zhong",
            "Zhongxuan Luo",
            "Xin Fan"
        ]
    },
    {
        "title": "Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts",
        "url": "http://arxiv.org/abs/2308.08810",
        "abstract": "Test-time adaptation (TTA) aims to adapt a pre-trained model to the target\ndomain in a batch-by-batch manner during inference. While label distributions\noften exhibit imbalances in real-world scenarios, most previous TTA approaches\ntypically assume that both source and target domain datasets have balanced\nlabel distribution. Due to the fact that certain classes appear more frequently\nin certain domains (e.g., buildings in cities, trees in forests), it is natural\nthat the label distribution shifts as the domain changes. However, we discover\nthat the majority of existing TTA methods fail to address the coexistence of\ncovariate and label shifts. To tackle this challenge, we propose a novel label\nshift adapter that can be incorporated into existing TTA approaches to deal\nwith label shifts during the TTA process effectively. Specifically, we estimate\nthe label distribution of the target domain to feed it into the label shift\nadapter. Subsequently, the label shift adapter produces optimal parameters for\nthe target label distribution. By predicting only the parameters for a part of\nthe pre-trained source model, our approach is computationally efficient and can\nbe easily applied, regardless of the model architectures. Through extensive\nexperiments, we demonstrate that integrating our strategy with TTA approaches\nleads to substantial performance improvements under the joint presence of label\nand covariate shifts.",
        "authors": [
            "Sunghyun Park",
            "Seunghan Yang",
            "Jaegul Choo",
            "Sungrack Yun"
        ]
    },
    {
        "title": "Dynamic Snake Convolution Based on Topological Geometric Constraints for Tubular Structure Segmentation",
        "url": "http://arxiv.org/abs/2307.08388",
        "abstract": "Accurate segmentation of topological tubular structures, such as blood\nvessels and roads, is crucial in various fields, ensuring accuracy and\nefficiency in downstream tasks. However, many factors complicate the task,\nincluding thin local structures and variable global morphologies. In this work,\nwe note the specificity of tubular structures and use this knowledge to guide\nour DSCNet to simultaneously enhance perception in three stages: feature\nextraction, feature fusion, and loss constraint. First, we propose a dynamic\nsnake convolution to accurately capture the features of tubular structures by\nadaptively focusing on slender and tortuous local structures. Subsequently, we\npropose a multi-view feature fusion strategy to complement the attention to\nfeatures from multiple perspectives during feature fusion, ensuring the\nretention of important information from different global morphologies. Finally,\na continuity constraint loss function, based on persistent homology, is\nproposed to constrain the topological continuity of the segmentation better.\nExperiments on 2D and 3D datasets show that our DSCNet provides better accuracy\nand continuity on the tubular structure segmentation task compared with several\nmethods. Our codes will be publicly available.",
        "authors": [
            "Yaolei Qi",
            "Yuting He",
            "Xiaoming Qi",
            "Yuan Zhang",
            "Guanyu Yang"
        ]
    },
    {
        "title": "Unsupervised Open-Vocabulary Object Localization in Videos",
        "url": "http://arxiv.org/abs/2309.09858",
        "abstract": "In this paper, we show that recent advances in video representation learning\nand pre-trained vision-language models allow for substantial improvements in\nself-supervised video object localization. We propose a method that first\nlocalizes objects in videos via a slot attention approach and then assigns text\nto the obtained slots. The latter is achieved by an unsupervised way to read\nlocalized semantic information from the pre-trained CLIP model. The resulting\nvideo object localization is entirely unsupervised apart from the implicit\nannotation contained in CLIP, and it is effectively the first unsupervised\napproach that yields good results on regular video benchmarks.",
        "authors": [
            "Ke Fan",
            "Zechen Bai",
            "Tianjun Xiao",
            "Dominik Zietlow",
            "Max Horn",
            "Zixu Zhao",
            "Carl-Johann Simon-Gabriel",
            "Mike Zheng Shou",
            "Francesco Locatello",
            "Bernt Schiele",
            "Thomas Brox",
            "Zheng Zhang",
            "Yanwei Fu",
            "Tong He"
        ]
    },
    {
        "title": "Dataset Quantization",
        "url": "http://arxiv.org/abs/2308.10524",
        "abstract": "State-of-the-art deep neural networks are trained with large amounts\n(millions or even billions) of data. The expensive computation and memory costs\nmake it difficult to train them on limited hardware resources, especially for\nrecent popular large language models (LLM) and computer vision models (CV).\nRecent popular dataset distillation methods are thus developed, aiming to\nreduce the number of training samples via synthesizing small-scale datasets via\ngradient matching. However, as the gradient calculation is coupled with the\nspecific network architecture, the synthesized dataset is biased and performs\npoorly when used for training unseen architectures. To address these\nlimitations, we present dataset quantization (DQ), a new framework to compress\nlarge-scale datasets into small subsets which can be used for training any\nneural network architectures. Extensive experiments demonstrate that DQ is able\nto generate condensed small datasets for training unseen network architectures\nwith state-of-the-art compression ratios for lossless model training. To the\nbest of our knowledge, DQ is the first method that can successfully distill\nlarge-scale datasets such as ImageNet-1k with a state-of-the-art compression\nratio. Notably, with 60% data from ImageNet and 20% data from Alpaca's\ninstruction tuning data, the models can be trained with negligible or no\nperformance drop for both vision tasks (including classification, semantic\nsegmentation, and object detection) as well as language tasks (including\ninstruction tuning tasks such as BBH and DROP).",
        "authors": [
            "Daquan Zhou",
            "Kai Wang",
            "Jianyang Gu",
            "Xiangyu Peng",
            "Dongze Lian",
            "Yifan Zhang",
            "Yang You",
            "Jiashi Feng"
        ]
    },
    {
        "title": "Learning to Upsample by Learning to Sample",
        "url": "http://arxiv.org/abs/2308.15085",
        "abstract": "We present DySample, an ultra-lightweight and effective dynamic upsampler.\nWhile impressive performance gains have been witnessed from recent kernel-based\ndynamic upsamplers such as CARAFE, FADE, and SAPA, they introduce much\nworkload, mostly due to the time-consuming dynamic convolution and the\nadditional sub-network used to generate dynamic kernels. Further, the need for\nhigh-res feature guidance of FADE and SAPA somehow limits their application\nscenarios. To address these concerns, we bypass dynamic convolution and\nformulate upsampling from the perspective of point sampling, which is more\nresource-efficient and can be easily implemented with the standard built-in\nfunction in PyTorch. We first showcase a naive design, and then demonstrate how\nto strengthen its upsampling behavior step by step towards our new upsampler,\nDySample. Compared with former kernel-based dynamic upsamplers, DySample\nrequires no customized CUDA package and has much fewer parameters, FLOPs, GPU\nmemory, and latency. Besides the light-weight characteristics, DySample\noutperforms other upsamplers across five dense prediction tasks, including\nsemantic segmentation, object detection, instance segmentation, panoptic\nsegmentation, and monocular depth estimation. Code is available at\nhttps://github.com/tiny-smart/dysample.",
        "authors": [
            "Wenze Liu",
            "Hao Lu",
            "Hongtao Fu",
            "Zhiguo Cao"
        ]
    },
    {
        "title": "LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models",
        "url": "http://arxiv.org/abs/2303.11589",
        "abstract": "Creating graphic layouts is a fundamental step in graphic designs. In this\nwork, we present a novel generative model named LayoutDiffusion for automatic\nlayout generation. As layout is typically represented as a sequence of discrete\ntokens, LayoutDiffusion models layout generation as a discrete denoising\ndiffusion process. It learns to reverse a mild forward process, in which\nlayouts become increasingly chaotic with the growth of forward steps and\nlayouts in the neighboring steps do not differ too much. Designing such a mild\nforward process is however very challenging as layout has both categorical\nattributes and ordinal attributes. To tackle the challenge, we summarize three\ncritical factors for achieving a mild forward process for the layout, i.e.,\nlegality, coordinate proximity and type disruption. Based on the factors, we\npropose a block-wise transition matrix coupled with a piece-wise linear noise\nschedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion\noutperforms state-of-the-art approaches significantly. Moreover, it enables two\nconditional layout generation tasks in a plug-and-play manner without\nre-training and achieves better performance than existing methods.",
        "authors": [
            "Junyi Zhang",
            "Jiaqi Guo",
            "Shizhao Sun",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ]
    },
    {
        "title": "Efficiently Robustify Pre-Trained Models",
        "url": "http://arxiv.org/abs/2309.07499",
        "abstract": "A recent trend in deep learning algorithms has been towards training large\nscale models, having high parameter count and trained on big dataset. However,\nrobustness of such large scale models towards real-world settings is still a\nless-explored topic. In this work, we first benchmark the performance of these\nmodels under different perturbations and datasets thereby representing\nreal-world shifts, and highlight their degrading performance under these\nshifts. We then discuss on how complete model fine-tuning based existing\nrobustification schemes might not be a scalable option given very large scale\nnetworks and can also lead them to forget some of the desired characterstics.\nFinally, we propose a simple and cost-effective method to solve this problem,\ninspired by knowledge transfer literature. It involves robustifying smaller\nmodels, at a lower computation cost, and then use them as teachers to tune a\nfraction of these large scale networks, reducing the overall computational\noverhead. We evaluate our proposed method under various vision perturbations\nincluding ImageNet-C,R,S,A datasets and also for transfer learning, zero-shot\nevaluation setups on different datasets. Benchmark results show that our method\nis able to induce robustness to these large scale models efficiently, requiring\nsignificantly lower time and also preserves the transfer learning, zero-shot\nproperties of the original model which none of the existing methods are able to\nachieve.",
        "authors": [
            "Nishant Jain",
            "Harkirat Behl",
            "Yogesh Singh Rawat",
            "Vibhav Vineet"
        ]
    },
    {
        "title": "Efficient Video Prediction via Sparsely Conditioned Flow Matching",
        "url": "http://arxiv.org/abs/2211.14575",
        "abstract": "We introduce a novel generative model for video prediction based on latent\nflow matching, an efficient alternative to diffusion-based models. In contrast\nto prior work, we keep the high costs of modeling the past during training and\ninference at bay by conditioning only on a small random set of past frames at\neach integration step of the image generation process. Moreover, to enable the\ngeneration of high-resolution videos and to speed up the training, we work in\nthe latent space of a pretrained VQGAN. Finally, we propose to approximate the\ninitial condition of the flow ODE with the previous noisy frame. This allows to\nreduce the number of integration steps and hence, speed up the sampling at\ninference time. We call our model Random frame conditioned flow Integration for\nVidEo pRediction, or, in short, RIVER. We show that RIVER achieves superior or\non par performance compared to prior work on common video prediction\nbenchmarks, while requiring an order of magnitude fewer computational\nresources.",
        "authors": [
            "Aram Davtyan",
            "Sepehr Sameni",
            "Paolo Favaro"
        ]
    },
    {
        "title": "Surface Normal Clustering for Implicit Representation of Manhattan Scenes",
        "url": "http://arxiv.org/abs/2212.01331",
        "abstract": "Novel view synthesis and 3D modeling using implicit neural field\nrepresentation are shown to be very effective for calibrated multi-view\ncameras. Such representations are known to benefit from additional geometric\nand semantic supervision. Most existing methods that exploit additional\nsupervision require dense pixel-wise labels or localized scene priors. These\nmethods cannot benefit from high-level vague scene priors provided in terms of\nscenes' descriptions. In this work, we aim to leverage the geometric prior of\nManhattan scenes to improve the implicit neural radiance field representations.\nMore precisely, we assume that only the knowledge of the indoor scene (under\ninvestigation) being Manhattan is known -- with no additional information\nwhatsoever -- with an unknown Manhattan coordinate frame. Such high-level prior\nis used to self-supervise the surface normals derived explicitly in the\nimplicit neural fields. Our modeling allows us to cluster the derived normals\nand exploit their orthogonality constraints for self-supervision. Our\nexhaustive experiments on datasets of diverse indoor scenes demonstrate the\nsignificant benefit of the proposed method over the established baselines. The\nsource code is available at\nhttps://github.com/nikola3794/normal-clustering-nerf.",
        "authors": [
            "Nikola Popovic",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ]
    },
    {
        "title": "Distracting Downpour: Adversarial Weather Attacks for Motion Estimation",
        "url": "http://arxiv.org/abs/2305.06716",
        "abstract": "Current adversarial attacks on motion estimation, or optical flow, optimize\nsmall per-pixel perturbations, which are unlikely to appear in the real world.\nIn contrast, adverse weather conditions constitute a much more realistic threat\nscenario. Hence, in this work, we present a novel attack on motion estimation\nthat exploits adversarially optimized particles to mimic weather effects like\nsnowflakes, rain streaks or fog clouds. At the core of our attack framework is\na differentiable particle rendering system that integrates particles (i)\nconsistently over multiple time steps (ii) into the 3D space (iii) with a\nphoto-realistic appearance. Through optimization, we obtain adversarial weather\nthat significantly impacts the motion estimation. Surprisingly, methods that\npreviously showed good robustness towards small per-pixel perturbations are\nparticularly vulnerable to adversarial weather. At the same time, augmenting\nthe training with non-optimized weather increases a method's robustness towards\nweather effects and improves generalizability at almost no additional cost. Our\ncode will be available at https://github.com/cv-stuttgart/DistractingDownpour.",
        "authors": [
            "Jenny Schmalfuss",
            "Lukas Mehl",
            "Andr\u00e9s Bruhn"
        ]
    },
    {
        "title": "Generalized Differentiable RANSAC",
        "url": "http://arxiv.org/abs/2212.13185",
        "abstract": "We propose $\\nabla$-RANSAC, a generalized differentiable RANSAC that allows\nlearning the entire randomized robust estimation pipeline. The proposed\napproach enables the use of relaxation techniques for estimating the gradients\nin the sampling distribution, which are then propagated through a\ndifferentiable solver. The trainable quality function marginalizes over the\nscores from all the models estimated within $\\nabla$-RANSAC to guide the\nnetwork learning accurate and useful inlier probabilities or to train feature\ndetection and matching networks. Our method directly maximizes the probability\nof drawing a good hypothesis, allowing us to learn better sampling\ndistributions. We test $\\nabla$-RANSAC on various real-world scenarios on\nfundamental and essential matrix estimation, and 3D point cloud registration,\noutdoors and indoors, with handcrafted and learning-based features. It is\nsuperior to the state-of-the-art in terms of accuracy while running at a\nsimilar speed to its less accurate alternatives. The code and trained models\nare available at https://github.com/weitong8591/differentiable_ransac.",
        "authors": [
            "Tong Wei",
            "Yash Patel",
            "Alexander Shekhovtsov",
            "Jiri Matas",
            "Daniel Barath"
        ]
    },
    {
        "title": "Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging",
        "url": "http://arxiv.org/abs/2306.11316",
        "abstract": "We consider the problem of video snapshot compressive imaging (SCI), where\nsequential high-speed frames are modulated by different masks and captured by a\nsingle measurement. The underlying principle of reconstructing multi-frame\nimages from only one single measurement is to solve an ill-posed problem. By\ncombining optimization algorithms and neural networks, deep unfolding networks\n(DUNs) score tremendous achievements in solving inverse problems. In this\npaper, our proposed model is under the DUN framework and we propose a 3D\nConvolution-Transformer Mixture (CTM) module with a 3D efficient and scalable\nattention model plugged in, which helps fully learn the correlation between\ntemporal and spatial dimensions by virtue of Transformer. To our best\nknowledge, this is the first time that Transformer is employed to video SCI\nreconstruction. Besides, to further investigate the high-frequency information\nduring the reconstruction process which are neglected in previous studies, we\nintroduce variance estimation characterizing the uncertainty on a\npixel-by-pixel basis. Extensive experimental results demonstrate that our\nproposed method achieves state-of-the-art (SOTA) (with a 1.2dB gain in PSNR\nover previous SOTA algorithm) results. We will release the code.",
        "authors": [
            "Siming Zheng",
            "Xin Yuan"
        ]
    },
    {
        "title": "ResQ: Residual Quantization for Video Perception",
        "url": "http://arxiv.org/abs/2308.09511",
        "abstract": "This paper accelerates video perception, such as semantic segmentation and\nhuman pose estimation, by levering cross-frame redundancies. Unlike the\nexisting approaches, which avoid redundant computations by warping the past\nfeatures using optical-flow or by performing sparse convolutions on frame\ndifferences, we approach the problem from a new perspective: low-bit\nquantization. We observe that residuals, as the difference in network\nactivations between two neighboring frames, exhibit properties that make them\nhighly quantizable. Based on this observation, we propose a novel quantization\nscheme for video networks coined as Residual Quantization. ResQ extends the\nstandard, frame-by-frame, quantization scheme by incorporating temporal\ndependencies that lead to better performance in terms of accuracy vs.\nbit-width. Furthermore, we extend our model to dynamically adjust the bit-width\nproportional to the amount of changes in the video. We demonstrate the\nsuperiority of our model, against the standard quantization and existing\nefficient video perception models, using various architectures on semantic\nsegmentation and human pose estimation benchmarks.",
        "authors": [
            "Davide Abati",
            "Haitam Ben Yahia",
            "Markus Nagel",
            "Amirhossein Habibian"
        ]
    },
    {
        "title": "End-to-End Diffusion Latent Optimization Improves Classifier Guidance",
        "url": "http://arxiv.org/abs/2303.13703",
        "abstract": "Classifier guidance -- using the gradients of an image classifier to steer\nthe generations of a diffusion model -- has the potential to dramatically\nexpand the creative control over image generation and editing. However,\ncurrently classifier guidance requires either training new noise-aware models\nto obtain accurate gradients or using a one-step denoising approximation of the\nfinal generation, which leads to misaligned gradients and sub-optimal control.\nWe highlight this approximation's shortcomings and propose a novel guidance\nmethod: Direct Optimization of Diffusion Latents (DOODL), which enables\nplug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of\na pre-trained classifier on the true generated pixels, using an invertible\ndiffusion process to achieve memory-efficient backpropagation. Showcasing the\npotential of more precise guidance, DOODL outperforms one-step classifier\nguidance on computational and human evaluation metrics across different forms\nof guidance: using CLIP guidance to improve generations of complex prompts from\nDrawBench, using fine-grained visual classifiers to expand the vocabulary of\nStable Diffusion, enabling image-conditioned generation with a CLIP visual\nencoder, and improving image aesthetics using an aesthetic scoring network.\nCode at https://github.com/salesforce/DOODL.",
        "authors": [
            "Bram Wallace",
            "Akash Gokul",
            "Stefano Ermon",
            "Nikhil Naik"
        ]
    },
    {
        "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
        "url": "http://arxiv.org/abs/2304.01480",
        "abstract": "Recent works on 3D reconstruction from posed images have demonstrated that\ndirect inference of scene-level 3D geometry without test-time optimization is\nfeasible using deep neural networks, showing remarkable promise and high\nefficiency. However, the reconstructed geometry, typically represented as a 3D\ntruncated signed distance function (TSDF), is often coarse without fine\ngeometric details. To address this problem, we propose three effective\nsolutions for improving the fidelity of inference-based 3D reconstructions. We\nfirst present a resolution-agnostic TSDF supervision strategy to provide the\nnetwork with a more accurate learning signal during training, avoiding the\npitfalls of TSDF interpolation seen in previous work. We then introduce a depth\nguidance strategy using multi-view depth estimates to enhance the scene\nrepresentation and recover more accurate surfaces. Finally, we develop a novel\narchitecture for the final layers of the network, conditioning the output TSDF\nprediction on high-resolution image features in addition to coarse voxel\nfeatures, enabling sharper reconstruction of fine details. Our method,\nFineRecon, produces smooth and highly accurate reconstructions, showing\nsignificant improvements across multiple depth and 3D reconstruction metrics.",
        "authors": [
            "Noah Stier",
            "Anurag Ranjan",
            "Alex Colburn",
            "Yajie Yan",
            "Liang Yang",
            "Fangchang Ma",
            "Baptiste Angles"
        ]
    },
    {
        "title": "Navigating to Objects Specified by Images",
        "url": "http://arxiv.org/abs/2304.01192",
        "abstract": "Images are a convenient way to specify which particular object instance an\nembodied agent should navigate to. Solving this task requires semantic visual\nreasoning and exploration of unknown environments. We present a system that can\nperform this task in both simulation and the real world. Our modular method\nsolves sub-tasks of exploration, goal instance re-identification, goal\nlocalization, and local navigation. We re-identify the goal instance in\negocentric vision using feature-matching and localize the goal instance by\nprojecting matched features to a map. Each sub-task is solved using\noff-the-shelf components requiring zero fine-tuning. On the HM3D\nInstanceImageNav benchmark, this system outperforms a baseline end-to-end RL\npolicy 7x and a state-of-the-art ImageNav model 2.3x (56% vs 25% success). We\ndeploy this system to a mobile robot platform and demonstrate effective\nreal-world performance, achieving an 88% success rate across a home and an\noffice environment.",
        "authors": [
            "Jacob Krantz",
            "Theophile Gervet",
            "Karmesh Yadav",
            "Austin Wang",
            "Chris Paxton",
            "Roozbeh Mottaghi",
            "Dhruv Batra",
            "Jitendra Malik",
            "Stefan Lee",
            "Devendra Singh Chaplot"
        ]
    },
    {
        "title": "LATR: 3D Lane Detection from Monocular Images with Transformer",
        "url": "http://arxiv.org/abs/2308.04583",
        "abstract": "3D lane detection from monocular images is a fundamental yet challenging task\nin autonomous driving. Recent advances primarily rely on structural 3D\nsurrogates (e.g., bird's eye view) built from front-view image features and\ncamera parameters. However, the depth ambiguity in monocular images inevitably\ncauses misalignment between the constructed surrogate feature map and the\noriginal image, posing a great challenge for accurate lane detection. To\naddress the above issue, we present a novel LATR model, an end-to-end 3D lane\ndetector that uses 3D-aware front-view features without transformed view\nrepresentation. Specifically, LATR detects 3D lanes via cross-attention based\non query and key-value pairs, constructed using our lane-aware query generator\nand dynamic 3D ground positional embedding. On the one hand, each query is\ngenerated based on 2D lane-aware features and adopts a hybrid embedding to\nenhance lane information. On the other hand, 3D space information is injected\nas positional embedding from an iteratively-updated 3D ground plane. LATR\noutperforms previous state-of-the-art methods on both synthetic Apollo,\nrealistic OpenLane and ONCE-3DLanes by large margins (e.g., 11.4 gain in terms\nof F1 score on OpenLane). Code will be released at\nhttps://github.com/JMoonr/LATR .",
        "authors": [
            "Yueru Luo",
            "Chaoda Zheng",
            "Xu Yan",
            "Tang Kun",
            "Chao Zheng",
            "Shuguang Cui",
            "Zhen Li"
        ]
    },
    {
        "title": "Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation",
        "url": "http://arxiv.org/abs/2308.03282",
        "abstract": "The scene graph generation (SGG) task is designed to identify the predicates\nbased on the subject-object pairs.However,existing datasets generally include\ntwo imbalance cases: one is the class imbalance from the predicted predicates\nand another is the context imbalance from the given subject-object pairs, which\npresents significant challenges for SGG. Most existing methods focus on the\nimbalance of the predicted predicate while ignoring the imbalance of the\nsubject-object pairs, which could not achieve satisfactory results. To address\nthe two imbalance cases, we propose a novel Environment Invariant Curriculum\nRelation learning (EICR) method, which can be applied in a plug-and-play\nfashion to existing SGG methods. Concretely, to remove the imbalance of the\nsubject-object pairs, we first construct different distribution environments\nfor the subject-object pairs and learn a model invariant to the environment\nchanges. Then, we construct a class-balanced curriculum learning strategy to\nbalance the different environments to remove the predicate imbalance.\nComprehensive experiments conducted on VG and GQA datasets demonstrate that our\nEICR framework can be taken as a general strategy for various SGG models, and\nachieve significant improvements.",
        "authors": [
            "Yukuan Min",
            "Aming Wu",
            "Cheng Deng"
        ]
    },
    {
        "title": "Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving",
        "url": "http://arxiv.org/abs/2305.02008",
        "abstract": "Existing datasets for autonomous driving (AD) often lack diversity and\nlong-range capabilities, focusing instead on 360{\\deg} perception and temporal\nreasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a\nlarge-scale and diverse multimodal dataset collected over two years in various\nEuropean countries, covering an area 9x that of existing datasets. ZOD boasts\nthe highest range and resolution sensors among comparable datasets, coupled\nwith detailed keyframe annotations for 2D and 3D objects (up to 245m), road\ninstance/semantic segmentation, traffic sign recognition, and road\nclassification. We believe that this unique combination will facilitate\nbreakthroughs in long-range perception and multi-task learning. The dataset is\ncomposed of Frames, Sequences, and Drives, designed to encompass both data\ndiversity and support for spatio-temporal learning, sensor fusion,\nlocalization, and mapping. Frames consist of 100k curated camera images with\ntwo seconds of other supporting sensor data, while the 1473 Sequences and 29\nDrives include the entire sensor suite for 20 seconds and a few minutes,\nrespectively. ZOD is the only large-scale AD dataset released under a\npermissive license, allowing for both research and commercial use. More\ninformation, and an extensive devkit, can be found at https://zod.zenseact.com",
        "authors": [
            "Mina Alibeigi",
            "William Ljungbergh",
            "Adam Tonderski",
            "Georg Hess",
            "Adam Lilja",
            "Carl Lindstrom",
            "Daria Motorniuk",
            "Junsheng Fu",
            "Jenny Widahl",
            "Christoffer Petersson"
        ]
    },
    {
        "title": "Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization",
        "url": "http://arxiv.org/abs/2308.09391",
        "abstract": "Domain generalization (DG) is proposed to deal with the issue of domain\nshift, which occurs when statistical differences exist between source and\ntarget domains. However, most current methods do not account for a common\nrealistic scenario where the source and target domains have different classes.\nTo overcome this deficiency, open set domain generalization (OSDG) then emerges\nas a more practical setting to recognize unseen classes in unseen domains. An\nintuitive approach is to use multiple one-vs-all classifiers to define decision\nboundaries for each class and reject the outliers as unknown. However, the\nsignificant class imbalance between positive and negative samples often causes\nthe boundaries biased towards positive ones, resulting in misclassification for\nknown samples in the unseen target domain. In this paper, we propose a novel\nmeta-learning-based framework called dualistic MEta-learning with joint\nDomaIn-Class matching (MEDIC), which considers gradient matching towards\ninter-domain and inter-class splits simultaneously to find a generalizable\nboundary balanced for all tasks. Experimental results demonstrate that MEDIC\nnot only outperforms previous methods in open set scenarios, but also maintains\ncompetitive close set generalization ability at the same time. Our code is\navailable at https://github.com/zzwdx/MEDIC.",
        "authors": [
            "Xiran Wang",
            "Jian Zhang",
            "Lei Qi",
            "Yinghuan Shi"
        ]
    },
    {
        "title": "Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples",
        "url": "http://arxiv.org/abs/2307.16361",
        "abstract": "Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to\nadversarial examples, threatening their practical deployment. Despite the many\nresearch endeavors have been made to tackle this issue in recent years, the\ndiversity of adversarial examples on 3D point clouds makes them more\nchallenging to defend against than those on 2D images. For examples, attackers\ncan generate adversarial examples by adding, shifting, or removing points.\nConsequently, existing defense strategies are hard to counter unseen point\ncloud adversarial examples. In this paper, we first establish a comprehensive,\nand rigorous point cloud adversarial robustness benchmark to evaluate\nadversarial robustness, which can provide a detailed understanding of the\neffects of the defense and attack methods. We then collect existing defense\ntricks in point cloud adversarial defenses and then perform extensive and\nsystematic experiments to identify an effective combination of these tricks.\nFurthermore, we propose a hybrid training augmentation methods that consider\nvarious types of point cloud adversarial examples to adversarial training,\nsignificantly improving the adversarial robustness. By combining these tricks,\nwe construct a more robust defense framework achieving an average accuracy of\n83.45\\% against various attacks, demonstrating its capability to enabling\nrobust learners. Our codebase are open-sourced on:\n\\url{https://github.com/qiufan319/benchmark_pc_attack.git}.",
        "authors": [
            "Qiufan Ji",
            "Lin Wang",
            "Cong Shi",
            "Shengshan Hu",
            "Yingying Chen",
            "Lichao Sun"
        ]
    },
    {
        "title": "Poincare ResNet",
        "url": "http://arxiv.org/abs/2303.14027",
        "abstract": "This paper introduces an end-to-end residual network that operates entirely\non the Poincar\\'e ball model of hyperbolic space. Hyperbolic learning has\nrecently shown great potential for visual understanding, but is currently only\nperformed in the penultimate layer(s) of deep networks. All visual\nrepresentations are still learned through standard Euclidean networks. In this\npaper we investigate how to learn hyperbolic representations of visual data\ndirectly from the pixel-level. We propose Poincar\\'e ResNet, a hyperbolic\ncounterpart of the celebrated residual network, starting from Poincar\\'e 2D\nconvolutions up to Poincar\\'e residual connections. We identify three\nroadblocks for training convolutional networks entirely in hyperbolic space and\npropose a solution for each: (i) Current hyperbolic network initializations\ncollapse to the origin, limiting their applicability in deeper networks. We\nprovide an identity-based initialization that preserves norms over many layers.\n(ii) Residual networks rely heavily on batch normalization, which comes with\nexpensive Fr\\'echet mean calculations in hyperbolic space. We introduce\nPoincar\\'e midpoint batch normalization as a faster and equally effective\nalternative. (iii) Due to the many intermediate operations in Poincar\\'e\nlayers, we lastly find that the computation graphs of deep learning libraries\nblow up, limiting our ability to train on deep hyperbolic networks. We provide\nmanual backward derivations of core hyperbolic operations to maintain\nmanageable computation graphs.",
        "authors": [
            "Max van Spengler",
            "Erwin Berkhout",
            "Pascal Mettes"
        ]
    },
    {
        "title": "SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection",
        "url": "http://arxiv.org/abs/2208.13930",
        "abstract": "We address the problem of out-of-distribution (OOD) detection for the task of\nobject detection. We show that residual convolutional layers with batch\nnormalisation produce Sensitivity-Aware FEatures (SAFE) that are consistently\npowerful for distinguishing in-distribution from out-of-distribution\ndetections. We extract SAFE vectors for every detected object, and train a\nmultilayer perceptron on the surrogate task of distinguishing adversarially\nperturbed from clean in-distribution examples. This circumvents the need for\nrealistic OOD training data, computationally expensive generative models, or\nretraining of the base object detector. SAFE outperforms the state-of-the-art\nOOD object detectors on multiple benchmarks by large margins, e.g. reducing the\nFPR95 by an absolute 30.6% from 48.3% to 17.7% on the OpenImages dataset.",
        "authors": [
            "Samuel Wilson",
            "Tobias Fischer",
            "Feras Dayoub",
            "Dimity Miller",
            "Niko S\u00fcnderhauf"
        ]
    },
    {
        "title": "SimFIR: A Simple Framework for Fisheye Image Rectification with Self-supervised Representation Learning",
        "url": "http://arxiv.org/abs/2308.09040",
        "abstract": "In fisheye images, rich distinct distortion patterns are regularly\ndistributed in the image plane. These distortion patterns are independent of\nthe visual content and provide informative cues for rectification. To make the\nbest of such rectification cues, we introduce SimFIR, a simple framework for\nfisheye image rectification based on self-supervised representation learning.\nTechnically, we first split a fisheye image into multiple patches and extract\ntheir representations with a Vision Transformer (ViT). To learn fine-grained\ndistortion representations, we then associate different image patches with\ntheir specific distortion patterns based on the fisheye model, and further\nsubtly design an innovative unified distortion-aware pretext task for their\nlearning. The transfer performance on the downstream rectification task is\nremarkably boosted, which verifies the effectiveness of the learned\nrepresentations. Extensive experiments are conducted, and the quantitative and\nqualitative results demonstrate the superiority of our method over the\nstate-of-the-art algorithms as well as its strong generalization ability on\nreal-world fisheye images.",
        "authors": [
            "Hao Feng",
            "Wendi Wang",
            "Jiajun Deng",
            "Wengang Zhou",
            "Li Li",
            "Houqiang Li"
        ]
    },
    {
        "title": "Subclass-balancing Contrastive Learning for Long-tailed Recognition",
        "url": "http://arxiv.org/abs/2306.15925",
        "abstract": "Long-tailed recognition with imbalanced class distribution naturally emerges\nin practical machine learning applications. Existing methods such as data\nreweighing, resampling, and supervised contrastive learning enforce the class\nbalance with a price of introducing imbalance between instances of head class\nand tail class, which may ignore the underlying rich semantic substructures of\nthe former and exaggerate the biases in the latter. We overcome these drawbacks\nby a novel ``subclass-balancing contrastive learning (SBCL)'' approach that\nclusters each head class into multiple subclasses of similar sizes as the tail\nclasses and enforce representations to capture the two-layer class hierarchy\nbetween the original classes and their subclasses. Since the clustering is\nconducted in the representation space and updated during the course of\ntraining, the subclass labels preserve the semantic substructures of head\nclasses. Meanwhile, it does not overemphasize tail class samples, so each\nindividual instance contribute to the representation learning equally. Hence,\nour method achieves both the instance- and subclass-balance, while the original\nclass labels are also learned through contrastive learning among subclasses\nfrom different classes. We evaluate SBCL over a list of long-tailed benchmark\ndatasets and it achieves the state-of-the-art performance. In addition, we\npresent extensive analyses and ablation studies of SBCL to verify its\nadvantages.",
        "authors": [
            "Chengkai Hou",
            "Jieyu Zhang",
            "Haonan Wang",
            "Tianyi Zhou"
        ]
    },
    {
        "title": "Generalized Lightness Adaptation with Channel Selective Normalization",
        "url": "http://arxiv.org/abs/2308.13783",
        "abstract": "Lightness adaptation is vital to the success of image processing to avoid\nunexpected visual deterioration, which covers multiple aspects, e.g., low-light\nimage enhancement, image retouching, and inverse tone mapping. Existing methods\ntypically work well on their trained lightness conditions but perform poorly in\nunknown ones due to their limited generalization ability. To address this\nlimitation, we propose a novel generalized lightness adaptation algorithm that\nextends conventional normalization techniques through a channel filtering\ndesign, dubbed Channel Selective Normalization (CSNorm). The proposed CSNorm\npurposely normalizes the statistics of lightness-relevant channels and keeps\nother channels unchanged, so as to improve feature generalization and\ndiscrimination. To optimize CSNorm, we propose an alternating training strategy\nthat effectively identifies lightness-relevant channels. The model equipped\nwith our CSNorm only needs to be trained on one lightness condition and can be\nwell generalized to unknown lightness conditions. Experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of CSNorm in\nenhancing the generalization ability for the existing lightness adaptation\nmethods. Code is available at https://github.com/mdyao/CSNorm.",
        "authors": [
            "Mingde Yao",
            "Jie Huang",
            "Xin Jin",
            "Ruikang Xu",
            "Shenglong Zhou",
            "Man Zhou",
            "Zhiwei Xiong"
        ]
    },
    {
        "title": "Omnidirectional Information Gathering for Knowledge Transfer-Based Audio-Visual Navigation",
        "url": "http://arxiv.org/abs/2308.10306",
        "abstract": "Audio-visual navigation is an audio-targeted wayfinding task where a robot\nagent is entailed to travel a never-before-seen 3D environment towards the\nsounding source. In this article, we present ORAN, an omnidirectional\naudio-visual navigator based on cross-task navigation skill transfer. In\nparticular, ORAN sharpens its two basic abilities for a such challenging task,\nnamely wayfinding and audio-visual information gathering. First, ORAN is\ntrained with a confidence-aware cross-task policy distillation (CCPD) strategy.\nCCPD transfers the fundamental, point-to-point wayfinding skill that is well\ntrained on the large-scale PointGoal task to ORAN, so as to help ORAN to better\nmaster audio-visual navigation with far fewer training samples. To improve the\nefficiency of knowledge transfer and address the domain gap, CCPD is made to be\nadaptive to the decision confidence of the teacher policy. Second, ORAN is\nequipped with an omnidirectional information gathering (OIG) mechanism, i.e.,\ngleaning visual-acoustic observations from different directions before\ndecision-making. As a result, ORAN yields more robust navigation behaviour.\nTaking CCPD and OIG together, ORAN significantly outperforms previous\ncompetitors. After the model ensemble, we got 1st in Soundspaces Challenge\n2022, improving SPL and SR by 53% and 35% relatively.",
        "authors": [
            "Jinyu Chen",
            "Wenguan Wang",
            "Si Liu",
            "Hongsheng Li",
            "Yi Yang"
        ]
    },
    {
        "title": "Dynamic Mesh-Aware Radiance Fields",
        "url": "http://arxiv.org/abs/2309.04581",
        "abstract": "Embedding polygonal mesh assets within photorealistic Neural Radience Fields\n(NeRF) volumes, such that they can be rendered and their dynamics simulated in\na physically consistent manner with the NeRF, is under-explored from the system\nperspective of integrating NeRF into the traditional graphics pipeline. This\npaper designs a two-way coupling between mesh and NeRF during rendering and\nsimulation. We first review the light transport equations for both mesh and\nNeRF, then distill them into an efficient algorithm for updating radiance and\nthroughput along a cast ray with an arbitrary number of bounces. To resolve the\ndiscrepancy between the linear color space that the path tracer assumes and the\nsRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range\n(HDR) images. We also present a strategy to estimate light sources and cast\nshadows on the NeRF. Finally, we consider how the hybrid surface-volumetric\nformulation can be efficiently integrated with a high-performance physics\nsimulator that supports cloth, rigid and soft bodies. The full rendering and\nsimulation system can be run on a GPU at interactive rates. We show that a\nhybrid system approach outperforms alternatives in visual realism for mesh\ninsertion, because it allows realistic light transport from volumetric NeRF\nmedia onto surfaces, which affects the appearance of reflective/refractive\nsurfaces and illumination of diffuse surfaces informed by the dynamic scene.",
        "authors": [
            "Yi-Ling Qiao",
            "Alexander Gao",
            "Yiran Xu",
            "Yue Feng",
            "Jia-Bin Huang",
            "Ming C. Lin"
        ]
    },
    {
        "title": "Learning Support and Trivial Prototypes for Interpretable Image Classification",
        "url": "http://arxiv.org/abs/2301.04011",
        "abstract": "Prototypical part network (ProtoPNet) methods have been designed to achieve\ninterpretable classification by associating predictions with a set of training\nprototypes, which we refer to as trivial prototypes because they are trained to\nlie far from the classification boundary in the feature space. Note that it is\npossible to make an analogy between ProtoPNet and support vector machine (SVM)\ngiven that the classification from both methods relies on computing similarity\nwith a set of training points (i.e., trivial prototypes in ProtoPNet, and\nsupport vectors in SVM). However, while trivial prototypes are located far from\nthe classification boundary, support vectors are located close to this\nboundary, and we argue that this discrepancy with the well-established SVM\ntheory can result in ProtoPNet models with inferior classification accuracy. In\nthis paper, we aim to improve the classification of ProtoPNet with a new method\nto learn support prototypes that lie near the classification boundary in the\nfeature space, as suggested by the SVM theory. In addition, we target the\nimprovement of classification results with a new model, named ST-ProtoPNet,\nwhich exploits our support prototypes and the trivial prototypes to provide\nmore effective classification. Experimental results on CUB-200-2011, Stanford\nCars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves\nstate-of-the-art classification accuracy and interpretability results. We also\nshow that the proposed support prototypes tend to be better localised in the\nobject of interest rather than in the background region.",
        "authors": [
            "Chong Wang",
            "Yuyuan Liu",
            "Yuanhong Chen",
            "Fengbei Liu",
            "Yu Tian",
            "Davis J. McCarthy",
            "Helen Frazer",
            "Gustavo Carneiro"
        ]
    },
    {
        "title": "GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization",
        "url": "http://arxiv.org/abs/2308.04699",
        "abstract": "Federated Learning (FL) has recently emerged as a promising distributed\nmachine learning framework to preserve clients' privacy, by allowing multiple\nclients to upload the gradients calculated from their local data to a central\nserver. Recent studies find that the exchanged gradients also take the risk of\nprivacy leakage, e.g., an attacker can invert the shared gradients and recover\nsensitive data against an FL system by leveraging pre-trained generative\nadversarial networks (GAN) as prior knowledge. However, performing gradient\ninversion attacks in the latent space of the GAN model limits their expression\nability and generalizability. To tackle these challenges, we propose\n\\textbf{G}radient \\textbf{I}nversion over \\textbf{F}eature \\textbf{D}omains\n(GIFD), which disassembles the GAN model and searches the feature domains of\nthe intermediate layers. Instead of optimizing only over the initial latent\ncode, we progressively change the optimized layer, from the initial latent\nspace to intermediate layers closer to the output images. In addition, we\ndesign a regularizer to avoid unreal image generation by adding a small ${l_1}$\nball constraint to the searching range. We also extend GIFD to the\nout-of-distribution (OOD) setting, which weakens the assumption that the\ntraining sets of GANs and FL tasks obey the same data distribution. Extensive\nexperiments demonstrate that our method can achieve pixel-level reconstruction\nand is superior to the existing methods. Notably, GIFD also shows great\ngeneralizability under different defense strategy settings and batch sizes.",
        "authors": [
            "Hao Fang",
            "Bin Chen",
            "Xuan Wang",
            "Zhi Wang",
            "Shu-Tao Xia"
        ]
    },
    {
        "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection",
        "url": "http://arxiv.org/abs/2307.11077",
        "abstract": "The paradigm of large-scale pre-training followed by downstream fine-tuning\nhas been widely employed in various object detection algorithms. In this paper,\nwe reveal discrepancies in data, model, and task between the pre-training and\nfine-tuning procedure in existing practices, which implicitly limit the\ndetector's performance, generalization ability, and convergence speed. To this\nend, we propose AlignDet, a unified pre-training framework that can be adapted\nto various existing detectors to alleviate the discrepancies. AlignDet\ndecouples the pre-training process into two stages, i.e., image-domain and\nbox-domain pre-training. The image-domain pre-training optimizes the detection\nbackbone to capture holistic visual abstraction, and box-domain pre-training\nlearns instance-level semantics and task-aware concepts to initialize the parts\nout of the backbone. By incorporating the self-supervised pre-trained\nbackbones, we can pre-train all modules for various detectors in an\nunsupervised paradigm. As depicted in Figure 1, extensive experiments\ndemonstrate that AlignDet can achieve significant improvements across diverse\nprotocols, such as detection algorithm, model backbone, data setting, and\ntraining schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by\n2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.",
        "authors": [
            "Ming Li",
            "Jie Wu",
            "Xionghui Wang",
            "Chen Chen",
            "Jie Qin",
            "Xuefeng Xiao",
            "Rui Wang",
            "Min Zheng",
            "Xin Pan"
        ]
    },
    {
        "title": "Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction",
        "url": "http://arxiv.org/abs/2309.03900",
        "abstract": "Deep learning is commonly used to reconstruct HDR images from LDR images. LDR\nstack-based methods are used for single-image HDR reconstruction, generating an\nHDR image from a deep learning-generated LDR stack. However, current methods\ngenerate the stack with predetermined exposure values (EVs), which may limit\nthe quality of HDR reconstruction. To address this, we propose the continuous\nexposure value representation (CEVR), which uses an implicit function to\ngenerate LDR images with arbitrary EVs, including those unseen during training.\nOur approach generates a continuous stack with more images containing diverse\nEVs, significantly improving HDR reconstruction. We use a cycle training\nstrategy to supervise the model in generating continuous EV LDR images without\ncorresponding ground truths. Our CEVR model outperforms existing methods, as\ndemonstrated by experimental results.",
        "authors": [
            "Su-Kai Chen",
            "Hung-Lin Yen",
            "Yu-Lun Liu",
            "Min-Hung Chen",
            "Hou-Ning Hu",
            "Wen-Hsiao Peng",
            "Yen-Yu Lin"
        ]
    },
    {
        "title": "DREAM: Efficient Dataset Distillation by Representative Matching",
        "url": "http://arxiv.org/abs/2302.14416",
        "abstract": "Dataset distillation aims to synthesize small datasets with little\ninformation loss from original large-scale ones for reducing storage and\ntraining costs. Recent state-of-the-art methods mainly constrain the sample\nsynthesis process by matching synthetic images and the original ones regarding\ngradients, embedding distributions, or training trajectories. Although there\nare various matching objectives, currently the strategy for selecting original\nimages is limited to naive random sampling.\n  We argue that random sampling overlooks the evenness of the selected sample\ndistribution, which may result in noisy or biased matching targets.\n  Besides, the sample diversity is also not constrained by random sampling.\nThese factors together lead to optimization instability in the distilling\nprocess and degrade the training efficiency. Accordingly, we propose a novel\nmatching strategy named as \\textbf{D}ataset distillation by\n\\textbf{RE}present\\textbf{A}tive \\textbf{M}atching (DREAM), where only\nrepresentative original images are selected for matching. DREAM is able to be\neasily plugged into popular dataset distillation frameworks and reduce the\ndistilling iterations by more than 8 times without performance drop. Given\nsufficient training time, DREAM further provides significant improvements and\nachieves state-of-the-art performances.",
        "authors": [
            "Yanqing Liu",
            "Jianyang Gu",
            "Kai Wang",
            "Zheng Zhu",
            "Wei Jiang",
            "Yang You"
        ]
    },
    {
        "title": "Focus on Your Target: A Dual Teacher-Student Framework for Domain-Adaptive Semantic Segmentation",
        "url": "http://arxiv.org/abs/2303.09083",
        "abstract": "We study unsupervised domain adaptation (UDA) for semantic segmentation.\nCurrently, a popular UDA framework lies in self-training which endows the model\nwith two-fold abilities: (i) learning reliable semantics from the labeled\nimages in the source domain, and (ii) adapting to the target domain via\ngenerating pseudo labels on the unlabeled images. We find that, by\ndecreasing/increasing the proportion of training samples from the target\ndomain, the 'learning ability' is strengthened/weakened while the 'adapting\nability' goes in the opposite direction, implying a conflict between these two\nabilities, especially for a single model. To alleviate the issue, we propose a\nnovel dual teacher-student (DTS) framework and equip it with a bidirectional\nlearning strategy. By increasing the proportion of target-domain data, the\nsecond teacher-student model learns to 'Focus on Your Target' while the first\nmodel is not affected. DTS is easily plugged into existing self-training\napproaches. In a standard UDA scenario (training on synthetic, labeled data and\nreal, unlabeled data), DTS shows consistent gains over the baselines and sets\nnew state-of-the-art results of 76.5\\% and 75.1\\% mIoUs on\nGTAv$\\rightarrow$Cityscapes and SYNTHIA$\\rightarrow$Cityscapes, respectively.",
        "authors": [
            "Xinyue Huo",
            "Lingxi Xie",
            "Wengang Zhou",
            "Houqiang Li",
            "Qi Tian"
        ]
    },
    {
        "title": "Enhanced Meta Label Correction for Coping with Label Corruption",
        "url": "http://arxiv.org/abs/2305.12961",
        "abstract": "Traditional methods for learning with the presence of noisy labels have\nsuccessfully handled datasets with artificially injected noise but still fall\nshort of adequately handling real-world noise. With the increasing use of\nmeta-learning in the diverse fields of machine learning, researchers leveraged\nauxiliary small clean datasets to meta-correct the training labels.\nNonetheless, existing meta-label correction approaches are not fully exploiting\ntheir potential. In this study, we propose an Enhanced Meta Label Correction\napproach abbreviated as EMLC for the learning with noisy labels (LNL) problem.\nWe re-examine the meta-learning process and introduce faster and more accurate\nmeta-gradient derivations. We propose a novel teacher architecture tailored\nexplicitly to the LNL problem, equipped with novel training objectives. EMLC\noutperforms prior approaches and achieves state-of-the-art results in all\nstandard benchmarks. Notably, EMLC enhances the previous art on the noisy\nreal-world dataset Clothing1M by $1.52\\%$ while requiring $\\times 0.5$ the time\nper epoch and with much faster convergence of the meta-objective when compared\nto the baseline approach.",
        "authors": [
            "Mitchell Keren Taraday",
            "Chaim Baskin"
        ]
    },
    {
        "title": "Dense Text-to-Image Generation with Attention Modulation",
        "url": "http://arxiv.org/abs/2308.12964",
        "abstract": "Existing text-to-image diffusion models struggle to synthesize realistic\nimages given dense captions, where each text prompt provides a detailed\ndescription for a specific image region. To address this, we propose\nDenseDiffusion, a training-free method that adapts a pre-trained text-to-image\nmodel to handle such dense captions while offering control over the scene\nlayout. We first analyze the relationship between generated images' layouts and\nthe pre-trained model's intermediate attention maps. Next, we develop an\nattention modulation method that guides objects to appear in specific regions\naccording to layout guidance. Without requiring additional fine-tuning or\ndatasets, we improve image generation performance given dense captions\nregarding both automatic and human evaluation scores. In addition, we achieve\nsimilar-quality visual results with models specifically trained with layout\nconditions.",
        "authors": [
            "Yunji Kim",
            "Jiyoung Lee",
            "Jin-Hwa Kim",
            "Jung-Woo Ha",
            "Jun-Yan Zhu"
        ]
    },
    {
        "title": "HumanMAC: Masked Motion Completion for Human Motion Prediction",
        "url": "http://arxiv.org/abs/2302.03665",
        "abstract": "Human motion prediction is a classical problem in computer vision and\ncomputer graphics, which has a wide range of practical applications. Previous\neffects achieve great empirical performance based on an encoding-decoding\nstyle. The methods of this style work by first encoding previous motions to\nlatent representations and then decoding the latent representations into\npredicted motions. However, in practice, they are still unsatisfactory due to\nseveral issues, including complicated loss constraints, cumbersome training\nprocesses, and scarce switch of different categories of motions in prediction.\nIn this paper, to address the above issues, we jump out of the foregoing style\nand propose a novel framework from a new perspective. Specifically, our\nframework works in a masked completion fashion. In the training stage, we learn\na motion diffusion model that generates motions from random noise. In the\ninference stage, with a denoising procedure, we make motion prediction\nconditioning on observed motions to output more continuous and controllable\npredictions. The proposed framework enjoys promising algorithmic properties,\nwhich only needs one loss in optimization and is trained in an end-to-end\nmanner. Additionally, it accomplishes the switch of different categories of\nmotions effectively, which is significant in realistic tasks, e.g., the\nanimation task. Comprehensive experiments on benchmarks confirm the superiority\nof the proposed framework. The project page is available at\nhttps://lhchen.top/Human-MAC.",
        "authors": [
            "Ling-Hao Chen",
            "Jiawei Zhang",
            "Yewen Li",
            "Yiren Pang",
            "Xiaobo Xia",
            "Tongliang Liu"
        ]
    },
    {
        "title": "Will Large-scale Generative Models Corrupt Future Datasets?",
        "url": "http://arxiv.org/abs/2211.08095",
        "abstract": "Recently proposed large-scale text-to-image generative models such as\nDALL$\\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and\nrealistic images from users' prompts. Not limited to the research community,\nordinary Internet users enjoy these generative models, and consequently, a\ntremendous amount of generated images have been shared on the Internet.\nMeanwhile, today's success of deep learning in the computer vision field owes a\nlot to images collected from the Internet. These trends lead us to a research\nquestion: \"\\textbf{will such generated images impact the quality of future\ndatasets and the performance of computer vision models positively or\nnegatively?}\" This paper empirically answers this question by simulating\ncontamination. Namely, we generate ImageNet-scale and COCO-scale datasets using\na state-of-the-art generative model and evaluate models trained with\n\"contaminated\" datasets on various tasks, including image classification and\nimage generation. Throughout experiments, we conclude that generated images\nnegatively affect downstream performance, while the significance depends on\ntasks and the amount of generated images. The generated datasets and the codes\nfor experiments will be publicly released for future research. Generated\ndatasets and source codes are available from\n\\url{https://github.com/moskomule/dataset-contamination}.",
        "authors": [
            "Ryuichiro Hataya",
            "Han Bao",
            "Hiromi Arai"
        ]
    },
    {
        "title": "Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2308.07648",
        "abstract": "In text-video retrieval, recent works have benefited from the powerful\nlearning capabilities of pre-trained text-image foundation models (e.g., CLIP)\nby adapting them to the video domain. A critical problem for them is how to\neffectively capture the rich semantics inside the video using the image encoder\nof CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal\nmodeling techniques to fuse the text information into video frame\nrepresentations, which, however, incurs severe efficiency issues in large-scale\nretrieval systems as the video representations must be recomputed online for\nevery text query. In this paper, we discard this problematic cross-modal fusion\nprocess and aim to learn semantically-enhanced representations purely from the\nvideo, so that the video representations can be computed offline and reused for\ndifferent texts. Concretely, we first introduce a spatial-temporal \"Prompt\nCube\" into the CLIP image encoder and iteratively switch it within the encoder\nlayers to efficiently incorporate the global video semantics into frame\nrepresentations. We then propose to apply an auxiliary video captioning\nobjective to train the frame representations, which facilitates the learning of\ndetailed video semantics by providing fine-grained guidance in the semantic\nspace. With a naive temporal fusion strategy (i.e., mean-pooling) on the\nenhanced frame representations, we obtain state-of-the-art performances on\nthree benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.",
        "authors": [
            "Chaorui Deng",
            "Qi Chen",
            "Pengda Qin",
            "Da Chen",
            "Qi Wu"
        ]
    },
    {
        "title": "Video Action Recognition with Attentive Semantic Units",
        "url": "http://arxiv.org/abs/2303.09756",
        "abstract": "Visual-Language Models (VLMs) have significantly advanced action video\nrecognition. Supervised by the semantics of action labels, recent works adapt\nthe visual branch of VLMs to learn video representations. Despite the\neffectiveness proved by these works, we believe that the potential of VLMs has\nyet to be fully harnessed. In light of this, we exploit the semantic units (SU)\nhiding behind the action labels and leverage their correlations with\nfine-grained items in frames for more accurate action recognition. SUs are\nentities extracted from the language descriptions of the entire action set,\nincluding body parts, objects, scenes, and motions. To further enhance the\nalignments between visual contents and the SUs, we introduce a multi-region\nmodule (MRA) to the visual branch of the VLM. The MRA allows the perception of\nregion-aware visual features beyond the original global feature. Our method\nadaptively attends to and selects relevant SUs with visual features of frames.\nWith a cross-modal decoder, the selected SUs serve to decode spatiotemporal\nvideo representations. In summary, the SUs as the medium can boost\ndiscriminative ability and transferability. Specifically, in fully-supervised\nlearning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2\nfew-shot experiments, our method surpassed the previous state-of-the-art by\n+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.",
        "authors": [
            "Yifei Chen",
            "Dapeng Chen",
            "Ruijin Liu",
            "Hao Li",
            "Wei Peng"
        ]
    },
    {
        "title": "Sentence Attention Blocks for Answer Grounding",
        "url": "http://arxiv.org/abs/2309.11593",
        "abstract": "Answer grounding is the task of locating relevant visual evidence for the\nVisual Question Answering task. While a wide variety of attention methods have\nbeen introduced for this task, they suffer from the following three problems:\ndesigns that do not allow the usage of pre-trained networks and do not benefit\nfrom large data pre-training, custom designs that are not based on\nwell-grounded previous designs, therefore limiting the learning power of the\nnetwork, or complicated designs that make it challenging to re-implement or\nimprove them. In this paper, we propose a novel architectural block, which we\nterm Sentence Attention Block, to solve these problems. The proposed block\nre-calibrates channel-wise image feature-maps by explicitly modeling\ninter-dependencies between the image feature-maps and sentence embedding. We\nvisually demonstrate how this block filters out irrelevant feature-maps\nchannels based on sentence embedding. We start our design with a well-known\nattention method, and by making minor modifications, we improve the results to\nachieve state-of-the-art accuracy. The flexibility of our method makes it easy\nto use different pre-trained backbone networks, and its simplicity makes it\neasy to understand and be re-implemented. We demonstrate the effectiveness of\nour method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We\nperform multiple ablation studies to show the effectiveness of our design\nchoices.",
        "authors": [
            "Seyedalireza Khoshsirat",
            "Chandra Kambhamettu"
        ]
    },
    {
        "title": "Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos",
        "url": "http://arxiv.org/abs/2303.08345",
        "abstract": "Video temporal grounding aims to pinpoint a video segment that matches the\nquery description. Despite the recent advance in short-form videos\n(\\textit{e.g.}, in minutes), temporal grounding in long videos (\\textit{e.g.},\nin hours) is still at its early stage. To address this challenge, a common\npractice is to employ a sliding window, yet can be inefficient and inflexible\ndue to the limited number of frames within the window. In this work, we propose\nan end-to-end framework for fast temporal grounding, which is able to model an\nhours-long video with \\textbf{one-time} network execution. Our pipeline is\nformulated in a coarse-to-fine manner, where we first extract context knowledge\nfrom non-overlapped video clips (\\textit{i.e.}, anchors), and then supplement\nthe anchors that highly response to the query with detailed content knowledge.\nBesides the remarkably high pipeline efficiency, another advantage of our\napproach is the capability of capturing long-range temporal correlation, thanks\nto modeling the entire video as a whole, and hence facilitates more accurate\ngrounding. Experimental results suggest that, on the long-form video datasets\nMAD and Ego4d, our method significantly outperforms state-of-the-arts, and\nachieves \\textbf{14.6$\\times$} / \\textbf{102.8$\\times$} higher efficiency\nrespectively. Project can be found at\n\\url{https://github.com/afcedf/SOONet.git}.",
        "authors": [
            "Yulin Pan",
            "Xiangteng He",
            "Biao Gong",
            "Yiliang Lv",
            "Yujun Shen",
            "Yuxin Peng",
            "Deli Zhao"
        ]
    },
    {
        "title": "VoroMesh: Learning Watertight Surface Meshes with Voronoi Diagrams",
        "url": "http://arxiv.org/abs/2308.14616",
        "abstract": "In stark contrast to the case of images, finding a concise, learnable\ndiscrete representation of 3D surfaces remains a challenge. In particular,\nwhile polygon meshes are arguably the most common surface representation used\nin geometry processing, their irregular and combinatorial structure often make\nthem unsuitable for learning-based applications. In this work, we present\nVoroMesh, a novel and differentiable Voronoi-based representation of watertight\n3D shape surfaces. From a set of 3D points (called generators) and their\nassociated occupancy, we define our boundary representation through the Voronoi\ndiagram of the generators as the subset of Voronoi faces whose two associated\n(equidistant) generators are of opposite occupancy: the resulting polygon mesh\nforms a watertight approximation of the target shape's boundary. To learn the\nposition of the generators, we propose a novel loss function, dubbed VoroLoss,\nthat minimizes the distance from ground truth surface samples to the closest\nfaces of the Voronoi diagram which does not require an explicit construction of\nthe entire Voronoi diagram. A direct optimization of the Voroloss to obtain\ngenerators on the Thingi32 dataset demonstrates the geometric efficiency of our\nrepresentation compared to axiomatic meshing algorithms and recent\nlearning-based mesh representations. We further use VoroMesh in a\nlearning-based mesh prediction task from input SDF grids on the ABC dataset,\nand show comparable performance to state-of-the-art methods while guaranteeing\nclosed output surfaces free of self-intersections.",
        "authors": [
            "Nissim Maruani",
            "Roman Klokov",
            "Maks Ovsjanikov",
            "Pierre Alliez",
            "Mathieu Desbrun"
        ]
    },
    {
        "title": "Smoothness Similarity Regularization for Few-Shot GAN Adaptation",
        "url": "http://arxiv.org/abs/2308.09717",
        "abstract": "The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model to\na small dataset with very few training images. While existing methods perform\nwell when the dataset for pre-training is structurally similar to the target\ndataset, the approaches suffer from training instabilities or memorization\nissues when the objects in the two domains have a very different structure. To\nmitigate this limitation, we propose a new smoothness similarity regularization\nthat transfers the inherently learned smoothness of the pre-trained GAN to the\nfew-shot target domain even if the two domains are very different. We evaluate\nour approach by adapting an unconditional and a class-conditional GAN to\ndiverse few-shot target domains. Our proposed method significantly outperforms\nprior few-shot GAN adaptation methods in the challenging case of structurally\ndissimilar source-target domains, while performing on par with the state of the\nart for similar source-target domains.",
        "authors": [
            "Vadim Sushko",
            "Ruyu Wang",
            "Juergen Gall"
        ]
    },
    {
        "title": "Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding",
        "url": "http://arxiv.org/abs/2307.09267",
        "abstract": "3D visual grounding involves finding a target object in a 3D scene that\ncorresponds to a given sentence query. Although many approaches have been\nproposed and achieved impressive performance, they all require dense\nobject-sentence pair annotations in 3D point clouds, which are both\ntime-consuming and expensive. To address the problem that fine-grained\nannotated data is difficult to obtain, we propose to leverage weakly supervised\nannotations to learn the 3D visual grounding model, i.e., only coarse\nscene-sentence correspondences are used to learn object-sentence links. To\naccomplish this, we design a novel semantic matching model that analyzes the\nsemantic similarity between object proposals and sentences in a coarse-to-fine\nmanner. Specifically, we first extract object proposals and coarsely select the\ntop-K candidates based on feature and class similarity matrices. Next, we\nreconstruct the masked keywords of the sentence using each candidate one by\none, and the reconstructed accuracy finely reflects the semantic similarity of\neach candidate to the query. Additionally, we distill the coarse-to-fine\nsemantic matching knowledge into a typical two-stage 3D visual grounding model,\nwhich reduces inference costs and improves performance by taking full advantage\nof the well-studied structure of the existing architectures. We conduct\nextensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the\neffectiveness of our proposed method.",
        "authors": [
            "Zehan Wang",
            "Haifeng Huang",
            "Yang Zhao",
            "Linjun Li",
            "Xize Cheng",
            "Yichen Zhu",
            "Aoxiong Yin",
            "Zhou Zhao"
        ]
    },
    {
        "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
        "url": "http://arxiv.org/abs/2304.06712",
        "abstract": "Large-scale Vision-Language Models, such as CLIP, learn powerful image-text\nrepresentations that have found numerous applications, from zero-shot\nclassification to text-to-image generation. Despite that, their capabilities\nfor solving novel discriminative tasks via prompting fall behind those of large\nlanguage models, such as GPT-3. Here we explore the idea of visual prompt\nengineering for solving computer vision tasks beyond classification by editing\nin image space instead of text. In particular, we discover an emergent ability\nof CLIP, where, by simply drawing a red circle around an object, we can direct\nthe model's attention to that region, while also maintaining global\ninformation. We show the power of this simple approach by achieving\nstate-of-the-art in zero-shot referring expressions comprehension and strong\nperformance in keypoint localization tasks. Finally, we draw attention to some\npotential ethical concerns of large language-vision models.",
        "authors": [
            "Aleksandar Shtedritski",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ]
    },
    {
        "title": "MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation",
        "url": "http://arxiv.org/abs/2308.11185",
        "abstract": "Previous research has studied the task of segmenting cinematic videos into\nscenes and into narrative acts. However, these studies have overlooked the\nessential task of multimodal alignment and fusion for effectively and\nefficiently processing long-form videos (>60min). In this paper, we introduce\nMultimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematic\nlong-video segmentation. MEGA tackles the challenge by leveraging multiple\nmedia modalities. The method coarsely aligns inputs of variable lengths and\ndifferent modalities with alignment positional encoding. To maintain temporal\nsynchronization while reducing computation, we further introduce an enhanced\nbottleneck fusion layer which uses temporal alignment. Additionally, MEGA\nemploys a novel contrastive loss to synchronize and transfer labels across\nmodalities, enabling act segmentation from labeled synopsis sentences on video\nshots. Our experimental results show that MEGA outperforms state-of-the-art\nmethods on MovieNet dataset for scene segmentation (with an Average Precision\nimprovement of +1.19%) and on TRIPOD dataset for act segmentation (with a Total\nAgreement improvement of +5.51%)",
        "authors": [
            "Najmeh Sadoughi",
            "Xinyu Li",
            "Avijit Vajpayee",
            "David Fan",
            "Bing Shuai",
            "Hector Santos-Villalobos",
            "Vimal Bhat",
            "Rohith MV"
        ]
    },
    {
        "title": "DiffRate : Differentiable Compression Rate for Efficient Vision Transformers",
        "url": "http://arxiv.org/abs/2305.17997",
        "abstract": "Token compression aims to speed up large-scale vision transformers (e.g.\nViTs) by pruning (dropping) or merging tokens. It is an important but\nchallenging task. Although recent advanced approaches achieved great success,\nthey need to carefully handcraft a compression rate (i.e. number of tokens to\nremove), which is tedious and leads to sub-optimal performance. To tackle this\nproblem, we propose Differentiable Compression Rate (DiffRate), a novel token\ncompression method that has several appealing properties prior arts do not\nhave. First, DiffRate enables propagating the loss function's gradient onto the\ncompression ratio, which is considered as a non-differentiable hyperparameter\nin previous work. In this case, different layers can automatically learn\ndifferent compression rates layer-wisely without extra overhead. Second, token\npruning and merging can be naturally performed simultaneously in DiffRate,\nwhile they were isolated in previous works. Third, extensive experiments\ndemonstrate that DiffRate achieves state-of-the-art performance. For example,\nby applying the learned layer-wise compression rates to an off-the-shelf ViT-H\n(MAE) model, we achieve a 40% FLOPs reduction and a 1.5x throughput\nimprovement, with a minor accuracy drop of 0.16% on ImageNet without\nfine-tuning, even outperforming previous methods with fine-tuning. Codes and\nmodels are available at https://github.com/OpenGVLab/DiffRate.",
        "authors": [
            "Mengzhao Chen",
            "Wenqi Shao",
            "Peng Xu",
            "Mingbao Lin",
            "Kaipeng Zhang",
            "Fei Chao",
            "Rongrong Ji",
            "Yu Qiao",
            "Ping Luo"
        ]
    },
    {
        "title": "zPROBE: Zero Peek Robustness Checks for Federated Learning",
        "url": "http://arxiv.org/abs/2206.12100",
        "abstract": "Privacy-preserving federated learning allows multiple users to jointly train\na model with coordination of a central server. The server only learns the final\naggregation result, thus the users' (private) training data is not leaked from\nthe individual model updates. However, keeping the individual updates private\nallows malicious users to perform Byzantine attacks and degrade the accuracy\nwithout being detected. Best existing defenses against Byzantine workers rely\non robust rank-based statistics, e.g., median, to find malicious updates.\nHowever, implementing privacy-preserving rank-based statistics is nontrivial\nand not scalable in the secure domain, as it requires sorting all individual\nupdates. We establish the first private robustness check that uses high break\npoint rank-based statistics on aggregated model updates. By exploiting\nrandomized clustering, we significantly improve the scalability of our defense\nwithout compromising privacy. We leverage our statistical bounds in\nzero-knowledge proofs to detect and remove malicious updates without revealing\nthe private user updates. Our novel framework, zPROBE, enables Byzantine\nresilient and secure federated learning. Empirical evaluations demonstrate that\nzPROBE provides a low overhead solution to defend against state-of-the-art\nByzantine attacks while preserving privacy.",
        "authors": [
            "Zahra Ghodsi",
            "Mojan Javaheripi",
            "Nojan Sheybani",
            "Xinqiao Zhang",
            "Ke Huang",
            "Farinaz Koushanfar"
        ]
    },
    {
        "title": "LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference",
        "url": "http://arxiv.org/abs/2307.12217",
        "abstract": "We propose a novel method, LoLep, which regresses Locally-Learned planes from\na single RGB image to represent scenes accurately, thus generating better novel\nviews. Without the depth information, regressing appropriate plane locations is\na challenging problem. To solve this issue, we pre-partition the disparity\nspace into bins and design a disparity sampler to regress local offsets for\nmultiple planes in each bin. However, only using such a sampler makes the\nnetwork not convergent; we further propose two optimizing strategies that\ncombine with different disparity distributions of datasets and propose an\nocclusion-aware reprojection loss as a simple yet effective geometric\nsupervision technique. We also introduce a self-attention mechanism to improve\nocclusion inference and present a Block-Sampling Self-Attention (BS-SA) module\nto address the problem of applying self-attention to large feature maps. We\ndemonstrate the effectiveness of our approach and generate state-of-the-art\nresults on different datasets. Compared to MINE, our approach has an LPIPS\nreduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate the\nperformance on real-world images and demonstrate the benefits.",
        "authors": [
            "Cong Wang",
            "Yu-Ping Wang",
            "Dinesh Manocha"
        ]
    },
    {
        "title": "Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation",
        "url": "http://arxiv.org/abs/2303.10457",
        "abstract": "Continual Test-Time Adaptation (CTTA) generalizes conventional Test-Time\nAdaptation (TTA) by assuming that the target domain is dynamic over time rather\nthan stationary. In this paper, we explore Multi-Modal Continual Test-Time\nAdaptation (MM-CTTA) as a new extension of CTTA for 3D semantic segmentation.\nThe key to MM-CTTA is to adaptively attend to the reliable modality while\navoiding catastrophic forgetting during continual domain shifts, which is out\nof the capability of previous TTA or CTTA methods. To fulfill this gap, we\npropose an MM-CTTA method called Continual Cross-Modal Adaptive Clustering\n(CoMAC) that addresses this task from two perspectives. On one hand, we propose\nan adaptive dual-stage mechanism to generate reliable cross-modal predictions\nby attending to the reliable modality based on the class-wise feature-centroid\ndistance in the latent space. On the other hand, to perform test-time\nadaptation without catastrophic forgetting, we design class-wise momentum\nqueues that capture confident target features for adaptation while\nstochastically restoring pseudo-source features to revisit source knowledge. We\nfurther introduce two new benchmarks to facilitate the exploration of MM-CTTA\nin the future. Our experimental results show that our method achieves\nstate-of-the-art performance on both benchmarks.",
        "authors": [
            "Haozhi Cao",
            "Yuecong Xu",
            "Jianfei Yang",
            "Pengyu Yin",
            "Shenghai Yuan",
            "Lihua Xie"
        ]
    },
    {
        "title": "Heterogeneous Forgetting Compensation for Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2308.03374",
        "abstract": "Class-incremental learning (CIL) has achieved remarkable successes in\nlearning new classes consecutively while overcoming catastrophic forgetting on\nold categories. However, most existing CIL methods unreasonably assume that all\nold categories have the same forgetting pace, and neglect negative influence of\nforgetting heterogeneity among different old classes on forgetting\ncompensation. To surmount the above challenges, we develop a novel\nHeterogeneous Forgetting Compensation (HFC) model, which can resolve\nheterogeneous forgetting of easy-to-forget and hard-to-forget old categories\nfrom both representation and gradient aspects. Specifically, we design a\ntask-semantic aggregation block to alleviate heterogeneous forgetting from\nrepresentation aspect. It aggregates local category information within each\ntask to learn task-shared global representations. Moreover, we develop two\nnovel plug-and-play losses: a gradient-balanced forgetting compensation loss\nand a gradient-balanced relation distillation loss to alleviate forgetting from\ngradient aspect. They consider gradient-balanced compensation to rectify\nforgetting heterogeneity of old categories and heterogeneous relation\nconsistency. Experiments on several representative datasets illustrate\neffectiveness of our HFC model. The code is available at\nhttps://github.com/JiahuaDong/HFC.",
        "authors": [
            "Jiahua Dong",
            "Wenqi Liang",
            "Yang Cong",
            "Gan Sun"
        ]
    },
    {
        "title": "FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs",
        "url": "http://arxiv.org/abs/2301.06719",
        "abstract": "Efficient detectors for edge devices are often optimized for parameters or\nspeed count metrics, which remain in weak correlation with the energy of\ndetectors.\n  However, some vision applications of convolutional neural networks, such as\nalways-on surveillance cameras, are critical for energy constraints.\n  This paper aims to serve as a baseline by designing detectors to reach\ntradeoffs between energy and performance from two perspectives:\n  1) We extensively analyze various CNNs to identify low-energy architectures,\nincluding selecting activation functions, convolutions operators, and feature\nfusion structures on necks. These underappreciated details in past work\nseriously affect the energy consumption of detectors;\n  2) To break through the dilemmatic energy-performance problem, we propose a\nbalanced detector driven by energy using discovered low-energy components named\n\\textit{FemtoDet}.\n  In addition to the novel construction, we improve FemtoDet by considering\nconvolutions and training strategy optimizations.\n  Specifically, we develop a new instance boundary enhancement (IBE) module for\nconvolution optimization to overcome the contradiction between the limited\ncapacity of CNNs and detection tasks in diverse spatial representations, and\npropose a recursive warm-restart (RecWR) for optimizing training strategy to\nescape the sub-optimization of light-weight detectors by considering the data\nshift produced in popular augmentations.\n  As a result, FemtoDet with only 68.77k parameters achieves a competitive\nscore of 46.3 AP50 on PASCAL VOC and 1.11 W $\\&$ 64.47 FPS on Qualcomm\nSnapdragon 865 CPU platforms.\n  Extensive experiments on COCO and TJU-DHD datasets indicate that the proposed\nmethod achieves competitive results in diverse scenes.",
        "authors": [
            "Peng Tu",
            "Xu Xie",
            "Guo AI",
            "Yuexiang Li",
            "Yawen Huang",
            "Yefeng Zheng"
        ]
    },
    {
        "title": "Generative Prompt Model for Weakly Supervised Object Localization",
        "url": "http://arxiv.org/abs/2307.09756",
        "abstract": "Weakly supervised object localization (WSOL) remains challenging when\nlearning object localization models from image category labels. Conventional\nmethods that discriminatively train activation models ignore representative yet\nless discriminative object parts. In this study, we propose a generative prompt\nmodel (GenPromp), defining the first generative pipeline to localize less\ndiscriminative object parts by formulating WSOL as a conditional image\ndenoising procedure. During training, GenPromp converts image category labels\nto learnable prompt embeddings which are fed to a generative model to\nconditionally recover the input image with noise and learn representative\nembeddings. During inference, enPromp combines the representative embeddings\nwith discriminative embeddings (queried from an off-the-shelf vision-language\nmodel) for both representative and discriminative capacity. The combined\nembeddings are finally used to generate multi-scale high-quality attention\nmaps, which facilitate localizing full object extent. Experiments on\nCUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best\ndiscriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline\nfor WSOL with the generative model. Code is available at\nhttps://github.com/callsys/GenPromp.",
        "authors": [
            "Yuzhong Zhao",
            "Qixiang Ye",
            "Weijia Wu",
            "Chunhua Shen",
            "Fang Wan"
        ]
    },
    {
        "title": "ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation",
        "url": "http://arxiv.org/abs/2203.07706",
        "abstract": "We present a GAN-based Transformer for general action-conditioned 3D human\nmotion generation, including not only single-person actions but also\nmulti-person interactive actions. Our approach consists of a powerful\nAction-conditioned motion TransFormer (ActFormer) under a GAN training scheme,\nequipped with a Gaussian Process latent prior. Such a design combines the\nstrong spatio-temporal representation capacity of Transformer, superiority in\ngenerative modeling of GAN, and inherent temporal correlations from the latent\nprior. Furthermore, ActFormer can be naturally extended to multi-person motions\nby alternately modeling temporal correlations and human interactions with\nTransformer encoders. To further facilitate research on multi-person motion\ngeneration, we introduce a new synthetic dataset of complex multi-person combat\nbehaviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the\nproposed combat dataset show that our method can adapt to various human motion\nrepresentations and achieve superior performance over the state-of-the-art\nmethods on both single-person and multi-person motion generation tasks,\ndemonstrating a promising step towards a general human motion generator.",
        "authors": [
            "Liang Xu",
            "Ziyang Song",
            "Dongliang Wang",
            "Jing Su",
            "Zhicheng Fang",
            "Chenjing Ding",
            "Weihao Gan",
            "Yichao Yan",
            "Xin Jin",
            "Xiaokang Yang",
            "Wenjun Zeng",
            "Wei Wu"
        ]
    },
    {
        "title": "Hiding Visual Information via Obfuscating Adversarial Perturbations",
        "url": "http://arxiv.org/abs/2209.15304",
        "abstract": "Growing leakage and misuse of visual information raise security and privacy\nconcerns, which promotes the development of information protection. Existing\nadversarial perturbations-based methods mainly focus on the de-identification\nagainst deep learning models. However, the inherent visual information of the\ndata has not been well protected. In this work, inspired by the Type-I\nadversarial attack, we propose an adversarial visual information hiding method\nto protect the visual privacy of data. Specifically, the method generates\nobfuscating adversarial perturbations to obscure the visual information of the\ndata. Meanwhile, it maintains the hidden objectives to be correctly predicted\nby models. In addition, our method does not modify the parameters of the\napplied model, which makes it flexible for different scenarios. Experimental\nresults on the recognition and classification tasks demonstrate that the\nproposed method can effectively hide visual information and hardly affect the\nperformances of models. The code is available in the supplementary material.",
        "authors": [
            "Zhigang Su",
            "Dawei Zhou",
            "Nannan Wangu",
            "Decheng Li",
            "Zhen Wang",
            "Xinbo Gao"
        ]
    },
    {
        "title": "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement",
        "url": "http://arxiv.org/abs/2303.17569",
        "abstract": "We propose a novel unsupervised backlit image enhancement method, abbreviated\nas CLIP-LIT, by exploring the potential of Contrastive Language-Image\nPre-Training (CLIP) for pixel-level image enhancement. We show that the\nopen-world CLIP prior not only aids in distinguishing between backlit and\nwell-lit images, but also in perceiving heterogeneous regions with different\nluminance, facilitating the optimization of the enhancement network. Unlike\nhigh-level and image manipulation tasks, directly applying CLIP to enhancement\ntasks is non-trivial, owing to the difficulty in finding accurate prompts. To\nsolve this issue, we devise a prompt learning framework that first learns an\ninitial prompt pair by constraining the text-image similarity between the\nprompt (negative/positive sample) and the corresponding image (backlit\nimage/well-lit image) in the CLIP latent space. Then, we train the enhancement\nnetwork based on the text-image similarity between the enhanced result and the\ninitial prompt pair. To further improve the accuracy of the initial prompt\npair, we iteratively fine-tune the prompt learning framework to reduce the\ndistribution gaps between the backlit images, enhanced results, and well-lit\nimages via rank learning, boosting the enhancement performance. Our method\nalternates between updating the prompt learning framework and enhancement\nnetwork until visually pleasing results are achieved. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nvisual quality and generalization ability, without requiring any paired data.",
        "authors": [
            "Zhexin Liang",
            "Chongyi Li",
            "Shangchen Zhou",
            "Ruicheng Feng",
            "Chen Change Loy"
        ]
    },
    {
        "title": "UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction",
        "url": "http://arxiv.org/abs/2302.13987",
        "abstract": "In recent years, many video tasks have achieved breakthroughs by utilizing\nthe vision transformer and establishing spatial-temporal decoupling for feature\nextraction. Although multi-view 3D reconstruction also faces multiple images as\ninput, it cannot immediately inherit their success due to completely ambiguous\nassociations between unstructured views. There is not usable prior\nrelationship, which is similar to the temporally-coherence property in a video.\nTo solve this problem, we propose a novel transformer network for Unstructured\nMultiple Images (UMIFormer). It exploits transformer blocks for decoupled\nintra-view encoding and designed blocks for token rectification that mine the\ncorrelation between similar tokens from different views to achieve decoupled\ninter-view encoding. Afterward, all tokens acquired from various branches are\ncompressed into a fixed-size compact representation while preserving rich\ninformation for reconstruction by leveraging the similarities between tokens.\nWe empirically demonstrate on ShapeNet and confirm that our decoupled learning\nmethod is adaptable for unstructured multiple images. Meanwhile, the\nexperiments also verify our model outperforms existing SOTA methods by a large\nmargin. Code will be available at https://github.com/GaryZhu1996/UMIFormer.",
        "authors": [
            "Zhenwei Zhu",
            "Liying Yang",
            "Ning Li",
            "Chaohao Jiang",
            "Yanyan Liang"
        ]
    },
    {
        "title": "Locally Stylized Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.10684",
        "abstract": "In recent years, there has been increasing interest in applying stylization\non 3D scenes from a reference style image, in particular onto neural radiance\nfields (NeRF). While performing stylization directly on NeRF guarantees\nappearance consistency over arbitrary novel views, it is a challenging problem\nto guide the transfer of patterns from the style image onto different parts of\nthe NeRF scene. In this work, we propose a stylization framework for NeRF based\non local style transfer. In particular, we use a hash-grid encoding to learn\nthe embedding of the appearance and geometry components, and show that the\nmapping defined by the hash table allows us to control the stylization to a\ncertain extent. Stylization is then achieved by optimizing the appearance\nbranch while keeping the geometry branch fixed. To support local style\ntransfer, we propose a new loss function that utilizes a segmentation network\nand bipartite matching to establish region correspondences between the style\nimage and the content images obtained from volume rendering. Our experiments\nshow that our method yields plausible stylization results with novel view\nsynthesis while having flexible controllability via manipulating and\ncustomizing the region correspondences.",
        "authors": [
            "Hong-Wing Pang",
            "Binh-Son Hua",
            "Sai-Kit Yeung"
        ]
    },
    {
        "title": "InterFormer: Real-time Interactive Image Segmentation",
        "url": "http://arxiv.org/abs/2304.02942",
        "abstract": "Interactive image segmentation enables annotators to efficiently perform\npixel-level annotation for segmentation tasks. However, the existing\ninteractive segmentation pipeline suffers from inefficient computations of\ninteractive models because of the following two issues. First, annotators'\nlater click is based on models' feedback of annotators' former click. This\nserial interaction is unable to utilize model's parallelism capabilities.\nSecond, in each interaction step, the model handles the invariant image along\nwith the sparse variable clicks, resulting in a process that's highly\nrepetitive and redundant. For efficient computations, we propose a method named\nInterFormer that follows a new pipeline to address these issues. InterFormer\nextracts and preprocesses the computationally time-consuming part i.e. image\nprocessing from the existing process. Specifically, InterFormer employs a large\nvision transformer (ViT) on high-performance devices to preprocess images in\nparallel, and then uses a lightweight module called interactive multi-head self\nattention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module's\ndeployment on low-power devices extends the practical application of\ninteractive segmentation. The I-MSA module utilizes the preprocessed features\nto efficiently response to the annotator inputs in real-time. The experiments\non several datasets demonstrate the effectiveness of InterFormer, which\noutperforms previous interactive segmentation models in terms of computational\nefficiency and segmentation quality, achieve real-time high-quality interactive\nsegmentation on CPU-only devices. The code is available at\nhttps://github.com/YouHuang67/InterFormer.",
        "authors": [
            "You Huang",
            "Hao Yang",
            "Ke Sun",
            "Shengchuan Zhang",
            "Liujuan Cao",
            "Guannan Jiang",
            "Rongrong Ji"
        ]
    },
    {
        "title": "A Simple Framework for Open-Vocabulary Segmentation and Detection",
        "url": "http://arxiv.org/abs/2303.08131",
        "abstract": "We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection\nframework that jointly learns from different segmentation and detection\ndatasets. To bridge the gap of vocabulary and annotation granularity, we first\nintroduce a pre-trained text encoder to encode all the visual concepts in two\ntasks and learn a common semantic space for them. This gives us reasonably good\nresults compared with the counterparts trained on segmentation task only. To\nfurther reconcile them, we locate two discrepancies: $i$) task discrepancy --\nsegmentation requires extracting masks for both foreground objects and\nbackground stuff, while detection merely cares about the former; $ii$) data\ndiscrepancy -- box and mask annotations are with different spatial granularity,\nand thus not directly interchangeable. To address these issues, we propose a\ndecoupled decoding to reduce the interference between foreground/background and\na conditioned mask decoding to assist in generating masks for given boxes. To\nthis end, we develop a simple encoder-decoder model encompassing all three\ntechniques and train it jointly on COCO and Objects365. After pre-training, our\nmodel exhibits competitive or stronger zero-shot transferability for both\nsegmentation and detection. Specifically, OpenSeeD beats the state-of-the-art\nmethod for open-vocabulary instance and panoptic segmentation across 5\ndatasets, and outperforms previous work for open-vocabulary detection on LVIS\nand ODinW under similar settings. When transferred to specific tasks, our model\nachieves new SoTA for panoptic segmentation on COCO and ADE20K, and instance\nsegmentation on ADE20K and Cityscapes.\n  Finally, we note that OpenSeeD is the first to explore the potential of joint\ntraining on segmentation and detection, and hope it can be received as a strong\nbaseline for developing a single model for both tasks in open world.",
        "authors": [
            "Hao Zhang",
            "Feng Li",
            "Xueyan Zou",
            "Shilong Liu",
            "Chunyuan Li",
            "Jianfeng Gao",
            "Jianwei Yang",
            "Lei Zhang"
        ]
    },
    {
        "title": "UATVR: Uncertainty-Adaptive Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2301.06309",
        "abstract": "With the explosive growth of web videos and emerging large-scale\nvision-language pre-training models, e.g., CLIP, retrieving videos of interest\nwith text instructions has attracted increasing attention. A common practice is\nto transfer text-video pairs to the same embedding space and craft cross-modal\ninteractions with certain entities in specific granularities for semantic\ncorrespondence. Unfortunately, the intrinsic uncertainties of optimal entity\ncombinations in appropriate granularities for cross-modal queries are\nunderstudied, which is especially critical for modalities with hierarchical\nsemantics, e.g., video, text, etc. In this paper, we propose an\nUncertainty-Adaptive Text-Video Retrieval approach, termed UATVR, which models\neach look-up as a distribution matching procedure. Concretely, we add\nadditional learnable tokens in the encoders to adaptively aggregate\nmulti-grained semantics for flexible high-level reasoning. In the refined\nembedding space, we represent text-video pairs as probabilistic distributions\nwhere prototypes are sampled for matching evaluation. Comprehensive experiments\non four benchmarks justify the superiority of our UATVR, which achieves new\nstate-of-the-art results on MSR-VTT (50.8%), VATEX (64.5%), MSVD (49.7%), and\nDiDeMo (45.8%). The code is available at https://github.com/bofang98/UATVR.",
        "authors": [
            "Bo Fang",
            "Wenhao Wu",
            "Chang Liu",
            "Yu Zhou",
            "Yuxin Song",
            "Weiping Wang",
            "Xiangbo Shu",
            "Xiangyang Ji",
            "Jingdong Wang"
        ]
    },
    {
        "title": "Deep Directly-Trained Spiking Neural Networks for Object Detection",
        "url": "http://arxiv.org/abs/2307.11411",
        "abstract": "Spiking neural networks (SNNs) are brain-inspired energy-efficient models\nthat encode information in spatiotemporal dynamics. Recently, deep SNNs trained\ndirectly have shown great success in achieving high performance on\nclassification tasks with very few time steps. However, how to design a\ndirectly-trained SNN for the regression task of object detection still remains\na challenging problem. To address this problem, we propose EMS-YOLO, a novel\ndirectly-trained SNN framework for object detection, which is the first trial\nto train a deep SNN with surrogate gradients for object detection rather than\nANN-SNN conversion strategies. Specifically, we design a full-spike residual\nblock, EMS-ResNet, which can effectively extend the depth of the\ndirectly-trained SNN with low power consumption. Furthermore, we theoretically\nanalyze and prove the EMS-ResNet could avoid gradient vanishing or exploding.\nThe results demonstrate that our approach outperforms the state-of-the-art\nANN-SNN conversion methods (at least 500 time steps) in extremely fewer time\nsteps (only 4 time steps). It is shown that our model could achieve comparable\nperformance to the ANN with the same architecture while consuming 5.83 times\nless energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.",
        "authors": [
            "Qiaoyi Su",
            "Yuhong Chou",
            "Yifan Hu",
            "Jianing Li",
            "Shijie Mei",
            "Ziyang Zhang",
            "Guoqi Li"
        ]
    },
    {
        "title": "Online Prototype Learning for Online Continual Learning",
        "url": "http://arxiv.org/abs/2308.00301",
        "abstract": "Online continual learning (CL) studies the problem of learning continuously\nfrom a single-pass data stream while adapting to new data and mitigating\ncatastrophic forgetting. Recently, by storing a small subset of old data,\nreplay-based methods have shown promising performance. Unlike previous methods\nthat focus on sample storage or knowledge distillation against catastrophic\nforgetting, this paper aims to understand why the online learning models fail\nto generalize well from a new perspective of shortcut learning. We identify\nshortcut learning as the key limiting factor for online CL, where the learned\nfeatures may be biased, not generalizable to new tasks, and may have an adverse\nimpact on knowledge distillation. To tackle this issue, we present the online\nprototype learning (OnPro) framework for online CL. First, we propose online\nprototype equilibrium to learn representative features against shortcut\nlearning and discriminative features to avoid class confusion, ultimately\nachieving an equilibrium status that separates all seen classes well while\nlearning new classes. Second, with the feedback of online prototypes, we devise\na novel adaptive prototypical feedback mechanism to sense the classes that are\neasily misclassified and then enhance their boundaries. Extensive experimental\nresults on widely-used benchmark datasets demonstrate the superior performance\nof OnPro over the state-of-the-art baseline methods. Source code is available\nat https://github.com/weilllllls/OnPro.",
        "authors": [
            "Yujie Wei",
            "Jiaxin Ye",
            "Zhizhong Huang",
            "Junping Zhang",
            "Hongming Shan"
        ]
    },
    {
        "title": "ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs",
        "url": "http://arxiv.org/abs/2304.14401",
        "abstract": "While NeRF-based human representations have shown impressive novel view\nsynthesis results, most methods still rely on a large number of images / views\nfor training. In this work, we propose a novel animatable NeRF called\nActorsNeRF. It is first pre-trained on diverse human subjects, and then adapted\nwith few-shot monocular video frames for a new actor with unseen poses.\nBuilding on previous generalizable NeRFs with parameter sharing using a ConvNet\nencoder, ActorsNeRF further adopts two human priors to capture the large human\nappearance, shape, and pose variations. Specifically, in the encoded feature\nspace, we will first align different human subjects in a category-level\ncanonical space, and then align the same human from different frames in an\ninstance-level canonical space for rendering. We quantitatively and\nqualitatively demonstrate that ActorsNeRF significantly outperforms the\nexisting state-of-the-art on few-shot generalization to new people and poses on\nmultiple datasets. Project Page: https://jitengmu.github.io/ActorsNeRF/",
        "authors": [
            "Jiteng Mu",
            "Shen Sang",
            "Nuno Vasconcelos",
            "Xiaolong Wang"
        ]
    },
    {
        "title": "SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation",
        "url": "http://arxiv.org/abs/2303.12236",
        "abstract": "We present a cascaded diffusion model based on a part-level implicit 3D\nrepresentation. Our model achieves state-of-the-art generation quality and also\nenables part-level shape editing and manipulation without any additional\ntraining in conditional setup. Diffusion models have demonstrated impressive\ncapabilities in data generation as well as zero-shot completion and editing via\na guided reverse process. Recent research on 3D diffusion models has focused on\nimproving their generation capabilities with various data representations,\nwhile the absence of structural information has limited their capability in\ncompletion and editing tasks. We thus propose our novel diffusion model using a\npart-level implicit representation. To effectively learn diffusion with\nhigh-dimensional embedding vectors of parts, we propose a cascaded framework,\nlearning diffusion first on a low-dimensional subspace encoding extrinsic\nparameters of parts and then on the other high-dimensional subspace encoding\nintrinsic attributes. In the experiments, we demonstrate the outperformance of\nour method compared with the previous ones both in generation and part-level\ncompletion and manipulation tasks.",
        "authors": [
            "Juil Koo",
            "Seungwoo Yoo",
            "Minh Hieu Nguyen",
            "Minhyuk Sung"
        ]
    },
    {
        "title": "COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability",
        "url": "http://arxiv.org/abs/2309.07926",
        "abstract": "Recently, neural network (NN)-based image compression studies have actively\nbeen made and has shown impressive performance in comparison to traditional\nmethods. However, most of the works have focused on non-scalable image\ncompression (single-layer coding) while spatially scalable image compression\nhas drawn less attention although it has many applications. In this paper, we\npropose a novel NN-based spatially scalable image compression method, called\nCOMPASS, which supports arbitrary-scale spatial scalability. Our proposed\nCOMPASS has a very flexible structure where the number of layers and their\nrespective scale factors can be arbitrarily determined during inference. To\nreduce the spatial redundancy between adjacent layers for arbitrary scale\nfactors, our COMPASS adopts an inter-layer arbitrary scale prediction method,\ncalled LIFF, based on implicit neural representation. We propose a combined RD\nloss function to effectively train multiple layers. Experimental results show\nthat our COMPASS achieves BD-rate gain of -58.33% and -47.17% at maximum\ncompared to SHVC and the state-of-the-art NN-based spatially scalable image\ncompression method, respectively, for various combinations of scale factors.\nOur COMPASS also shows comparable or even better coding efficiency than the\nsingle-layer coding for various scale factors.",
        "authors": [
            "Jongmin Park",
            "Jooyoung Lee",
            "Munchurl Kim"
        ]
    },
    {
        "title": "Score-Based Diffusion Models as Principled Priors for Inverse Imaging",
        "url": "http://arxiv.org/abs/2304.11751",
        "abstract": "Priors are essential for reconstructing images from noisy and/or incomplete\nmeasurements. The choice of the prior determines both the quality and\nuncertainty of recovered images. We propose turning score-based diffusion\nmodels into principled image priors (\"score-based priors\") for analyzing a\nposterior of images given measurements. Previously, probabilistic priors were\nlimited to handcrafted regularizers and simple distributions. In this work, we\nempirically validate the theoretically-proven probability function of a\nscore-based diffusion model. We show how to sample from resulting posteriors by\nusing this probability function for variational inference. Our results,\nincluding experiments on denoising, deblurring, and interferometric imaging,\nsuggest that score-based priors enable principled inference with a\nsophisticated, data-driven image prior.",
        "authors": [
            "Berthy T. Feng",
            "Jamie Smith",
            "Michael Rubinstein",
            "Huiwen Chang",
            "Katherine L. Bouman",
            "William T. Freeman"
        ]
    },
    {
        "title": "Multiscale Structure Guided Diffusion for Image Deblurring",
        "url": "http://arxiv.org/abs/2212.01789",
        "abstract": "Diffusion Probabilistic Models (DPMs) have recently been employed for image\ndeblurring, formulated as an image-conditioned generation process that maps\nGaussian noise to the high-quality image, conditioned on the blurry input.\nImage-conditioned DPMs (icDPMs) have shown more realistic results than\nregression-based methods when trained on pairwise in-domain data. However,\ntheir robustness in restoring images is unclear when presented with\nout-of-domain images as they do not impose specific degradation models or\nintermediate constraints. To this end, we introduce a simple yet effective\nmultiscale structure guidance as an implicit bias that informs the icDPM about\nthe coarse structure of the sharp image at the intermediate layers. This guided\nformulation leads to a significant improvement of the deblurring results,\nparticularly on unseen domain. The guidance is extracted from the latent space\nof a regression network trained to predict the clean-sharp target at multiple\nlower resolutions, thus maintaining the most salient sharp structures. With\nboth the blurry input and multiscale guidance, the icDPM model can better\nunderstand the blur and recover the clean image. We evaluate a single-dataset\ntrained model on diverse datasets and demonstrate more robust deblurring\nresults with fewer artifacts on unseen data. Our method outperforms existing\nbaselines, achieving state-of-the-art perceptual quality while keeping\ncompetitive distortion metrics.",
        "authors": [
            "Mengwei Ren",
            "Mauricio Delbracio",
            "Hossein Talebi",
            "Guido Gerig",
            "Peyman Milanfar"
        ]
    },
    {
        "title": "CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network",
        "url": "http://arxiv.org/abs/2303.16874",
        "abstract": "Estimating the 6-DoF pose of a rigid object from a single RGB image is a\ncrucial yet challenging task. Recent studies have shown the great potential of\ndense correspondence-based solutions, yet improvements are still needed to\nreach practical deployment. In this paper, we propose a novel pose estimation\nalgorithm named CheckerPose, which improves on three main aspects. Firstly,\nCheckerPose densely samples 3D keypoints from the surface of the 3D object and\nfinds their 2D correspondences progressively in the 2D image. Compared to\nprevious solutions that conduct dense sampling in the image space, our strategy\nenables the correspondence searching in a 2D grid (i.e., pixel coordinate).\nSecondly, for our 3D-to-2D correspondence, we design a compact binary code\nrepresentation for 2D image locations. This representation not only allows for\nprogressive correspondence refinement but also converts the correspondence\nregression to a more efficient classification problem. Thirdly, we adopt a\ngraph neural network to explicitly model the interactions among the sampled 3D\nkeypoints, further boosting the reliability and accuracy of the\ncorrespondences. Together, these novel components make CheckerPose a strong\npose estimation algorithm. When evaluated on the popular Linemod, Linemod-O,\nand YCB-V object pose estimation benchmarks, CheckerPose clearly boosts the\naccuracy of correspondence-based methods and achieves state-of-the-art\nperformances. Code is available at https://github.com/RuyiLian/CheckerPose.",
        "authors": [
            "Ruyi Lian",
            "Haibin Ling"
        ]
    },
    {
        "title": "ASIC: Aligning Sparse in-the-wild Image Collections",
        "url": "http://arxiv.org/abs/2303.16201",
        "abstract": "We present a method for joint alignment of sparse in-the-wild image\ncollections of an object category. Most prior works assume either ground-truth\nkeypoint annotations or a large dataset of images of a single object category.\nHowever, neither of the above assumptions hold true for the long-tail of the\nobjects present in the world. We present a self-supervised technique that\ndirectly optimizes on a sparse collection of images of a particular\nobject/object category to obtain consistent dense correspondences across the\ncollection. We use pairwise nearest neighbors obtained from deep features of a\npre-trained vision transformer (ViT) model as noisy and sparse keypoint matches\nand make them dense and accurate matches by optimizing a neural network that\njointly maps the image collection into a learned canonical grid. Experiments on\nCUB and SPair-71k benchmarks demonstrate that our method can produce globally\nconsistent and higher quality correspondences across the image collection when\ncompared to existing self-supervised methods. Code and other material will be\nmade available at \\url{https://kampta.github.io/asic}.",
        "authors": [
            "Kamal Gupta",
            "Varun Jampani",
            "Carlos Esteves",
            "Abhinav Shrivastava",
            "Ameesh Makadia",
            "Noah Snavely",
            "Abhishek Kar"
        ]
    },
    {
        "title": "Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation",
        "url": "http://arxiv.org/abs/2211.14512",
        "abstract": "Semantic segmentation models classify pixels into a set of known\n(``in-distribution'') visual classes. When deployed in an open world, the\nreliability of these models depends on their ability not only to classify\nin-distribution pixels but also to detect out-of-distribution (OoD) pixels.\nHistorically, the poor OoD detection performance of these models has motivated\nthe design of methods based on model re-training using synthetic training\nimages that include OoD visual objects. Although successful, these re-trained\nmethods have two issues: 1) their in-distribution segmentation accuracy may\ndrop during re-training, and 2) their OoD detection accuracy does not\ngeneralise well to new contexts (e.g., country surroundings) outside the\ntraining set (e.g., city surroundings). In this paper, we mitigate these issues\nwith: (i) a new residual pattern learning (RPL) module that assists the\nsegmentation model to detect OoD pixels without affecting the inlier\nsegmentation performance; and (ii) a novel context-robust contrastive learning\n(CoroCL) that enforces RPL to robustly detect OoD pixels among various\ncontexts. Our approach improves by around 10\\% FPR and 7\\% AuPRC the previous\nstate-of-the-art in Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly\ndatasets. Our code is available at: https://github.com/yyliu01/RPL.",
        "authors": [
            "Yuyuan Liu",
            "Choubo Ding",
            "Yu Tian",
            "Guansong Pang",
            "Vasileios Belagiannis",
            "Ian Reid",
            "Gustavo Carneiro"
        ]
    },
    {
        "title": "Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning",
        "url": "http://arxiv.org/abs/2308.04016",
        "abstract": "Compositional zero-shot learning (CZSL) aims to recognize unseen compositions\nwith prior knowledge of known primitives (attribute and object). Previous works\nfor CZSL often suffer from grasping the contextuality between attribute and\nobject, as well as the discriminability of visual features, and the long-tailed\ndistribution of real-world compositional data. We propose a simple and scalable\nframework called Composition Transformer (CoT) to address these issues. CoT\nemploys object and attribute experts in distinctive manners to generate\nrepresentative embeddings, using the visual network hierarchically. The object\nexpert extracts representative object embeddings from the final layer in a\nbottom-up manner, while the attribute expert makes attribute embeddings in a\ntop-down manner with a proposed object-guided attention module that models\ncontextuality explicitly. To remedy biased prediction caused by imbalanced data\ndistribution, we develop a simple minority attribute augmentation (MAA) that\nsynthesizes virtual samples by mixing two images and oversampling minority\nattribute classes. Our method achieves SoTA performance on several benchmarks,\nincluding MIT-States, C-GQA, and VAW-CZSL. We also demonstrate the\neffectiveness of CoT in improving visual discrimination and addressing the\nmodel bias from the imbalanced data distribution. The code is available at\nhttps://github.com/HanjaeKim98/CoT.",
        "authors": [
            "Hanjae Kim",
            "Jiyoung Lee",
            "Seongheon Park",
            "Kwanghoon Sohn"
        ]
    },
    {
        "title": "Event Camera Data Pre-training",
        "url": "http://arxiv.org/abs/2301.01928",
        "abstract": "This paper proposes a pre-trained neural network for handling event camera\ndata. Our model is a self-supervised learning framework, and uses paired event\ncamera data and natural RGB images for training.\n  Our method contains three modules connected in a sequence: i) a family of\nevent data augmentations, generating meaningful event images for\nself-supervised training; ii) a conditional masking strategy to sample\ninformative event patches from event images, encouraging our model to capture\nthe spatial layout of a scene and accelerating training; iii) a contrastive\nlearning approach, enforcing the similarity of embeddings between matching\nevent images, and between paired event and RGB images. An embedding projection\nloss is proposed to avoid the model collapse when enforcing the event image\nembedding similarities. A probability distribution alignment loss is proposed\nto encourage the event image to be consistent with its paired RGB image in the\nfeature space.\n  Transfer learning performance on downstream tasks shows the superiority of\nour method over state-of-the-art methods. For example, we achieve top-1\naccuracy at 64.83% on the N-ImageNet dataset.",
        "authors": [
            "Yan Yang",
            "Liyuan Pan",
            "Liu Liu"
        ]
    },
    {
        "title": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective",
        "url": "http://arxiv.org/abs/2304.06813",
        "abstract": "Out-of-distribution (OOD) detection aims to identify test examples that do\nnot belong to the training distribution and are thus unlikely to be predicted\nreliably. Despite a plethora of existing works, most of them focused only on\nthe scenario where OOD examples come from semantic shift (e.g., unseen\ncategories), ignoring other possible causes (e.g., covariate shift). In this\npaper, we present a novel, unifying framework to study OOD detection in a\nbroader scope. Instead of detecting OOD examples from a particular cause, we\npropose to detect examples that a deployed machine learning model (e.g., an\nimage classifier) is unable to predict correctly. That is, whether a test\nexample should be detected and rejected or not is ``model-specific''. We show\nthat this framework unifies the detection of OOD examples caused by semantic\nshift and covariate shift, and closely addresses the concern of applying a\nmachine learning model to uncontrolled environments. We provide an extensive\nanalysis that involves a variety of models (e.g., different architectures and\ntraining strategies), sources of OOD examples, and OOD detection approaches,\nand reveal several insights into improving and understanding OOD detection in\nuncontrolled environments.",
        "authors": [
            "Reza Averly",
            "Wei-Lun Chao"
        ]
    },
    {
        "title": "One-shot Implicit Animatable Avatars with Model-based Priors",
        "url": "http://arxiv.org/abs/2212.02469",
        "abstract": "Existing neural rendering methods for creating human avatars typically either\nrequire dense input signals such as video or multi-view images, or leverage a\nlearned prior from large-scale specific 3D human datasets such that\nreconstruction can be performed with sparse-view inputs. Most of these methods\nfail to achieve realistic reconstruction when only a single image is available.\nTo enable the data-efficient creation of realistic animatable 3D humans, we\npropose ELICIT, a novel method for learning human-specific neural radiance\nfields from a single image. Inspired by the fact that humans can effortlessly\nestimate the body geometry and imagine full-body clothing from a single image,\nwe leverage two priors in ELICIT: 3D geometry prior and visual semantic prior.\nSpecifically, ELICIT utilizes the 3D body shape geometry prior from a skinned\nvertex-based template model (i.e., SMPL) and implements the visual clothing\nsemantic prior with the CLIP-based pretrained models. Both priors are used to\njointly guide the optimization for creating plausible content in the invisible\nareas. Taking advantage of the CLIP models, ELICIT can use text descriptions to\ngenerate text-conditioned unseen regions. In order to further improve visual\ndetails, we propose a segmentation-based sampling strategy that locally refines\ndifferent parts of the avatar. Comprehensive evaluations on multiple popular\nbenchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT\nhas outperformed strong baseline methods of avatar creation when only a single\nimage is available. The code is public for research purposes at\nhttps://huangyangyi.github.io/ELICIT/.",
        "authors": [
            "Yangyi Huang",
            "Hongwei Yi",
            "Weiyang Liu",
            "Haofan Wang",
            "Boxi Wu",
            "Wenxiao Wang",
            "Binbin Lin",
            "Debing Zhang",
            "Deng Cai"
        ]
    },
    {
        "title": "MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.04829",
        "abstract": "Recently, semantic segmentation models trained with image-level text\nsupervision have shown promising results in challenging open-world scenarios.\nHowever, these models still face difficulties in learning fine-grained semantic\nalignment at the pixel level and predicting accurate object masks. To address\nthis issue, we propose MixReorg, a novel and straightforward pre-training\nparadigm for semantic segmentation that enhances a model's ability to\nreorganize patches mixed across images, exploring both local visual relevance\nand global semantic coherence. Our approach involves generating fine-grained\npatch-text pairs data by mixing image patches while preserving the\ncorrespondence between patches and text. The model is then trained to minimize\nthe segmentation loss of the mixed images and the two contrastive losses of the\noriginal and restored features. With MixReorg as a mask learner, conventional\ntext-supervised semantic segmentation models can achieve highly generalizable\npixel-semantic alignment ability, which is crucial for open-world segmentation.\nAfter training with large-scale image-text data, MixReorg models can be applied\ndirectly to segment visual objects of arbitrary categories, without the need\nfor further fine-tuning. Our proposed framework demonstrates strong performance\non popular zero-shot semantic segmentation benchmarks, outperforming GroupViT\nby significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012,\nPASCAL Context, MS COCO, and ADE20K, respectively.",
        "authors": [
            "Kaixin Cai",
            "Pengzhen Ren",
            "Yi Zhu",
            "Hang Xu",
            "Jianzhuang Liu",
            "Changlin Li",
            "Guangrun Wang",
            "Xiaodan Liang"
        ]
    },
    {
        "title": "Under-Display Camera Image Restoration with Scattering Effect",
        "url": "http://arxiv.org/abs/2308.04163",
        "abstract": "The under-display camera (UDC) provides consumers with a full-screen visual\nexperience without any obstruction due to notches or punched holes. However,\nthe semi-transparent nature of the display inevitably introduces the severe\ndegradation into UDC images. In this work, we address the UDC image restoration\nproblem with the specific consideration of the scattering effect caused by the\ndisplay. We explicitly model the scattering effect by treating the display as a\npiece of homogeneous scattering medium. With the physical model of the\nscattering effect, we improve the image formation pipeline for the image\nsynthesis to construct a realistic UDC dataset with ground truths. To suppress\nthe scattering effect for the eventual UDC image recovery, a two-branch\nrestoration network is designed. More specifically, the scattering branch\nleverages global modeling capabilities of the channel-wise self-attention to\nestimate parameters of the scattering effect from degraded images. While the\nimage branch exploits the local representation advantage of CNN to recover\nclear scenes, implicitly guided by the scattering branch. Extensive experiments\nare conducted on both real-world and synthesized data, demonstrating the\nsuperiority of the proposed method over the state-of-the-art UDC restoration\ntechniques. The source code and dataset are available at\n\\url{https://github.com/NamecantbeNULL/SRUDC}.",
        "authors": [
            "Binbin Song",
            "Xiangyu Chen",
            "Shuning Xu",
            "Jiantao Zhou"
        ]
    },
    {
        "title": "PRANC: Pseudo RAndom Networks for Compacting Deep Models",
        "url": "http://arxiv.org/abs/2206.08464",
        "abstract": "We demonstrate that a deep model can be reparametrized as a linear\ncombination of several randomly initialized and frozen deep models in the\nweight space. During training, we seek local minima that reside within the\nsubspace spanned by these random models (i.e., `basis' networks). Our\nframework, PRANC, enables significant compaction of a deep model. The model can\nbe reconstructed using a single scalar `seed,' employed to generate the\npseudo-random `basis' networks, together with the learned linear mixture\ncoefficients.\n  In practical applications, PRANC addresses the challenge of efficiently\nstoring and communicating deep models, a common bottleneck in several\nscenarios, including multi-agent learning, continual learners, federated\nsystems, and edge devices, among others. In this study, we employ PRANC to\ncondense image classification models and compress images by compacting their\nassociated implicit neural networks. PRANC outperforms baselines with a large\nmargin on image classification when compressing a deep model almost $100$\ntimes. Moreover, we show that PRANC enables memory-efficient inference by\ngenerating layer-wise weights on the fly. The source code of PRANC is here:\n\\url{https://github.com/UCDvision/PRANC}",
        "authors": [
            "Parsa Nooralinejad",
            "Ali Abbasi",
            "Soroush Abbasi Koohpayegani",
            "Kossar Pourahmadi Meibodi",
            "Rana Muhammad Shahroz Khan",
            "Soheil Kolouri",
            "Hamed Pirsiavash"
        ]
    },
    {
        "title": "Clutter Detection and Removal in 3D Scenes with View-Consistent Inpainting",
        "url": "http://arxiv.org/abs/2304.03763",
        "abstract": "Removing clutter from scenes is essential in many applications, ranging from\nprivacy-concerned content filtering to data augmentation. In this work, we\npresent an automatic system that removes clutter from 3D scenes and inpaints\nwith coherent geometry and texture. We propose techniques for its two key\ncomponents: 3D segmentation from shared properties and 3D inpainting, both of\nwhich are important problems. The definition of 3D scene clutter\n(frequently-moving objects) is not well captured by commonly-studied object\ncategories in computer vision. To tackle the lack of well-defined clutter\nannotations, we group noisy fine-grained labels, leverage virtual rendering,\nand impose an instance-level area-sensitive loss. Once clutter is removed, we\ninpaint geometry and texture in the resulting holes by merging inpainted RGB-D\nimages. This requires novel voting and pruning strategies that guarantee\nmulti-view consistency across individually inpainted images for mesh\nreconstruction. Experiments on ScanNet and Matterport dataset show that our\nmethod outperforms baselines for clutter segmentation and 3D inpainting, both\nvisually and quantitatively.",
        "authors": [
            "Fangyin Wei",
            "Thomas Funkhouser",
            "Szymon Rusinkiewicz"
        ]
    },
    {
        "title": "PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning",
        "url": "http://arxiv.org/abs/2211.11682",
        "abstract": "Large-scale pre-trained models have shown promising open-world performance\nfor both vision and language tasks. However, their transferred capacity on 3D\npoint clouds is still limited and only constrained to the classification task.\nIn this paper, we first collaborate CLIP and GPT to be a unified 3D open-world\nlearner, named as PointCLIP V2, which fully unleashes their potential for\nzero-shot 3D classification, segmentation, and detection. To better align 3D\ndata with the pre-trained language knowledge, PointCLIP V2 contains two key\ndesigns. For the visual end, we prompt CLIP via a shape projection module to\ngenerate more realistic depth maps, narrowing the domain gap between projected\npoint clouds with natural images. For the textual end, we prompt the GPT model\nto generate 3D-specific text as the input of CLIP's textual encoder. Without\nany training in 3D domains, our approach significantly surpasses PointCLIP by\n+42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D\nclassification. On top of that, V2 can be extended to few-shot 3D\nclassification, zero-shot 3D part segmentation, and 3D object detection in a\nsimple manner, demonstrating our generalization ability for unified 3D\nopen-world learning.",
        "authors": [
            "Xiangyang Zhu",
            "Renrui Zhang",
            "Bowei He",
            "Ziyu Guo",
            "Ziyao Zeng",
            "Zipeng Qin",
            "Shanghang Zhang",
            "Peng Gao"
        ]
    },
    {
        "title": "VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation",
        "url": "http://arxiv.org/abs/2303.08340",
        "abstract": "We introduce VideoFlow, a novel optical flow estimation framework for videos.\nIn contrast to previous methods that learn to estimate optical flow from two\nframes, VideoFlow concurrently estimates bi-directional optical flows for\nmultiple frames that are available in videos by sufficiently exploiting\ntemporal cues. We first propose a TRi-frame Optical Flow (TROF) module that\nestimates bi-directional optical flows for the center frame in a three-frame\nmanner. The information of the frame triplet is iteratively fused onto the\ncenter frame. To extend TROF for handling more frames, we further propose a\nMOtion Propagation (MOP) module that bridges multiple TROFs and propagates\nmotion features between adjacent TROFs. With the iterative flow estimation\nrefinement, the information fused in individual TROFs can be propagated into\nthe whole sequence via MOP. By effectively exploiting video information,\nVideoFlow presents extraordinary performance, ranking 1st on all public\nbenchmarks. On the Sintel benchmark, VideoFlow achieves 1.649 and 0.991 average\nend-point-error (AEPE) on the final and clean passes, a 15.1% and 7.6% error\nreduction from the best-published results (1.943 and 1.073 from FlowFormer++).\nOn the KITTI-2015 benchmark, VideoFlow achieves an F1-all error of 3.65%, a\n19.2% error reduction from the best-published result (4.52% from FlowFormer++).\nCode is released at \\url{https://github.com/XiaoyuShi97/VideoFlow}.",
        "authors": [
            "Xiaoyu Shi",
            "Zhaoyang Huang",
            "Weikang Bian",
            "Dasong Li",
            "Manyuan Zhang",
            "Ka Chun Cheung",
            "Simon See",
            "Hongwei Qin",
            "Jifeng Dai",
            "Hongsheng Li"
        ]
    },
    {
        "title": "Identification of Systematic Errors of Image Classifiers on Rare Subgroups",
        "url": "http://arxiv.org/abs/2303.05072",
        "abstract": "Despite excellent average-case performance of many image classifiers, their\nperformance can substantially deteriorate on semantically coherent subgroups of\nthe data that were under-represented in the training data. These systematic\nerrors can impact both fairness for demographic minority groups as well as\nrobustness and safety under domain shift. A major challenge is to identify such\nsubgroups with subpar performance when the subgroups are not annotated and\ntheir occurrence is very rare. We leverage recent advances in text-to-image\nmodels and search in the space of textual descriptions of subgroups (\"prompts\")\nfor subgroups where the target model has low performance on the\nprompt-conditioned synthesized data. To tackle the exponentially growing number\nof subgroups, we employ combinatorial testing. We denote this procedure as\nPromptAttack as it can be interpreted as an adversarial attack in a prompt\nspace. We study subgroup coverage and identifiability with PromptAttack in a\ncontrolled setting and find that it identifies systematic errors with high\naccuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identify\nnovel systematic errors on rare subgroups.",
        "authors": [
            "Jan Hendrik Metzen",
            "Robin Hutmacher",
            "N. Grace Hua",
            "Valentyn Boreiko",
            "Dan Zhang"
        ]
    },
    {
        "title": "Hierarchical Spatio-Temporal Representation Learning for Gait Recognition",
        "url": "http://arxiv.org/abs/2307.09856",
        "abstract": "Gait recognition is a biometric technique that identifies individuals by\ntheir unique walking styles, which is suitable for unconstrained environments\nand has a wide range of applications. While current methods focus on exploiting\nbody part-based representations, they often neglect the hierarchical\ndependencies between local motion patterns. In this paper, we propose a\nhierarchical spatio-temporal representation learning (HSTL) framework for\nextracting gait features from coarse to fine. Our framework starts with a\nhierarchical clustering analysis to recover multi-level body structures from\nthe whole body to local details. Next, an adaptive region-based motion\nextractor (ARME) is designed to learn region-independent motion features. The\nproposed HSTL then stacks multiple ARMEs in a top-down manner, with each ARME\ncorresponding to a specific partition level of the hierarchy. An adaptive\nspatio-temporal pooling (ASTP) module is used to capture gait features at\ndifferent levels of detail to perform hierarchical feature mapping. Finally, a\nframe-level temporal aggregation (FTA) module is employed to reduce redundant\ninformation in gait sequences through multi-scale temporal downsampling.\nExtensive experiments on CASIA-B, OUMVLP, GREW, and Gait3D datasets demonstrate\nthat our method outperforms the state-of-the-art while maintaining a reasonable\nbalance between model accuracy and complexity.",
        "authors": [
            "Lei Wang",
            "Bo Liu",
            "Fangfang Liang",
            "Bincheng Wang"
        ]
    },
    {
        "title": "NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation",
        "url": "http://arxiv.org/abs/2304.11342",
        "abstract": "3D representation disentanglement aims to identify, decompose, and manipulate\nthe underlying explanatory factors of 3D data, which helps AI fundamentally\nunderstand our 3D world. This task is currently under-explored and poses great\nchallenges: (i) the 3D representations are complex and in general contains much\nmore information than 2D image; (ii) many 3D representations are not well\nsuited for gradient-based optimization, let alone disentanglement. To address\nthese challenges, we use NeRF as a differentiable 3D representation, and\nintroduce a self-supervised Navigation to identify interpretable semantic\ndirections in the latent space. To our best knowledge, this novel method,\ndubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement\nwithout any priors or supervisions. Specifically, NaviNeRF is built upon the\ngenerative NeRF pipeline, and equipped with an Outer Navigation Branch and an\nInner Refinement Branch. They are complementary -- the outer navigation is to\nidentify global-view semantic directions, and the inner refinement dedicates to\nfine-grained attributes. A synergistic loss is further devised to coordinate\ntwo branches. Extensive experiments demonstrate that NaviNeRF has a superior\nfine-grained 3D disentanglement ability than the previous 3D-aware models. Its\nperformance is also comparable to editing-oriented models relying on semantic\nor geometry priors.",
        "authors": [
            "Baao Xie",
            "Bohan Li",
            "Zequn Zhang",
            "Junting Dong",
            "Xin Jin",
            "Jingyu Yang",
            "Wenjun Zeng"
        ]
    },
    {
        "title": "CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification",
        "url": "http://arxiv.org/abs/2307.16634",
        "abstract": "This paper presents a CLIP-based unsupervised learning method for\nannotation-free multi-label image classification, including three stages:\ninitialization, training, and inference. At the initialization stage, we take\nfull advantage of the powerful CLIP model and propose a novel approach to\nextend CLIP for multi-label predictions based on global-local image-text\nsimilarity aggregation. To be more specific, we split each image into snippets\nand leverage CLIP to generate the similarity vector for the whole image\n(global) as well as each snippet (local). Then a similarity aggregator is\nintroduced to leverage the global and local similarity vectors. Using the\naggregated similarity scores as the initial pseudo labels at the training\nstage, we propose an optimization framework to train the parameters of the\nclassification network and refine pseudo labels for unobserved labels. During\ninference, only the classification network is used to predict the labels of the\ninput image. Extensive experiments show that our method outperforms\nstate-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC\n2012, and NUS datasets and even achieves comparable results to weakly\nsupervised classification methods.",
        "authors": [
            "Rabab Abdelfattah",
            "Qing Guo",
            "Xiaoguang Li",
            "Xiaofeng Wang",
            "Song Wang"
        ]
    },
    {
        "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
        "url": "http://arxiv.org/abs/2303.16203",
        "abstract": "The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification, which we call Diffusion Classifier,\nattains strong results on a variety of benchmarks and outperforms alternative\nmethods of extracting knowledge from diffusion models. Although a gap remains\nbetween generative and discriminative approaches on zero-shot recognition\ntasks, our diffusion-based approach has significantly stronger multimodal\ncompositional reasoning ability than competing discriminative approaches.\nFinally, we use Diffusion Classifier to extract standard classifiers from\nclass-conditional diffusion models trained on ImageNet. Our models achieve\nstrong classification performance using only weak augmentations and exhibit\nqualitatively better \"effective robustness\" to distribution shift. Overall, our\nresults are a step toward using generative over discriminative models for\ndownstream tasks. Results and visualizations at\nhttps://diffusion-classifier.github.io/",
        "authors": [
            "Alexander C. Li",
            "Mihir Prabhudesai",
            "Shivam Duggal",
            "Ellis Brown",
            "Deepak Pathak"
        ]
    },
    {
        "title": "Backpropagation Path Search On Adversarial Transferability",
        "url": "http://arxiv.org/abs/2308.07625",
        "abstract": "Deep neural networks are vulnerable to adversarial examples, dictating the\nimperativeness to test the model's robustness before deployment. Transfer-based\nattackers craft adversarial examples against surrogate models and transfer them\nto victim models deployed in the black-box situation. To enhance the\nadversarial transferability, structure-based attackers adjust the\nbackpropagation path to avoid the attack from overfitting the surrogate model.\nHowever, existing structure-based attackers fail to explore the convolution\nmodule in CNNs and modify the backpropagation graph heuristically, leading to\nlimited effectiveness. In this paper, we propose backPropagation pAth Search\n(PAS), solving the aforementioned two problems. We first propose SkipConv to\nadjust the backpropagation path of convolution by structural\nreparameterization. To overcome the drawback of heuristically designed\nbackpropagation paths, we further construct a DAG-based search space, utilize\none-step approximation for path evaluation and employ Bayesian Optimization to\nsearch for the optimal path. We conduct comprehensive experiments in a wide\nrange of transfer settings, showing that PAS improves the attack success rate\nby a huge margin for both normally trained and defense models.",
        "authors": [
            "Zhuoer Xu",
            "Zhangxuan Gu",
            "Jianping Zhang",
            "Shiwen Cui",
            "Changhua Meng",
            "Weiqiang Wang"
        ]
    },
    {
        "title": "Image-Free Classifier Injection for Zero-Shot Classification",
        "url": "http://arxiv.org/abs/2308.10599",
        "abstract": "Zero-shot learning models achieve remarkable results on image classification\nfor samples from classes that were not seen during training. However, such\nmodels must be trained from scratch with specialised methods: therefore, access\nto a training dataset is required when the need for zero-shot classification\narises. In this paper, we aim to equip pre-trained models with zero-shot\nclassification capabilities without the use of image data. We achieve this with\nour proposed Image-free Classifier Injection with Semantics (ICIS) that injects\nclassifiers for new, unseen classes into pre-trained classification models in a\npost-hoc fashion without relying on image data. Instead, the existing\nclassifier weights and simple class-wise descriptors, such as class names or\nattributes, are used. ICIS has two encoder-decoder networks that learn to\nreconstruct classifier weights from descriptors (and vice versa), exploiting\n(cross-)reconstruction and cosine losses to regularise the decoding process.\nNotably, ICIS can be cheaply trained and applied directly on top of pre-trained\nclassification models. Experiments on benchmark ZSL datasets show that ICIS\nproduces unseen classifier weights that achieve strong (generalised) zero-shot\nclassification performance. Code is available at\nhttps://github.com/ExplainableML/ImageFreeZSL .",
        "authors": [
            "Anders Christensen",
            "Massimiliano Mancini",
            "A. Sophia Koepke",
            "Ole Winther",
            "Zeynep Akata"
        ]
    },
    {
        "title": "CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No",
        "url": "http://arxiv.org/abs/2308.12213",
        "abstract": "Out-of-distribution (OOD) detection refers to training the model on an\nin-distribution (ID) dataset to classify whether the input images come from\nunknown classes. Considerable effort has been invested in designing various OOD\ndetection methods based on either convolutional neural networks or\ntransformers. However, zero-shot OOD detection methods driven by CLIP, which\nonly require class names for ID, have received less attention. This paper\npresents a novel method, namely CLIP saying no (CLIPN), which empowers the\nlogic of saying no within CLIP. Our key motivation is to equip CLIP with the\ncapability of distinguishing OOD and ID samples using positive-semantic prompts\nand negation-semantic prompts. Specifically, we design a novel learnable no\nprompt and a no text encoder to capture negation semantics within images.\nSubsequently, we introduce two loss functions: the image-text binary-opposite\nloss and the text semantic-opposite loss, which we use to teach CLIPN to\nassociate images with no prompts, thereby enabling it to identify unknown\nsamples. Furthermore, we propose two threshold-free inference algorithms to\nperform OOD detection by utilizing negation semantics from no prompts and the\ntext encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6\nOOD datasets) for the OOD detection task demonstrate that CLIPN, based on\nViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in\nterms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN\ncan serve as a solid foundation for effectively leveraging CLIP in downstream\nOOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.",
        "authors": [
            "Hualiang Wang",
            "Yi Li",
            "Huifeng Yao",
            "Xiaomeng Li"
        ]
    },
    {
        "title": "Semantically Structured Image Compression via Irregular Group-Based Decoupling",
        "url": "http://arxiv.org/abs/2305.02586",
        "abstract": "Image compression techniques typically focus on compressing rectangular\nimages for human consumption, however, resulting in transmitting redundant\ncontent for downstream applications. To overcome this limitation, some previous\nworks propose to semantically structure the bitstream, which can meet specific\napplication requirements by selective transmission and reconstruction.\nNevertheless, they divide the input image into multiple rectangular regions\naccording to semantics and ignore avoiding information interaction among them,\ncausing waste of bitrate and distorted reconstruction of region boundaries. In\nthis paper, we propose to decouple an image into multiple groups with irregular\nshapes based on a customized group mask and compress them independently. Our\ngroup mask describes the image at a finer granularity, enabling significant\nbitrate saving by reducing the transmission of redundant content. Moreover, to\nensure the fidelity of selective reconstruction, this paper proposes the\nconcept of group-independent transform that maintain the independence among\ndistinct groups. And we instantiate it by the proposed Group-Independent\nSwin-Block (GI Swin-Block). Experimental results demonstrate that our framework\nstructures the bitstream with negligible cost, and exhibits superior\nperformance on both visual quality and intelligent task supporting.",
        "authors": [
            "Ruoyu Feng",
            "Yixin Gao",
            "Xin Jin",
            "Runsen Feng",
            "Zhibo Chen"
        ]
    },
    {
        "title": "Unsupervised Object Localization with Representer Point Selection",
        "url": "http://arxiv.org/abs/2309.04172",
        "abstract": "We propose a novel unsupervised object localization method that allows us to\nexplain the predictions of the model by utilizing self-supervised pre-trained\nmodels without additional finetuning. Existing unsupervised and self-supervised\nobject localization methods often utilize class-agnostic activation maps or\nself-similarity maps of a pre-trained model. Although these maps can offer\nvaluable information for localization, their limited ability to explain how the\nmodel makes predictions remains challenging. In this paper, we propose a simple\nyet effective unsupervised object localization method based on representer\npoint selection, where the predictions of the model can be represented as a\nlinear combination of representer values of training points. By selecting\nrepresenter points, which are the most important examples for the model\npredictions, our model can provide insights into how the model predicts the\nforeground object by providing relevant examples as well as their importance.\nOur method outperforms the state-of-the-art unsupervised and self-supervised\nobject localization methods on various datasets with significant margins and\neven outperforms recent weakly supervised and few-shot methods.",
        "authors": [
            "Yeonghwan Song",
            "Seokwoo Jang",
            "Dina Katabi",
            "Jeany Son"
        ]
    },
    {
        "title": "SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics",
        "url": "http://arxiv.org/abs/2309.10972",
        "abstract": "Accurately determining salient regions of an image is challenging when\nlabeled data is scarce. DINO-based self-supervised approaches have recently\nleveraged meaningful image semantics captured by patch-wise features for\nlocating foreground objects. Recent methods have also incorporated intuitive\npriors and demonstrated value in unsupervised methods for object partitioning.\nIn this paper, we propose SEMPART, which jointly infers coarse and fine\nbi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART\npreserves fine boundary details using graph-driven regularization and\nsuccessfully distills the coarse mask semantics into the fine mask. Our salient\nobject detection and single object localization findings suggest that SEMPART\nproduces high-quality masks rapidly without additional post-processing and\nbenefits from co-optimizing the coarse and fine branches.",
        "authors": [
            "Sriram Ravindran",
            "Debraj Basu"
        ]
    },
    {
        "title": "Flatness-Aware Minimization for Domain Generalization",
        "url": "http://arxiv.org/abs/2307.11108",
        "abstract": "Domain generalization (DG) seeks to learn robust models that generalize well\nunder unknown distribution shifts. As a critical aspect of DG, optimizer\nselection has not been explored in depth. Currently, most DG methods follow the\nwidely used benchmark, DomainBed, and utilize Adam as the default optimizer for\nall datasets. However, we reveal that Adam is not necessarily the optimal\nchoice for the majority of current DG methods and datasets. Based on the\nperspective of loss landscape flatness, we propose a novel approach,\nFlatness-Aware Minimization for Domain Generalization (FAD), which can\nefficiently optimize both zeroth-order and first-order flatness simultaneously\nfor DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD)\ngeneralization error and convergence. Our experimental results demonstrate the\nsuperiority of FAD on various DG datasets. Additionally, we confirm that FAD is\ncapable of discovering flatter optima in comparison to other zeroth-order and\nfirst-order flatness-aware optimization methods.",
        "authors": [
            "Xingxuan Zhang",
            "Renzhe Xu",
            "Han Yu",
            "Yancheng Dong",
            "Pengfei Tian",
            "Peng Cu"
        ]
    },
    {
        "title": "ProtoFL: Unsupervised Federated Learning via Prototypical Distillation",
        "url": "http://arxiv.org/abs/2307.12450",
        "abstract": "Federated learning (FL) is a promising approach for enhancing data privacy\npreservation, particularly for authentication systems. However, limited round\ncommunications, scarce representation, and scalability pose significant\nchallenges to its deployment, hindering its full potential. In this paper, we\npropose 'ProtoFL', Prototypical Representation Distillation based unsupervised\nFederated Learning to enhance the representation power of a global model and\nreduce round communication costs. Additionally, we introduce a local one-class\nclassifier based on normalizing flows to improve performance with limited data.\nOur study represents the first investigation of using FL to improve one-class\nclassification performance. We conduct extensive experiments on five widely\nused benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and\nKeystroke-Dynamics, to demonstrate the superior performance of our proposed\nframework over previous methods in the literature.",
        "authors": [
            "Hansol Kim",
            "Youngjun Kwak",
            "Minyoung Jung",
            "Jinho Shin",
            "Youngsung Kim",
            "Changick Kim"
        ]
    },
    {
        "title": "Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation",
        "url": "http://arxiv.org/abs/2303.10451",
        "abstract": "For video models to be transferred and applied seamlessly across video tasks\nin varied environments, Video Unsupervised Domain Adaptation (VUDA) has been\nintroduced to improve the robustness and transferability of video models.\nHowever, current VUDA methods rely on a vast amount of high-quality unlabeled\ntarget data, which may not be available in real-world cases. We thus consider a\nmore realistic \\textit{Few-Shot Video-based Domain Adaptation} (FSVDA) scenario\nwhere we adapt video models with only a few target video samples. While a few\nmethods have touched upon Few-Shot Domain Adaptation (FSDA) in images and in\nFSVDA, they rely primarily on spatial augmentation for target domain expansion\nwith alignment performed statistically at the instance level. However, videos\ncontain more knowledge in terms of rich temporal and semantic information,\nwhich should be fully considered while augmenting target domains and performing\nalignment in FSVDA. We propose a novel SSA2lign to address FSVDA at the snippet\nlevel, where the target domain is expanded through a simple snippet-level\naugmentation followed by the attentive alignment of snippets both semantically\nand statistically, where semantic alignment of snippets is conducted through\nmultiple perspectives. Empirical results demonstrate state-of-the-art\nperformance of SSA2lign across multiple cross-domain action recognition\nbenchmarks.",
        "authors": [
            "Yuecong Xu",
            "Jianfei Yang",
            "Yunjiao Zhou",
            "Zhenghua Chen",
            "Min Wu",
            "Xiaoli Li"
        ]
    },
    {
        "title": "Preserving Tumor Volumes for Unsupervised Medical Image Registration",
        "url": "http://arxiv.org/abs/2309.10153",
        "abstract": "Medical image registration is a critical task that estimates the spatial\ncorrespondence between pairs of images. However, current traditional and\ndeep-learning-based methods rely on similarity measures to generate a deforming\nfield, which often results in disproportionate volume changes in dissimilar\nregions, especially in tumor regions. These changes can significantly alter the\ntumor size and underlying anatomy, which limits the practical use of image\nregistration in clinical diagnosis. To address this issue, we have formulated\nimage registration with tumors as a constraint problem that preserves tumor\nvolumes while maximizing image similarity in other normal regions. Our proposed\nstrategy involves a two-stage process. In the first stage, we use\nsimilarity-based registration to identify potential tumor regions by their\nvolume change, generating a soft tumor mask accordingly. In the second stage,\nwe propose a volume-preserving registration with a novel adaptive\nvolume-preserving loss that penalizes the change in size adaptively based on\nthe masks calculated from the previous stage. Our approach balances image\nsimilarity and volume preservation in different regions, i.e., normal and tumor\nregions, by using soft tumor masks to adjust the imposition of\nvolume-preserving loss on each one. This ensures that the tumor volume is\npreserved during the registration process. We have evaluated our strategy on\nvarious datasets and network architectures, demonstrating that our method\nsuccessfully preserves the tumor volume while achieving comparable registration\nresults with state-of-the-art methods. Our codes is available at:\n\\url{https://dddraxxx.github.io/Volume-Preserving-Registration/}.",
        "authors": [
            "Qihua Dong",
            "Hao Du",
            "Ying Song",
            "Yan Xu",
            "Jing Liao"
        ]
    },
    {
        "title": "Multi-label Affordance Mapping from Egocentric Vision",
        "url": "http://arxiv.org/abs/2309.02120",
        "abstract": "Accurate affordance detection and segmentation with pixel precision is an\nimportant piece in many complex systems based on interactions, such as robots\nand assitive devices. We present a new approach to affordance perception which\nenables accurate multi-label segmentation. Our approach can be used to\nautomatically extract grounded affordances from first person videos of\ninteractions using a 3D map of the environment providing pixel level precision\nfor the affordance location. We use this method to build the largest and most\ncomplete dataset on affordances based on the EPIC-Kitchen dataset, EPIC-Aff,\nwhich provides interaction-grounded, multi-label, metric and spatial affordance\nannotations. Then, we propose a new approach to affordance segmentation based\non multi-label detection which enables multiple affordances to co-exists in the\nsame space, for example if they are associated with the same object. We present\nseveral strategies of multi-label detection using several segmentation\narchitectures. The experimental results highlight the importance of the\nmulti-label detection. Finally, we show how our metric representation can be\nexploited for build a map of interaction hotspots in spatial action-centric\nzones and use that representation to perform a task-oriented navigation.",
        "authors": [
            "Lorenzo Mur-Labadia",
            "Jose J. Guerrero",
            "Ruben Martinez-Cantin"
        ]
    },
    {
        "title": "Unified Adversarial Patch for Cross-Modal Attacks in the Physical World",
        "url": "http://arxiv.org/abs/2307.07859",
        "abstract": "Recently, physical adversarial attacks have been presented to evade\nDNNs-based object detectors. To ensure the security, many scenarios are\nsimultaneously deployed with visible sensors and infrared sensors, leading to\nthe failures of these single-modal physical attacks. To show the potential\nrisks under such scenes, we propose a unified adversarial patch to perform\ncross-modal physical attacks, i.e., fooling visible and infrared object\ndetectors at the same time via a single patch. Considering different imaging\nmechanisms of visible and infrared sensors, our work focuses on modeling the\nshapes of adversarial patches, which can be captured in different modalities\nwhen they change. To this end, we design a novel boundary-limited shape\noptimization to achieve the compact and smooth shapes, and thus they can be\neasily implemented in the physical world. In addition, to balance the fooling\ndegree between visible detector and infrared detector during the optimization\nprocess, we propose a score-aware iterative evaluation, which can guide the\nadversarial patch to iteratively reduce the predicted scores of the multi-modal\nsensors. We finally test our method against the one-stage detector: YOLOv3 and\nthe two-stage detector: Faster RCNN. Results show that our unified patch\nachieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More\nimportantly, we verify the effective attacks in the physical world when visible\nand infrared sensors shoot the objects under various settings like different\nangles, distances, postures, and scenes.",
        "authors": [
            "Xingxing Wei",
            "Yao Huang",
            "Yitong Sun",
            "Jie Yu"
        ]
    },
    {
        "title": "Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples",
        "url": "http://arxiv.org/abs/2307.10062",
        "abstract": "Deploying deep visual models can lead to performance drops due to the\ndiscrepancies between source and target distributions. Several approaches\nleverage labeled source data to estimate target domain accuracy, but accessing\nlabeled source data is often prohibitively difficult due to data\nconfidentiality or resource limitations on serving devices. Our work proposes a\nnew framework to estimate model accuracy on unlabeled target data without\naccess to source data. We investigate the feasibility of using pseudo-labels\nfor accuracy estimation and evolve this idea into adopting recent advances in\nsource-free domain adaptation algorithms. Our approach measures the\ndisagreement rate between the source hypothesis and the target pseudo-labeling\nfunction, adapted from the source hypothesis. We mitigate the impact of\nerroneous pseudo-labels that may arise due to a high ideal joint hypothesis\nrisk by employing adaptive adversarial perturbation on the input of the target\nmodel. Our proposed source-free framework effectively addresses the challenging\ndistribution shift scenarios and outperforms existing methods requiring source\ndata and labels for training.",
        "authors": [
            "JoonHo Lee",
            "Jae Oh Woo",
            "Hankyu Moon",
            "Kwonho Lee"
        ]
    },
    {
        "title": "SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-Time Performance on Mobile Device",
        "url": "http://arxiv.org/abs/2308.08137",
        "abstract": "With the rapid development of AI hardware accelerators, applying deep\nlearning-based algorithms to solve various low-level vision tasks on mobile\ndevices has gradually become possible. However, two main problems still need to\nbe solved: task-specific algorithms make it difficult to integrate them into a\nsingle neural network architecture, and large amounts of parameters make it\ndifficult to achieve real-time inference. To tackle these problems, we propose\na novel network, SYENet, with only $~$6K parameters, to handle multiple\nlow-level vision tasks on mobile devices in a real-time manner. The SYENet\nconsists of two asymmetrical branches with simple building blocks. To\neffectively connect the results by asymmetrical branches, a Quadratic\nConnection Unit(QCU) is proposed. Furthermore, to improve performance, a new\nOutlier-Aware Loss is proposed to process the image. The proposed method proves\nits superior performance with the best PSNR as compared with other networks in\nreal-time applications such as Image Signal Processing(ISP), Low-Light\nEnhancement(LLE), and Super-Resolution(SR) with 2K60FPS throughput on Qualcomm\n8 Gen 1 mobile SoC(System-on-Chip). Particularly, for ISP task, SYENet got the\nhighest score in MAI 2022 Learned Smartphone ISP challenge.",
        "authors": [
            "Weiran Gou",
            "Ziyao Yi",
            "Yan Xiang",
            "Shaoqing Li",
            "Zibin Liu",
            "Dehui Kong",
            "Ke Xu"
        ]
    },
    {
        "title": "MATE: Masked Autoencoders are Online 3D Test-Time Learners",
        "url": "http://arxiv.org/abs/2211.11432",
        "abstract": "Our MATE is the first Test-Time-Training (TTT) method designed for 3D data,\nwhich makes deep networks trained for point cloud classification robust to\ndistribution shifts occurring in test data. Like existing TTT methods from the\n2D image domain, MATE also leverages test data for adaptation. Its test-time\nobjective is that of a Masked Autoencoder: a large portion of each test point\ncloud is removed before it is fed to the network, tasked with reconstructing\nthe full point cloud. Once the network is updated, it is used to classify the\npoint cloud. We test MATE on several 3D object classification datasets and show\nthat it significantly improves robustness of deep networks to several types of\ncorruptions commonly occurring in 3D point clouds. We show that MATE is very\nefficient in terms of the fraction of points it needs for the adaptation. It\ncan effectively adapt given as few as 5% of tokens of each test sample, making\nit extremely lightweight. Our experiments show that MATE also achieves\ncompetitive performance by adapting sparsely on the test data, which further\nreduces its computational overhead, making it ideal for real-time applications.",
        "authors": [
            "M. Jehanzeb Mirza",
            "Inkyu Shin",
            "Wei Lin",
            "Andreas Schriebl",
            "Kunyang Sun",
            "Jaesung Choe",
            "Horst Possegger",
            "Mateusz Kozinski",
            "In So Kweon",
            "Kun-Jin Yoon",
            "Horst Bischof"
        ]
    },
    {
        "title": "EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment",
        "url": "http://arxiv.org/abs/2309.01151",
        "abstract": "Vision-language models such as CLIP have boosted the performance of\nopen-vocabulary object detection, where the detector is trained on base\ncategories but required to detect novel categories. Existing methods leverage\nCLIP's strong zero-shot recognition ability to align object-level embeddings\nwith textual embeddings of categories. However, we observe that using CLIP for\nobject-level alignment results in overfitting to base categories, i.e., novel\ncategories most similar to base categories have particularly poor performance\nas they are recognized as similar base categories. In this paper, we first\nidentify that the loss of critical fine-grained local image semantics hinders\nexisting methods from attaining strong base-to-novel generalization. Then, we\npropose Early Dense Alignment (EDA) to bridge the gap between generalizable\nlocal semantics and object-level prediction. In EDA, we use object-level\nsupervision to learn the dense-level rather than object-level alignment to\nmaintain the local fine-grained semantics. Extensive experiments demonstrate\nour superior performance to competing approaches under the same strict setting\nand without using external training resources, i.e., improving the +8.4% novel\nbox AP50 on COCO and +3.9% rare mask AP on LVIS.",
        "authors": [
            "Cheng Shi",
            "Sibei Yang"
        ]
    },
    {
        "title": "MixPath: A Unified Approach for One-shot Neural Architecture Search",
        "url": "http://arxiv.org/abs/2001.05887",
        "abstract": "Blending multiple convolutional kernels is proved advantageous in neural\narchitecture design. However, current two-stage neural architecture search\nmethods are mainly limited to single-path search spaces. How to efficiently\nsearch models of multi-path structures remains a difficult problem. In this\npaper, we are motivated to train a one-shot multi-path supernet to accurately\nevaluate the candidate architectures. Specifically, we discover that in the\nstudied search spaces, feature vectors summed from multiple paths are nearly\nmultiples of those from a single path. Such disparity perturbs the supernet\ntraining and its ranking ability. Therefore, we propose a novel mechanism\ncalled Shadow Batch Normalization (SBN) to regularize the disparate feature\nstatistics. Extensive experiments prove that SBNs are capable of stabilizing\nthe optimization and improving ranking performance. We call our unified\nmulti-path one-shot approach as MixPath, which generates a series of models\nthat achieve state-of-the-art results on ImageNet.",
        "authors": [
            "Xiangxiang Chu",
            "Shun Lu",
            "Xudong Li",
            "Bo Zhang"
        ]
    },
    {
        "title": "Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts",
        "url": "http://arxiv.org/abs/2308.11793",
        "abstract": "Cross-scene generalizable NeRF models, which can directly synthesize novel\nviews of unseen scenes, have become a new spotlight of the NeRF field. Several\nexisting attempts rely on increasingly end-to-end \"neuralized\" architectures,\ni.e., replacing scene representation and/or rendering modules with performant\nneural networks such as transformers, and turning novel view synthesis into a\nfeed-forward inference pipeline. While those feedforward \"neuralized\"\narchitectures still do not fit diverse scenes well out of the box, we propose\nto bridge them with the powerful Mixture-of-Experts (MoE) idea from large\nlanguage models (LLMs), which has demonstrated superior generalization ability\nby balancing between larger overall model capacity and flexible per-instance\nspecialization. Starting from a recent generalizable NeRF architecture called\nGNT, we first demonstrate that MoE can be neatly plugged in to enhance the\nmodel. We further customize a shared permanent expert and a geometry-aware\nconsistency loss to enforce cross-scene consistency and spatial smoothness\nrespectively, which are essential for generalizable view synthesis. Our\nproposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has\nexperimentally shown state-of-the-art results when transferring to unseen\nscenes, indicating remarkably better cross-scene generalization in both\nzero-shot and few-shot settings. Our codes are available at\nhttps://github.com/VITA-Group/GNT-MOVE.",
        "authors": [
            "Wenyan Cong",
            "Hanxue Liang",
            "Peihao Wang",
            "Zhiwen Fan",
            "Tianlong Chen",
            "Mukund Varma",
            "Yi Wang",
            "Zhangyang Wang"
        ]
    },
    {
        "title": "Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers",
        "url": "http://arxiv.org/abs/2304.11335",
        "abstract": "Current arbitrary style transfer models are limited to either image or video\ndomains. In order to achieve satisfying image and video style transfers, two\ndifferent models are inevitably required with separate training processes on\nimage and video domains, respectively. In this paper, we show that this can be\nprecluded by introducing UniST, a Unified Style Transfer framework for both\nimages and videos. At the core of UniST is a domain interaction transformer\n(DIT), which first explores context information within the specific domain and\nthen interacts contextualized domain information for joint learning. In\nparticular, DIT enables exploration of temporal information from videos for the\nimage style transfer task and meanwhile allows rich appearance texture from\nimages for video style transfer, thus leading to mutual benefits. Considering\nheavy computation of traditional multi-head self-attention, we present a simple\nyet effective axial multi-head self-attention (AMSA) for DIT, which improves\ncomputational efficiency while maintains style transfer performance. To verify\nthe effectiveness of UniST, we conduct extensive experiments on both image and\nvideo style transfer tasks and show that UniST performs favorably against\nstate-of-the-art approaches on both tasks. Code is available at\nhttps://github.com/NevSNev/UniST.",
        "authors": [
            "Bohai Gu",
            "Heng Fan",
            "Libo Zhang"
        ]
    },
    {
        "title": "Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling",
        "url": "http://arxiv.org/abs/2307.07944",
        "abstract": "Unsupervised domain adaptation (DA) with the aid of pseudo labeling\ntechniques has emerged as a crucial approach for domain-adaptive 3D object\ndetection. While effective, existing DA methods suffer from a substantial drop\nin performance when applied to a multi-class training setting, due to the\nco-existence of low-quality pseudo labels and class imbalance issues. In this\npaper, we address this challenge by proposing a novel ReDB framework tailored\nfor learning to detect all classes at once. Our approach produces Reliable,\nDiverse, and class-Balanced pseudo 3D boxes to iteratively guide the\nself-training on a distributionally different target domain. To alleviate\ndisruptions caused by the environmental discrepancy (e.g., beam numbers), the\nproposed cross-domain examination (CDE) assesses the correctness of pseudo\nlabels by copy-pasting target instances into a source environment and measuring\nthe prediction consistency. To reduce computational overhead and mitigate the\nobject shift (e.g., scales and point densities), we design an overlapped boxes\ncounting (OBC) metric that allows to uniformly downsample pseudo-labeled\nobjects across different geometric characteristics. To confront the issue of\ninter-class imbalance, we progressively augment the target point clouds with a\nclass-balanced set of pseudo-labeled target instances and source objects, which\nboosts recognition accuracies on both frequently appearing and rare classes.\nExperimental results on three benchmark datasets using both voxel-based (i.e.,\nSECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our\nproposed ReDB approach outperforms existing 3D domain adaptation methods by a\nlarge margin, improving 23.15% mAP on the nuScenes $\\rightarrow$ KITTI task.\nThe code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.",
        "authors": [
            "Zhuoxiao Chen",
            "Yadan Luo",
            "Zheng Wang",
            "Mahsa Baktashmotlagh",
            "Zi Huang"
        ]
    },
    {
        "title": "Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models",
        "url": "http://arxiv.org/abs/2303.17169",
        "abstract": "Prompt learning has become one of the most efficient paradigms for adapting\nlarge pre-trained vision-language models to downstream tasks. Current\nstate-of-the-art methods, like CoOp and ProDA, tend to adopt soft prompts to\nlearn an appropriate prompt for each specific task. Recent CoCoOp further\nboosts the base-to-new generalization performance via an image-conditional\nprompt. However, it directly fuses identical image semantics to prompts of\ndifferent labels and significantly weakens the discrimination among different\nclasses as shown in our experiments. Motivated by this observation, we first\npropose a class-aware text prompt (CTP) to enrich generated prompts with\nlabel-related image information. Unlike CoCoOp, CTP can effectively involve\nimage semantics and avoid introducing extra ambiguities into different prompts.\nOn the other hand, instead of reserving the complete image representations, we\npropose text-guided feature tuning (TFT) to make the image branch attend to\nclass-related representation. A contrastive loss is employed to align such\naugmented text and image representations on downstream tasks. In this way, the\nimage-to-text CTP and text-to-image TFT can be mutually promoted to enhance the\nadaptation of VLMs for downstream tasks. Extensive experiments demonstrate that\nour method outperforms the existing methods by a significant margin.\nEspecially, compared to CoCoOp, we achieve an average improvement of 4.03% on\nnew classes and 3.19% on harmonic-mean over eleven classification benchmarks.",
        "authors": [
            "Sifan Long",
            "Zhen Zhao",
            "Junkun Yuan",
            "Zichang Tan",
            "Jiangjiang Liu",
            "Luping Zhou",
            "Shengsheng Wang",
            "Jingdong Wang"
        ]
    },
    {
        "title": "Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory",
        "url": "http://arxiv.org/abs/2309.03696",
        "abstract": "Human Object Interaction (HOI) detection aims to localize and infer the\nrelationships between a human and an object. Arguably, training supervised\nmodels for this task from scratch presents challenges due to the performance\ndrop over rare classes and the high computational cost and time required to\nhandle long-tailed distributions of HOIs in complex HOI scenes in realistic\nsettings. This observation motivates us to design an HOI detector that can be\ntrained even with long-tailed labeled data and can leverage existing knowledge\nfrom pre-trained models. Inspired by the powerful generalization ability of the\nlarge Vision-Language Models (VLM) on classification and retrieval tasks, we\npropose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).\nADA-CM has two operating modes. The first mode makes it tunable without\nlearning new parameters in a training-free paradigm. Its second mode\nincorporates an instance-aware adapter mechanism that can further efficiently\nboost performance if updating a lightweight set of parameters can be afforded.\nOur proposed method achieves competitive results with state-of-the-art on the\nHICO-DET and V-COCO datasets with much less training time. Code can be found at\nhttps://github.com/ltttpku/ADA-CM.",
        "authors": [
            "Ting Lei",
            "Fabian Caba",
            "Qingchao Chen",
            "Hailin Jin",
            "Yuxin Peng",
            "Yang Liu"
        ]
    },
    {
        "title": "NeMF: Inverse Volume Rendering with Neural Microflake Field",
        "url": "http://arxiv.org/abs/2304.00782",
        "abstract": "Recovering the physical attributes of an object's appearance from its images\ncaptured under an unknown illumination is challenging yet essential for\nphoto-realistic rendering. Recent approaches adopt the emerging implicit scene\nrepresentations and have shown impressive results.However, they unanimously\nadopt a surface-based representation,and hence can not well handle scenes with\nvery complex geometry, translucent object and etc. In this paper, we propose to\nconduct inverse volume rendering, in contrast to surface-based, by representing\na scene using microflake volume, which assumes the space is filled with\ninfinite small flakes and light reflects or scatters at each spatial location\naccording to microflake distributions. We further adopt the coordinate networks\nto implicitly encode the microflake volume, and develop a differentiable\nmicroflake volume renderer to train the network in an end-to-end way in\nprinciple.Our NeMF enables effective recovery of appearance attributes for\nhighly complex geometry and scattering object, enables high-quality relighting,\nmaterial editing, and especially simulates volume rendering effects, such as\nscattering, which is infeasible for surface-based approaches.",
        "authors": [
            "Youjia Zhang",
            "Teng Xu",
            "Junqing Yu",
            "Yuteng Ye",
            "Junle Wang",
            "Yanqing Jing",
            "Jingyi Yu",
            "Wei Yang"
        ]
    },
    {
        "title": "Attentive Mask CLIP",
        "url": "http://arxiv.org/abs/2212.08653",
        "abstract": "Image token removal is an efficient augmentation strategy for reducing the\ncost of computing image features. However, this efficient augmentation strategy\nhas been found to adversely affect the accuracy of CLIP-based training. We\nhypothesize that removing a large portion of image tokens may improperly\ndiscard the semantic content associated with a given text description, thus\nconstituting an incorrect pairing target in CLIP training. To address this\nissue, we propose an attentive token removal approach for CLIP training, which\nretains tokens with a high semantic correlation to the text description. The\ncorrelation scores are computed in an online fashion using the EMA version of\nthe visual encoder. Our experiments show that the proposed attentive masking\napproach performs better than the previous method of random token removal for\nCLIP training. The approach also makes it efficient to apply multiple\naugmentation views to the image, as well as introducing instance contrastive\nlearning tasks between these views into the CLIP framework. Compared to other\nCLIP improvements that combine different pre-training targets such as SLIP and\nMaskCLIP, our method is not only more effective, but also much more efficient.\nSpecifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\\%$\ntop-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$\nand $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are\n$+1.1\\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being\n$2.30\\times$ faster. An efficient version of our approach running $1.16\\times$\nfaster than the plain CLIP model achieves significant gains of $+5.3\\%$,\n$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.",
        "authors": [
            "Yifan Yang",
            "Weiquan Huang",
            "Yixuan Wei",
            "Houwen Peng",
            "Xinyang Jiang",
            "Huiqiang Jiang",
            "Fangyun Wei",
            "Yin Wang",
            "Han Hu",
            "Lili Qiu",
            "Yuqing Yang"
        ]
    },
    {
        "title": "DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction",
        "url": "http://arxiv.org/abs/2211.12340",
        "abstract": "Limited-Angle Computed Tomography (LACT) is a non-destructive evaluation\ntechnique used in a variety of applications ranging from security to medicine.\nThe limited angle coverage in LACT is often a dominant source of severe\nartifacts in the reconstructed images, making it a challenging inverse problem.\nWe present DOLCE, a new deep model-based framework for LACT that uses a\nconditional diffusion model as an image prior. Diffusion models are a recent\nclass of deep generative models that are relatively easy to train due to their\nimplementation as image denoisers. DOLCE can form high-quality images from\nseverely under-sampled data by integrating data-consistency updates with the\nsampling updates of a diffusion model, which is conditioned on the transformed\nlimited-angle data. We show through extensive experimentation on several\nchallenging real LACT datasets that, the same pre-trained DOLCE model achieves\nthe SOTA performance on drastically different types of images. Additionally, we\nshow that, unlike standard LACT reconstruction methods, DOLCE naturally enables\nthe quantification of the reconstruction uncertainty by generating multiple\nsamples consistent with the measured data.",
        "authors": [
            "Jiaming Liu",
            "Rushil Anirudh",
            "Jayaraman J. Thiagarajan",
            "Stewart He",
            "K. Aditya Mohan",
            "Ulugbek S. Kamilov",
            "Hyojin Kim"
        ]
    },
    {
        "title": "Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition",
        "url": "http://arxiv.org/abs/2309.12042",
        "abstract": "For improving image composition and aesthetic quality, most existing methods\nmodulate the captured images by striking out redundant content near the image\nborders. However, such image cropping methods are limited in the range of image\nviews. Some methods have been suggested to extrapolate the images and predict\ncropping boxes from the extrapolated image. Nonetheless, the synthesized\nextrapolated regions may be included in the cropped image, making the image\ncomposition result not real and potentially with degraded image quality. In\nthis paper, we circumvent this issue by presenting a joint framework for both\nunbounded recommendation of camera view and image composition (i.e., UNIC). In\nthis way, the cropped image is a sub-image of the image acquired by the\npredicted camera view, and thus can be guaranteed to be real and consistent in\nimage quality. Specifically, our framework takes the current camera preview\nframe as input and provides a recommendation for view adjustment, which\ncontains operations unlimited by the image borders, such as zooming in or out\nand camera movement. To improve the prediction accuracy of view adjustment\nprediction, we further extend the field of view by feature extrapolation. After\none or several times of view adjustments, our method converges and results in\nboth a camera view and a bounding box showing the image composition\nrecommendation. Extensive experiments are conducted on the datasets constructed\nupon existing image cropping datasets, showing the effectiveness of our UNIC in\nunbounded recommendation of camera view and image composition. The source code,\ndataset, and pretrained models is available at\nhttps://github.com/liuxiaoyu1104/UNIC.",
        "authors": [
            "Xiaoyu Liu",
            "Ming Liu",
            "Junyi Li",
            "Shuai Liu",
            "Xiaotao Wang",
            "Lei Lei",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing",
        "url": "http://arxiv.org/abs/2304.08465",
        "abstract": "Despite the success in large-scale text-to-image generation and\ntext-conditioned image editing, existing methods still struggle to produce\nconsistent generation and editing results. For example, generation approaches\nusually fail to synthesize multiple images of the same objects/characters but\nwith different views or poses. Meanwhile, existing editing methods either fail\nto achieve effective complex non-rigid editing while maintaining the overall\ntextures and identity, or require time-consuming fine-tuning to capture the\nimage-specific appearance. In this paper, we develop MasaCtrl, a tuning-free\nmethod to achieve consistent image generation and complex non-rigid image\nediting simultaneously. Specifically, MasaCtrl converts existing self-attention\nin diffusion models into mutual self-attention, so that it can query correlated\nlocal contents and textures from source images for consistency. To further\nalleviate the query confusion between foreground and background, we propose a\nmask-guided mutual self-attention strategy, where the mask can be easily\nextracted from the cross-attention maps. Extensive experiments show that the\nproposed MasaCtrl can produce impressive results in both consistent image\ngeneration and complex non-rigid real image editing.",
        "authors": [
            "Mingdeng Cao",
            "Xintao Wang",
            "Zhongang Qi",
            "Ying Shan",
            "Xiaohu Qie",
            "Yinqiang Zheng"
        ]
    },
    {
        "title": "Understanding Hessian Alignment for Domain Generalization",
        "url": "http://arxiv.org/abs/2308.11778",
        "abstract": "Out-of-distribution (OOD) generalization is a critical ability for deep\nlearning models in many real-world scenarios including healthcare and\nautonomous vehicles. Recently, different techniques have been proposed to\nimprove OOD generalization. Among these methods, gradient-based regularizers\nhave shown promising performance compared with other competitors. Despite this\nsuccess, our understanding of the role of Hessian and gradient alignment in\ndomain generalization is still limited. To address this shortcoming, we analyze\nthe role of the classifier's head Hessian matrix and gradient in domain\ngeneralization using recent OOD theory of transferability. Theoretically, we\nshow that spectral norm between the classifier's head Hessian matrices across\ndomains is an upper bound of the transfer measure, a notion of distance between\ntarget and source domains. Furthermore, we analyze all the attributes that get\naligned when we encourage similarity between Hessians and gradients. Our\nanalysis explains the success of many regularizers like CORAL, IRM, V-REx,\nFish, IGA, and Fishr as they regularize part of the classifier's head Hessian\nand/or gradient. Finally, we propose two simple yet effective methods to match\nthe classifier's head Hessians and gradients in an efficient way, based on the\nHessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and\nwithout directly calculating Hessians. We validate the OOD generalization\nability of proposed methods in different scenarios, including transferability,\nsevere correlation shift, label shift and diversity shift. Our results show\nthat Hessian alignment methods achieve promising performance on various OOD\nbenchmarks. The code is available at\n\\url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.",
        "authors": [
            "Sobhan Hemati",
            "Guojun Zhang",
            "Amir Estiri",
            "Xi Chen"
        ]
    },
    {
        "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models",
        "url": "http://arxiv.org/abs/2305.10474",
        "abstract": "Despite tremendous progress in generating high-quality images using diffusion\nmodels, synthesizing a sequence of animated frames that are both photorealistic\nand temporally coherent is still in its infancy. While off-the-shelf\nbillion-scale datasets for image generation are available, collecting similar\nvideo data of the same scale is still challenging. Also, training a video\ndiffusion model is computationally much more expensive than its image\ncounterpart. In this work, we explore finetuning a pretrained image diffusion\nmodel with video data as a practical solution for the video synthesis task. We\nfind that naively extending the image noise prior to video noise prior in video\ndiffusion leads to sub-optimal performance. Our carefully designed video noise\nprior leads to substantially better performance. Extensive experimental\nvalidation shows that our model, Preserve Your Own Correlation (PYoCo), attains\nSOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It\nalso achieves SOTA video generation quality on the small-scale UCF-101\nbenchmark with a $10\\times$ smaller model using significantly less computation\nthan the prior art.",
        "authors": [
            "Songwei Ge",
            "Seungjun Nah",
            "Guilin Liu",
            "Tyler Poon",
            "Andrew Tao",
            "Bryan Catanzaro",
            "David Jacobs",
            "Jia-Bin Huang",
            "Ming-Yu Liu",
            "Yogesh Balaji"
        ]
    },
    {
        "title": "Joint-Relation Transformer for Multi-Person Motion Prediction",
        "url": "http://arxiv.org/abs/2308.04808",
        "abstract": "Multi-person motion prediction is a challenging problem due to the dependency\nof motion on both individual past movements and interactions with other people.\nTransformer-based methods have shown promising results on this task, but they\nmiss the explicit relation representation between joints, such as skeleton\nstructure and pairwise distance, which is crucial for accurate interaction\nmodeling. In this paper, we propose the Joint-Relation Transformer, which\nutilizes relation information to enhance interaction modeling and improve\nfuture motion prediction. Our relation information contains the relative\ndistance and the intra-/inter-person physical constraints. To fuse relation and\njoint information, we design a novel joint-relation fusion layer with\nrelation-aware attention to update both features. Additionally, we supervise\nthe relation information by forecasting future distance. Experiments show that\nour method achieves a 13.4% improvement of 900ms VIM on 3DPW-SoMoF/RC and\n17.8%/12.0% improvement of 3s MPJPE on CMU-Mpcap/MuPoTS-3D dataset.",
        "authors": [
            "Qingyao Xu",
            "Weibo Mao",
            "Jingze Gong",
            "Chenxin Xu",
            "Siheng Chen",
            "Weidi Xie",
            "Ya Zhang",
            "Yanfeng Wang"
        ]
    },
    {
        "title": "Revisiting Vision Transformer from the View of Path Ensemble",
        "url": "http://arxiv.org/abs/2308.06548",
        "abstract": "Vision Transformers (ViTs) are normally regarded as a stack of transformer\nlayers. In this work, we propose a novel view of ViTs showing that they can be\nseen as ensemble networks containing multiple parallel paths with different\nlengths. Specifically, we equivalently transform the traditional cascade of\nmulti-head self-attention (MSA) and feed-forward network (FFN) into three\nparallel paths in each transformer layer. Then, we utilize the identity\nconnection in our new transformer form and further transform the ViT into an\nexplicit multi-path ensemble network. From the new perspective, these paths\nperform two functions: the first is to provide the feature for the classifier\ndirectly, and the second is to provide the lower-level feature representation\nfor subsequent longer paths. We investigate the influence of each path for the\nfinal prediction and discover that some paths even pull down the performance.\nTherefore, we propose the path pruning and EnsembleScale skills for\nimprovement, which cut out the underperforming paths and re-weight the ensemble\ncomponents, respectively, to optimize the path combination and make the short\npaths focus on providing high-quality representation for subsequent paths. We\nalso demonstrate that our path combination strategies can help ViTs go deeper\nand act as high-pass filters to filter out partial low-frequency signals. To\nfurther enhance the representation of paths served for subsequent paths,\nself-distillation is applied to transfer knowledge from the long paths to the\nshort paths. This work calls for more future research to explain and design\nViTs from new perspectives.",
        "authors": [
            "Shuning Chang",
            "Pichao Wang",
            "Hao Luo",
            "Fan Wang",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "TMA: Temporal Motion Aggregation for Event-based Optical Flow",
        "url": "http://arxiv.org/abs/2303.11629",
        "abstract": "Event cameras have the ability to record continuous and detailed trajectories\nof objects with high temporal resolution, thereby providing intuitive motion\ncues for optical flow estimation. Nevertheless, most existing learning-based\napproaches for event optical flow estimation directly remould the paradigm of\nconventional images by representing the consecutive event stream as static\nframes, ignoring the inherent temporal continuity of event data. In this paper,\nwe argue that temporal continuity is a vital element of event-based optical\nflow and propose a novel Temporal Motion Aggregation (TMA) approach to unlock\nits potential. Technically, TMA comprises three components: an event splitting\nstrategy to incorporate intermediate motion information underlying the temporal\ncontext, a linear lookup strategy to align temporally fine-grained motion\nfeatures and a novel motion pattern aggregation module to emphasize consistent\npatterns for motion feature enhancement. By incorporating temporally\nfine-grained motion information, TMA can derive better flow estimates than\nexisting methods at early stages, which not only enables TMA to obtain more\naccurate final predictions, but also greatly reduces the demand for a number of\nrefinements. Extensive experiments on DSEC-Flow and MVSEC datasets verify the\neffectiveness and superiority of our TMA. Remarkably, compared to E-RAFT, TMA\nachieves a 6\\% improvement in accuracy and a 40\\% reduction in inference time\non DSEC-Flow. Code will be available at \\url{https://github.com/ispc-lab/TMA}.",
        "authors": [
            "Haotian Liu",
            "Guang Chen",
            "Sanqing Qu",
            "Yanping Zhang",
            "Zhijun Li",
            "Alois Knoll",
            "Changjun Jiang"
        ]
    },
    {
        "title": "Ablating Concepts in Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2303.13516",
        "abstract": "Large-scale text-to-image diffusion models can generate high-fidelity images\nwith powerful compositional ability. However, these models are typically\ntrained on an enormous amount of Internet data, often containing copyrighted\nmaterial, licensed images, and personal photos. Furthermore, they have been\nfound to replicate the style of various living artists or memorize exact\ntraining samples. How can we remove such copyrighted concepts or images without\nretraining the model from scratch? To achieve this goal, we propose an\nefficient method of ablating concepts in the pretrained model, i.e., preventing\nthe generation of a target concept. Our algorithm learns to match the image\ndistribution for a target style, instance, or text prompt we wish to ablate to\nthe distribution corresponding to an anchor concept. This prevents the model\nfrom generating target concepts given its text condition. Extensive experiments\nshow that our method can successfully prevent the generation of the ablated\nconcept while preserving closely related concepts in the model.",
        "authors": [
            "Nupur Kumari",
            "Bingliang Zhang",
            "Sheng-Yu Wang",
            "Eli Shechtman",
            "Richard Zhang",
            "Jun-Yan Zhu"
        ]
    },
    {
        "title": "Motion-Guided Masking for Spatiotemporal Representation Learning",
        "url": "http://arxiv.org/abs/2308.12962",
        "abstract": "Several recent works have directly extended the image masked autoencoder\n(MAE) with random masking into video domain, achieving promising results.\nHowever, unlike images, both spatial and temporal information are important for\nvideo understanding. This suggests that the random masking strategy that is\ninherited from the image MAE is less effective for video MAE. This motivates\nthe design of a novel masking algorithm that can more efficiently make use of\nvideo saliency. Specifically, we propose a motion-guided masking algorithm\n(MGM) which leverages motion vectors to guide the position of each mask over\ntime. Crucially, these motion-based correspondences can be directly obtained\nfrom information stored in the compressed format of the video, which makes our\nmethod efficient and scalable. On two challenging large-scale video benchmarks\n(Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and\nachieve up to +$1.3\\%$ improvement compared to previous state-of-the-art\nmethods. Additionally, our MGM achieves equivalent performance to previous\nvideo MAE using up to $66\\%$ fewer training epochs. Lastly, we show that MGM\ngeneralizes better to downstream transfer learning and domain adaptation tasks\non the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\\%$\nimprovement compared to baseline methods.",
        "authors": [
            "David Fan",
            "Jue Wang",
            "Shuai Liao",
            "Yi Zhu",
            "Vimal Bhat",
            "Hector Santos-Villalobos",
            "Rohith MV",
            "Xinyu Li"
        ]
    },
    {
        "title": "Masked Diffusion Transformer is a Strong Image Synthesizer",
        "url": "http://arxiv.org/abs/2303.14389",
        "abstract": "Despite its success in image synthesis, we observe that diffusion\nprobabilistic models (DPMs) often lack contextual reasoning ability to learn\nthe relations among object parts in an image, leading to a slow learning\nprocess. To solve this issue, we propose a Masked Diffusion Transformer (MDT)\nthat introduces a mask latent modeling scheme to explicitly enhance the DPMs'\nability of contextual relation learning among object semantic parts in an\nimage. During training, MDT operates on the latent space to mask certain\ntokens. Then, an asymmetric masking diffusion transformer is designed to\npredict masked tokens from unmasked ones while maintaining the diffusion\ngeneration process. Our MDT can reconstruct the full information of an image\nfrom its incomplete contextual input, thus enabling it to learn the associated\nrelations among image tokens. Experimental results show that MDT achieves\nsuperior image synthesis performance, e.g. a new SoTA FID score on the ImageNet\ndataset, and has about 3x faster learning speed than the previous SoTA DiT. The\nsource code is released at https://github.com/sail-sg/MDT.",
        "authors": [
            "Shanghua Gao",
            "Pan Zhou",
            "Ming-Ming Cheng",
            "Shuicheng Yan"
        ]
    },
    {
        "title": "Urban Radiance Field Representation with Deformable Neural Mesh Primitives",
        "url": "http://arxiv.org/abs/2307.10776",
        "abstract": "Neural Radiance Fields (NeRFs) have achieved great success in the past few\nyears. However, most current methods still require intensive resources due to\nray marching-based rendering. To construct urban-level radiance fields\nefficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to\nparameterize the entire scene with such primitives. The DNMP is a flexible and\ncompact neural variant of classic mesh representation, which enjoys both the\nefficiency of rasterization-based rendering and the powerful neural\nrepresentation capability for photo-realistic image synthesis. Specifically, a\nDNMP consists of a set of connected deformable mesh vertices with paired vertex\nfeatures to parameterize the geometry and radiance information of a local area.\nTo constrain the degree of freedom for optimization and lower the storage\nbudgets, we enforce the shape of each primitive to be decoded from a relatively\nlow-dimensional latent space. The rendering colors are decoded from the vertex\nfeatures (interpolated with rasterization) by a view-dependent MLP. The DNMP\nprovides a new paradigm for urban-level scene representation with appealing\nproperties: $(1)$ High-quality rendering. Our method achieves leading\nperformance for novel view synthesis in urban scenarios. $(2)$ Low\ncomputational costs. Our representation enables fast rendering (2.07ms/1k\npixels) and low peak memory usage (110MB/1k pixels). We also present a\nlightweight version that can run 33$\\times$ faster than vanilla NeRFs, and\ncomparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).\nProject page: \\href{https://dnmp.github.io/}{https://dnmp.github.io/}.",
        "authors": [
            "Fan Lu",
            "Yan Xu",
            "Guang Chen",
            "Hongsheng Li",
            "Kwan-Yee Lin",
            "Changjun Jiang"
        ]
    },
    {
        "title": "Adaptive Frequency Filters As Efficient Global Token Mixers",
        "url": "http://arxiv.org/abs/2307.14008",
        "abstract": "Recent vision transformers, large-kernel CNNs and MLPs have attained\nremarkable successes in broad vision tasks thanks to their effective\ninformation fusion in the global scope. However, their efficient deployments,\nespecially on mobile devices, still suffer from noteworthy challenges due to\nthe heavy computational costs of self-attention mechanisms, large kernels, or\nfully connected layers. In this work, we apply conventional convolution theorem\nto deep learning for addressing this and reveal that adaptive frequency filters\ncan serve as efficient global token mixers. With this insight, we propose\nAdaptive Frequency Filtering (AFF) token mixer. This neural operator transfers\na latent representation to the frequency domain via a Fourier transform and\nperforms semantic-adaptive frequency filtering via an elementwise\nmultiplication, which mathematically equals to a token mixing operation in the\noriginal latent space with a dynamic convolution kernel as large as the spatial\nresolution of this latent representation. We take AFF token mixers as primary\nneural operators to build a lightweight neural network, dubbed AFFNet.\nExtensive experiments demonstrate the effectiveness of our proposed AFF token\nmixer and show that AFFNet achieve superior accuracy and efficiency trade-offs\ncompared to other lightweight network designs on broad visual tasks, including\nvisual recognition and dense prediction tasks.",
        "authors": [
            "Zhipeng Huang",
            "Zhizheng Zhang",
            "Cuiling Lan",
            "Zheng-Jun Zha",
            "Yan Lu",
            "Baining Guo"
        ]
    },
    {
        "title": "Referring Image Segmentation Using Text Supervision",
        "url": "http://arxiv.org/abs/2308.14575",
        "abstract": "Existing Referring Image Segmentation (RIS) methods typically require\nexpensive pixel-level or box-level annotations for supervision. In this paper,\nwe observe that the referring texts used in RIS already provide sufficient\ninformation to localize the target object. Hence, we propose a novel\nweakly-supervised RIS framework to formulate the target localization problem as\na classification process to differentiate between positive and negative text\nexpressions. While the referring text expressions for an image are used as\npositive expressions, the referring text expressions from other images can be\nused as negative expressions for this image. Our framework has three main\nnovelties. First, we propose a bilateral prompt method to facilitate the\nclassification process, by harmonizing the domain discrepancy between visual\nand linguistic features. Second, we propose a calibration method to reduce\nnoisy background information and improve the correctness of the response maps\nfor target object localization. Third, we propose a positive response map\nselection strategy to generate high-quality pseudo-labels from the enhanced\nresponse maps, for training a segmentation network for RIS inference. For\nevaluation, we propose a new metric to measure localization accuracy.\nExperiments on four benchmarks show that our framework achieves promising\nperformances to existing fully-supervised RIS methods while outperforming\nstate-of-the-art weakly-supervised methods adapted from related areas. Code is\navailable at https://github.com/fawnliu/TRIS.",
        "authors": [
            "Fang Liu",
            "Yuhao Liu",
            "Yuqiu Kong",
            "Ke Xu",
            "Lihe Zhang",
            "Baocai Yin",
            "Gerhard Hancke",
            "Rynson Lau"
        ]
    },
    {
        "title": "Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction",
        "url": "http://arxiv.org/abs/2303.13796",
        "abstract": "As it is hard to calibrate single-view RGB images in the wild, existing 3D\nhuman mesh reconstruction (3DHMR) methods either use a constant large focal\nlength or estimate one based on the background environment context, which can\nnot tackle the problem of the torso, limb, hand or face distortion caused by\nperspective camera projection when the camera is close to the human body. The\nnaive focal length assumptions can harm this task with the incorrectly\nformulated projection matrices. To solve this, we propose Zolly, the first\n3DHMR method focusing on perspective-distorted images. Our approach begins with\nanalysing the reason for perspective distortion, which we find is mainly caused\nby the relative location of the human body to the camera center. We propose a\nnew camera model and a novel 2D representation, termed distortion image, which\ndescribes the 2D dense distortion scale of the human body. We then estimate the\ndistance from distortion scale features rather than environment context\nfeatures. Afterwards, we integrate the distortion feature with image features\nto reconstruct the body mesh. To formulate the correct projection matrix and\nlocate the human body position, we simultaneously use perspective and\nweak-perspective projection loss. Since existing datasets could not handle this\ntask, we propose the first synthetic dataset PDHuman and extend two real-world\ndatasets tailored for this task, all containing perspective-distorted human\nimages. Extensive experiments show that Zolly outperforms existing\nstate-of-the-art methods on both perspective-distorted datasets and the\nstandard benchmark (3DPW).",
        "authors": [
            "Wenjia Wang",
            "Yongtao Ge",
            "Haiyi Mei",
            "Zhongang Cai",
            "Qingping Sun",
            "Yanjun Wang",
            "Chunhua Shen",
            "Lei Yang",
            "Taku Komura"
        ]
    },
    {
        "title": "Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",
        "url": "http://arxiv.org/abs/2304.12315",
        "abstract": "This paper aims for high-performance offline LiDAR-based 3D object detection.\nWe first observe that experienced human annotators annotate objects from a\ntrack-centric perspective. They first label the objects with clear shapes in a\ntrack, and then leverage the temporal coherence to infer the annotations of\nobscure objects. Drawing inspiration from this, we propose a high-performance\noffline detector in a track-centric perspective instead of the conventional\nobject-centric perspective. Our method features a bidirectional tracking module\nand a track-centric learning module. Such a design allows our detector to infer\nand refine a complete track once the object is detected at a certain moment. We\nrefer to this characteristic as \"onCe detecTed, neveR Lost\" and name the\nproposed system CTRL. Extensive experiments demonstrate the remarkable\nperformance of our method, surpassing the human-level annotating accuracy and\nthe previous state-of-the-art methods in the highly competitive Waymo Open\nDataset without model ensemble. The code will be made publicly available at\nhttps://github.com/tusen-ai/SST.",
        "authors": [
            "Lue Fan",
            "Yuxue Yang",
            "Yiming Mao",
            "Feng Wang",
            "Yuntao Chen",
            "Naiyan Wang",
            "Zhaoxiang Zhang"
        ]
    },
    {
        "title": "Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach",
        "url": "http://arxiv.org/abs/2309.02429",
        "abstract": "Estimating the transferability of publicly available pretrained models to a\ntarget task has assumed an important place for transfer learning tasks in\nrecent years. Existing efforts propose metrics that allow a user to choose one\nmodel from a pool of pre-trained models without having to fine-tune each model\nindividually and identify one explicitly. With the growth in the number of\navailable pre-trained models and the popularity of model ensembles, it also\nbecomes essential to study the transferability of multiple-source models for a\ngiven target task. The few existing efforts study transferability in such\nmulti-source ensemble settings using just the outputs of the classification\nlayer and neglect possible domain or task mismatch. Moreover, they overlook the\nmost important factor while selecting the source models, viz., the cohesiveness\nfactor between them, which can impact the performance and confidence in the\nprediction of the ensemble. To address these gaps, we propose a novel Optimal\ntranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the\ntransferability of an ensemble of models to a downstream task. OSBORN\ncollectively accounts for image domain difference, task difference, and\ncohesiveness of models in the ensemble to provide reliable estimates of\ntransferability. We gauge the performance of OSBORN on both image\nclassification and semantic segmentation tasks. Our setup includes 28 source\ndatasets, 11 target datasets, 5 model architectures, and 2 pre-training\nmethods. We benchmark our method against current state-of-the-art metrics\nMS-LEEP and E-LEEP, and outperform them consistently using the proposed\napproach.",
        "authors": [
            "Vimal K B",
            "Saketh Bachu",
            "Tanmay Garg",
            "Niveditha Lakshmi Narasimhan",
            "Raghavan Konuru",
            "Vineeth N Balasubramanian"
        ]
    },
    {
        "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers",
        "url": "http://arxiv.org/abs/2308.13494",
        "abstract": "Vision Transformers achieve impressive accuracy across a range of visual\nrecognition tasks. Unfortunately, their accuracy frequently comes with high\ncomputational costs. This is a particular issue in video recognition, where\nmodels are often applied repeatedly across frames or temporal chunks. In this\nwork, we exploit temporal redundancy between subsequent inputs to reduce the\ncost of Transformers for video processing. We describe a method for identifying\nand re-processing only those tokens that have changed significantly over time.\nOur proposed family of models, Eventful Transformers, can be converted from\nexisting Transformers (often without any re-training) and give adaptive control\nover the compute cost at runtime. We evaluate our method on large-scale\ndatasets for video object detection (ImageNet VID) and action recognition\n(EPIC-Kitchens 100). Our approach leads to significant computational savings\n(on the order of 2-4x) with only minor reductions in accuracy.",
        "authors": [
            "Matthew Dutson",
            "Yin Li",
            "Mohit Gupta"
        ]
    },
    {
        "title": "DiffIR: Efficient Diffusion Model for Image Restoration",
        "url": "http://arxiv.org/abs/2303.09472",
        "abstract": "Diffusion model (DM) has achieved SOTA performance by modeling the image\nsynthesis process into a sequential application of a denoising network.\nHowever, different from image synthesis, image restoration (IR) has a strong\nconstraint to generate results in accordance with ground-truth. Thus, for IR,\ntraditional DMs running massive iterations on a large model to estimate whole\nimages or feature maps is inefficient. To address this issue, we propose an\nefficient DM for IR (DiffIR), which consists of a compact IR prior extraction\nnetwork (CPEN), dynamic IR transformer (DIRformer), and denoising network.\nSpecifically, DiffIR has two training stages: pretraining and training DM. In\npretraining, we input ground-truth images into CPEN$_{S1}$ to capture a compact\nIR prior representation (IPR) to guide DIRformer. In the second stage, we train\nthe DM to directly estimate the same IRP as pretrained CPEN$_{S1}$ only using\nLQ images. We observe that since the IPR is only a compact vector, DiffIR can\nuse fewer iterations than traditional DM to obtain accurate estimations and\ngenerate more stable and realistic results. Since the iterations are few, our\nDiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoising\nnetwork, which can further reduce the estimation error influence. We conduct\nextensive experiments on several IR tasks and achieve SOTA performance while\nconsuming less computational costs. Code is available at\n\\url{https://github.com/Zj-BinXia/DiffIR}.",
        "authors": [
            "Bin Xia",
            "Yulun Zhang",
            "Shiyin Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Yapeng Tian",
            "Wenming Yang",
            "Luc Van Gool"
        ]
    },
    {
        "title": "MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope",
        "url": "http://arxiv.org/abs/2302.05294",
        "abstract": "Explaining the predictions of deep neural nets has been a topic of great\ninterest in the computer vision literature. While several gradient-based\ninterpretation schemes have been proposed to reveal the influential variables\nin a neural net's prediction, standard gradient-based interpretation frameworks\nhave been commonly observed to lack robustness to input perturbations and\nflexibility for incorporating prior knowledge of sparsity and group-sparsity\nstructures. In this work, we propose MoreauGrad as an interpretation scheme\nbased on the classifier neural net's Moreau envelope. We demonstrate that\nMoreauGrad results in a smooth and robust interpretation of a multi-layer\nneural network and can be efficiently computed through first-order optimization\nmethods. Furthermore, we show that MoreauGrad can be naturally combined with\n$L_1$-norm regularization techniques to output a sparse or group-sparse\nexplanation which are prior conditions applicable to a wide range of deep\nlearning applications. We empirically evaluate the proposed MoreauGrad scheme\non standard computer vision datasets, showing the qualitative and quantitative\nsuccess of the MoreauGrad approach in comparison to standard gradient-based\ninterpretation methods.",
        "authors": [
            "Jingwei Zhang",
            "Farzan Farnia"
        ]
    },
    {
        "title": "Class-Incremental Grouping Network for Continual Audio-Visual Learning",
        "url": "http://arxiv.org/abs/2309.05281",
        "abstract": "Continual learning is a challenging problem in which models need to be\ntrained on non-stationary data across sequential tasks for class-incremental\nlearning. While previous methods have focused on using either regularization or\nrehearsal-based frameworks to alleviate catastrophic forgetting in image\nclassification, they are limited to a single modality and cannot learn compact\nclass-aware cross-modal representations for continual audio-visual learning. To\naddress this gap, we propose a novel class-incremental grouping network (CIGN)\nthat can learn category-wise semantic features to achieve continual\naudio-visual learning. Our CIGN leverages learnable audio-visual class tokens\nand audio-visual grouping to continually aggregate class-aware features.\nAdditionally, it utilizes class tokens distillation and continual grouping to\nprevent forgetting parameters learned from previous tasks, thereby improving\nthe model's ability to capture discriminative audio-visual categories. We\nconduct extensive experiments on VGGSound-Instruments, VGGSound-100, and\nVGG-Sound Sources benchmarks. Our experimental results demonstrate that the\nCIGN achieves state-of-the-art audio-visual class-incremental learning\nperformance. Code is available at https://github.com/stoneMo/CIGN.",
        "authors": [
            "Shentong Mo",
            "Weiguo Pian",
            "Yapeng Tian"
        ]
    },
    {
        "title": "Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction",
        "url": "http://arxiv.org/abs/2306.05872",
        "abstract": "Generating realistic human 3D reconstructions using image or video data is\nessential for various communication and entertainment applications. While\nexisting methods achieved impressive results for body and facial regions,\nrealistic hair modeling still remains challenging due to its high mechanical\ncomplexity. This work proposes an approach capable of accurate hair geometry\nreconstruction at a strand level from a monocular video or multi-view images\ncaptured in uncontrolled lighting conditions. Our method has two stages, with\nthe first stage performing joint reconstruction of coarse hair and bust shapes\nand hair orientation using implicit volumetric representations. The second\nstage then estimates a strand-level hair reconstruction by reconciling in a\nsingle optimization process the coarse volumetric constraints with hair strand\nand hairstyle priors learned from the synthetic data. To further increase the\nreconstruction fidelity, we incorporate image-based losses into the fitting\nprocess using a new differentiable renderer. The combined system, named Neural\nHaircut, achieves high realism and personalization of the reconstructed\nhairstyles.",
        "authors": [
            "Vanessa Sklyarova",
            "Jenya Chelishev",
            "Andreea Dogaru",
            "Igor Medvedev",
            "Victor Lempitsky",
            "Egor Zakharov"
        ]
    },
    {
        "title": "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance",
        "url": "http://arxiv.org/abs/2210.00939",
        "abstract": "Denoising diffusion models (DDMs) have attracted attention for their\nexceptional generation quality and diversity. This success is largely\nattributed to the use of class- or text-conditional diffusion guidance methods,\nsuch as classifier and classifier-free guidance. In this paper, we present a\nmore comprehensive perspective that goes beyond the traditional guidance\nmethods. From this generalized perspective, we introduce novel condition- and\ntraining-free strategies to enhance the quality of generated images. As a\nsimple solution, blur guidance improves the suitability of intermediate samples\nfor their fine-scale information and structures, enabling diffusion models to\ngenerate higher quality samples with a moderate guidance scale. Improving upon\nthis, Self-Attention Guidance (SAG) uses the intermediate self-attention maps\nof diffusion models to enhance their stability and efficacy. Specifically, SAG\nadversarially blurs only the regions that diffusion models attend to at each\niteration and guides them accordingly. Our experimental results show that our\nSAG improves the performance of various diffusion models, including ADM, IDDPM,\nStable Diffusion, and DiT. Moreover, combining SAG with conventional guidance\nmethods leads to further improvement.",
        "authors": [
            "Susung Hong",
            "Gyuseong Lee",
            "Wooseok Jang",
            "Seungryong Kim"
        ]
    },
    {
        "title": "Evaluating Data Attribution for Text-to-Image Models",
        "url": "http://arxiv.org/abs/2306.09345",
        "abstract": "While large text-to-image models are able to synthesize \"novel\" images, these\nimages are necessarily a reflection of the training data. The problem of data\nattribution in such models -- which of the images in the training set are most\nresponsible for the appearance of a given generated image -- is a difficult yet\nimportant one. As an initial step toward this problem, we evaluate attribution\nthrough \"customization\" methods, which tune an existing large-scale model\ntoward a given exemplar object or style. Our key insight is that this allows us\nto efficiently create synthetic images that are computationally influenced by\nthe exemplar by construction. With our new dataset of such exemplar-influenced\nimages, we are able to evaluate various data attribution algorithms and\ndifferent possible feature spaces. Furthermore, by training on our dataset, we\ncan tune standard models, such as DINO, CLIP, and ViT, toward the attribution\nproblem. Even though the procedure is tuned towards small exemplar sets, we\nshow generalization to larger sets. Finally, by taking into account the\ninherent uncertainty of the problem, we can assign soft attribution scores over\na set of training images.",
        "authors": [
            "Sheng-Yu Wang",
            "Alexei A. Efros",
            "Jun-Yan Zhu",
            "Richard Zhang"
        ]
    },
    {
        "title": "Delta Denoising Score",
        "url": "http://arxiv.org/abs/2304.07090",
        "abstract": "We introduce Delta Denoising Score (DDS), a novel scoring function for\ntext-based image editing that guides minimal modifications of an input image\ntowards the content described in a target prompt. DDS leverages the rich\ngenerative prior of text-to-image diffusion models and can be used as a loss\nterm in an optimization problem to steer an image towards a desired direction\ndictated by a text. DDS utilizes the Score Distillation Sampling (SDS)\nmechanism for the purpose of image editing. We show that using only SDS often\nproduces non-detailed and blurry outputs due to noisy gradients. To address\nthis issue, DDS uses a prompt that matches the input image to identify and\nremove undesired erroneous directions of SDS. Our key premise is that SDS\nshould be zero when calculated on pairs of matched prompts and images, meaning\nthat if the score is non-zero, its gradients can be attributed to the erroneous\ncomponent of SDS. Our analysis demonstrates the competence of DDS for text\nbased image-to-image translation. We further show that DDS can be used to train\nan effective zero-shot image translation model. Experimental results indicate\nthat DDS outperforms existing methods in terms of stability and quality,\nhighlighting its potential for real-world applications in text-based image\nediting.",
        "authors": [
            "Amir Hertz",
            "Kfir Aberman",
            "Daniel Cohen-Or"
        ]
    },
    {
        "title": "Hierarchical Prior Mining for Non-local Multi-View Stereo",
        "url": "http://arxiv.org/abs/2303.09758",
        "abstract": "As a fundamental problem in computer vision, multi-view stereo (MVS) aims at\nrecovering the 3D geometry of a target from a set of 2D images. Recent advances\nin MVS have shown that it is important to perceive non-local structured\ninformation for recovering geometry in low-textured areas. In this work, we\npropose a Hierarchical Prior Mining for Non-local Multi-View Stereo (HPM-MVS).\nThe key characteristics are the following techniques that exploit non-local\ninformation to assist MVS: 1) A Non-local Extensible Sampling Pattern (NESP),\nwhich is able to adaptively change the size of sampled areas without becoming\nsnared in locally optimal solutions. 2) A new approach to leverage non-local\nreliable points and construct a planar prior model based on K-Nearest Neighbor\n(KNN), to obtain potential hypotheses for the regions where prior construction\nis challenging. 3) A Hierarchical Prior Mining (HPM) framework, which is used\nto mine extensive non-local prior information at different scales to assist 3D\nmodel recovery, this strategy can achieve a considerable balance between the\nreconstruction of details and low-textured areas. Experimental results on the\nETH3D and Tanks \\& Temples have verified the superior performance and strong\ngeneralization capability of our method. Our code will be released.",
        "authors": [
            "Chunlin Ren",
            "Qingshan Xu",
            "Shikun Zhang",
            "Jiaqi Yang"
        ]
    },
    {
        "title": "Generative Multiplane Neural Radiance for 3D-Aware Image Generation",
        "url": "http://arxiv.org/abs/2304.01172",
        "abstract": "We present a method to efficiently generate 3D-aware high-resolution images\nthat are view-consistent across multiple target views. The proposed multiplane\nneural radiance model, named GMNR, consists of a novel {\\alpha}-guided\nview-dependent representation ({\\alpha}-VdR) module for learning view-dependent\ninformation. The {\\alpha}-VdR module, faciliated by an {\\alpha}-guided pixel\nsampling technique, computes the view-dependent representation efficiently by\nlearning viewing direction and position coefficients. Moreover, we propose a\nview-consistency loss to enforce photometric similarity across multiple views.\nThe GMNR model can generate 3D-aware high-resolution images that are\nviewconsistent across multiple camera poses, while maintaining the\ncomputational efficiency in terms of both training and inference time.\nExperiments on three datasets demonstrate the effectiveness of the proposed\nmodules, leading to favorable results in terms of both generation quality and\ninference time, compared to existing approaches. Our GMNR model generates\n3D-aware images of 1024 X 1024 pixels with 17.6 FPS on a single V100. Code :\nhttps://github.com/VIROBO-15/GMNR",
        "authors": [
            "Amandeep Kumar",
            "Ankan Kumar Bhunia",
            "Sanath Narayan",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Ming-Hsuan Yang",
            "Fahad Shahbaz Khan"
        ]
    },
    {
        "title": "Simple Baselines for Interactive Video Retrieval with Questions and Answers",
        "url": "http://arxiv.org/abs/2308.10402",
        "abstract": "To date, the majority of video retrieval systems have been optimized for a\n\"single-shot\" scenario in which the user submits a query in isolation, ignoring\nprevious interactions with the system. Recently, there has been renewed\ninterest in interactive systems to enhance retrieval, but existing approaches\nare complex and deliver limited gains in performance. In this work, we revisit\nthis topic and propose several simple yet effective baselines for interactive\nvideo retrieval via question-answering. We employ a VideoQA model to simulate\nuser interactions and show that this enables the productive study of the\ninteractive retrieval task without access to ground truth dialogue data.\nExperiments on MSR-VTT, MSVD, and AVSD show that our framework using\nquestion-based interaction significantly improves the performance of text-based\nvideo retrieval systems.",
        "authors": [
            "Kaiqu Liang",
            "Samuel Albanie"
        ]
    },
    {
        "title": "Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings",
        "url": "http://arxiv.org/abs/2308.12894",
        "abstract": "Semantic segmentation is a computer vision task that associates a label with\neach pixel in an image. Modern approaches tend to introduce class embeddings\ninto semantic segmentation for deeply utilizing category semantics, and regard\nsupervised class masks as final predictions. In this paper, we explore the\nmechanism of class embeddings and have an insight that more explicit and\nmeaningful class embeddings can be generated based on class masks purposely.\nFollowing this observation, we propose ECENet, a new segmentation paradigm, in\nwhich class embeddings are obtained and enhanced explicitly during interacting\nwith multi-stage image features. Based on this, we revisit the traditional\ndecoding process and explore inverted information flow between segmentation\nmasks and class embeddings. Furthermore, to ensure the discriminability and\ninformativity of features from backbone, we propose a Feature Reconstruction\nmodule, which combines intrinsic and diverse branches together to ensure the\nconcurrence of diversity and redundancy in features. Experiments show that our\nECENet outperforms its counterparts on the ADE20K dataset with much less\ncomputational cost and achieves new state-of-the-art results on PASCAL-Context\ndataset. The code will be released at https://gitee.com/mindspore/models and\nhttps://github.com/Carol-lyh/ECENet.",
        "authors": [
            "Yuhe Liu",
            "Chuanjian Liu",
            "Kai Han",
            "Quan Tang",
            "Zengchang Qin"
        ]
    },
    {
        "title": "Going Denser with Open-Vocabulary Part Segmentation",
        "url": "http://arxiv.org/abs/2305.11173",
        "abstract": "Object detection has been expanded from a limited number of categories to\nopen vocabulary. Moving forward, a complete intelligent vision system requires\nunderstanding more fine-grained object descriptions, object parts. In this\npaper, we propose a detector with the ability to predict both open-vocabulary\nobjects and their part segmentation. This ability comes from two designs.\nFirst, we train the detector on the joint of part-level, object-level and\nimage-level data to build the multi-granularity alignment between language and\nimage. Second, we parse the novel object into its parts by its dense semantic\ncorrespondence with the base object. These two designs enable the detector to\nlargely benefit from various data sources and foundation models. In\nopen-vocabulary part segmentation experiments, our method outperforms the\nbaseline by 3.3$\\sim$7.3 mAP in cross-dataset generalization on PartImageNet,\nand improves the baseline by 7.3 novel AP$_{50}$ in cross-category\ngeneralization on Pascal Part. Finally, we train a detector that generalizes to\na wide range of part segmentation datasets while achieving better performance\nthan dataset-specific training.",
        "authors": [
            "Peize Sun",
            "Shoufa Chen",
            "Chenchen Zhu",
            "Fanyi Xiao",
            "Ping Luo",
            "Saining Xie",
            "Zhicheng Yan"
        ]
    },
    {
        "title": "Learning to Identify Critical States for Reinforcement Learning from Videos",
        "url": "http://arxiv.org/abs/2308.07795",
        "abstract": "Recent work on deep reinforcement learning (DRL) has pointed out that\nalgorithmic information about good policies can be extracted from offline data\nwhich lack explicit information about executed actions. For example, videos of\nhumans or robots may convey a lot of implicit information about rewarding\naction sequences, but a DRL machine that wants to profit from watching such\nvideos must first learn by itself to identify and recognize relevant\nstates/actions/rewards. Without relying on ground-truth annotations, our new\nmethod called Deep State Identifier learns to predict returns from episodes\nencoded as videos. Then it uses a kind of mask-based sensitivity analysis to\nextract/identify important critical states. Extensive experiments showcase our\nmethod's potential for understanding and improving agent behavior. The source\ncode and the generated datasets are available at\nhttps://github.com/AI-Initiative-KAUST/VideoRLCS.",
        "authors": [
            "Haozhe Liu",
            "Mingchen Zhuge",
            "Bing Li",
            "Yuhui Wang",
            "Francesco Faccio",
            "Bernard Ghanem",
            "J\u00fcrgen Schmidhuber"
        ]
    },
    {
        "title": "Editing Implicit Assumptions in Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2303.08084",
        "abstract": "Text-to-image diffusion models often make implicit assumptions about the\nworld when generating images. While some assumptions are useful (e.g., the sky\nis blue), they can also be outdated, incorrect, or reflective of social biases\npresent in the training data. Thus, there is a need to control these\nassumptions without requiring explicit user input or costly re-training. In\nthis work, we aim to edit a given implicit assumption in a pre-trained\ndiffusion model. Our Text-to-Image Model Editing method, TIME for short,\nreceives a pair of inputs: a \"source\" under-specified prompt for which the\nmodel makes an implicit assumption (e.g., \"a pack of roses\"), and a\n\"destination\" prompt that describes the same setting, but with a specified\ndesired attribute (e.g., \"a pack of blue roses\"). TIME then updates the model's\ncross-attention layers, as these layers assign visual meaning to textual\ntokens. We edit the projection matrices in these layers such that the source\nprompt is projected close to the destination prompt. Our method is highly\nefficient, as it modifies a mere 2.2% of the model's parameters in under one\nsecond. To evaluate model editing approaches, we introduce TIMED (TIME\nDataset), containing 147 source and destination prompt pairs from various\ndomains. Our experiments (using Stable Diffusion) show that TIME is successful\nin model editing, generalizes well for related prompts unseen during editing,\nand imposes minimal effect on unrelated generations.",
        "authors": [
            "Hadas Orgad",
            "Bahjat Kawar",
            "Yonatan Belinkov"
        ]
    },
    {
        "title": "Reconstructing Interacting Hands with Interaction Prior from Monocular Images",
        "url": "http://arxiv.org/abs/2308.14082",
        "abstract": "Reconstructing interacting hands from monocular images is indispensable in\nAR/VR applications. Most existing solutions rely on the accurate localization\nof each skeleton joint. However, these methods tend to be unreliable due to the\nsevere occlusion and confusing similarity among adjacent hand parts. This also\ndefies human perception because humans can quickly imitate an interaction\npattern without localizing all joints. Our key idea is to first construct a\ntwo-hand interaction prior and recast the interaction reconstruction task as\nthe conditional sampling from the prior. To expand more interaction states, a\nlarge-scale multimodal dataset with physical plausibility is proposed. Then a\nVAE is trained to further condense these interaction patterns as latent codes\nin a prior distribution. When looking for image cues that contribute to\ninteraction prior sampling, we propose the interaction adjacency heatmap (IAH).\nCompared with a joint-wise heatmap for localization, IAH assigns denser visible\nfeatures to those invisible joints. Compared with an all-in-one visible\nheatmap, it provides more fine-grained local interaction information in each\ninteraction region. Finally, the correlations between the extracted features\nand corresponding interaction codes are linked by the ViT module. Comprehensive\nevaluations on benchmark datasets have verified the effectiveness of this\nframework. The code and dataset are publicly available at\nhttps://github.com/binghui-z/InterPrior_pytorch",
        "authors": [
            "Binghui Zuo",
            "Zimeng Zhao",
            "Wenqian Sun",
            "Wei Xie",
            "Zhou Xue",
            "Yangang Wang"
        ]
    },
    {
        "title": "How Much Temporal Long-Term Context is Needed for Action Segmentation?",
        "url": "http://arxiv.org/abs/2308.11358",
        "abstract": "Modeling long-term context in videos is crucial for many fine-grained tasks\nincluding temporal action segmentation. An interesting question that is still\nopen is how much long-term temporal context is needed for optimal performance.\nWhile transformers can model the long-term context of a video, this becomes\ncomputationally prohibitive for long videos. Recent works on temporal action\nsegmentation thus combine temporal convolutional networks with self-attentions\nthat are computed only for a local temporal window. While these approaches show\ngood results, their performance is limited by their inability to capture the\nfull context of a video. In this work, we try to answer how much long-term\ntemporal context is required for temporal action segmentation by introducing a\ntransformer-based model that leverages sparse attention to capture the full\ncontext of a video. We compare our model with the current state of the art on\nthree datasets for temporal action segmentation, namely 50Salads, Breakfast,\nand Assembly101. Our experiments show that modeling the full context of a video\nis necessary to obtain the best performance for temporal action segmentation.",
        "authors": [
            "Emad Bahrami",
            "Gianpiero Francesca",
            "Juergen Gall"
        ]
    },
    {
        "title": "3D VR Sketch Guided 3D Shape Prototyping and Exploration",
        "url": "http://arxiv.org/abs/2306.10830",
        "abstract": "3D shape modeling is labor-intensive, time-consuming, and requires years of\nexpertise. To facilitate 3D shape modeling, we propose a 3D shape generation\nnetwork that takes a 3D VR sketch as a condition. We assume that sketches are\ncreated by novices without art training and aim to reconstruct geometrically\nrealistic 3D shapes of a given category. To handle potential sketch ambiguity,\nour method creates multiple 3D shapes that align with the original sketch's\nstructure. We carefully design our method, training the model step-by-step and\nleveraging multi-modal 3D shape representation to support training with limited\ntraining data. To guarantee the realism of generated 3D shapes we leverage the\nnormalizing flow that models the distribution of the latent space of 3D shapes.\nTo encourage the fidelity of the generated 3D shapes to an input sketch, we\npropose a dedicated loss that we deploy at different stages of the training\nprocess. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.",
        "authors": [
            "Ling Luo",
            "Pinaki Nath Chowdhury",
            "Tao Xiang",
            "Yi-Zhe Song",
            "Yulia Gryaditskaya"
        ]
    },
    {
        "title": "Generative Novel View Synthesis with 3D-Aware Diffusion Models",
        "url": "http://arxiv.org/abs/2304.02602",
        "abstract": "We present a diffusion-based model for 3D-aware generative novel view\nsynthesis from as few as a single input image. Our model samples from the\ndistribution of possible renderings consistent with the input and, even in the\npresence of ambiguity, is capable of rendering diverse and plausible novel\nviews. To achieve this, our method makes use of existing 2D diffusion backbones\nbut, crucially, incorporates geometry priors in the form of a 3D feature\nvolume. This latent feature field captures the distribution over possible scene\nrepresentations and improves our method's ability to generate view-consistent\nnovel renderings. In addition to generating novel views, our method has the\nability to autoregressively synthesize 3D-consistent sequences. We demonstrate\nstate-of-the-art results on synthetic renderings and room-scale scenes; we also\nshow compelling results for challenging, real-world objects.",
        "authors": [
            "Eric R. Chan",
            "Koki Nagano",
            "Matthew A. Chan",
            "Alexander W. Bergman",
            "Jeong Joon Park",
            "Axel Levy",
            "Miika Aittala",
            "Shalini De Mello",
            "Tero Karras",
            "Gordon Wetzstein"
        ]
    },
    {
        "title": "MDCS: More Diverse Experts with Consistency Self-distillation for Long-tailed Recognition",
        "url": "http://arxiv.org/abs/2308.09922",
        "abstract": "Recently, multi-expert methods have led to significant improvements in\nlong-tail recognition (LTR). We summarize two aspects that need further\nenhancement to contribute to LTR boosting: (1) More diverse experts; (2) Lower\nmodel variance. However, the previous methods didn't handle them well. To this\nend, we propose More Diverse experts with Consistency Self-distillation (MDCS)\nto bridge the gap left by earlier methods. Our MDCS approach consists of two\ncore components: Diversity Loss (DL) and Consistency Self-distillation (CS). In\ndetail, DL promotes diversity among experts by controlling their focus on\ndifferent categories. To reduce the model variance, we employ KL divergence to\ndistill the richer knowledge of weakly augmented instances for the experts'\nself-distillation. In particular, we design Confident Instance Sampling (CIS)\nto select the correctly classified instances for CS to avoid biased/noisy\nknowledge. In the analysis and ablation study, we demonstrate that our method\ncompared with previous work can effectively increase the diversity of experts,\nsignificantly reduce the variance of the model, and improve recognition\naccuracy. Moreover, the roles of our DL and CS are mutually reinforcing and\ncoupled: the diversity of experts benefits from the CS, and the CS cannot\nachieve remarkable results without the DL. Experiments show our MDCS\noutperforms the state-of-the-art by 1% $\\sim$ 2% on five popular long-tailed\nbenchmarks, including CIFAR10-LT, CIFAR100-LT, ImageNet-LT, Places-LT, and\niNaturalist 2018. The code is available at https://github.com/fistyee/MDCS.",
        "authors": [
            "Qihao Zhao",
            "Chen Jiang",
            "Wei Hu",
            "Fan Zhang",
            "Jun Liu"
        ]
    },
    {
        "title": "LVOS: A Benchmark for Long-term Video Object Segmentation",
        "url": "http://arxiv.org/abs/2211.10181",
        "abstract": "Existing video object segmentation (VOS) benchmarks focus on short-term\nvideos which just last about 3-5 seconds and where objects are visible most of\nthe time. These videos are poorly representative of practical applications, and\nthe absence of long-term datasets restricts further investigation of VOS on the\napplication in realistic scenarios. So, in this paper, we present a new\nbenchmark dataset named \\textbf{LVOS}, which consists of 220 videos with a\ntotal duration of 421 minutes. To the best of our knowledge, LVOS is the first\ndensely annotated long-term VOS dataset. The videos in our LVOS last 1.59\nminutes on average, which is 20 times longer than videos in existing VOS\ndatasets. Each video includes various attributes, especially challenges\nderiving from the wild, such as long-term reappearing and cross-temporal\nsimilar objeccts.Based on LVOS, we assess existing video object segmentation\nalgorithms and propose a Diverse Dynamic Memory network (DDMemory) that\nconsists of three complementary memory banks to exploit temporal information\nadequately. The experimental results demonstrate the strength and weaknesses of\nprior methods, pointing promising directions for further study. Data and code\nare available at https://lingyihongfd.github.io/lvos.github.io/.",
        "authors": [
            "Lingyi Hong",
            "Wenchao Chen",
            "Zhongying Liu",
            "Wei Zhang",
            "Pinxue Guo",
            "Zhaoyu Chen",
            "Wenqiang Zhang"
        ]
    },
    {
        "title": "Diffusion Model as Representation Learner",
        "url": "http://arxiv.org/abs/2308.10916",
        "abstract": "Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive\nresults on various generative tasks.Despite its promises, the learned\nrepresentations of pre-trained DPMs, however, have not been fully understood.\nIn this paper, we conduct an in-depth investigation of the representation power\nof DPMs, and propose a novel knowledge transfer method that leverages the\nknowledge acquired by generative DPMs for recognition tasks. Our study begins\nby examining the feature space of DPMs, revealing that DPMs are inherently\ndenoising autoencoders that balance the representation learning with\nregularizing model capacity. To this end, we introduce a novel knowledge\ntransfer paradigm named RepFusion. Our paradigm extracts representations at\ndifferent time steps from off-the-shelf DPMs and dynamically employs them as\nsupervision for student networks, in which the optimal time is determined\nthrough reinforcement learning. We evaluate our approach on several image\nclassification, semantic segmentation, and landmark detection benchmarks, and\ndemonstrate that it outperforms state-of-the-art methods. Our results uncover\nthe potential of DPMs as a powerful tool for representation learning and\nprovide insights into the usefulness of generative models beyond sample\ngeneration. The code is available at\n\\url{https://github.com/Adamdad/Repfusion}.",
        "authors": [
            "Xingyi Yang",
            "Xinchao Wang"
        ]
    },
    {
        "title": "Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs",
        "url": "http://arxiv.org/abs/2304.10532",
        "abstract": "Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such\nas floaters or flawed geometry when rendered outside the camera trajectory.\nExisting evaluation protocols often do not capture these effects, since they\nusually only assess image quality at every 8th frame of the training capture.\nTo push forward progress in novel-view synthesis, we propose a new dataset and\nevaluation procedure, where two camera trajectories are recorded of the scene:\none used for training, and the other for evaluation. In this more challenging\nin-the-wild setting, we find that existing hand-crafted regularizers do not\nremove floaters nor improve scene geometry. Thus, we propose a 3D\ndiffusion-based method that leverages local 3D priors and a novel density-based\nscore distillation sampling loss to discourage artifacts during NeRF\noptimization. We show that this data-driven prior removes floaters and improves\nscene geometry for casual captures.",
        "authors": [
            "Frederik Warburg",
            "Ethan Weber",
            "Matthew Tancik",
            "Aleksander Holynski",
            "Angjoo Kanazawa"
        ]
    },
    {
        "title": "Document Understanding Dataset and Evaluation (DUDE)",
        "url": "http://arxiv.org/abs/2305.08455",
        "abstract": "We call on the Document AI (DocAI) community to reevaluate current\nmethodologies and embrace the challenge of creating more practically-oriented\nbenchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to\nremediate the halted research progress in understanding visually-rich documents\n(VRDs). We present a new dataset with novelties related to types of questions,\nanswers, and document layouts based on multi-industry, multi-domain, and\nmulti-page VRDs of various origins, and dates. Moreover, we are pushing the\nboundaries of current methods by creating multi-task and multi-domain\nevaluation setups that more accurately simulate real-world situations where\npowerful generalization and adaptation under low-resource settings are desired.\nDUDE aims to set a new standard as a more practical, long-standing benchmark\nfor the community, and we hope that it will lead to future extensions and\ncontributions that address real-world challenges. Finally, our work illustrates\nthe importance of finding more efficient ways to model language, images, and\nlayout in DocAI.",
        "authors": [
            "Jordy Van Landeghem",
            "Rub\u00e9n Tito",
            "\u0141ukasz Borchmann",
            "Micha\u0142 Pietruszka",
            "Pawe\u0142 J\u00f3ziak",
            "Rafa\u0142 Powalski",
            "Dawid Jurkiewicz",
            "Micka\u00ebl Coustaty",
            "Bertrand Ackaert",
            "Ernest Valveny",
            "Matthew Blaschko",
            "Sien Moens",
            "Tomasz Stanis\u0142awek"
        ]
    },
    {
        "title": "ALWOD: Active Learning for Weakly-Supervised Object Detection",
        "url": "http://arxiv.org/abs/2309.07914",
        "abstract": "Object detection (OD), a crucial vision task, remains challenged by the lack\nof large training datasets with precise object localization labels. In this\nwork, we propose ALWOD, a new framework that addresses this problem by fusing\nactive learning (AL) with weakly and semi-supervised object detection\nparadigms. Because the performance of AL critically depends on the model\ninitialization, we propose a new auxiliary image generator strategy that\nutilizes an extremely small labeled set, coupled with a large weakly tagged set\nof images, as a warm-start for AL. We then propose a new AL acquisition\nfunction, another critical factor in AL success, that leverages the\nstudent-teacher OD pair disagreement and uncertainty to effectively propose the\nmost informative images to annotate. Finally, to complete the AL loop, we\nintroduce a new labeling task delegated to human annotators, based on selection\nand correction of model-proposed detections, which is both rapid and effective\nin labeling the informative images. We demonstrate, across several challenging\nbenchmarks, that ALWOD significantly narrows the gap between the ODs trained on\nfew partially labeled but strategically selected image instances and those that\nrely on the fully-labeled data. Our code is publicly available on\nhttps://github.com/seqam-lab/ALWOD.",
        "authors": [
            "Yuting Wang",
            "Velibor Ilic",
            "Jiatong Li",
            "Branislav Kisacanin",
            "Vladimir Pavlovic"
        ]
    },
    {
        "title": "Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.04952",
        "abstract": "Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic\nSegmentation (FSS) to simultaneously segment unseen classes and seen classes\nduring evaluation. Previous works leverage additional branch or prototypical\naggregation to eliminate the constrained setting of FSS. However,\nrepresentation division and embedding prejudice, which heavily results in poor\nperformance of GFSS, have not been synthetical considered. We address the\naforementioned problems by jointing the prototypical kernel learning and\nopen-set foreground perception. Specifically, a group of learnable kernels is\nproposed to perform segmentation with each kernel in charge of a stuff class.\nThen, we explore to merge the prototypical learning to the update of base-class\nkernels, which is consistent with the prototype knowledge aggregation of\nfew-shot novel classes. In addition, a foreground contextual perception module\ncooperating with conditional bias based inference is adopted to perform\nclass-agnostic as well as open-set foreground detection, thus to mitigate the\nembedding prejudice and prevent novel targets from being misclassified as\nbackground. Moreover, we also adjust our method to the Class Incremental\nFew-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel\nclasses in a incremental stream. Extensive experiments on PASCAL-5i and\nCOCO-20i datasets demonstrate that our method performs better than previous\nstate-of-the-art.",
        "authors": [
            "Kai Huang",
            "Feigege Wang",
            "Ye Xi",
            "Yutao Gao"
        ]
    },
    {
        "title": "CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos",
        "url": "http://arxiv.org/abs/2303.09713",
        "abstract": "Visual information is central to conversation: body gestures and physical\nbehaviour, for example, contribute to meaning that transcends words alone. To\ndate, however, most neural conversational models are limited to just text. We\nintroduce CHAMPAGNE, a generative model of conversations that can account for\nvisual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a\nlarge-scale corpus of 18M video-based dialogues. YTD-18M is constructed from\nweb videos: crucial to our data collection pipeline is a pretrained language\nmodel that converts error-prone automatic transcripts to a cleaner dialogue\nformat while maintaining meaning. Human evaluation reveals that YTD-18M is more\nsensible and specific than prior resources (MMDialog, 1M dialogues), while\nmaintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE\nlearns to conduct conversation from YTD-18M; and 2) when fine-tuned, it\nachieves state-of-the-art results on four vision-language tasks focused on\nreal-world conversations. We release data, models, and code.",
        "authors": [
            "Seungju Han",
            "Jack Hessel",
            "Nouha Dziri",
            "Yejin Choi",
            "Youngjae Yu"
        ]
    },
    {
        "title": "SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding",
        "url": "http://arxiv.org/abs/2211.15660",
        "abstract": "Remote sensing images are useful for a wide variety of planet monitoring\napplications, from tracking deforestation to tackling illegal fishing. The\nEarth is extremely diverse -- the amount of potential tasks in remote sensing\nimages is massive, and the sizes of features range from several kilometers to\njust tens of centimeters. However, creating generalizable computer vision\nmethods is a challenge in part due to the lack of a large-scale dataset that\ncaptures these diverse features for many tasks. In this paper, we present\nSatlasPretrain, a remote sensing dataset that is large in both breadth and\nscale, combining Sentinel-2 and NAIP images with 302M labels under 137\ncategories and seven label types. We evaluate eight baselines and a proposed\nmethod on SatlasPretrain, and find that there is substantial room for\nimprovement in addressing research challenges specific to remote sensing,\nincluding processing image time series that consist of images from very\ndifferent types of sensors, and taking advantage of long-range spatial context.\nMoreover, we find that pre-training on SatlasPretrain substantially improves\nperformance on downstream tasks, increasing average accuracy by 18% over\nImageNet and 6% over the next best baseline. The dataset, pre-trained model\nweights, and code are available at https://satlas-pretrain.allen.ai/.",
        "authors": [
            "Favyen Bastani",
            "Piper Wolters",
            "Ritwik Gupta",
            "Joe Ferdinando",
            "Aniruddha Kembhavi"
        ]
    },
    {
        "title": "Empowering Low-Light Image Enhancer through Customized Learnable Priors",
        "url": "http://arxiv.org/abs/2309.01958",
        "abstract": "Deep neural networks have achieved remarkable progress in enhancing low-light\nimages by improving their brightness and eliminating noise. However, most\nexisting methods construct end-to-end mapping networks heuristically,\nneglecting the intrinsic prior of image enhancement task and lacking\ntransparency and interpretability. Although some unfolding solutions have been\nproposed to relieve these issues, they rely on proximal operator networks that\ndeliver ambiguous and implicit priors. In this work, we propose a paradigm for\nlow-light image enhancement that explores the potential of customized learnable\npriors to improve the transparency of the deep unfolding paradigm. Motivated by\nthe powerful feature representation capability of Masked Autoencoder (MAE), we\ncustomize MAE-based illumination and noise priors and redevelop them from two\nperspectives: 1) \\textbf{structure flow}: we train the MAE from a normal-light\nimage to its illumination properties and then embed it into the proximal\noperator design of the unfolding architecture; and m2) \\textbf{optimization\nflow}: we train MAE from a normal-light image to its gradient representation\nand then employ it as a regularization term to constrain noise in the model\noutput. These designs improve the interpretability and representation\ncapability of the model.Extensive experiments on multiple low-light image\nenhancement datasets demonstrate the superiority of our proposed paradigm over\nstate-of-the-art methods. Code is available at\nhttps://github.com/zheng980629/CUE.",
        "authors": [
            "Naishan Zheng",
            "Man Zhou",
            "Yanmeng Dong",
            "Xiangyu Rui",
            "Jie Huang",
            "Chongyi Li",
            "Feng Zhao"
        ]
    },
    {
        "title": "TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation",
        "url": "http://arxiv.org/abs/2307.14611",
        "abstract": "We propose TextManiA, a text-driven manifold augmentation method that\nsemantically enriches visual feature spaces, regardless of class distribution.\nTextManiA augments visual data with intra-class semantic perturbation by\nexploiting easy-to-understand visually mimetic words, i.e., attributes. This\nwork is built on an interesting hypothesis that general language models, e.g.,\nBERT and GPT, encompass visual information to some extent, even without\ntraining on visual training data. Given the hypothesis, TextManiA transfers\npre-trained text representation obtained from a well-established large language\nencoder to a target visual feature space being learned. Our extensive analysis\nhints that the language encoder indeed encompasses visual information at least\nuseful to augment visual representation. Our experiments demonstrate that\nTextManiA is particularly powerful in scarce samples with class imbalance as\nwell as even distribution. We also show compatibility with the label mix-based\napproaches in evenly distributed scarce data.",
        "authors": [
            "Moon Ye-Bin",
            "Jisoo Kim",
            "Hongyeob Kim",
            "Kilho Son",
            "Tae-Hyun Oh"
        ]
    },
    {
        "title": "Guiding Image Captioning Models Toward More Specific Captions",
        "url": "http://arxiv.org/abs/2307.16686",
        "abstract": "Image captioning is conventionally formulated as the task of generating\ncaptions for images that match the distribution of reference image-caption\npairs. However, reference captions in standard captioning datasets are short\nand may not uniquely identify the images they describe. These problems are\nfurther exacerbated when models are trained directly on image-alt text pairs\ncollected from the internet. In this work, we show that it is possible to\ngenerate more specific captions with minimal changes to the training process.\nWe implement classifier-free guidance for an autoregressive captioning model by\nfine-tuning it to estimate both conditional and unconditional distributions\nover captions. The guidance scale applied at decoding controls a trade-off\nbetween maximizing $p(\\mathrm{caption}|\\mathrm{image})$ and\n$p(\\mathrm{image}|\\mathrm{caption})$. Compared to standard greedy decoding,\ndecoding with a guidance scale of 2 substantially improves reference-free\nmetrics such as CLIPScore (0.808 vs. 0.775) and caption$\\to$image retrieval\nperformance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens\nstandard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We\nfurther explore the use of language models to guide the decoding process,\nobtaining small improvements over the Pareto frontier of reference-free vs.\nreference-based captioning metrics that arises from classifier-free guidance,\nand substantially improving the quality of captions generated from a model\ntrained only on minimally curated web data.",
        "authors": [
            "Simon Kornblith",
            "Lala Li",
            "Zirui Wang",
            "Thao Nguyen"
        ]
    },
    {
        "title": "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images",
        "url": "http://arxiv.org/abs/2303.07274",
        "abstract": "Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io",
        "authors": [
            "Nitzan Bitton-Guetta",
            "Yonatan Bitton",
            "Jack Hessel",
            "Ludwig Schmidt",
            "Yuval Elovici",
            "Gabriel Stanovsky",
            "Roy Schwartz"
        ]
    },
    {
        "title": "FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models",
        "url": "http://arxiv.org/abs/2308.05733",
        "abstract": "3D scene reconstruction is a long-standing vision task. Existing approaches\ncan be categorized into geometry-based and learning-based methods. The former\nleverages multi-view geometry but can face catastrophic failures due to the\nreliance on accurate pixel correspondence across views. The latter was\nproffered to mitigate these issues by learning 2D or 3D representation\ndirectly. However, without a large-scale video or 3D training data, it can\nhardly generalize to diverse real-world scenarios due to the presence of tens\nof millions or even billions of optimization parameters in the deep network.\nRecently, robust monocular depth estimation models trained with large-scale\ndatasets have been proven to possess weak 3D geometry prior, but they are\ninsufficient for reconstruction due to the unknown camera parameters, the\naffine-invariant property, and inter-frame inconsistency. Here, we propose a\nnovel test-time optimization approach that can transfer the robustness of\naffine-invariant depth models such as LeReS to challenging diverse scenes while\nensuring inter-frame consistency, with only dozens of parameters to optimize\nper video frame. Specifically, our approach involves freezing the pre-trained\naffine-invariant depth model's depth predictions, rectifying them by optimizing\nthe unknown scale-shift values with a geometric consistency alignment module,\nand employing the resulting scale-consistent depth maps to robustly obtain\ncamera poses and achieve dense scene reconstruction, even in low-texture\nregions. Experiments show that our method achieves state-of-the-art\ncross-dataset reconstruction on five zero-shot testing datasets.",
        "authors": [
            "Guangkai Xu",
            "Wei Yin",
            "Hao Chen",
            "Chunhua Shen",
            "Kai Cheng",
            "Feng Zhao"
        ]
    },
    {
        "title": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation",
        "url": "http://arxiv.org/abs/2212.03741",
        "abstract": "Generating full-body and multi-genre dance sequences from given music is a\nchallenging task, due to the limitations of existing datasets and the inherent\ncomplexity of the fine-grained hand motion and dance genres. To address these\nproblems, we propose FineDance, which contains 14.6 hours of music-dance paired\ndata, with fine-grained hand motions, fine-grained genres (22 dance genres),\nand accurate posture. To the best of our knowledge, FineDance is the largest\nmusic-dance paired dataset with the most dance genres. Additionally, to address\nmonotonous and unnatural hand movements existing in previous methods, we\npropose a full-body dance generation network, which utilizes the diverse\ngeneration capabilities of the diffusion model to solve monotonous problems,\nand use expert nets to solve unreal problems. To further enhance the\ngenre-matching and long-term stability of generated dances, we propose a\nGenre&Coherent aware Retrieval Module. Besides, we propose a novel metric named\nGenre Matching Score to evaluate the genre-matching degree between dance and\nmusic. Quantitative and qualitative experiments demonstrate the quality of\nFineDance, and the state-of-the-art performance of FineNet. The FineDance\nDataset and more qualitative samples can be found at our website.",
        "authors": [
            "Ronghui Li",
            "Junfan Zhao",
            "Yachao Zhang",
            "Mingyang Su",
            "Zeping Ren",
            "Han Zhang",
            "Yansong Tang",
            "Xiu Li"
        ]
    },
    {
        "title": "AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation",
        "url": "http://arxiv.org/abs/2303.13953",
        "abstract": "Both indoor and outdoor environments are inherently structured and\nrepetitive. Traditional modeling pipelines keep an asset library storing unique\nobject templates, which is both versatile and memory efficient in practice.\nInspired by this observation, we propose AssetField, a novel neural scene\nrepresentation that learns a set of object-aware ground feature planes to\nrepresent the scene, where an asset library storing template feature patches\ncan be constructed in an unsupervised manner. Unlike existing methods which\nrequire object masks to query spatial points for object editing, our ground\nfeature plane representation offers a natural visualization of the scene in the\nbird-eye view, allowing a variety of operations (e.g. translation, duplication,\ndeformation) on objects to configure a new scene. With the template feature\npatches, group editing is enabled for scenes with many recurring items to avoid\nrepetitive work on object individuals. We show that AssetField not only\nachieves competitive performance for novel-view synthesis but also generates\nrealistic renderings for new scene configurations.",
        "authors": [
            "Yuanbo Xiangli",
            "Linning Xu",
            "Xingang Pan",
            "Nanxuan Zhao",
            "Bo Dai",
            "Dahua Lin"
        ]
    },
    {
        "title": "Improving Online Lane Graph Extraction by Object-Lane Clustering",
        "url": "http://arxiv.org/abs/2307.10947",
        "abstract": "Autonomous driving requires accurate local scene understanding information.\nTo this end, autonomous agents deploy object detection and online BEV lane\ngraph extraction methods as a part of their perception stack. In this work, we\npropose an architecture and loss formulation to improve the accuracy of local\nlane graph estimates by using 3D object detection outputs. The proposed method\nlearns to assign the objects to centerlines by considering the centerlines as\ncluster centers and the objects as data points to be assigned a probability\ndistribution over the cluster centers. This training scheme ensures direct\nsupervision on the relationship between lanes and objects, thus leading to\nbetter performance. The proposed method improves lane graph estimation\nsubstantially over state-of-the-art methods. The extensive ablations show that\nour method can achieve significant performance improvements by using the\noutputs of existing 3D object detection methods. Since our method uses the\ndetection outputs rather than detection method intermediate representations, a\nsingle model of our method can use any detection method at test time.",
        "authors": [
            "Yigit Baran Can",
            "Alexander Liniger",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ]
    },
    {
        "title": "SAGA: Spectral Adversarial Geometric Attack on 3D Meshes",
        "url": "http://arxiv.org/abs/2211.13775",
        "abstract": "A triangular mesh is one of the most popular 3D data representations. As\nsuch, the deployment of deep neural networks for mesh processing is widely\nspread and is increasingly attracting more attention. However, neural networks\nare prone to adversarial attacks, where carefully crafted inputs impair the\nmodel's functionality. The need to explore these vulnerabilities is a\nfundamental factor in the future development of 3D-based applications.\nRecently, mesh attacks were studied on the semantic level, where classifiers\nare misled to produce wrong predictions. Nevertheless, mesh surfaces possess\ncomplex geometric attributes beyond their semantic meaning, and their analysis\noften includes the need to encode and reconstruct the geometry of the shape.\n  We propose a novel framework for a geometric adversarial attack on a 3D mesh\nautoencoder. In this setting, an adversarial input mesh deceives the\nautoencoder by forcing it to reconstruct a different geometric shape at its\noutput. The malicious input is produced by perturbing a clean shape in the\nspectral domain. Our method leverages the spectral decomposition of the mesh\nalong with additional mesh-related properties to obtain visually credible\nresults that consider the delicacy of surface distortions. Our code is publicly\navailable at https://github.com/StolikTomer/SAGA.",
        "authors": [
            "Tomer Stolik",
            "Itai Lang",
            "Shai Avidan"
        ]
    },
    {
        "title": "All in Tokens: Unifying Output Space of Visual Tasks via Soft Token",
        "url": "http://arxiv.org/abs/2301.02229",
        "abstract": "Unlike language tasks, where the output space is usually limited to a set of\ntokens, the output space of visual tasks is more complicated, making it\ndifficult to build a unified visual model for various visual tasks. In this\npaper, we seek to unify the output space of visual tasks, so that we can also\nbuild a unified model for visual tasks. To this end, we demonstrate a single\nunified model that simultaneously handles two typical visual tasks of instance\nsegmentation and depth estimation, which have discrete/fixed-length and\ncontinuous/varied-length outputs, respectively. We propose several new\ntechniques that take into account the particularity of visual tasks: 1) Soft\ntoken. We employ soft token to represent the task output. Unlike hard tokens in\nthe common VQ-VAE which are assigned one-hot to discrete\ncodebooks/vocabularies, the soft token is assigned softly to the codebook\nembeddings. Soft token can improve the accuracy of both the next token\ninference and decoding of the task output; 2) Mask augmentation. Many visual\ntasks have corruption, undefined or invalid values in label annotations, i.e.,\noccluded area of depth maps. We show that a mask augmentation technique can\ngreatly benefit these tasks. With these new techniques and other designs, we\nshow that the proposed general-purpose task-solver can perform both instance\nsegmentation and depth estimation well. Particularly, we achieve 0.279 RMSE on\nthe specific task of NYUv2 depth estimation, setting a new record on this\nbenchmark. The general-purpose task-solver, dubbed AiT, is available at\n\\url{https://github.com/SwinTransformer/AiT}.",
        "authors": [
            "Jia Ning",
            "Chen Li",
            "Zheng Zhang",
            "Zigang Geng",
            "Qi Dai",
            "Kun He",
            "Han Hu"
        ]
    },
    {
        "title": "Learning Navigational Visual Representations with Semantic Map Supervision",
        "url": "http://arxiv.org/abs/2307.12335",
        "abstract": "Being able to perceive the semantics and the spatial structure of the\nenvironment is essential for visual navigation of a household robot. However,\nmost existing works only employ visual backbones pre-trained either with\nindependent images for classification or with self-supervised learning methods\nto adapt to the indoor navigation domain, neglecting the spatial relationships\nthat are essential to the learning of navigation. Inspired by the behavior that\nhumans naturally build semantically and spatially meaningful cognitive maps in\ntheir brains during navigation, in this paper, we propose a novel\nnavigational-specific visual representation learning method by contrasting the\nagent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual\ntransformer as the backbone encoder and train the model with data collected\nfrom the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning\ntransfers the compact and rich information from a map, such as objects,\nstructure and transition, to the agent's egocentric representations for\nnavigation. Experiments show that agents using our learned representations on\nobject-goal navigation outperform recent visual pre-training methods. Moreover,\nour representations significantly improve vision-and-language navigation in\ncontinuous environments for both high-level and low-level action spaces,\nachieving new state-of-the-art results of 47% SR and 41% SPL on the test\nserver.",
        "authors": [
            "Yicong Hong",
            "Yang Zhou",
            "Ruiyi Zhang",
            "Franck Dernoncourt",
            "Trung Bui",
            "Stephen Gould",
            "Hao Tan"
        ]
    },
    {
        "title": "LDL: Line Distance Functions for Panoramic Localization",
        "url": "http://arxiv.org/abs/2308.13989",
        "abstract": "We introduce LDL, a fast and robust algorithm that localizes a panorama to a\n3D map using line segments. LDL focuses on the sparse structural information of\nlines in the scene, which is robust to illumination changes and can potentially\nenable efficient computation. While previous line-based localization approaches\ntend to sacrifice accuracy or computation time, our method effectively observes\nthe holistic distribution of lines within panoramic images and 3D maps.\nSpecifically, LDL matches the distribution of lines with 2D and 3D line\ndistance functions, which are further decomposed along principal directions of\nlines to increase the expressiveness. The distance functions provide coarse\npose estimates by comparing the distributional information, where the poses are\nfurther optimized using conventional local feature matching. As our pipeline\nsolely leverages line geometry and local features, it does not require costly\nadditional training of line-specific features or correspondence matching.\nNevertheless, our method demonstrates robust performance on challenging\nscenarios including object layout changes, illumination shifts, and large-scale\nscenes, while exhibiting fast pose search terminating within a matter of\nmilliseconds. We thus expect our method to serve as a practical solution for\nline-based localization, and complement the well-established point-based\nparadigm. The code for LDL is available through the following link:\nhttps://github.com/82magnolia/panoramic-localization.",
        "authors": [
            "Junho Kim",
            "Changwoon Choi",
            "Hojun Jang",
            "Young Min Kim"
        ]
    },
    {
        "title": "TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception",
        "url": "http://arxiv.org/abs/2306.05085",
        "abstract": "This work aims for transferring a Transformer-based image compression codec\nfrom human perception to machine perception without fine-tuning the codec. We\npropose a transferable Transformer-based image compression framework, termed\nTransTIC. Inspired by visual prompt tuning, TransTIC adopts an\ninstance-specific prompt generator to inject instance-specific prompts to the\nencoder and task-specific prompts to the decoder. Extensive experiments show\nthat our proposed method is capable of transferring the base codec to various\nmachine tasks and outperforms the competing methods significantly. To our best\nknowledge, this work is the first attempt to utilize prompting on the low-level\nimage compression task.",
        "authors": [
            "Yi-Hsin Chen",
            "Ying-Chieh Weng",
            "Chia-Hao Kao",
            "Cheng Chien",
            "Wei-Chen Chiu",
            "Wen-Hsiao Peng"
        ]
    },
    {
        "title": "CHORUS : Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images",
        "url": "http://arxiv.org/abs/2308.12288",
        "abstract": "We present a method for teaching machines to understand and model the\nunderlying spatial common sense of diverse human-object interactions in 3D in a\nself-supervised way. This is a challenging task, as there exist specific\nmanifolds of the interactions that can be considered human-like and natural,\nbut the human pose and the geometry of objects can vary even for similar\ninteractions. Such diversity makes the annotating task of 3D interactions\ndifficult and hard to scale, which limits the potential to reason about that in\na supervised way. One way of learning the 3D spatial relationship between\nhumans and objects during interaction is by showing multiple 2D images captured\nfrom different viewpoints when humans interact with the same type of objects.\nThe core idea of our method is to leverage a generative model that produces\nhigh-quality 2D images from an arbitrary text prompt input as an \"unbounded\"\ndata generator with effective controllability and view diversity. Despite its\nimperfection of the image quality over real images, we demonstrate that the\nsynthesized images are sufficient to learn the 3D human-object spatial\nrelations. We present multiple strategies to leverage the synthesized images,\nincluding (1) the first method to leverage a generative image model for 3D\nhuman-object spatial relation learning; (2) a framework to reason about the 3D\nspatial relations from inconsistent 2D cues in a self-supervised manner via 3D\noccupancy reasoning with pose canonicalization; (3) semantic clustering to\ndisambiguate different types of interactions with the same object types; and\n(4) a novel metric to assess the quality of 3D spatial learning of interaction.",
        "authors": [
            "Sookwan Han",
            "Hanbyul Joo"
        ]
    },
    {
        "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data",
        "url": "http://arxiv.org/abs/2308.11194",
        "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained\non datasets consisting of image-caption pairs obtained from the web. However,\nreal-world multimodal datasets, such as healthcare data, are significantly more\ncomplex: each image (e.g. X-ray) is often paired with text (e.g. physician\nreport) that describes many distinct attributes occurring in fine-grained\nregions of the image. We refer to these samples as exhibiting high pairwise\ncomplexity, since each image-text pair can be decomposed into a large number of\nregion-attribute pairings. The extent to which VLMs can capture fine-grained\nrelationships between image regions and textual attributes when trained on such\ndata has not been previously evaluated. The first key contribution of this work\nis to demonstrate through systematic evaluations that as the pairwise\ncomplexity of the training dataset increases, standard VLMs struggle to learn\nregion-attribute relationships, exhibiting performance degradations of up to\n37% on retrieval tasks. In order to address this issue, we introduce ViLLA as\nour second key contribution. ViLLA, which is trained to capture fine-grained\nregion-attribute relationships from complex datasets, involves two components:\n(a) a lightweight, self-supervised mapping model to decompose image-text\nsamples into region-attribute pairs, and (b) a contrastive VLM to learn\nrepresentations from generated region-attribute pairs. We demonstrate with\nexperiments across four domains (synthetic, product, medical, and natural\nimages) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks,\nsuch as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP\npoints on LVIS) and retrieval (up to 14.2 R-Precision points).",
        "authors": [
            "Maya Varma",
            "Jean-Benoit Delbrouck",
            "Sarah Hooper",
            "Akshay Chaudhari",
            "Curtis Langlotz"
        ]
    },
    {
        "title": "Towards Unifying Medical Vision-and-Language Pre-Training via Soft Prompts",
        "url": "http://arxiv.org/abs/2302.08958",
        "abstract": "Medical vision-and-language pre-training (Med-VLP) has shown promising\nimprovements on many downstream medical tasks owing to its applicability to\nextracting generic representations from medical images and texts. Practically,\nthere exist two typical types, \\textit{i.e.}, the fusion-encoder type and the\ndual-encoder type, depending on whether a heavy fusion module is used. The\nformer is superior at multi-modal tasks owing to the sufficient interaction\nbetween modalities; the latter is good at uni-modal and cross-modal tasks due\nto the single-modality encoding ability. To take advantage of these two types,\nwe propose an effective yet straightforward scheme named PTUnifier to unify the\ntwo types. We first unify the input format by introducing visual and textual\nprompts, which serve as a feature bank that stores the most representative\nimages/texts. By doing so, a single model could serve as a \\textit{foundation\nmodel} that processes various tasks adopting different input formats\n(\\textit{i.e.}, image-only, text-only, and image-text-pair). Furthermore, we\nconstruct a prompt pool (instead of static ones) to improve diversity and\nscalability. Experimental results show that our approach achieves\nstate-of-the-art results on a broad range of tasks, spanning uni-modal tasks\n(\\textit{i.e.}, image/text classification and text summarization), cross-modal\ntasks (\\textit{i.e.}, image-to-text generation and image-text/text-image\nretrieval), and multi-modal tasks (\\textit{i.e.}, visual question answering),\ndemonstrating the effectiveness of our approach. Note that the adoption of\nprompts is orthogonal to most existing Med-VLP approaches and could be a\nbeneficial and complementary extension to these approaches.",
        "authors": [
            "Zhihong Chen",
            "Shizhe Diao",
            "Benyou Wang",
            "Guanbin Li",
            "Xiang Wan"
        ]
    },
    {
        "title": "A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition",
        "url": "http://arxiv.org/abs/2303.13505",
        "abstract": "The goal of building a benchmark (suite of datasets) is to provide a unified\nprotocol for fair evaluation and thus facilitate the evolution of a specific\narea. Nonetheless, we point out that existing protocols of action recognition\ncould yield partial evaluations due to several limitations. To comprehensively\nprobe the effectiveness of spatiotemporal representation learning, we introduce\nBEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18\nvideo datasets grouped into 5 categories (anomaly, gesture, daily, sports, and\ninstructional), which covers a diverse set of real-world applications. With\nBEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both\nsupervised and self-supervised learning. We also report transfer performance\nvia standard finetuning, few-shot finetuning, and unsupervised domain\nadaptation. Our observation suggests that current state-of-the-art cannot\nsolidly guarantee high performance on datasets close to real-world\napplications, and we hope BEAR can serve as a fair and challenging evaluation\nbenchmark to gain insights on building next-generation spatiotemporal learners.\nOur dataset, code, and models are released at:\nhttps://github.com/AndongDeng/BEAR",
        "authors": [
            "Andong Deng",
            "Taojiannan Yang",
            "Chen Chen"
        ]
    },
    {
        "title": "Video Background Music Generation: Dataset, Method and Evaluation",
        "url": "http://arxiv.org/abs/2211.11248",
        "abstract": "Music is essential when editing videos, but selecting music manually is\ndifficult and time-consuming. Thus, we seek to automatically generate\nbackground music tracks given video input. This is a challenging task since it\nrequires music-video datasets, efficient architectures for video-to-music\ngeneration, and reasonable metrics, none of which currently exist. To close\nthis gap, we introduce a complete recipe including dataset, benchmark model,\nand evaluation metric for video background music generation. We present SymMV,\na video and symbolic music dataset with various musical annotations. To the\nbest of our knowledge, it is the first video-music dataset with rich musical\nannotations. We also propose a benchmark video background music generation\nframework named V-MusProd, which utilizes music priors of chords, melody, and\naccompaniment along with video-music relations of semantic, color, and motion\nfeatures. To address the lack of objective metrics for video-music\ncorrespondence, we design a retrieval-based metric VMCP built upon a powerful\nvideo-music representation learning model. Experiments show that with our\ndataset, V-MusProd outperforms the state-of-the-art method in both music\nquality and correspondence with videos. We believe our dataset, benchmark\nmodel, and evaluation metric will boost the development of video background\nmusic generation. Our dataset and code are available at\nhttps://github.com/zhuole1025/SymMV.",
        "authors": [
            "Le Zhuo",
            "Zhaokai Wang",
            "Baisen Wang",
            "Yue Liao",
            "Chenxi Bao",
            "Stanley Peng",
            "Songhao Han",
            "Aixi Zhang",
            "Fei Fang",
            "Si Liu"
        ]
    },
    {
        "title": "HoloFusion: Towards Photo-realistic 3D Generative Modeling",
        "url": "http://arxiv.org/abs/2308.14244",
        "abstract": "Diffusion-based image generators can now produce high-quality and diverse\nsamples, but their success has yet to fully translate to 3D generation:\nexisting diffusion methods can either generate low-resolution but 3D consistent\noutputs, or detailed 2D views of 3D objects but with potential structural\ndefects and lacking view consistency or realism. We present HoloFusion, a\nmethod that combines the best of these approaches to produce high-fidelity,\nplausible, and diverse 3D samples while learning from a collection of\nmulti-view 2D images only. The method first generates coarse 3D samples using a\nvariant of the recently proposed HoloDiffusion generator. Then, it\nindependently renders and upsamples a large number of views of the coarse 3D\nmodel, super-resolves them to add detail, and distills those into a single,\nhigh-fidelity implicit 3D representation, which also ensures view consistency\nof the final renders. The super-resolution network is trained as an integral\npart of HoloFusion, end-to-end, and the final distillation uses a new sampling\nscheme to capture the space of super-resolved signals. We compare our method\nagainst existing baselines, including DreamFusion, Get3D, EG3D, and\nHoloDiffusion, and achieve, to the best of our knowledge, the most realistic\nresults on the challenging CO3Dv2 dataset.",
        "authors": [
            "Animesh Karnewar",
            "Niloy J. Mitra",
            "Andrea Vedaldi",
            "David Novotny"
        ]
    },
    {
        "title": "Improving Continuous Sign Language Recognition with Cross-Lingual Signs",
        "url": "http://arxiv.org/abs/2308.10809",
        "abstract": "This work dedicates to continuous sign language recognition (CSLR), which is\na weakly supervised task dealing with the recognition of continuous signs from\nvideos, without any prior knowledge about the temporal boundaries between\nconsecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing\napproaches typically train CSLR models on a monolingual corpus, which is orders\nof magnitude smaller than that of speech recognition. In this work, we explore\nthe feasibility of utilizing multilingual sign language corpora to facilitate\nmonolingual CSLR. Our work is built upon the observation of cross-lingual\nsigns, which originate from different sign languages but have similar visual\nsignals (e.g., hand shape and motion). The underlying idea of our approach is\nto identify the cross-lingual signs in one sign language and properly leverage\nthem as auxiliary training data to improve the recognition capability of\nanother. To achieve the goal, we first build two sign language dictionaries\ncontaining isolated signs that appear in two datasets. Then we identify the\nsign-to-sign mappings between two sign languages via a well-optimized isolated\nsign language recognition model. At last, we train a CSLR model on the\ncombination of the target data with original labels and the auxiliary data with\nmapped labels. Experimentally, our approach achieves state-of-the-art\nperformance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.",
        "authors": [
            "Fangyun Wei",
            "Yutong Chen"
        ]
    },
    {
        "title": "Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation",
        "url": "http://arxiv.org/abs/2308.00356",
        "abstract": "Given a composite image, image harmonization aims to adjust the foreground\nillumination to be consistent with background. Previous methods have explored\ntransforming foreground features to achieve competitive performance. In this\nwork, we show that using global information to guide foreground feature\ntransformation could achieve significant improvement. Besides, we propose to\ntransfer the foreground-background relation from real images to composite\nimages, which can provide intermediate supervision for the transformed encoder\nfeatures. Additionally, considering the drawbacks of existing harmonization\ndatasets, we also contribute a ccHarmony dataset which simulates the natural\nillumination variation. Extensive experiments on iHarmony4 and our contributed\ndataset demonstrate the superiority of our method. Our ccHarmony dataset is\nreleased at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.",
        "authors": [
            "Li Niu",
            "Linfeng Tan",
            "Xinhao Tao",
            "Junyan Cao",
            "Fengjun Guo",
            "Teng Long",
            "Liqing Zhang"
        ]
    },
    {
        "title": "RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration",
        "url": "http://arxiv.org/abs/2303.12384",
        "abstract": "Although point cloud registration has achieved remarkable advances in\nobject-level and indoor scenes, large-scale registration methods are rarely\nexplored. Challenges mainly arise from the huge point number, complex\ndistribution, and outliers of outdoor LiDAR scans. In addition, most existing\nregistration works generally adopt a two-stage paradigm: They first find\ncorrespondences by extracting discriminative local features and then leverage\nestimators (eg. RANSAC) to filter outliers, which are highly dependent on\nwell-designed descriptors and post-processing choices. To address these\nproblems, we propose an end-to-end transformer network (RegFormer) for\nlarge-scale point cloud alignment without any further post-processing.\nSpecifically, a projection-aware hierarchical transformer is proposed to\ncapture long-range dependencies and filter outliers by extracting point\nfeatures globally. Our transformer has linear complexity, which guarantees high\nefficiency even for large-scale scenes. Furthermore, to effectively reduce\nmismatches, a bijective association transformer is designed for regressing the\ninitial transformation. Extensive experiments on KITTI and NuScenes datasets\ndemonstrate that our RegFormer achieves competitive performance in terms of\nboth accuracy and efficiency.",
        "authors": [
            "Jiuming Liu",
            "Guangming Wang",
            "Zhe Liu",
            "Chaokang Jiang",
            "Marc Pollefeys",
            "Hesheng Wang"
        ]
    },
    {
        "title": "Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation",
        "url": "http://arxiv.org/abs/2308.04549",
        "abstract": "Transformers have become the primary backbone of the computer vision\ncommunity due to their impressive performance. However, the unfriendly\ncomputation cost impedes their potential in the video recognition domain. To\noptimize the speed-accuracy trade-off, we propose Semantic-aware Temporal\nAccumulation score (STA) to prune spatio-temporal tokens integrally. STA score\nconsiders two critical factors: temporal redundancy and semantic importance.\nThe former depicts a specific region based on whether it is a new occurrence or\na seen entity by aggregating token-to-token similarity in consecutive frames\nwhile the latter evaluates each token based on its contribution to the overall\nprediction. As a result, tokens with higher scores of STA carry more temporal\nredundancy as well as lower semantics thus being pruned. Based on the STA\nscore, we are able to progressively prune the tokens without introducing any\nadditional parameters or requiring further re-training. We directly apply the\nSTA module to off-the-shelf ViT and VideoSwin backbones, and the empirical\nresults on Kinetics-400 and Something-Something V2 achieve over 30% computation\nreduction with a negligible ~0.2% accuracy drop. The code is released at\nhttps://github.com/Mark12Ding/STA.",
        "authors": [
            "Shuangrui Ding",
            "Peisen Zhao",
            "Xiaopeng Zhang",
            "Rui Qian",
            "Hongkai Xiong",
            "Qi Tian"
        ]
    },
    {
        "title": "VQ3D: Learning a 3D-Aware Generative Model on ImageNet",
        "url": "http://arxiv.org/abs/2302.06833",
        "abstract": "Recent work has shown the possibility of training generative models of 3D\ncontent from 2D image collections on small datasets corresponding to a single\nobject class, such as human faces, animal faces, or cars. However, these models\nstruggle on larger, more complex datasets. To model diverse and unconstrained\nimage collections such as ImageNet, we present VQ3D, which introduces a\nNeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1\nallows for the reconstruction of an input image and the ability to change the\ncamera position around the image, and our Stage 2 allows for the generation of\nnew 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images\nfrom the 1000-class ImageNet dataset of 1.2 million training images. We achieve\nan ImageNet generation FID score of 16.8, compared to 69.8 for the next best\nbaseline method.",
        "authors": [
            "Kyle Sargent",
            "Jing Yu Koh",
            "Han Zhang",
            "Huiwen Chang",
            "Charles Herrmann",
            "Pratul Srinivasan",
            "Jiajun Wu",
            "Deqing Sun"
        ]
    },
    {
        "title": "Cross-Ray Neural Radiance Fields for Novel-View Synthesis from Unconstrained Image Collections",
        "url": "http://arxiv.org/abs/2307.08093",
        "abstract": "Neural Radiance Fields (NeRF) is a revolutionary approach for rendering\nscenes by sampling a single ray per pixel and it has demonstrated impressive\ncapabilities in novel-view synthesis from static scene images. However, in\npractice, we usually need to recover NeRF from unconstrained image collections,\nwhich poses two challenges: 1) the images often have dynamic changes in\nappearance because of different capturing time and camera settings; 2) the\nimages may contain transient objects such as humans and cars, leading to\nocclusion and ghosting artifacts. Conventional approaches seek to address these\nchallenges by locally utilizing a single ray to synthesize a color of a pixel.\nIn contrast, humans typically perceive appearance and objects by globally\nutilizing information across multiple pixels. To mimic the perception process\nof humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leverages\ninteractive information across multiple rays to synthesize occlusion-free novel\nviews with the same appearances as the images. Specifically, to model varying\nappearances, we first propose to represent multiple rays with a novel cross-ray\nfeature and then recover the appearance by fusing global statistics, i.e.,\nfeature covariance of the rays and the image appearance. Moreover, to avoid\nocclusion introduced by transient objects, we propose a transient objects\nhandler and introduce a grid sampling strategy for masking out the transient\nobjects. We theoretically find that leveraging correlation across multiple rays\npromotes capturing more global information. Moreover, extensive experimental\nresults on large real-world datasets verify the effectiveness of CR-NeRF.",
        "authors": [
            "Yifan Yang",
            "Shuhai Zhang",
            "Zixiong Huang",
            "Yubing Zhang",
            "Mingkui Tan"
        ]
    },
    {
        "title": "SPACE: Speech-driven Portrait Animation with Controllable Expression",
        "url": "http://arxiv.org/abs/2211.09809",
        "abstract": "Animating portraits using speech has received growing attention in recent\nyears, with various creative and practical use cases. An ideal generated video\nshould have good lip sync with the audio, natural facial expressions and head\nmotions, and high frame quality. In this work, we present SPACE, which uses\nspeech and a single image to generate high-resolution, and expressive videos\nwith realistic head pose, without requiring a driving video. It uses a\nmulti-stage approach, combining the controllability of facial landmarks with\nthe high-quality synthesis power of a pretrained face generator. SPACE also\nallows for the control of emotions and their intensities. Our method\noutperforms prior methods in objective metrics for image quality and facial\nmotions and is strongly preferred by users in pair-wise comparisons. The\nproject website is available at https://deepimagination.cc/SPACE/",
        "authors": [
            "Siddharth Gururani",
            "Arun Mallya",
            "Ting-Chun Wang",
            "Rafael Valle",
            "Ming-Yu Liu"
        ]
    },
    {
        "title": "Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures",
        "url": "http://arxiv.org/abs/2210.01887",
        "abstract": "Human pose transfer synthesizes new view(s) of a person for a given pose.\nRecent work achieves this via self-reconstruction, which disentangles a\nperson's pose and texture information by breaking the person down into parts,\nthen recombines them for reconstruction. However, part-level disentanglement\npreserves some pose information that can create unwanted artifacts. In this\npaper, we propose Pose Transfer by Permuting Textures (PT$^2$), an approach for\nself-driven human pose transfer that disentangles pose from texture at the\npatch-level. Specifically, we remove pose from an input image by permuting\nimage patches so only texture information remains. Then we reconstruct the\ninput image by sampling from the permuted textures for patch-level\ndisentanglement. To reduce noise and recover clothing shape information from\nthe permuted patches, we employ encoders with multiple kernel sizes in a triple\nbranch network. On DeepFashion and Market-1501, PT$^2$ reports significant\ngains on automatic metrics over other self-driven methods, and even outperforms\nsome fully-supervised methods. A user study also reports images generated by\nour method are preferred in 68% of cases over self-driven approaches from prior\nwork. Code is available at https://github.com/NannanLi999/pt_square.",
        "authors": [
            "Nannan Li",
            "Kevin J. Shih",
            "Bryan A. Plummer"
        ]
    },
    {
        "title": "VAD: Vectorized Scene Representation for Efficient Autonomous Driving",
        "url": "http://arxiv.org/abs/2303.12077",
        "abstract": "Autonomous driving requires a comprehensive understanding of the surrounding\nenvironment for reliable trajectory planning. Previous works rely on dense\nrasterized scene representation (e.g., agent occupancy and semantic map) to\nperform planning, which is computationally intensive and misses the\ninstance-level structure information. In this paper, we propose VAD, an\nend-to-end vectorized paradigm for autonomous driving, which models the driving\nscene as a fully vectorized representation. The proposed vectorized paradigm\nhas two significant advantages. On one hand, VAD exploits the vectorized agent\nmotion and map elements as explicit instance-level planning constraints which\neffectively improves planning safety. On the other hand, VAD runs much faster\nthan previous end-to-end planning methods by getting rid of\ncomputation-intensive rasterized representation and hand-designed\npost-processing steps. VAD achieves state-of-the-art end-to-end planning\nperformance on the nuScenes dataset, outperforming the previous best method by\na large margin. Our base model, VAD-Base, greatly reduces the average collision\nrate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny,\ngreatly improves the inference speed (up to 9.3x) while achieving comparable\nplanning performance. We believe the excellent performance and the high\nefficiency of VAD are critical for the real-world deployment of an autonomous\ndriving system. Code and models are available at https://github.com/hustvl/VAD\nfor facilitating future research.",
        "authors": [
            "Bo Jiang",
            "Shaoyu Chen",
            "Qing Xu",
            "Bencheng Liao",
            "Jiajie Chen",
            "Helong Zhou",
            "Qian Zhang",
            "Wenyu Liu",
            "Chang Huang",
            "Xinggang Wang"
        ]
    },
    {
        "title": "Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation",
        "url": "http://arxiv.org/abs/2303.11329",
        "abstract": "The images and sounds that we perceive undergo subtle but geometrically\nconsistent changes as we rotate our heads. In this paper, we use these cues to\nsolve a problem we call Sound Localization from Motion (SLfM): jointly\nestimating camera rotation and localizing sound sources. We learn to solve\nthese tasks solely through self-supervision. A visual model predicts camera\nrotation from a pair of images, while an audio model predicts the direction of\nsound sources from binaural sounds. We train these models to generate\npredictions that agree with one another. At test time, the models can be\ndeployed independently. To obtain a feature representation that is well-suited\nto solving this challenging problem, we also propose a method for learning an\naudio-visual representation through cross-view binauralization: estimating\nbinaural sound from one view, given images and sound from another. Our model\ncan successfully estimate accurate rotations on both real and synthetic scenes,\nand localize sound sources with accuracy competitive with state-of-the-art\nself-supervised approaches. Project site: https://ificl.github.io/SLfM/",
        "authors": [
            "Ziyang Chen",
            "Shengyi Qian",
            "Andrew Owens"
        ]
    },
    {
        "title": "Batch-based Model Registration for Fast 3D Sherd Reconstruction",
        "url": "http://arxiv.org/abs/2211.06897",
        "abstract": "3D reconstruction techniques have widely been used for digital documentation\nof archaeological fragments. However, efficient digital capture of fragments\nremains as a challenge. In this work, we aim to develop a portable,\nhigh-throughput, and accurate reconstruction system for efficient digitization\nof fragments excavated in archaeological sites. To realize high-throughput\ndigitization of large numbers of objects, an effective strategy is to perform\nscanning and reconstruction in batches. However, effective batch-based scanning\nand reconstruction face two key challenges: 1) how to correlate partial scans\nof the same object from multiple batch scans, and 2) how to register and\nreconstruct complete models from partial scans that exhibit only small\noverlaps. To tackle these two challenges, we develop a new batch-based matching\nalgorithm that pairs the front and back sides of the fragments, and a new\nBilateral Boundary ICP algorithm that can register partial scans sharing very\nnarrow overlapping regions. Extensive validation in labs and testing in\nexcavation sites demonstrate that these designs enable efficient batch-based\nscanning for fragments. We show that such a batch-based scanning and\nreconstruction pipeline can have immediate applications on digitizing sherds in\narchaeological excavations. Our project page:\nhttps://jiepengwang.github.io/FIRES/.",
        "authors": [
            "Jiepeng Wang",
            "Congyi Zhang",
            "Peng Wang",
            "Xin Li",
            "Peter J. Cobb",
            "Christian Theobalt",
            "Wenping Wang"
        ]
    },
    {
        "title": "HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details",
        "url": "http://arxiv.org/abs/2303.11225",
        "abstract": "3D Morphable Models (3DMMs) demonstrate great potential for reconstructing\nfaithful and animatable 3D facial surfaces from a single image. The facial\nsurface is influenced by the coarse shape, as well as the static detail (e,g.,\nperson-specific appearance) and dynamic detail (e.g., expression-driven\nwrinkles). Previous work struggles to decouple the static and dynamic details\nthrough image-level supervision, leading to reconstructions that are not\nrealistic. In this paper, we aim at high-fidelity 3D face reconstruction and\npropose HiFace to explicitly model the static and dynamic details.\nSpecifically, the static detail is modeled as the linear combination of a\ndisplacement basis, while the dynamic detail is modeled as the linear\ninterpolation of two displacement maps with polarized expressions. We exploit\nseveral loss functions to jointly learn the coarse shape and fine details with\nboth synthetic and real-world datasets, which enable HiFace to reconstruct\nhigh-fidelity 3D shapes with animatable details. Extensive quantitative and\nqualitative experiments demonstrate that HiFace presents state-of-the-art\nreconstruction quality and faithfully recovers both the static and dynamic\ndetails. Our project page can be found at https://project-hiface.github.io.",
        "authors": [
            "Zenghao Chai",
            "Tianke Zhang",
            "Tianyu He",
            "Xu Tan",
            "Tadas Baltru\u0161aitis",
            "HsiangTao Wu",
            "Runnan Li",
            "Sheng Zhao",
            "Chun Yuan",
            "Jiang Bian"
        ]
    },
    {
        "title": "Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance",
        "url": "http://arxiv.org/abs/2308.05986",
        "abstract": "Given a set of pre-trained models, how can we quickly and accurately find the\nmost useful pre-trained model for a downstream task? Transferability\nmeasurement is to quantify how transferable is a pre-trained model learned on a\nsource task to a target task. It is used for quickly ranking pre-trained models\nfor a given task and thus becomes a crucial step for transfer learning.\nExisting methods measure transferability as the discrimination ability of a\nsource model for a target data before transfer learning, which cannot\naccurately estimate the fine-tuning performance. Some of them restrict the\napplication of transferability measurement in selecting the best supervised\npre-trained models that have classifiers. It is important to have a general\nmethod for measuring transferability that can be applied in a variety of\nsituations, such as selecting the best self-supervised pre-trained models that\ndo not have classifiers, and selecting the best transferring layer for a target\ntask. In this work, we propose TMI (TRANSFERABILITY MEASUREMENT WITH\nINTRA-CLASS FEATURE VARIANCE), a fast and accurate algorithm to measure\ntransferability. We view transferability as the generalization of a pre-trained\nmodel on a target task by measuring intra-class feature variance. Intra-class\nvariance evaluates the adaptability of the model to a new task, which measures\nhow transferable the model is. Compared to previous studies that estimate how\ndiscriminative the models are, intra-class variance is more accurate than those\nas it does not require an optimal feature extractor and classifier. Extensive\nexperiments on real-world datasets show that TMI outperforms competitors for\nselecting the top-5 best models, and exhibits consistently better correlation\nin 13 out of 17 cases.",
        "authors": [
            "Huiwen Xu",
            "U Kang"
        ]
    },
    {
        "title": "Deformable Model-Driven Neural Rendering for High-Fidelity 3D Reconstruction of Human Heads Under Low-View Settings",
        "url": "http://arxiv.org/abs/2303.13855",
        "abstract": "Reconstructing 3D human heads in low-view settings presents technical\nchallenges, mainly due to the pronounced risk of overfitting with limited views\nand high-frequency signals. To address this, we propose geometry decomposition\nand adopt a two-stage, coarse-to-fine training strategy, allowing for\nprogressively capturing high-frequency geometric details. We represent 3D human\nheads using the zero level-set of a combined signed distance field, comprising\na smooth template, a non-rigid deformation, and a high-frequency displacement\nfield. The template captures features that are independent of both identity and\nexpression and is co-trained with the deformation network across multiple\nindividuals with sparse and randomly selected views. The displacement field,\ncapturing individual-specific details, undergoes separate training for each\nperson. Our network training does not require 3D supervision or object masks.\nExperimental results demonstrate the effectiveness and robustness of our\ngeometry decomposition and two-stage training strategy. Our method outperforms\nexisting neural rendering approaches in terms of reconstruction accuracy and\nnovel view synthesis under low-view settings. Moreover, the pre-trained\ntemplate serves a good initialization for our model when encountering unseen\nindividuals.",
        "authors": [
            "Baixin Xu",
            "Jiarui Zhang",
            "Kwan-Yee Lin",
            "Chen Qian",
            "Ying He"
        ]
    },
    {
        "title": "Prompt Tuning Inversion for Text-driven Image Editing Using Diffusion Models",
        "url": "http://arxiv.org/abs/2305.04441",
        "abstract": "Recently large-scale language-image models (e.g., text-guided diffusion\nmodels) have considerably improved the image generation capabilities to\ngenerate photorealistic images in various domains. Based on this success,\ncurrent image editing methods use texts to achieve intuitive and versatile\nmodification of images. To edit a real image using diffusion models, one must\nfirst invert the image to a noisy latent from which an edited image is sampled\nwith a target text prompt. However, most methods lack one of the following:\nuser-friendliness (e.g., additional masks or precise descriptions of the input\nimage are required), generalization to larger domains, or high fidelity to the\ninput image. In this paper, we design an accurate and quick inversion\ntechnique, Prompt Tuning Inversion, for text-driven image editing.\nSpecifically, our proposed editing method consists of a reconstruction stage\nand an editing stage. In the first stage, we encode the information of the\ninput image into a learnable conditional embedding via Prompt Tuning Inversion.\nIn the second stage, we apply classifier-free guidance to sample the edited\nimage, where the conditional embedding is calculated by linearly interpolating\nbetween the target embedding and the optimized one obtained in the first stage.\nThis technique ensures a superior trade-off between editability and high\nfidelity to the input image of our method. For example, we can change the color\nof a specific object while preserving its original shape and background under\nthe guidance of only a target text prompt. Extensive experiments on ImageNet\ndemonstrate the superior editing performance of our method compared to the\nstate-of-the-art baselines.",
        "authors": [
            "Wenkai Dong",
            "Song Xue",
            "Xiaoyue Duan",
            "Shumin Han"
        ]
    },
    {
        "title": "CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion",
        "url": "http://arxiv.org/abs/2307.07938",
        "abstract": "Semantic scene completion (SSC) requires an accurate understanding of the\ngeometric and semantic relationships between the objects in the 3D scene for\nreasoning the occluded objects. The popular SSC methods voxelize the 3D\nobjects, allowing the deep 3D convolutional network (3D CNN) to learn the\nobject relationships from the complex scenes. However, the current networks\nlack the controllable kernels to model the object relationship across multiple\nviews, where appropriate views provide the relevant information for suggesting\nthe existence of the occluded objects. In this paper, we propose Cross-View\nSynthesis Transformer (CVSformer), which consists of Multi-View Feature\nSynthesis and Cross-View Transformer for learning cross-view object\nrelationships. In the multi-view feature synthesis, we use a set of 3D\nconvolutional kernels rotated differently to compute the multi-view features\nfor each voxel. In the cross-view transformer, we employ the cross-view fusion\nto comprehensively learn the cross-view relationships, which form useful\ninformation for enhancing the features of individual views. We use the enhanced\nfeatures to predict the geometric occupancies and semantic labels of all\nvoxels. We evaluate CVSformer on public datasets, where CVSformer yields\nstate-of-the-art results.",
        "authors": [
            "Haotian Dong",
            "Enhui Ma",
            "Lubo Wang",
            "Miaohui Wang",
            "Wuyuan Xie",
            "Qing Guo",
            "Ping Li",
            "Lingyu Liang",
            "Kairui Yang",
            "Di Lin"
        ]
    },
    {
        "title": "UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields",
        "url": "http://arxiv.org/abs/2303.14167",
        "abstract": "Generating photorealistic images with controllable camera pose and scene\ncontents is essential for many applications including AR/VR and simulation.\nDespite the fact that rapid progress has been made in 3D-aware generative\nmodels, most existing methods focus on object-centric images and are not\napplicable to generating urban scenes for free camera viewpoint control and\nscene editing. To address this challenging task, we propose UrbanGIRAFFE, which\nuses a coarse 3D panoptic prior, including the layout distribution of\nuncountable stuff and countable objects, to guide a 3D-aware generative model.\nOur model is compositional and controllable as it breaks down the scene into\nstuff, objects, and sky. Using stuff prior in the form of semantic voxel grids,\nwe build a conditioned stuff generator that effectively incorporates the coarse\nsemantic and geometry information. The object layout prior further allows us to\nlearn an object generator from cluttered scenes. With proper loss functions,\nour approach facilitates photorealistic 3D-aware image synthesis with diverse\ncontrollability, including large camera movement, stuff editing, and object\nmanipulation. We validate the effectiveness of our model on both synthetic and\nreal-world datasets, including the challenging KITTI-360 dataset.",
        "authors": [
            "Yuanbo Yang",
            "Yifei Yang",
            "Hanlei Guo",
            "Rong Xiong",
            "Yue Wang",
            "Yiyi Liao"
        ]
    },
    {
        "title": "UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation",
        "url": "http://arxiv.org/abs/2309.14335",
        "abstract": "Human generation has achieved significant progress. Nonetheless, existing\nmethods still struggle to synthesize specific regions such as faces and hands.\nWe argue that the main reason is rooted in the training data. A holistic human\ndataset inevitably has insufficient and low-resolution information on local\nparts. Therefore, we propose to use multi-source datasets with various\nresolution images to jointly learn a high-resolution human generative model.\nHowever, multi-source data inherently a) contains different parts that do not\nspatially align into a coherent human, and b) comes with different scales. To\ntackle these challenges, we propose an end-to-end framework, UnitedHuman, that\nempowers continuous GAN with the ability to effectively utilize multi-source\ndata for high-resolution human generation. Specifically, 1) we design a\nMulti-Source Spatial Transformer that spatially aligns multi-source images to\nfull-body space with a human parametric model. 2) Next, a continuous GAN is\nproposed with global-structural guidance and CutMix consistency. Patches from\ndifferent datasets are then sampled and transformed to supervise the training\nof this scale-invariant generative model. Extensive experiments demonstrate\nthat our model jointly learned from multi-source data achieves superior quality\nthan those learned from a holistic dataset.",
        "authors": [
            "Jianglin Fu",
            "Shikai Li",
            "Yuming Jiang",
            "Kwan-Yee Lin",
            "Wayne Wu",
            "Ziwei Liu"
        ]
    },
    {
        "title": "Active Neural Mapping",
        "url": "http://arxiv.org/abs/2308.16246",
        "abstract": "We address the problem of active mapping with a continually-learned neural\nscene representation, namely Active Neural Mapping. The key lies in actively\nfinding the target space to be explored with efficient agent movement, thus\nminimizing the map uncertainty on-the-fly within a previously unseen\nenvironment. In this paper, we examine the weight space of the\ncontinually-learned neural field, and show empirically that the neural\nvariability, the prediction robustness against random weight perturbation, can\nbe directly utilized to measure the instant uncertainty of the neural map.\nTogether with the continuous geometric information inherited in the neural map,\nthe agent can be guided to find a traversable path to gradually gain knowledge\nof the environment. We present for the first time an active mapping system with\na coordinate-based implicit neural representation for online scene\nreconstruction. Experiments in the visually-realistic Gibson and Matterport3D\nenvironment demonstrate the efficacy of the proposed method.",
        "authors": [
            "Zike Yan",
            "Haoxiang Yang",
            "Hongbin Zha"
        ]
    },
    {
        "title": "Density-invariant Features for Distant Point Cloud Registration",
        "url": "http://arxiv.org/abs/2307.09788",
        "abstract": "Registration of distant outdoor LiDAR point clouds is crucial to extending\nthe 3D vision of collaborative autonomous vehicles, and yet is challenging due\nto small overlapping area and a huge disparity between observed point\ndensities. In this paper, we propose Group-wise Contrastive Learning (GCL)\nscheme to extract density-invariant geometric features to register distant\noutdoor LiDAR point clouds. We mark through theoretical analysis and\nexperiments that, contrastive positives should be independent and identically\ndistributed (i.i.d.), in order to train densityinvariant feature extractors. We\npropose upon the conclusion a simple yet effective training scheme to force the\nfeature of multiple point clouds in the same spatial location (referred to as\npositive groups) to be similar, which naturally avoids the sampling bias\nintroduced by a pair of point clouds to conform with the i.i.d. principle. The\nresulting fully-convolutional feature extractor is more powerful and\ndensity-invariant than state-of-the-art methods, improving the registration\nrecall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and\n26.9%, respectively. Code is available at https://github.com/liuQuan98/GCL.",
        "authors": [
            "Quan Liu",
            "Hongzi Zhu",
            "Yunsong Zhou",
            "Hongyang Li",
            "Shan Chang",
            "Minyi Guo"
        ]
    },
    {
        "title": "UniverSeg: Universal Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2304.06131",
        "abstract": "While deep learning models have become the predominant method for medical\nimage segmentation, they are typically not capable of generalizing to unseen\nsegmentation tasks involving new anatomies, image modalities, or labels. Given\na new segmentation task, researchers generally have to train or fine-tune\nmodels, which is time-consuming and poses a substantial barrier for clinical\nresearchers, who often lack the resources and expertise to train neural\nnetworks. We present UniverSeg, a method for solving unseen medical\nsegmentation tasks without additional training. Given a query image and example\nset of image-label pairs that define a new segmentation task, UniverSeg employs\na new Cross-Block mechanism to produce accurate segmentation maps without the\nneed for additional training. To achieve generalization to new tasks, we have\ngathered and standardized a collection of 53 open-access medical segmentation\ndatasets with over 22,000 scans, which we refer to as MegaMedical. We used this\ncollection to train UniverSeg on a diverse set of anatomies and imaging\nmodalities. We demonstrate that UniverSeg substantially outperforms several\nrelated methods on unseen tasks, and thoroughly analyze and draw insights about\nimportant aspects of the proposed system. The UniverSeg source code and model\nweights are freely available at https://universeg.csail.mit.edu",
        "authors": [
            "Victor Ion Butoi",
            "Jose Javier Gonzalez Ortiz",
            "Tianyu Ma",
            "Mert R. Sabuncu",
            "John Guttag",
            "Adrian V. Dalca"
        ]
    },
    {
        "title": "RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning",
        "url": "http://arxiv.org/abs/2301.01661",
        "abstract": "The wide-angle lens shows appealing applications in VR technologies, but it\nintroduces severe radial distortion into its captured image. To recover the\nrealistic scene, previous works devote to rectifying the content of the\nwide-angle image. However, such a rectification solution inevitably distorts\nthe image boundary, which changes related geometric distributions and misleads\nthe current vision perception models. In this work, we explore constructing a\nwin-win representation on both content and boundary by contributing a new\nlearning model, i.e., Rectangling Rectification Network (RecRecNet). In\nparticular, we propose a thin-plate spline (TPS) module to formulate the\nnon-linear and non-rigid transformation for rectangling images. By learning the\ncontrol points on the rectified image, our model can flexibly warp the source\nstructure to the target domain and achieves an end-to-end unsupervised\ndeformation. To relieve the complexity of structure approximation, we then\ninspire our RecRecNet to learn the gradual deformation rules with a DoF (Degree\nof Freedom)-based curriculum learning. By increasing the DoF in each curriculum\nstage, namely, from similarity transformation (4-DoF) to homography\ntransformation (8-DoF), the network is capable of investigating more detailed\ndeformations, offering fast convergence on the final rectangling task.\nExperiments show the superiority of our solution over the compared methods on\nboth quantitative and qualitative evaluations. The code and dataset are\navailable at https://github.com/KangLiao929/RecRecNet.",
        "authors": [
            "Kang Liao",
            "Lang Nie",
            "Chunyu Lin",
            "Zishuo Zheng",
            "Yao Zhao"
        ]
    },
    {
        "title": "Neural Microfacet Fields for Inverse Rendering",
        "url": "http://arxiv.org/abs/2303.17806",
        "abstract": "We present Neural Microfacet Fields, a method for recovering materials,\ngeometry, and environment illumination from images of a scene. Our method uses\na microfacet reflectance model within a volumetric setting by treating each\nsample along the ray as a (potentially non-opaque) surface. Using surface-based\nMonte Carlo rendering in a volumetric setting enables our method to perform\ninverse rendering efficiently by combining decades of research in surface-based\nlight transport with recent advances in volume rendering for view synthesis.\nOur approach outperforms prior work in inverse rendering, capturing high\nfidelity geometry and high frequency illumination details; its novel view\nsynthesis results are on par with state-of-the-art methods that do not recover\nillumination or materials.",
        "authors": [
            "Alexander Mai",
            "Dor Verbin",
            "Falko Kuester",
            "Sara Fridovich-Keil"
        ]
    },
    {
        "title": "Understanding Self-attention Mechanism via Dynamical System Perspective",
        "url": "http://arxiv.org/abs/2308.09939",
        "abstract": "The self-attention mechanism (SAM) is widely used in various fields of\nartificial intelligence and has successfully boosted the performance of\ndifferent models. However, current explanations of this mechanism are mainly\nbased on intuitions and experiences, while there still lacks direct modeling\nfor how the SAM helps performance. To mitigate this issue, in this paper, based\non the dynamical system perspective of the residual neural network, we first\nshow that the intrinsic stiffness phenomenon (SP) in the high-precision\nsolution of ordinary differential equations (ODEs) also widely exists in\nhigh-performance neural networks (NN). Thus the ability of NN to measure SP at\nthe feature level is necessary to obtain high performance and is an important\nfactor in the difficulty of training NN. Similar to the adaptive step-size\nmethod which is effective in solving stiff ODEs, we show that the SAM is also a\nstiffness-aware step size adaptor that can enhance the model's representational\nability to measure intrinsic SP by refining the estimation of stiffness\ninformation and generating adaptive attention values, which provides a new\nunderstanding about why and how the SAM can benefit the model performance. This\nnovel perspective can also explain the lottery ticket hypothesis in SAM, design\nnew quantitative metrics of representational ability, and inspire a new\ntheoretic-inspired approach, StepNet. Extensive experiments on several popular\nbenchmarks demonstrate that StepNet can extract fine-grained stiffness\ninformation and measure SP accurately, leading to significant improvements in\nvarious visual tasks.",
        "authors": [
            "Zhongzhan Huang",
            "Mingfu Liang",
            "Jinghui Qin",
            "Shanshan Zhong",
            "Liang Lin"
        ]
    },
    {
        "title": "DETA: Denoised Task Adaptation for Few-Shot Learning",
        "url": "http://arxiv.org/abs/2303.06315",
        "abstract": "Test-time task adaptation in few-shot learning aims to adapt a pre-trained\ntask-agnostic model for capturing taskspecific knowledge of the test task, rely\nonly on few-labeled support samples. Previous approaches generally focus on\ndeveloping advanced algorithms to achieve the goal, while neglecting the\ninherent problems of the given support samples. In fact, with only a handful of\nsamples available, the adverse effect of either the image noise (a.k.a.\nX-noise) or the label noise (a.k.a. Y-noise) from support samples can be\nseverely amplified. To address this challenge, in this work we propose DEnoised\nTask Adaptation (DETA), a first, unified image- and label-denoising framework\northogonal to existing task adaptation approaches. Without extra supervision,\nDETA filters out task-irrelevant, noisy representations by taking advantage of\nboth global visual information and local region details of support samples. On\nthe challenging Meta-Dataset, DETA consistently improves the performance of a\nbroad spectrum of baseline methods applied on various pre-trained models.\nNotably, by tackling the overlooked image noise in Meta-Dataset, DETA\nestablishes new state-of-the-art results. Code is released at\nhttps://github.com/JimZAI/DETA.",
        "authors": [
            "Ji Zhang",
            "Lianli Gao",
            "Xu Luo",
            "Hengtao Shen",
            "Jingkuan Song"
        ]
    },
    {
        "title": "Diffusion Models as Masked Autoencoders",
        "url": "http://arxiv.org/abs/2304.03283",
        "abstract": "There has been a longstanding belief that generation can facilitate a true\nunderstanding of visual data. In line with this, we revisit generatively\npre-training visual representations in light of recent interest in denoising\ndiffusion models. While directly pre-training with diffusion models does not\nproduce strong representations, we condition diffusion models on masked input\nand formulate diffusion models as masked autoencoders (DiffMAE). Our approach\nis capable of (i) serving as a strong initialization for downstream recognition\ntasks, (ii) conducting high-quality image inpainting, and (iii) being\neffortlessly extended to video where it produces state-of-the-art\nclassification accuracy. We further perform a comprehensive study on the pros\nand cons of design choices and build connections between diffusion models and\nmasked autoencoders.",
        "authors": [
            "Chen Wei",
            "Karttikeya Mangalam",
            "Po-Yao Huang",
            "Yanghao Li",
            "Haoqi Fan",
            "Hu Xu",
            "Huiyu Wang",
            "Cihang Xie",
            "Alan Yuille",
            "Christoph Feichtenhofer"
        ]
    },
    {
        "title": "Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes",
        "url": "http://arxiv.org/abs/2309.08588",
        "abstract": "We present an approach to estimating camera rotation in crowded, real-world\nscenes from handheld monocular video. While camera rotation estimation is a\nwell-studied problem, no previous methods exhibit both high accuracy and\nacceptable speed in this setting. Because the setting is not addressed well by\nother datasets, we provide a new dataset and benchmark, with high-accuracy,\nrigorously verified ground truth, on 17 video sequences. Methods developed for\nwide baseline stereo (e.g., 5-point methods) perform poorly on monocular video.\nOn the other hand, methods used in autonomous driving (e.g., SLAM) leverage\nspecific sensor setups, specific motion models, or local optimization\nstrategies (lagging batch processing) and do not generalize well to handheld\nvideo. Finally, for dynamic scenes, commonly used robustification techniques\nlike RANSAC require large numbers of iterations, and become prohibitively slow.\nWe introduce a novel generalization of the Hough transform on SO(3) to\nefficiently and robustly find the camera rotation most compatible with optical\nflow. Among comparably fast methods, ours reduces error by almost 50\\% over the\nnext best, and is more accurate than any method, irrespective of speed. This\nrepresents a strong new performance point for crowded scenes, an important\nsetting for computer vision. The code and the dataset are available at\nhttps://fabiendelattre.com/robust-rotation-estimation.",
        "authors": [
            "Fabien Delattre",
            "David Dirnfeld",
            "Phat Nguyen",
            "Stephen Scarano",
            "Michael J. Jones",
            "Pedro Miraldo",
            "Erik Learned-Miller"
        ]
    },
    {
        "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
        "url": "http://arxiv.org/abs/2210.02390",
        "abstract": "Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\nCode available at: https://github.com/saic-fi/Bayesian-Prompt-Learning",
        "authors": [
            "Mohammad Mahdi Derakhshani",
            "Enrique Sanchez",
            "Adrian Bulat",
            "Victor Guilherme Turrisi da Costa",
            "Cees G. M. Snoek",
            "Georgios Tzimiropoulos",
            "Brais Martinez"
        ]
    },
    {
        "title": "One-Shot Recognition of Any Material Anywhere Using Contrastive Learning with Physics-Based Rendering",
        "url": "http://arxiv.org/abs/2212.00648",
        "abstract": "Visual recognition of materials and their states is essential for\nunderstanding most aspects of the world, from determining whether food is\ncooked, metal is rusted, or a chemical reaction has occurred. However, current\nimage recognition methods are limited to specific classes and properties and\ncan't handle the vast number of material states in the world. To address this,\nwe present MatSim: the first dataset and benchmark for computer vision-based\nrecognition of similarities and transitions between materials and textures,\nfocusing on identifying any material under any conditions using one or a few\nexamples. The dataset contains synthetic and natural images. The synthetic\nimages were rendered using giant collections of textures, objects, and\nenvironments generated by computer graphics artists. We use mixtures and\ngradual transitions between materials to allow the system to learn cases with\nsmooth transitions between states (like gradually cooked food). We also render\nimages with materials inside transparent containers to support beverage and\nchemistry lab use cases. We use this dataset to train a siamese net that\nidentifies the same material in different objects, mixtures, and environments.\nThe descriptor generated by this net can be used to identify the states of\nmaterials and their subclasses using a single image. We also present the first\nfew-shot material recognition benchmark with images from a wide range of\nfields, including the state of foods and drinks, types of grounds, and many\nother use cases. We show that a net trained on the MatSim synthetic dataset\noutperforms state-of-the-art models like Clip on the benchmark and also\nachieves good results on other unsupervised material classification tasks.",
        "authors": [
            "Manuel S. Drehwald",
            "Sagi Eppel",
            "Jolina Li",
            "Han Hao",
            "Alan Aspuru-Guzik"
        ]
    },
    {
        "title": "Rethinking Data Distillation: Do Not Overlook Calibration",
        "url": "http://arxiv.org/abs/2307.12463",
        "abstract": "Neural networks trained on distilled data often produce over-confident output\nand require correction by calibration methods. Existing calibration methods\nsuch as temperature scaling and mixup work well for networks trained on\noriginal large-scale data. However, we find that these methods fail to\ncalibrate networks trained on data distilled from large source datasets. In\nthis paper, we show that distilled data lead to networks that are not\ncalibratable due to (i) a more concentrated distribution of the maximum logits\nand (ii) the loss of information that is semantically meaningful but unrelated\nto classification tasks. To address this problem, we propose Masked Temperature\nScaling (MTS) and Masked Distillation Training (MDT) which mitigate the\nlimitations of distilled data and achieve better calibration results while\nmaintaining the efficiency of dataset distillation.",
        "authors": [
            "Dongyao Zhu",
            "Bowen Lei",
            "Jie Zhang",
            "Yanbo Fang",
            "Ruqi Zhang",
            "Yiqun Xie",
            "Dongkuan Xu"
        ]
    },
    {
        "title": "Accurate and Fast Compressed Video Captioning",
        "url": "http://arxiv.org/abs/2309.12867",
        "abstract": "Existing video captioning approaches typically require to first sample video\nframes from a decoded video and then conduct a subsequent process (e.g.,\nfeature extraction and/or captioning model learning). In this pipeline, manual\nframe sampling may ignore key information in videos and thus degrade\nperformance. Additionally, redundant information in the sampled frames may\nresult in low efficiency in the inference of video captioning. Addressing this,\nwe study video captioning from a different perspective in compressed domain,\nwhich brings multi-fold advantages over the existing pipeline: 1) Compared to\nraw images from the decoded video, the compressed video, consisting of\nI-frames, motion vectors and residuals, is highly distinguishable, which allows\nus to leverage the entire video for learning without manual sampling through a\nspecialized model design; 2) The captioning model is more efficient in\ninference as smaller and less redundant information is processed. We propose a\nsimple yet effective end-to-end transformer in the compressed domain for video\ncaptioning that enables learning from the compressed video for captioning. We\nshow that even with a simple design, our method can achieve state-of-the-art\nperformance on different benchmarks while running almost 2x faster than\nexisting approaches. Code is available at https://github.com/acherstyx/CoCap.",
        "authors": [
            "Yaojie Shen",
            "Xin Gu",
            "Kai Xu",
            "Heng Fan",
            "Longyin Wen",
            "Libo Zhang"
        ]
    },
    {
        "title": "TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models",
        "url": "http://arxiv.org/abs/2308.03906",
        "abstract": "We present a Multimodal Backdoor Defense technique TIJO (Trigger Inversion\nusing Joint Optimization). Recent work arXiv:2112.07668 has demonstrated\nsuccessful backdoor attacks on multimodal models for the Visual Question\nAnswering task. Their dual-key backdoor trigger is split across two modalities\n(image and text), such that the backdoor is activated if and only if the\ntrigger is present in both modalities. We propose TIJO that defends against\ndual-key attacks through a joint optimization that reverse-engineers the\ntrigger in both the image and text modalities. This joint optimization is\nchallenging in multimodal models due to the disconnected nature of the visual\npipeline which consists of an offline feature extractor, whose output is then\nfused with the text using a fusion module. The key insight enabling the joint\noptimization in TIJO is that the trigger inversion needs to be carried out in\nthe object detection box feature space as opposed to the pixel space. We\ndemonstrate the effectiveness of our method on the TrojVQA benchmark, where\nTIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to\n0.92 on multimodal dual-key backdoors. Furthermore, our method also improves\nupon the unimodal baselines on unimodal backdoors. We present ablation studies\nand qualitative results to provide insights into our algorithm such as the\ncritical importance of overlaying the inverted feature triggers on all visual\nfeatures during trigger inversion. The prototype implementation of TIJO is\navailable at https://github.com/SRI-CSL/TIJO.",
        "authors": [
            "Indranil Sur",
            "Karan Sikka",
            "Matthew Walmer",
            "Kaushik Koneripalli",
            "Anirban Roy",
            "Xiao Lin",
            "Ajay Divakaran",
            "Susmit Jha"
        ]
    },
    {
        "title": "Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting",
        "url": "http://arxiv.org/abs/2308.10315",
        "abstract": "In this paper, we investigate the adversarial robustness of vision\ntransformers that are equipped with BERT pretraining (e.g., BEiT, MAE). A\nsurprising observation is that MAE has significantly worse adversarial\nrobustness than other BERT pretraining methods. This observation drives us to\nrethink the basic differences between these BERT pretraining methods and how\nthese differences affect the robustness against adversarial perturbations. Our\nempirical analysis reveals that the adversarial robustness of BERT pretraining\nis highly related to the reconstruction target, i.e., predicting the raw pixels\nof masked image patches will degrade more adversarial robustness of the model\nthan predicting the semantic context, since it guides the model to concentrate\nmore on medium-/high-frequency components of images. Based on our analysis, we\nprovide a simple yet effective way to boost the adversarial robustness of MAE.\nThe basic idea is using the dataset-extracted domain knowledge to occupy the\nmedium-/high-frequency of images, thus narrowing the optimization space of\nadversarial perturbations. Specifically, we group the distribution of\npretraining data and optimize a set of cluster-specific visual prompts on\nfrequency domain. These prompts are incorporated with input images through\nprototype-based prompt selection during test period. Extensive evaluation shows\nthat our method clearly boost MAE's adversarial robustness while maintaining\nits clean performance on ImageNet-1k classification. Our code is available at:\nhttps://github.com/shikiw/RobustMAE.",
        "authors": [
            "Qidong Huang",
            "Xiaoyi Dong",
            "Dongdong Chen",
            "Yinpeng Chen",
            "Lu Yuan",
            "Gang Hua",
            "Weiming Zhang",
            "Nenghai Yu"
        ]
    },
    {
        "title": "VLSlice: Interactive Vision-and-Language Slice Discovery",
        "url": "http://arxiv.org/abs/2309.06703",
        "abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining\ncan learn generalizable models that are efficiently transferable to downstream\ntasks. While this may improve dataset-scale aggregate metrics, analyzing\nperformance around hand-crafted subgroups targeting specific bias dimensions\nreveals systemic undesirable behaviors. However, this subgroup analysis is\nfrequently stalled by annotation efforts, which require extensive time and\nresources to collect the necessary data. Prior art attempts to automatically\ndiscover subgroups to circumvent these constraints but typically leverages\nmodel behavior on existing task-specific annotations and rapidly degrades on\nmore complex inputs beyond \"tabular\" data, none of which study\nvision-and-language models. This paper presents VLSlice, an interactive system\nenabling user-guided discovery of coherent representation-level subgroups with\nconsistent visiolinguistic behavior, denoted as vision-and-language slices,\nfrom unlabeled image sets. We show that VLSlice enables users to quickly\ngenerate diverse high-coherency slices in a user study (n=22) and release the\ntool publicly.",
        "authors": [
            "Eric Slyman",
            "Minsuk Kahng",
            "Stefan Lee"
        ]
    },
    {
        "title": "Learning to Ground Instructional Articles in Videos through Narrations",
        "url": "http://arxiv.org/abs/2306.03802",
        "abstract": "In this paper we present an approach for localizing steps of procedural\nactivities in narrated how-to videos. To deal with the scarcity of labeled data\nat scale, we source the step descriptions from a language knowledge base\n(wikiHow) containing instructional articles for a large variety of procedural\ntasks. Without any form of manual supervision, our model learns to temporally\nground the steps of procedural articles in how-to videos by matching three\nmodalities: frames, narrations, and step descriptions. Specifically, our method\naligns steps to video by fusing information from two distinct pathways: i) {\\em\ndirect} alignment of step descriptions to frames, ii) {\\em indirect} alignment\nobtained by composing steps-to-narrations with narrations-to-video\ncorrespondences. Notably, our approach performs global temporal grounding of\nall steps in an article at once by exploiting order information, and is trained\nwith step pseudo-labels which are iteratively refined and aggressively\nfiltered. In order to validate our model we introduce a new evaluation\nbenchmark -- HT-Step -- obtained by manually annotating a 124-hour subset of\nHowTo100M\\footnote{A test server is accessible at\n\\url{https://eval.ai/web/challenges/challenge-page/2082}.} with steps sourced\nfrom wikiHow articles. Experiments on this benchmark as well as zero-shot\nevaluations on CrossTask demonstrate that our multi-modality alignment yields\ndramatic gains over several baselines and prior works. Finally, we show that\nour inner module for matching narration-to-video outperforms by a large margin\nthe state of the art on the HTM-Align narration-video alignment benchmark.",
        "authors": [
            "Effrosyni Mavroudi",
            "Triantafyllos Afouras",
            "Lorenzo Torresani"
        ]
    },
    {
        "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
        "url": "http://arxiv.org/abs/2307.07929",
        "abstract": "We present a new formulation for structured information extraction (SIE) from\nvisually rich documents. It aims to address the limitations of existing IOB\ntagging or graph-based formulations, which are either overly reliant on the\ncorrect ordering of input text or struggle with decoding a complex graph.\nInstead, motivated by anchor-based object detectors in vision, we represent an\nentity as an anchor word and a bounding box, and represent entity linking as\nthe association between anchor words. This is more robust to text ordering, and\nmaintains a compact graph for entity linking. The formulation motivates us to\nintroduce 1) a DOCument TRansformer (DocTr) that aims at detecting and\nassociating entity bounding boxes in visually rich documents, and 2) a simple\npre-training strategy that helps learn entity detection in the context of\nlanguage. Evaluations on three SIE benchmarks show the effectiveness of the\nproposed formulation, and the overall approach outperforms existing solutions.",
        "authors": [
            "Haofu Liao",
            "Aruni RoyChowdhury",
            "Weijian Li",
            "Ankan Bansal",
            "Yuting Zhang",
            "Zhuowen Tu",
            "Ravi Kumar Satzoda",
            "R. Manmatha",
            "Vijay Mahadevan"
        ]
    },
    {
        "title": "The Making and Breaking of Camouflage",
        "url": "http://arxiv.org/abs/2309.03899",
        "abstract": "Not all camouflages are equally effective, as even a partially visible\ncontour or a slight color difference can make the animal stand out and break\nits camouflage. In this paper, we address the question of what makes a\ncamouflage successful, by proposing three scores for automatically assessing\nits effectiveness. In particular, we show that camouflage can be measured by\nthe similarity between background and foreground features and boundary\nvisibility. We use these camouflage scores to assess and compare all available\ncamouflage datasets. We also incorporate the proposed camouflage score into a\ngenerative model as an auxiliary loss and show that effective camouflage images\nor videos can be synthesised in a scalable manner. The generated synthetic\ndataset is used to train a transformer-based model for segmenting camouflaged\nanimals in videos. Experimentally, we demonstrate state-of-the-art camouflage\nbreaking performance on the public MoCA-Mask benchmark.",
        "authors": [
            "Hala Lamdouar",
            "Weidi Xie",
            "Andrew Zisserman"
        ]
    },
    {
        "title": "MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation",
        "url": "http://arxiv.org/abs/2307.14336",
        "abstract": "We propose MAMo, a novel memory and attention frame-work for monocular video\ndepth estimation. MAMo can augment and improve any single-image depth\nestimation networks into video depth estimation models, enabling them to take\nadvantage of the temporal information to predict more accurate depth. In MAMo,\nwe augment model with memory which aids the depth prediction as the model\nstreams through the video. Specifically, the memory stores learned visual and\ndisplacement tokens of the previous time instances. This allows the depth\nnetwork to cross-reference relevant features from the past when predicting\ndepth on the current frame. We introduce a novel scheme to continuously update\nthe memory, optimizing it to keep tokens that correspond with both the past and\nthe present visual information. We adopt attention-based approach to process\nmemory features where we first learn the spatio-temporal relation among the\nresultant visual and displacement memory tokens using self-attention module.\nFurther, the output features of self-attention are aggregated with the current\nvisual features through cross-attention. The cross-attended features are\nfinally given to a decoder to predict depth on the current frame. Through\nextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and\nDDAD, we show that MAMo consistently improves monocular depth estimation\nnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video\ndepth estimation provides higher accuracy with lower latency, when omparing to\nSOTA cost-volume-based video depth models.",
        "authors": [
            "Rajeev Yasarla",
            "Hong Cai",
            "Jisoo Jeong",
            "Yunxiao Shi",
            "Risheek Garrepalli",
            "Fatih Porikli"
        ]
    },
    {
        "title": "Object as Query: Lifting Any 2D Object Detector to 3D Detection",
        "url": "http://arxiv.org/abs/2301.02364",
        "abstract": "3D object detection from multi-view images has drawn much attention over the\npast few years. Existing methods mainly establish 3D representations from\nmulti-view images and adopt a dense detection head for object detection, or\nemploy object queries distributed in 3D space to localize objects. In this\npaper, we design Multi-View 2D Objects guided 3D Object Detector (MV2D), which\ncan lift any 2D object detector to multi-view 3D object detection. Since 2D\ndetections can provide valuable priors for object existence, MV2D exploits 2D\ndetectors to generate object queries conditioned on the rich image semantics.\nThese dynamically generated queries help MV2D to recall objects in the field of\nview and show a strong capability of localizing 3D objects. For the generated\nqueries, we design a sparse cross attention module to force them to focus on\nthe features of specific objects, which suppresses interference from noises.\nThe evaluation results on the nuScenes dataset demonstrate the dynamic object\nqueries and sparse feature aggregation can promote 3D detection capability.\nMV2D also exhibits a state-of-the-art performance among existing methods. We\nhope MV2D can serve as a new baseline for future research. Code is available at\n\\url{https://github.com/tusen-ai/MV2D}.",
        "authors": [
            "Zitian Wang",
            "Zehao Huang",
            "Jiahui Fu",
            "Naiyan Wang",
            "Si Liu"
        ]
    },
    {
        "title": "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model",
        "url": "http://arxiv.org/abs/2211.08332",
        "abstract": "Recent advances in diffusion models have set an impressive milestone in many\ngeneration tasks, and trending works such as DALL-E2, Imagen, and Stable\nDiffusion have attracted great interest. Despite the rapid landscape changes,\nrecent new approaches focus on extensions and performance rather than capacity,\nthus requiring separate models for separate tasks. In this work, we expand the\nexisting single-flow diffusion pipeline into a multi-task multimodal network,\ndubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,\nimage-to-text, and variations in one unified model. The pipeline design of VD\ninstantiates a unified multi-flow diffusion framework, consisting of sharable\nand swappable layer modules that enable the crossmodal generality beyond images\nand text. Through extensive experiments, we demonstrate that VD successfully\nachieves the following: a) VD outperforms the baseline approaches and handles\nall its base tasks with competitive quality; b) VD enables novel extensions\nsuch as disentanglement of style and semantics, dual- and multi-context\nblending, etc.; c) The success of our multi-flow multimodal framework over\nimages and text may inspire further diffusion-based universal AI research. Our\ncode and models are open-sourced at\nhttps://github.com/SHI-Labs/Versatile-Diffusion.",
        "authors": [
            "Xingqian Xu",
            "Zhangyang Wang",
            "Eric Zhang",
            "Kai Wang",
            "Humphrey Shi"
        ]
    },
    {
        "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
        "url": "http://arxiv.org/abs/2307.07487",
        "abstract": "In this work, we introduce a self-supervised feature representation learning\nframework DreamTeacher that utilizes generative networks for pre-training\ndownstream image backbones. We propose to distill knowledge from a trained\ngenerative model into standard image backbones that have been well engineered\nfor specific perception tasks. We investigate two types of knowledge\ndistillation: 1) distilling learned generative features onto target image\nbackbones as an alternative to pretraining these backbones on large labeled\ndatasets such as ImageNet, and 2) distilling labels obtained from generative\nnetworks with task heads onto logits of target backbones. We perform extensive\nanalyses on multiple generative models, dense prediction benchmarks, and\nseveral pre-training regimes. We empirically find that our DreamTeacher\nsignificantly outperforms existing self-supervised representation learning\napproaches across the board. Unsupervised ImageNet pre-training with\nDreamTeacher leads to significant improvements over ImageNet classification\npre-training on downstream datasets, showcasing generative models, and\ndiffusion generative models specifically, as a promising approach to\nrepresentation learning on large, diverse datasets without requiring manual\nannotation.",
        "authors": [
            "Daiqing Li",
            "Huan Ling",
            "Amlan Kar",
            "David Acuna",
            "Seung Wook Kim",
            "Karsten Kreis",
            "Antonio Torralba",
            "Sanja Fidler"
        ]
    },
    {
        "title": "Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs",
        "url": "http://arxiv.org/abs/2303.14672",
        "abstract": "This paper aims to develop an accurate 3D geometry representation of\nsatellite images using satellite-ground image pairs. Our focus is on the\nchallenging problem of 3D-aware ground-views synthesis from a satellite image.\nWe draw inspiration from the density field representation used in volumetric\nneural rendering and propose a new approach, called Sat2Density. Our method\nutilizes the properties of ground-view panoramas for the sky and non-sky\nregions to learn faithful density fields of 3D scenes in a geometric\nperspective. Unlike other methods that require extra depth information during\ntraining, our Sat2Density can automatically learn accurate and faithful 3D\ngeometry via density representation without depth supervision. This advancement\nsignificantly improves the ground-view panorama synthesis task. Additionally,\nour study provides a new geometric perspective to understand the relationship\nbetween satellite and ground-view images in 3D space.",
        "authors": [
            "Ming Qian",
            "Jincheng Xiong",
            "Gui-Song Xia",
            "Nan Xue"
        ]
    },
    {
        "title": "Expressive Text-to-Image Generation with Rich Text",
        "url": "http://arxiv.org/abs/2304.06720",
        "abstract": "Plain text has become a prevalent interface for text-to-image synthesis.\nHowever, its limited customization options hinder users from accurately\ndescribing desired outputs. For example, plain text makes it hard to specify\ncontinuous quantities, such as the precise RGB color value or importance of\neach word. Furthermore, creating detailed text prompts for complex scenes is\ntedious for humans to write and challenging for text encoders to interpret. To\naddress these challenges, we propose using a rich-text editor supporting\nformats such as font style, size, color, and footnote. We extract each word's\nattributes from rich text to enable local style control, explicit token\nreweighting, precise color rendering, and detailed region synthesis. We achieve\nthese capabilities through a region-based diffusion process. We first obtain\neach word's region based on attention maps of a diffusion process using plain\ntext. For each region, we enforce its text attributes by creating\nregion-specific detailed prompts and applying region-specific guidance, and\nmaintain its fidelity against plain-text generation through region-based\ninjections. We present various examples of image generation from rich text and\ndemonstrate that our method outperforms strong baselines with quantitative\nevaluations.",
        "authors": [
            "Songwei Ge",
            "Taesung Park",
            "Jun-Yan Zhu",
            "Jia-Bin Huang"
        ]
    },
    {
        "title": "Learning Fine-Grained Features for Pixel-Wise Video Correspondences",
        "url": "http://arxiv.org/abs/2308.03040",
        "abstract": "Video analysis tasks rely heavily on identifying the pixels from different\nframes that correspond to the same visual target. To tackle this problem,\nrecent studies have advocated feature learning methods that aim to learn\ndistinctive representations to match the pixels, especially in a\nself-supervised fashion. Unfortunately, these methods have difficulties for\ntiny or even single-pixel visual targets. Pixel-wise video correspondences were\ntraditionally related to optical flows, which however lead to deterministic\ncorrespondences and lack robustness on real-world videos. We address the\nproblem of learning features for establishing pixel-wise correspondences.\nMotivated by optical flows as well as the self-supervised feature learning, we\npropose to use not only labeled synthetic videos but also unlabeled real-world\nvideos for learning fine-grained representations in a holistic framework. We\nadopt an adversarial learning scheme to enhance the generalization ability of\nthe learned features. Moreover, we design a coarse-to-fine framework to pursue\nhigh computational efficiency. Our experimental results on a series of\ncorrespondence-based tasks demonstrate that the proposed method outperforms\nstate-of-the-art rivals in both accuracy and efficiency.",
        "authors": [
            "Rui Li",
            "Shenglong Zhou",
            "Dong Liu"
        ]
    },
    {
        "title": "Learning to Learn: How to Continuously Teach Humans and Machines",
        "url": "http://arxiv.org/abs/2211.15470",
        "abstract": "Curriculum design is a fundamental component of education. For example, when\nwe learn mathematics at school, we build upon our knowledge of addition to\nlearn multiplication. These and other concepts must be mastered before our\nfirst algebra lesson, which also reinforces our addition and multiplication\nskills. Designing a curriculum for teaching either a human or a machine shares\nthe underlying goal of maximizing knowledge transfer from earlier to later\ntasks, while also minimizing forgetting of learned tasks. Prior research on\ncurriculum design for image classification focuses on the ordering of training\nexamples during a single offline task. Here, we investigate the effect of the\norder in which multiple distinct tasks are learned in a sequence. We focus on\nthe online class-incremental continual learning setting, where algorithms or\nhumans must learn image classes one at a time during a single pass through a\ndataset. We find that curriculum consistently influences learning outcomes for\nhumans and for multiple continual machine learning algorithms across several\nbenchmark datasets. We introduce a novel-object recognition dataset for human\ncurriculum learning experiments and observe that curricula that are effective\nfor humans are highly correlated with those that are effective for machines. As\nan initial step towards automated curriculum design for online\nclass-incremental learning, we propose a novel algorithm, dubbed Curriculum\nDesigner (CD), that designs and ranks curricula based on inter-class feature\nsimilarities. We find significant overlap between curricula that are\nempirically highly effective and those that are highly ranked by our CD. Our\nstudy establishes a framework for further research on teaching humans and\nmachines to learn continuously using optimized curricula.",
        "authors": [
            "Parantak Singh",
            "You Li",
            "Ankur Sikarwar",
            "Weixian Lei",
            "Daniel Gao",
            "Morgan Bruce Talbot",
            "Ying Sun",
            "Mike Zheng Shou",
            "Gabriel Kreiman",
            "Mengmi Zhang"
        ]
    },
    {
        "title": "TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration",
        "url": "http://arxiv.org/abs/2304.02419",
        "abstract": "We propose a novel task for generating 3D dance movements that simultaneously\nincorporate both text and music modalities. Unlike existing works that generate\ndance movements using a single modality such as music, our goal is to produce\nricher dance movements guided by the instructive information provided by the\ntext. However, the lack of paired motion data with both music and text\nmodalities limits the ability to generate dance movements that integrate both.\nTo alleviate this challenge, we propose to utilize a 3D human motion VQ-VAE to\nproject the motions of the two datasets into a latent space consisting of\nquantized vectors, which effectively mix the motion tokens from the two\ndatasets with different distributions for training. Additionally, we propose a\ncross-modal transformer to integrate text instructions into motion generation\narchitecture for generating 3D dance movements without degrading the\nperformance of music-conditioned dance generation. To better evaluate the\nquality of the generated motion, we introduce two novel metrics, namely Motion\nPrediction Distance (MPD) and Freezing Score (FS), to measure the coherence and\nfreezing percentage of the generated motion. Extensive experiments show that\nour approach can generate realistic and coherent dance movements conditioned on\nboth text and music while maintaining comparable performance with the two\nsingle modalities. Code is available at https://garfield-kh.github.io/TM2D/.",
        "authors": [
            "Kehong Gong",
            "Dongze Lian",
            "Heng Chang",
            "Chuan Guo",
            "Zihang Jiang",
            "Xinxin Zuo",
            "Michael Bi Mi",
            "Xinchao Wang"
        ]
    },
    {
        "title": "Bootstrap Motion Forecasting With Self-Consistent Constraints",
        "url": "http://arxiv.org/abs/2204.05859",
        "abstract": "We present a novel framework to bootstrap Motion forecasting with\nSelf-consistent Constraints (MISC). The motion forecasting task aims at\npredicting future trajectories of vehicles by incorporating spatial and\ntemporal information from the past. A key design of MISC is the proposed Dual\nConsistency Constraints that regularize the predicted trajectories under\nspatial and temporal perturbation during training. Also, to model the\nmulti-modality in motion forecasting, we design a novel self-ensembling scheme\nto obtain accurate teacher targets to enforce the self-constraints with\nmulti-modality supervision. With explicit constraints from multiple teacher\ntargets, we observe a clear improvement in the prediction performance.\nExtensive experiments on the Argoverse motion forecasting benchmark and Waymo\nOpen Motion dataset show that MISC significantly outperforms the\nstate-of-the-art methods. As the proposed strategies are general and can be\neasily incorporated into other motion forecasting approaches, we also\ndemonstrate that our proposed scheme consistently improves the prediction\nperformance of several existing methods.",
        "authors": [
            "Maosheng Ye",
            "Jiamiao Xu",
            "Xunnong Xu",
            "Tengfei Wang",
            "Tongyi Cao",
            "Qifeng Chen"
        ]
    },
    {
        "title": "WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2308.04826",
        "abstract": "Neural Radiance Field (NeRF) has shown impressive performance in novel view\nsynthesis via implicit scene representation. However, it usually suffers from\npoor scalability as requiring densely sampled images for each new scene.\nSeveral studies have attempted to mitigate this problem by integrating\nMulti-View Stereo (MVS) technique into NeRF while they still entail a\ncumbersome fine-tuning process for new scenes. Notably, the rendering quality\nwill drop severely without this fine-tuning process and the errors mainly\nappear around the high-frequency features. In the light of this observation, we\ndesign WaveNeRF, which integrates wavelet frequency decomposition into MVS and\nNeRF to achieve generalizable yet high-quality synthesis without any per-scene\noptimization. To preserve high-frequency information when generating 3D feature\nvolumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating\nthe discrete wavelet transform into the classical cascade MVS, which\ndisentangles high-frequency information explicitly. With that, disentangled\nfrequency features can be injected into classic NeRF via a novel hybrid neural\nrenderer to yield faithful high-frequency details, and an intuitive\nfrequency-guided sampling strategy can be designed to suppress artifacts around\nhigh-frequency regions. Extensive experiments over three widely studied\nbenchmarks show that WaveNeRF achieves superior generalizable radiance field\nmodeling when only given three images as input.",
        "authors": [
            "Muyu Xu",
            "Fangneng Zhan",
            "Jiahui Zhang",
            "Yingchen Yu",
            "Xiaoqin Zhang",
            "Christian Theobalt",
            "Ling Shao",
            "Shijian Lu"
        ]
    },
    {
        "title": "BoxSnake: Polygonal Instance Segmentation with Box Supervision",
        "url": "http://arxiv.org/abs/2303.11630",
        "abstract": "Box-supervised instance segmentation has gained much attention as it requires\nonly simple box annotations instead of costly mask or polygon annotations.\nHowever, existing box-supervised instance segmentation models mainly focus on\nmask-based frameworks. We propose a new end-to-end training technique, termed\nBoxSnake, to achieve effective polygonal instance segmentation using only box\nannotations for the first time. Our method consists of two loss functions: (1)\na point-based unary loss that constrains the bounding box of predicted polygons\nto achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss\nthat encourages the predicted polygons to fit the object boundaries. Compared\nwith the mask-based weakly-supervised methods, BoxSnake further reduces the\nperformance gap between the predicted segmentation and the bounding box, and\nshows significant superiority on the Cityscapes dataset. The code has been\navailable publicly.",
        "authors": [
            "Rui Yang",
            "Lin Song",
            "Yixiao Ge",
            "Xiu Li"
        ]
    },
    {
        "title": "Event-Guided Procedure Planning from Instructional Videos with Text Supervision",
        "url": "http://arxiv.org/abs/2308.08885",
        "abstract": "In this work, we focus on the task of procedure planning from instructional\nvideos with text supervision, where a model aims to predict an action sequence\nto transform the initial visual state into the goal visual state. A critical\nchallenge of this task is the large semantic gap between observed visual states\nand unobserved intermediate actions, which is ignored by previous works.\nSpecifically, this semantic gap refers to that the contents in the observed\nvisual states are semantically different from the elements of some action text\nlabels in a procedure. To bridge this semantic gap, we propose a novel\nevent-guided paradigm, which first infers events from the observed states and\nthen plans out actions based on both the states and predicted events. Our\ninspiration comes from that planning a procedure from an instructional video is\nto complete a specific event and a specific event usually involves specific\nactions. Based on the proposed paradigm, we contribute an Event-guided\nPrompting-based Procedure Planning (E3P) model, which encodes event information\ninto the sequential modeling process to support procedure planning. To further\nconsider the strong action associations within each event, our E3P adopts a\nmask-and-predict approach for relation mining, incorporating a probabilistic\nmasking scheme for regularization. Extensive experiments on three datasets\ndemonstrate the effectiveness of our proposed model.",
        "authors": [
            "An-Lan Wang",
            "Kun-Yu Lin",
            "Jia-Run Du",
            "Jingke Meng",
            "Wei-Shi Zheng"
        ]
    },
    {
        "title": "Foreground Object Search by Distilling Composite Image Feature",
        "url": "http://arxiv.org/abs/2308.04990",
        "abstract": "Foreground object search (FOS) aims to find compatible foreground objects for\na given background image, producing realistic composite image. We observe that\ncompetitive retrieval performance could be achieved by using a discriminator to\npredict the compatibility of composite image, but this approach has\nunaffordable time cost. To this end, we propose a novel FOS method via\ndistilling composite feature (DiscoFOS). Specifically, the abovementioned\ndiscriminator serves as teacher network. The student network employs two\nencoders to extract foreground feature and background feature. Their\ninteraction output is enforced to match the composite image feature from the\nteacher network. Additionally, previous works did not release their datasets,\nso we contribute two datasets for FOS task: S-FOSD dataset with synthetic\ncomposite images and R-FOSD dataset with real composite images. Extensive\nexperiments on our two datasets demonstrate the superiority of the proposed\nmethod over previous approaches. The dataset and code are available at\nhttps://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.",
        "authors": [
            "Bo Zhang",
            "Jiacheng Sui",
            "Li Niu"
        ]
    },
    {
        "title": "ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field",
        "url": "http://arxiv.org/abs/2211.13226",
        "abstract": "Physical simulations produce excellent predictions of weather effects. Neural\nradiance fields produce SOTA scene models. We describe a novel NeRF-editing\nprocedure that can fuse physical simulations with NeRF models of scenes,\nproducing realistic movies of physical phenomena in those scenes. Our\napplication -- Climate NeRF -- allows people to visualize what climate change\noutcomes will do to them. ClimateNeRF allows us to render realistic weather\neffects, including smog, snow, and flood. Results can be controlled with\nphysically meaningful variables like water level. Qualitative and quantitative\nstudies show that our simulated results are significantly more realistic than\nthose from SOTA 2D image editing and SOTA 3D NeRF stylization.",
        "authors": [
            "Yuan Li",
            "Zhi-Hao Lin",
            "David Forsyth",
            "Jia-Bin Huang",
            "Shenlong Wang"
        ]
    },
    {
        "title": "Generalized Few-Shot Point Cloud Segmentation via Geometric Words",
        "url": "http://arxiv.org/abs/2309.11222",
        "abstract": "Existing fully-supervised point cloud segmentation methods suffer in the\ndynamic testing environment with emerging new classes. Few-shot point cloud\nsegmentation algorithms address this problem by learning to adapt to new\nclasses at the sacrifice of segmentation accuracy for the base classes, which\nseverely impedes its practicality. This largely motivates us to present the\nfirst attempt at a more practical paradigm of generalized few-shot point cloud\nsegmentation, which requires the model to generalize to new categories with\nonly a few support point clouds and simultaneously retain the capability to\nsegment base classes. We propose the geometric words to represent geometric\ncomponents shared between the base and novel classes, and incorporate them into\na novel geometric-aware semantic representation to facilitate better\ngeneralization to the new classes without forgetting the old ones. Moreover, we\nintroduce geometric prototypes to guide the segmentation with geometric prior\nknowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate\nthe superior performance of our method over baseline methods. Our code is\navailable at: https://github.com/Pixie8888/GFS-3DSeg_GWs.",
        "authors": [
            "Yating Xu",
            "Conghui Hu",
            "Na Zhao",
            "Gim Hee Lee"
        ]
    },
    {
        "title": "Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection",
        "url": "http://arxiv.org/abs/2304.04442",
        "abstract": "Single-frame infrared small target (SIRST) detection aims at separating small\ntargets from clutter backgrounds on infrared images. Recently, deep learning\nbased methods have achieved promising performance on SIRST detection, but at\nthe cost of a large amount of training data with expensive pixel-level\nannotations. To reduce the annotation burden, we propose the first method to\nachieve SIRST detection with single-point supervision. The core idea of this\nwork is to recover the per-pixel mask of each target from the given single\npoint label by using clustering approaches, which looks simple but is indeed\nchallenging since targets are always insalient and accompanied with background\nclutters. To handle this issue, we introduce randomness to the clustering\nprocess by adding noise to the input images, and then obtain much more reliable\npseudo masks by averaging the clustered results. Thanks to this \"Monte Carlo\"\nclustering approach, our method can accurately recover pseudo masks and thus\nturn arbitrary fully supervised SIRST detection networks into weakly supervised\nones with only single point annotation. Experiments on four datasets\ndemonstrate that our method can be applied to existing SIRST detection networks\nto achieve comparable performance with their fully supervised counterparts,\nwhich reveals that single-point supervision is strong enough for SIRST\ndetection. Our code will be available at:\nhttps://github.com/YeRen123455/SIRST-Single-Point-Supervision.",
        "authors": [
            "Boyang Li",
            "Yingqian Wang",
            "Longguang Wang",
            "Fei Zhang",
            "Ting Liu",
            "Zaiping Lin",
            "Wei An",
            "Yulan Guo"
        ]
    },
    {
        "title": "TCOVIS: Temporally Consistent Online Video Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.11857",
        "abstract": "In recent years, significant progress has been made in video instance\nsegmentation (VIS), with many offline and online methods achieving\nstate-of-the-art performance. While offline methods have the advantage of\nproducing temporally consistent predictions, they are not suitable for\nreal-time scenarios. Conversely, online methods are more practical, but\nmaintaining temporal consistency remains a challenging task. In this paper, we\npropose a novel online method for video instance segmentation, called TCOVIS,\nwhich fully exploits the temporal information in a video clip. The core of our\nmethod consists of a global instance assignment strategy and a spatio-temporal\nenhancement module, which improve the temporal consistency of the features from\ntwo aspects. Specifically, we perform global optimal matching between the\npredictions and ground truth across the whole video clip, and supervise the\nmodel with the global optimal objective. We also capture the spatial feature\nand aggregate it with the semantic feature between frames, thus realizing the\nspatio-temporal enhancement. We evaluate our method on four widely adopted VIS\nbenchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve\nstate-of-the-art performance on all benchmarks without bells-and-whistles. For\ninstance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with\nResNet-50 and Swin-L backbones, respectively. Code is available at\nhttps://github.com/jun-long-li/TCOVIS.",
        "authors": [
            "Junlong Li",
            "Bingyao Yu",
            "Yongming Rao",
            "Jie Zhou",
            "Jiwen Lu"
        ]
    },
    {
        "title": "Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models",
        "url": "http://arxiv.org/abs/2304.10700",
        "abstract": "Novel view synthesis from a single input image is a challenging task, where\nthe goal is to generate a new view of a scene from a desired camera pose that\nmay be separated by a large motion. The highly uncertain nature of this\nsynthesis task due to unobserved elements within the scene (i.e. occlusion) and\noutside the field-of-view makes the use of generative models appealing to\ncapture the variety of possible outputs. In this paper, we propose a novel\ngenerative model capable of producing a sequence of photorealistic images\nconsistent with a specified camera trajectory, and a single starting image. Our\napproach is centred on an autoregressive conditional diffusion-based model\ncapable of interpolating visible scene elements, and extrapolating unobserved\nregions in a view, in a geometrically consistent manner. Conditioning is\nlimited to an image capturing a single camera view and the (relative) pose of\nthe new camera view. To measure the consistency over a sequence of generated\nviews, we introduce a new metric, the thresholded symmetric epipolar distance\n(TSED), to measure the number of consistent frame pairs in a sequence. While\nprevious methods have been shown to produce high quality images and consistent\nsemantics across pairs of views, we show empirically with our metric that they\nare often inconsistent with the desired camera poses. In contrast, we\ndemonstrate that our method produces both photorealistic and view-consistent\nimagery.",
        "authors": [
            "Jason J. Yu",
            "Fereshteh Forghani",
            "Konstantinos G. Derpanis",
            "Marcus A. Brubaker"
        ]
    },
    {
        "title": "What Can a Cook in Italy Teach a Mechanic in India? Action Recognition Generalisation Over Scenarios and Locations",
        "url": "http://arxiv.org/abs/2306.08713",
        "abstract": "We propose and address a new generalisation problem: can a model trained for\naction recognition successfully classify actions when they are performed within\na previously unseen scenario and in a previously unseen location? To answer\nthis question, we introduce the Action Recognition Generalisation Over\nscenarios and locations dataset (ARGO1M), which contains 1.1M video clips from\nthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. We\ndemonstrate recognition models struggle to generalise over 10 proposed test\nsplits, each of an unseen scenario in an unseen location. We thus propose CIR,\na method to represent each video as a Cross-Instance Reconstruction of videos\nfrom other domains. Reconstructions are paired with text narrations to guide\nthe learning of a domain generalisable representation. We provide extensive\nanalysis and ablations on ARGO1M that show CIR outperforms prior domain\ngeneralisation works on all test splits. Code and data:\nhttps://chiaraplizz.github.io/what-can-a-cook/.",
        "authors": [
            "Chiara Plizzari",
            "Toby Perrett",
            "Barbara Caputo",
            "Dima Damen"
        ]
    },
    {
        "title": "EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild",
        "url": "http://arxiv.org/abs/2308.16894",
        "abstract": "We present EMDB, the Electromagnetic Database of Global 3D Human Pose and\nShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPL\npose and shape parameters with global body and camera trajectories for\nin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors and\na hand-held iPhone to record a total of 58 minutes of motion data, distributed\nover 81 indoor and outdoor sequences and 10 participants. Together with\naccurate body poses and shapes, we also provide global camera poses and body\nroot trajectories. To construct EMDB, we propose a multi-stage optimization\nprocedure, which first fits SMPL to the 6-DoF EM measurements and then refines\nthe poses via image observations. To achieve high-quality results, we leverage\na neural implicit avatar model to reconstruct detailed human surface geometry\nand appearance, which allows for improved alignment and smoothness via a dense\npixel-level objective. Our evaluations, conducted with a multi-view volumetric\ncapture system, indicate that EMDB has an expected accuracy of 2.3 cm\npositional and 10.6 degrees angular error, surpassing the accuracy of previous\nin-the-wild datasets. We evaluate existing state-of-the-art monocular RGB\nmethods for camera-relative and global pose estimation on EMDB. EMDB is\npublicly available under https://ait.ethz.ch/emdb",
        "authors": [
            "Manuel Kaufmann",
            "Jie Song",
            "Chen Guo",
            "Kaiyue Shen",
            "Tianjian Jiang",
            "Chengcheng Tang",
            "Juan Zarate",
            "Otmar Hilliges"
        ]
    },
    {
        "title": "STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning",
        "url": "http://arxiv.org/abs/2308.10468",
        "abstract": "Scale variation is a deep-rooted problem in object counting, which has not\nbeen effectively addressed by existing scale-aware algorithms. An important\nfactor is that they typically involve cooperative learning across\nmulti-resolutions, which could be suboptimal for learning the most\ndiscriminative features from each scale. In this paper, we propose a novel\nmethod termed STEERER (\\textbf{S}elec\\textbf{T}iv\\textbf{E}\ninh\\textbf{ER}itance l\\textbf{E}a\\textbf{R}ning) that addresses the issue of\nscale variations in object counting. STEERER selects the most suitable scale\nfor patch objects to boost feature extraction and only inherits discriminative\nfeatures from lower to higher resolution progressively. The main insights of\nSTEERER are a dedicated Feature Selection and Inheritance Adaptor (FSIA), which\nselectively forwards scale-customized features at each scale, and a Masked\nSelection and Inheritance Loss (MSIL) that helps to achieve high-quality\ndensity maps across all scales. Our experimental results on nine datasets with\ncounting and localization tasks demonstrate the unprecedented scale\ngeneralization ability of STEERER. Code is available at\n\\url{https://github.com/taohan10200/STEERER}.",
        "authors": [
            "Tao Han",
            "Lei Bai",
            "Lingbo Liu",
            "Wanli Ouyang"
        ]
    },
    {
        "title": "Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation",
        "url": "http://arxiv.org/abs/2308.05441",
        "abstract": "We propose an experimental method for measuring bias in face recognition\nsystems. Existing methods to measure bias depend on benchmark datasets that are\ncollected in the wild and annotated for protected (e.g., race, gender) and\nnon-protected (e.g., pose, lighting) attributes. Such observational datasets\nonly permit correlational conclusions, e.g., \"Algorithm A's accuracy is\ndifferent on female and male faces in dataset X.\". By contrast, experimental\nmethods manipulate attributes individually and thus permit causal conclusions,\ne.g., \"Algorithm A's accuracy is affected by gender and skin color.\"\n  Our method is based on generating synthetic faces using a neural face\ngenerator, where each attribute of interest is modified independently while\nleaving all other attributes constant. Human observers crucially provide the\nground truth on perceptual identity similarity between synthetic image pairs.\nWe validate our method quantitatively by evaluating race and gender biases of\nthree research-grade face recognition models. Our synthetic pipeline reveals\nthat for these algorithms, accuracy is lower for Black and East Asian\npopulation subgroups. Our method can also quantify how perceptual changes in\nattributes affect face identity distances reported by these models. Our large\nsynthetic dataset, consisting of 48,000 synthetic face image pairs (10,200\nunique synthetic faces) and 555,000 human annotations (individual attributes\nand pairwise identity comparisons) is available to researchers in this\nimportant area.",
        "authors": [
            "Hao Liang",
            "Pietro Perona",
            "Guha Balakrishnan"
        ]
    },
    {
        "title": "Spatial-Aware Token for Weakly Supervised Object Localization",
        "url": "http://arxiv.org/abs/2303.10438",
        "abstract": "Weakly supervised object localization (WSOL) is a challenging task aiming to\nlocalize objects with only image-level supervision. Recent works apply visual\ntransformer to WSOL and achieve significant success by exploiting the\nlong-range feature dependency in self-attention mechanism. However, existing\ntransformer-based methods synthesize the classification feature maps as the\nlocalization map, which leads to optimization conflicts between classification\nand localization tasks. To address this problem, we propose to learn a\ntask-specific spatial-aware token (SAT) to condition localization in a weakly\nsupervised manner. Specifically, a spatial token is first introduced in the\ninput space to aggregate representations for localization task. Then a spatial\naware attention module is constructed, which allows spatial token to generate\nforeground probabilities of different patches by querying and to extract\nlocalization knowledge from the classification task. Besides, for the problem\nof sparse and unbalanced pixel-level supervision obtained from the image-level\nlabel, two spatial constraints, including batch area loss and normalization\nloss, are designed to compensate and enhance this supervision. Experiments show\nthat the proposed SAT achieves state-of-the-art performance on both CUB-200 and\nImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under the\nextreme setting of using only 1 image per class from ImageNet for training, SAT\nalready exceeds the SOTA method by 2.1% GT-known Loc. Code and models are\navailable at https://github.com/wpy1999/SAT.",
        "authors": [
            "Pingyu Wu",
            "Wei Zhai",
            "Yang Cao",
            "Jiebo Luo",
            "Zheng-Jun Zha"
        ]
    },
    {
        "title": "Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis",
        "url": "http://arxiv.org/abs/2304.03869",
        "abstract": "Diffusion-based models have achieved state-of-the-art performance on\ntext-to-image synthesis tasks. However, one critical limitation of these models\nis the low fidelity of generated images with respect to the text description,\nsuch as missing objects, mismatched attributes, and mislocated objects. One key\nreason for such inconsistencies is the inaccurate cross-attention to text in\nboth the spatial dimension, which controls at what pixel region an object\nshould appear, and the temporal dimension, which controls how different levels\nof details are added through the denoising steps. In this paper, we propose a\nnew text-to-image algorithm that adds explicit control over spatial-temporal\ncross-attention in diffusion models. We first utilize a layout predictor to\npredict the pixel regions for objects mentioned in the text. We then impose\nspatial attention control by combining the attention over the entire text\ndescription and that over the local description of the particular object in the\ncorresponding pixel region of that object. The temporal attention control is\nfurther added by allowing the combination weights to change at each denoising\nstep, and the combination weights are optimized to ensure high fidelity between\nthe image and the text. Experiments show that our method generates images with\nhigher fidelity compared to diffusion-model-based baselines without fine-tuning\nthe diffusion model. Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn.",
        "authors": [
            "Qiucheng Wu",
            "Yujian Liu",
            "Handong Zhao",
            "Trung Bui",
            "Zhe Lin",
            "Yang Zhang",
            "Shiyu Chang"
        ]
    },
    {
        "title": "C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction",
        "url": "http://arxiv.org/abs/2306.10003",
        "abstract": "There is an emerging effort to combine the two popular 3D frameworks using\nMulti-View Stereo (MVS) and Neural Implicit Surfaces (NIS) with a specific\nfocus on the few-shot / sparse view setting. In this paper, we introduce a\nnovel integration scheme that combines the multi-view stereo with neural signed\ndistance function representations, which potentially overcomes the limitations\nof both methods. MVS uses per-view depth estimation and cross-view fusion to\ngenerate accurate surfaces, while NIS relies on a common coordinate volume.\nBased on this strategy, we propose to construct per-view cost frustum for finer\ngeometry estimation, and then fuse cross-view frustums and estimate the\nimplicit signed distance functions to tackle artifacts that are due to noise\nand holes in the produced surface reconstruction. We further apply a cascade\nfrustum fusion strategy to effectively captures global-local information and\nstructural consistency. Finally, we apply cascade sampling and a\npseudo-geometric loss to foster stronger integration between the two\narchitectures. Extensive experiments demonstrate that our method reconstructs\nrobust surfaces and outperforms existing state-of-the-art methods.",
        "authors": [
            "Luoyuan Xu",
            "Tao Guan",
            "Yuesong Wang",
            "Wenkai Liu",
            "Zhaojie Zeng",
            "Junle Wang",
            "Wei Yang"
        ]
    },
    {
        "title": "Mesh2Tex: Generating Mesh Textures from Image Queries",
        "url": "http://arxiv.org/abs/2304.05868",
        "abstract": "Remarkable advances have been achieved recently in learning neural\nrepresentations that characterize object geometry, while generating textured\nobjects suitable for downstream applications and 3D rendering remains at an\nearly stage. In particular, reconstructing textured geometry from images of\nreal objects is a significant challenge -- reconstructed geometry is often\ninexact, making realistic texturing a significant challenge. We present\nMesh2Tex, which learns a realistic object texture manifold from uncorrelated\ncollections of 3D object geometry and photorealistic RGB images, by leveraging\na hybrid mesh-neural-field texture representation. Our texture representation\nenables compact encoding of high-resolution textures as a neural field in the\nbarycentric coordinate system of the mesh faces. The learned texture manifold\nenables effective navigation to generate an object texture for a given 3D\nobject geometry that matches to an input RGB image, which maintains robustness\neven under challenging real-world scenarios where the mesh geometry\napproximates an inexact match to the underlying geometry in the RGB image.\nMesh2Tex can effectively generate realistic object textures for an object mesh\nto match real images observations towards digitization of real environments,\nsignificantly improving over previous state of the art.",
        "authors": [
            "Alexey Bokhovkin",
            "Shubham Tulsiani",
            "Angela Dai"
        ]
    },
    {
        "title": "USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2303.07806",
        "abstract": "Seed area generation is usually the starting point of weakly supervised\nsemantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a\nmulti-label classification network is the de facto paradigm for seed area\ngeneration, but CAMs generated from Convolutional Neural Networks (CNNs) and\nTransformers are prone to be under- and over-activated, respectively, which\nmakes the strategies to refine CAMs for CNNs usually inappropriate for\nTransformers, and vice versa. In this paper, we propose a Unified optimization\nparadigm for Seed Area GEneration (USAGE) for both types of networks, in which\nthe objective function to be optimized consists of two terms: One is a\ngeneration loss, which controls the shape of seed areas by a temperature\nparameter following a deterministic principle for different types of networks;\nThe other is a regularization loss, which ensures the consistency between the\nseed areas that are generated by self-adaptive network adjustment from\ndifferent views, to overturn false activation in seed areas. Experimental\nresults show that USAGE consistently improves seed area generation for both\nCNNs and Transformers by large margins, e.g., outperforming state-of-the-art\nmethods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated\nseed areas on Transformers, we achieve state-of-the-art WSSS results on both\nPASCAL VOC and MS COCO.",
        "authors": [
            "Zelin Peng",
            "Guanchun Wang",
            "Lingxi Xie",
            "Dongsheng Jiang",
            "Wei Shen",
            "Qi Tian"
        ]
    },
    {
        "title": "NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction",
        "url": "http://arxiv.org/abs/2212.05231",
        "abstract": "Recent methods for neural surface representation and rendering, for example\nNeuS, have demonstrated the remarkably high-quality reconstruction of static\nscenes. However, the training of NeuS takes an extremely long time (8 hours),\nwhich makes it almost impossible to apply them to dynamic scenes with thousands\nof frames. We propose a fast neural surface reconstruction approach, called\nNeuS2, which achieves two orders of magnitude improvement in terms of\nacceleration without compromising reconstruction quality. To accelerate the\ntraining process, we parameterize a neural surface representation by\nmulti-resolution hash encodings and present a novel lightweight calculation of\nsecond-order derivatives tailored to our networks to leverage CUDA parallelism,\nachieving a factor two speed up. To further stabilize and expedite training, a\nprogressive learning strategy is proposed to optimize multi-resolution hash\nencodings from coarse to fine. We extend our method for fast training of\ndynamic scenes, with a proposed incremental training strategy and a novel\nglobal transformation prediction component, which allow our method to handle\nchallenging long sequences with large movements and deformations. Our\nexperiments on various datasets demonstrate that NeuS2 significantly\noutperforms the state-of-the-arts in both surface reconstruction accuracy and\ntraining speed for both static and dynamic scenes. The code is available at our\nwebsite: https://vcai.mpi-inf.mpg.de/projects/NeuS2/ .",
        "authors": [
            "Yiming Wang",
            "Qin Han",
            "Marc Habermann",
            "Kostas Daniilidis",
            "Christian Theobalt",
            "Lingjie Liu"
        ]
    },
    {
        "title": "Fast Full-frame Video Stabilization with Iterative Optimization",
        "url": "http://arxiv.org/abs/2307.12774",
        "abstract": "Video stabilization refers to the problem of transforming a shaky video into\na visually pleasing one. The question of how to strike a good trade-off between\nvisual quality and computational speed has remained one of the open challenges\nin video stabilization. Inspired by the analogy between wobbly frames and\njigsaw puzzles, we propose an iterative optimization-based learning approach\nusing synthetic datasets for video stabilization, which consists of two\ninteracting submodules: motion trajectory smoothing and full-frame outpainting.\nFirst, we develop a two-level (coarse-to-fine) stabilizing algorithm based on\nthe probabilistic flow field. The confidence map associated with the estimated\noptical flow is exploited to guide the search for shared regions through\nbackpropagation. Second, we take a divide-and-conquer approach and propose a\nnovel multiframe fusion strategy to render full-frame stabilized views. An\nimportant new insight brought about by our iterative optimization approach is\nthat the target video can be interpreted as the fixed point of nonlinear\nmapping for video stabilization. We formulate video stabilization as a problem\nof minimizing the amount of jerkiness in motion trajectories, which guarantees\nconvergence with the help of fixed-point theory. Extensive experimental results\nare reported to demonstrate the superiority of the proposed approach in terms\nof computational speed and visual quality. The code will be available on\nGitHub.",
        "authors": [
            "Weiyue Zhao",
            "Xin Li",
            "Zhan Peng",
            "Xianrui Luo",
            "Xinyi Ye",
            "Hao Lu",
            "Zhiguo Cao"
        ]
    },
    {
        "title": "Gender Artifacts in Visual Datasets",
        "url": "http://arxiv.org/abs/2206.09191",
        "abstract": "Gender biases are known to exist within large-scale visual datasets and can\nbe reflected or even amplified in downstream models. Many prior works have\nproposed methods for mitigating gender biases, often by attempting to remove\ngender expression information from images. To understand the feasibility and\npracticality of these approaches, we investigate what $\\textit{gender\nartifacts}$ exist within large-scale visual datasets. We define a\n$\\textit{gender artifact}$ as a visual cue that is correlated with gender,\nfocusing specifically on those cues that are learnable by a modern image\nclassifier and have an interpretable human corollary. Through our analyses, we\nfind that gender artifacts are ubiquitous in the COCO and OpenImages datasets,\noccurring everywhere from low-level information (e.g., the mean value of the\ncolor channels) to the higher-level composition of the image (e.g., pose and\nlocation of people). Given the prevalence of gender artifacts, we claim that\nattempts to remove gender artifacts from such datasets are largely infeasible.\nInstead, the responsibility lies with researchers and practitioners to be aware\nthat the distribution of images within datasets is highly gendered and hence\ndevelop methods which are robust to these distributional shifts across groups.",
        "authors": [
            "Nicole Meister",
            "Dora Zhao",
            "Angelina Wang",
            "Vikram V. Ramaswamy",
            "Ruth Fong",
            "Olga Russakovsky"
        ]
    },
    {
        "title": "Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery",
        "url": "http://arxiv.org/abs/2305.06144",
        "abstract": "In this paper, we address the problem of generalized category discovery\n(GCD), \\ie, given a set of images where part of them are labelled and the rest\nare not, the task is to automatically cluster the images in the unlabelled\ndata, leveraging the information from the labelled data, while the unlabelled\ndata contain images from the labelled classes and also new ones. GCD is similar\nto semi-supervised learning (SSL) but is more realistic and challenging, as SSL\nassumes all the unlabelled images are from the same classes as the labelled\nones. We also do not assume the class number in the unlabelled data is known\na-priori, making the GCD problem even harder. To tackle the problem of GCD\nwithout knowing the class number, we propose an EM-like framework that\nalternates between representation learning and class number estimation. We\npropose a semi-supervised variant of the Gaussian Mixture Model (GMM) with a\nstochastic splitting and merging mechanism to dynamically determine the\nprototypes by examining the cluster compactness and separability. With these\nprototypes, we leverage prototypical contrastive learning for representation\nlearning on the partially labelled data subject to the constraints imposed by\nthe labelled data. Our framework alternates between these two steps until\nconvergence. The cluster assignment for an unlabelled instance can then be\nretrieved by identifying its nearest prototype. We comprehensively evaluate our\nframework on both generic image classification datasets and challenging\nfine-grained object recognition datasets, achieving state-of-the-art\nperformance.",
        "authors": [
            "Bingchen Zhao",
            "Xin Wen",
            "Kai Han"
        ]
    },
    {
        "title": "Beating Backdoor Attack at Its Own Game",
        "url": "http://arxiv.org/abs/2307.15539",
        "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not\naffect the network's performance on clean data but would manipulate the network\nbehavior once a trigger pattern is added. Existing defense methods have greatly\nreduced attack success rate, but their prediction accuracy on clean data still\nlags behind a clean model by a large margin. Inspired by the stealthiness and\neffectiveness of backdoor attack, we propose a simple but highly effective\ndefense framework which injects non-adversarial backdoors targeting poisoned\nsamples. Following the general steps in backdoor attack, we detect a small set\nof suspected samples and then apply a poisoning strategy to them. The\nnon-adversarial backdoor, once triggered, suppresses the attacker's backdoor on\npoisoned data, but has limited influence on clean data. The defense can be\ncarried out during data preprocessing, without any modification to the standard\nend-to-end training pipeline. We conduct extensive experiments on multiple\nbenchmarks with different architectures and representative attacks. Results\ndemonstrate that our method achieves state-of-the-art defense effectiveness\nwith by far the lowest performance drop on clean data. Considering the\nsurprising defense ability displayed by our framework, we call for more\nattention to utilizing backdoor for backdoor defense. Code is available at\nhttps://github.com/damianliumin/non-adversarial_backdoor.",
        "authors": [
            "Min Liu",
            "Alberto Sangiovanni-Vincentelli",
            "Xiangyu Yue"
        ]
    },
    {
        "title": "Introducing Language Guidance in Prompt-based Continual Learning",
        "url": "http://arxiv.org/abs/2308.15827",
        "abstract": "Continual Learning aims to learn a single model on a sequence of tasks\nwithout having access to data from previous tasks. The biggest challenge in the\ndomain still remains catastrophic forgetting: a loss in performance on seen\nclasses of earlier tasks. Some existing methods rely on an expensive replay\nbuffer to store a chunk of data from previous tasks. This, while promising,\nbecomes expensive when the number of tasks becomes large or data can not be\nstored for privacy reasons. As an alternative, prompt-based methods have been\nproposed that store the task information in a learnable prompt pool. This\nprompt pool instructs a frozen image encoder on how to solve each task. While\nthe model faces a disjoint set of classes in each task in this setting, we\nargue that these classes can be encoded to the same embedding space of a\npre-trained language encoder. In this work, we propose Language Guidance for\nPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.\nLGCL is model agnostic and introduces language guidance at the task level in\nthe prompt pool and at the class level on the output feature of the vision\nencoder. We show with extensive experimentation that LGCL consistently improves\nthe performance of prompt-based continual learning methods to set a new\nstate-of-the art. LGCL achieves these performance improvements without needing\nany additional learnable parameters.",
        "authors": [
            "Muhammad Gul Zain Ali Khan",
            "Muhammad Ferjad Naeem",
            "Luc Van Gool",
            "Didier Stricker",
            "Federico Tombari",
            "Muhammad Zeshan Afzal"
        ]
    },
    {
        "title": "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition",
        "url": "http://arxiv.org/abs/2308.09694",
        "abstract": "We tackle the data scarcity challenge in few-shot point cloud recognition of\n3D objects by using a joint prediction from a conventional 3D model and a\nwell-trained 2D model. Surprisingly, such an ensemble, though seems trivial,\nhas hardly been shown effective in recent 2D-3D models. We find out the crux is\nthe less effective training for the ''joint hard samples'', which have high\nconfidence prediction on different wrong labels, implying that the 2D and 3D\nmodels do not collaborate well. To this end, our proposed invariant training\nstrategy, called InvJoint, does not only emphasize the training more on the\nhard samples, but also seeks the invariance between the conflicting 2D and 3D\nambiguous predictions. InvJoint can learn more collaborative 2D and 3D\nrepresentations for better ensemble. Extensive experiments on 3D shape\nclassification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, and\nshape retrieval with ShapeNet-Core validate the superiority of our InvJoint.",
        "authors": [
            "Xuanyu Yi",
            "Jiajun Deng",
            "Qianru Sun",
            "Xian-Sheng Hua",
            "Joo-Hwee Lim",
            "Hanwang Zhang"
        ]
    },
    {
        "title": "EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition",
        "url": "http://arxiv.org/abs/2308.10832",
        "abstract": "Visual Place Recognition is a task that aims to predict the place of an image\n(called query) based solely on its visual features. This is typically done\nthrough image retrieval, where the query is matched to the most similar images\nfrom a large database of geotagged photos, using learned global descriptors. A\nmajor challenge in this task is recognizing places seen from different\nviewpoints. To overcome this limitation, we propose a new method, called\nEigenPlaces, to train our neural network on images from different point of\nviews, which embeds viewpoint robustness into the learned global descriptors.\nThe underlying idea is to cluster the training data so as to explicitly present\nthe model with different views of the same points of interest. The selection of\nthis points of interest is done without the need for extra supervision. We then\npresent experiments on the most comprehensive set of datasets in literature,\nfinding that EigenPlaces is able to outperform previous state of the art on the\nmajority of datasets, while requiring 60\\% less GPU memory for training and\nusing 50\\% smaller descriptors. The code and trained models for EigenPlaces are\navailable at {\\small{\\url{https://github.com/gmberton/EigenPlaces}}}, while\nresults with any other baseline can be computed with the codebase at\n{\\small{\\url{https://github.com/gmberton/auto_VPR}}}.",
        "authors": [
            "Gabriele Berton",
            "Gabriele Trivigno",
            "Barbara Caputo",
            "Carlo Masone"
        ]
    },
    {
        "title": "FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation",
        "url": "http://arxiv.org/abs/2307.07245",
        "abstract": "Curvilinear object segmentation is critical for many applications. However,\nmanually annotating curvilinear objects is very time-consuming and error-prone,\nyielding insufficiently available annotated datasets for existing supervised\nmethods and domain adaptation methods. This paper proposes a self-supervised\ncurvilinear object segmentation method that learns robust and distinctive\nfeatures from fractals and unlabeled images (FreeCOS). The key contributions\ninclude a novel Fractal-FDA synthesis (FFS) module and a geometric information\nalignment (GIA) approach. FFS generates curvilinear structures based on the\nparametric Fractal L-system and integrates the generated structures into\nunlabeled images to obtain synthetic training images via Fourier Domain\nAdaptation. GIA reduces the intensity differences between the synthetic and\nunlabeled images by comparing the intensity order of a given pixel to the\nvalues of its nearby neighbors. Such image alignment can explicitly remove the\ndependency on absolute intensity values and enhance the inherent geometric\ncharacteristics which are common in both synthetic and real images. In\naddition, GIA aligns features of synthetic and real images via the prediction\nspace adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL).\nExtensive experimental results on four public datasets, i.e., XCAD, DRIVE,\nSTARE and CrackTree demonstrate that our method outperforms the\nstate-of-the-art unsupervised methods, self-supervised methods and traditional\nmethods by a large margin. The source code of this work is available at\nhttps://github.com/TY-Shi/FreeCOS.",
        "authors": [
            "Tianyi Shi",
            "Xiaohuan Ding",
            "Liang Zhang",
            "Xin Yang"
        ]
    },
    {
        "title": "RSFNet: A White-Box Image Retouching Approach using Region-Specific Color Filters",
        "url": "http://arxiv.org/abs/2303.08682",
        "abstract": "Retouching images is an essential aspect of enhancing the visual appeal of\nphotos. Although users often share common aesthetic preferences, their\nretouching methods may vary based on their individual preferences. Therefore,\nthere is a need for white-box approaches that produce satisfying results and\nenable users to conveniently edit their images simultaneously. Recent white-box\nretouching methods rely on cascaded global filters that provide image-level\nfilter arguments but cannot perform fine-grained retouching. In contrast,\ncolorists typically employ a divide-and-conquer approach, performing a series\nof region-specific fine-grained enhancements when using traditional tools like\nDavinci Resolve. We draw on this insight to develop a white-box framework for\nphoto retouching using parallel region-specific filters, called RSFNet. Our\nmodel generates filter arguments (e.g., saturation, contrast, hue) and\nattention maps of regions for each filter simultaneously. Instead of cascading\nfilters, RSFNet employs linear summations of filters, allowing for a more\ndiverse range of filter classes that can be trained more easily. Our\nexperiments demonstrate that RSFNet achieves state-of-the-art results, offering\nsatisfying aesthetic appeal and increased user convenience for editable\nwhite-box retouching.",
        "authors": [
            "Wenqi Ouyang",
            "Yi Dong",
            "Xiaoyang Kang",
            "Peiran Ren",
            "Xin Xu",
            "Xuansong Xie"
        ]
    },
    {
        "title": "Boosting Long-tailed Object Detection via Step-wise Learning on Smooth-tail Data",
        "url": "http://arxiv.org/abs/2305.12833",
        "abstract": "Real-world data tends to follow a long-tailed distribution, where the class\nimbalance results in dominance of the head classes during training. In this\npaper, we propose a frustratingly simple but effective step-wise learning\nframework to gradually enhance the capability of the model in detecting all\ncategories of long-tailed datasets. Specifically, we build smooth-tail data\nwhere the long-tailed distribution of categories decays smoothly to correct the\nbias towards head classes. We pre-train a model on the whole long-tailed data\nto preserve discriminability between all categories. We then fine-tune the\nclass-agnostic modules of the pre-trained model on the head class dominant\nreplay data to get a head class expert model with improved decision boundaries\nfrom all categories. Finally, we train a unified model on the tail class\ndominant replay data while transferring knowledge from the head class expert\nmodel to ensure accurate detection of all categories. Extensive experiments on\nlong-tailed datasets LVIS v0.5 and LVIS v1.0 demonstrate the superior\nperformance of our method, where we can improve the AP with ResNet-50 backbone\nfrom 27.0% to 30.3% AP, and especially for the rare categories from 15.5% to\n24.9% AP. Our best model using ResNet-101 backbone can achieve 30.7% AP, which\nsuppresses all existing detectors using the same backbone.",
        "authors": [
            "Na Dong",
            "Yongqiang Zhang",
            "Mingli Ding",
            "Gim Hee Lee"
        ]
    },
    {
        "title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors",
        "url": "http://arxiv.org/abs/2212.04248",
        "abstract": "In this paper, we introduce a simple and novel framework for one-shot\naudio-driven talking head generation. Unlike prior works that require\nadditional driving sources for controlled synthesis in a deterministic manner,\nwe instead probabilistically sample all the holistic lip-irrelevant facial\nmotions (i.e. pose, expression, blink, gaze, etc.) to semantically match the\ninput audio while still maintaining both the photo-realism of audio-lip\nsynchronization and the overall naturalness. This is achieved by our newly\nproposed audio-to-visual diffusion prior trained on top of the mapping between\naudio and disentangled non-lip facial representations. Thanks to the\nprobabilistic nature of the diffusion prior, one big advantage of our framework\nis it can synthesize diverse facial motion sequences given the same audio clip,\nwhich is quite user-friendly for many real applications. Through comprehensive\nevaluations on public benchmarks, we conclude that (1) our diffusion prior\noutperforms auto-regressive prior significantly on almost all the concerned\nmetrics; (2) our overall system is competitive with prior works in terms of\naudio-lip synchronization but can effectively sample rich and natural-looking\nlip-irrelevant facial motions while still semantically harmonized with the\naudio input.",
        "authors": [
            "Zhentao Yu",
            "Zixin Yin",
            "Deyu Zhou",
            "Duomin Wang",
            "Finn Wong",
            "Baoyuan Wang"
        ]
    },
    {
        "title": "Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples",
        "url": "http://arxiv.org/abs/2309.02041",
        "abstract": "Referring video object segmentation (RVOS), as a supervised learning task,\nrelies on sufficient annotated data for a given scene. However, in more\nrealistic scenarios, only minimal annotations are available for a new scene,\nwhich poses significant challenges to existing RVOS methods. With this in mind,\nwe propose a simple yet effective model with a newly designed cross-modal\naffinity (CMA) module based on a Transformer architecture. The CMA module\nbuilds multimodal affinity with a few samples, thus quickly learning new\nsemantic information, and enabling the model to adapt to different scenarios.\nSince the proposed method targets limited samples for new scenes, we generalize\nthe problem as - few-shot referring video object segmentation (FS-RVOS). To\nfoster research in this direction, we build up a new FS-RVOS benchmark based on\ncurrently available datasets. The benchmark covers a wide range and includes\nmultiple situations, which can maximally simulate real-world scenarios.\nExtensive experiments show that our model adapts well to different scenarios\nwith only a few samples, reaching state-of-the-art performance on the\nbenchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performance\nof 53.1 J and 54.8 F, which are 10% better than the baselines. Furthermore, we\nshow impressive results of 77.7 J and 74.8 F on Mini-Ref-SAIL-VOS, which are\nsignificantly better than the baselines. Code is publicly available at\nhttps://github.com/hengliusky/Few_shot_RVOS.",
        "authors": [
            "Guanghui Li",
            "Mingqi Gao",
            "Heng Liu",
            "Xiantong Zhen",
            "Feng Zheng"
        ]
    },
    {
        "title": "Human Part-wise 3D Motion Context Learning for Sign Language Recognition",
        "url": "http://arxiv.org/abs/2308.09305",
        "abstract": "In this paper, we propose P3D, the human part-wise motion context learning\nframework for sign language recognition. Our main contributions lie in two\ndimensions: learning the part-wise motion context and employing the pose\nensemble to utilize 2D and 3D pose jointly. First, our empirical observation\nimplies that part-wise context encoding benefits the performance of sign\nlanguage recognition. While previous methods of sign language recognition\nlearned motion context from the sequence of the entire pose, we argue that such\nmethods cannot exploit part-specific motion context. In order to utilize\npart-wise motion context, we propose the alternating combination of a part-wise\nencoding Transformer (PET) and a whole-body encoding Transformer (WET). PET\nencodes the motion contexts from a part sequence, while WET merges them into a\nunified context. By learning part-wise motion context, our P3D achieves\nsuperior performance on WLASL compared to previous state-of-the-art methods.\nSecond, our framework is the first to ensemble 2D and 3D poses for sign\nlanguage recognition. Since the 3D pose holds rich motion context and depth\ninformation to distinguish the words, our P3D outperformed the previous\nstate-of-the-art methods employing a pose ensemble.",
        "authors": [
            "Taeryung Lee",
            "Yeonguk Oh",
            "Kyoung Mu Lee"
        ]
    },
    {
        "title": "Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction",
        "url": "http://arxiv.org/abs/2308.11025",
        "abstract": "In recent years, huge progress has been made on learning neural implicit\nrepresentations from multi-view images for 3D reconstruction. As an additional\ninput complementing coordinates, using sinusoidal functions as positional\nencodings plays a key role in revealing high frequency details with\ncoordinate-based neural networks. However, high frequency positional encodings\nmake the optimization unstable, which results in noisy reconstructions and\nartifacts in empty space. To resolve this issue in a general sense, we\nintroduce to learn neural implicit representations with quantized coordinates,\nwhich reduces the uncertainty and ambiguity in the field during optimization.\nInstead of continuous coordinates, we discretize continuous coordinates into\ndiscrete coordinates using nearest interpolation among quantized coordinates\nwhich are obtained by discretizing the field in an extremely high resolution.\nWe use discrete coordinates and their positional encodings to learn implicit\nfunctions through volume rendering. This significantly reduces the variations\nin the sample space, and triggers more multi-view consistency constraints on\nintersections of rays from different views, which enables to infer implicit\nfunction in a more effective way. Our quantized coordinates do not bring any\ncomputational burden, and can seamlessly work upon the latest methods. Our\nevaluations under the widely used benchmarks show our superiority over the\nstate-of-the-art. Our code is available at\nhttps://github.com/MachinePerceptionLab/CQ-NIR.",
        "authors": [
            "Sijia Jiang",
            "Jing Hua",
            "Zhizhong Han"
        ]
    },
    {
        "title": "MAS: Towards Resource-Efficient Federated Multiple-Task Learning",
        "url": "http://arxiv.org/abs/2307.11285",
        "abstract": "Federated learning (FL) is an emerging distributed machine learning method\nthat empowers in-situ model training on decentralized edge devices. However,\nmultiple simultaneous FL tasks could overload resource-constrained devices. In\nthis work, we propose the first FL system to effectively coordinate and train\nmultiple simultaneous FL tasks. We first formalize the problem of training\nsimultaneous FL tasks. Then, we present our new approach, MAS (Merge and\nSplit), to optimize the performance of training multiple simultaneous FL tasks.\nMAS starts by merging FL tasks into an all-in-one FL task with a multi-task\narchitecture. After training for a few rounds, MAS splits the all-in-one FL\ntask into two or more FL tasks by using the affinities among tasks measured\nduring the all-in-one training. It then continues training each split of FL\ntasks based on model parameters from the all-in-one training. Extensive\nexperiments demonstrate that MAS outperforms other methods while reducing\ntraining time by 2x and reducing energy consumption by 40%. We hope this work\nwill inspire the community to further study and optimize training simultaneous\nFL tasks.",
        "authors": [
            "Weiming Zhuang",
            "Yonggang Wen",
            "Lingjuan Lyu",
            "Shuai Zhang"
        ]
    },
    {
        "title": "Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection",
        "url": "http://arxiv.org/abs/2308.14286",
        "abstract": "Knowledge distillation (KD) has shown potential for learning compact models\nin dense object detection. However, the commonly used softmax-based\ndistillation ignores the absolute classification scores for individual\ncategories. Thus, the optimum of the distillation loss does not necessarily\nlead to the optimal student classification scores for dense object detectors.\nThis cross-task protocol inconsistency is critical, especially for dense object\ndetectors, since the foreground categories are extremely imbalanced. To address\nthe issue of protocol differences between distillation and classification, we\npropose a novel distillation method with cross-task consistent protocols,\ntailored for the dense object detection. For classification distillation, we\naddress the cross-task protocol inconsistency problem by formulating the\nclassification logit maps in both teacher and student models as multiple\nbinary-classification maps and applying a binary-classification distillation\nloss to each map. For localization distillation, we design an IoU-based\nLocalization Distillation Loss that is free from specific network structures\nand can be compared with existing localization distillation losses. Our\nproposed method is simple but effective, and experimental results demonstrate\nits superiority over existing methods. Code is available at\nhttps://github.com/TinyTigerPan/BCKD.",
        "authors": [
            "Longrong Yang",
            "Xianpan Zhou",
            "Xuewei Li",
            "Liang Qiao",
            "Zheyang Li",
            "Ziwei Yang",
            "Gaoang Wang",
            "Xi Li"
        ]
    },
    {
        "title": "HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training",
        "url": "http://arxiv.org/abs/2212.14546",
        "abstract": "Video-language pre-training has advanced the performance of various\ndownstream video-language tasks. However, most previous methods directly\ninherit or adapt typical image-language pre-training paradigms to\nvideo-language pre-training, thus not fully exploiting the unique\ncharacteristic of video, i.e., temporal. In this paper, we propose a\nHierarchical Temporal-Aware video-language pre-training framework, HiTeA, with\ntwo novel pre-training tasks for modeling cross-modal alignment between moments\nand texts as well as the temporal relations of video-text pairs. Specifically,\nwe propose a cross-modal moment exploration task to explore moments in videos,\nwhich results in detailed video moment representation. Besides, the inherent\ntemporal relations are captured by aligning video-text pairs as a whole in\ndifferent time resolutions with multi-modal temporal relation exploration task.\nFurthermore, we introduce the shuffling test to evaluate the temporal reliance\nof datasets and video-language pre-training models. We achieve state-of-the-art\nresults on 15 well-established video-language understanding and generation\ntasks, especially on temporal-oriented datasets (e.g., SSv2-Template and\nSSv2-Label) with 8.6% and 11.1% improvement respectively. HiTeA also\ndemonstrates strong generalization ability when directly transferred to\ndownstream tasks in a zero-shot manner. Models and demo will be available on\nModelScope.",
        "authors": [
            "Qinghao Ye",
            "Guohai Xu",
            "Ming Yan",
            "Haiyang Xu",
            "Qi Qian",
            "Ji Zhang",
            "Fei Huang"
        ]
    },
    {
        "title": "Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models",
        "url": "http://arxiv.org/abs/2307.14061",
        "abstract": "Vision-language pre-training (VLP) models have shown vulnerability to\nadversarial examples in multimodal tasks. Furthermore, malicious adversaries\ncan be deliberately transferred to attack other black-box models. However,\nexisting work has mainly focused on investigating white-box attacks. In this\npaper, we present the first study to investigate the adversarial\ntransferability of recent VLP models. We observe that existing methods exhibit\nmuch lower transferability, compared to the strong attack performance in\nwhite-box settings. The transferability degradation is partly caused by the\nunder-utilization of cross-modal interactions. Particularly, unlike unimodal\nlearning, VLP models rely heavily on cross-modal interactions and the\nmultimodal alignments are many-to-many, e.g., an image can be described in\nvarious natural languages. To this end, we propose a highly transferable\nSet-level Guidance Attack (SGA) that thoroughly leverages modality interactions\nand incorporates alignment-preserving augmentation with cross-modal guidance.\nExperimental results demonstrate that SGA could generate adversarial examples\nthat can strongly transfer across different VLP models on multiple downstream\nvision-language tasks. On image-text retrieval, SGA significantly enhances the\nattack success rate for transfer attacks from ALBEF to TCL by a large margin\n(at least 9.78% and up to 30.21%), compared to the state-of-the-art.",
        "authors": [
            "Dong Lu",
            "Zhiqiang Wang",
            "Teng Wang",
            "Weili Guan",
            "Hongchang Gao",
            "Feng Zheng"
        ]
    },
    {
        "title": "AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration",
        "url": "http://arxiv.org/abs/2309.11170",
        "abstract": "In the current deep learning paradigm, the amount and quality of training\ndata are as critical as the network architecture and its training details.\nHowever, collecting, processing, and annotating real data at scale is\ndifficult, expensive, and time-consuming, particularly for tasks such as 3D\nobject registration. While synthetic datasets can be created, they require\nexpertise to design and include a limited number of categories. In this paper,\nwe introduce a new approach called AutoSynth, which automatically generates 3D\ntraining data for point cloud registration. Specifically, AutoSynth\nautomatically curates an optimal dataset by exploring a search space\nencompassing millions of potential datasets with diverse 3D shapes at a low\ncost.To achieve this, we generate synthetic 3D datasets by assembling shape\nprimitives, and develop a meta-learning strategy to search for the best\ntraining data for 3D registration on real point clouds. For this search to\nremain tractable, we replace the point cloud registration network with a much\nsmaller surrogate network, leading to a $4056.43$ times speedup. We demonstrate\nthe generality of our approach by implementing it with two different point\ncloud registration networks, BPNet and IDAM. Our results on TUD-L, LINEMOD and\nOccluded-LINEMOD evidence that a neural network trained on our searched dataset\nyields consistently better performance than the same one trained on the widely\nused ModelNet40 dataset.",
        "authors": [
            "Zheng Dang",
            "Mathieu Salzmann"
        ]
    },
    {
        "title": "Multimodal Distillation for Egocentric Action Recognition",
        "url": "http://arxiv.org/abs/2307.07483",
        "abstract": "The focal point of egocentric video understanding is modelling hand-object\ninteractions. Standard models, e.g. CNNs or Vision Transformers, which receive\nRGB frames as input perform well. However, their performance improves further\nby employing additional input modalities that provide complementary cues, such\nas object detections, optical flow, audio, etc. The added complexity of the\nmodality-specific modules, on the other hand, makes these models impractical\nfor deployment. The goal of this work is to retain the performance of such a\nmultimodal approach, while using only the RGB frames as input at inference\ntime. We demonstrate that for egocentric action recognition on the\nEpic-Kitchens and the Something-Something datasets, students which are taught\nby multimodal teachers tend to be more accurate and better calibrated than\narchitecturally equivalent models trained on ground truth labels in a unimodal\nor multimodal fashion. We further adopt a principled multimodal knowledge\ndistillation framework, allowing us to deal with issues which occur when\napplying multimodal knowledge distillation in a naive manner. Lastly, we\ndemonstrate the achieved reduction in computational complexity, and show that\nour approach maintains higher performance with the reduction of the number of\ninput views. We release our code at\nhttps://github.com/gorjanradevski/multimodal-distillation.",
        "authors": [
            "Gorjan Radevski",
            "Dusan Grujicic",
            "Marie-Francine Moens",
            "Matthew Blaschko",
            "Tinne Tuytelaars"
        ]
    },
    {
        "title": "Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects",
        "url": "http://arxiv.org/abs/2308.12590",
        "abstract": "Learning 3D shape representation with dense correspondence for deformable\nobjects is a fundamental problem in computer vision. Existing approaches often\nneed additional annotations of specific semantic domain, e.g., skeleton poses\nfor human bodies or animals, which require extra annotation effort and suffer\nfrom error accumulation, and they are limited to specific domain. In this\npaper, we propose a novel self-supervised approach to learn neural implicit\nshape representation for deformable objects, which can represent shapes with a\ntemplate shape and dense correspondence in 3D. Our method does not require the\npriors of skeleton and skinning weight, and only requires a collection of\nshapes represented in signed distance fields. To handle the large deformation,\nwe constrain the learned template shape in the same latent space with the\ntraining shapes, design a new formulation of local rigid constraint that\nenforces rigid transformation in local region and addresses local reflection\nissue, and present a new hierarchical rigid constraint to reduce the ambiguity\ndue to the joint learning of template shape and correspondences. Extensive\nexperiments show that our model can represent shapes with large deformations.\nWe also show that our shape representation can support two typical\napplications, such as texture transfer and shape editing, with competitive\nperformance. The code and models are available at\nhttps://iscas3dv.github.io/deformshape",
        "authors": [
            "Baowen Zhang",
            "Jiahe Li",
            "Xiaoming Deng",
            "Yinda Zhang",
            "Cuixia Ma",
            "Hongan Wang"
        ]
    },
    {
        "title": "Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning",
        "url": "http://arxiv.org/abs/2303.09410",
        "abstract": "Naturally controllable human-scene interaction (HSI) generation has an\nimportant role in various fields, such as VR/AR content creation and\nhuman-centered AI. However, existing methods are unnatural and unintuitive in\ntheir controllability, which heavily limits their application in practice.\nTherefore, we focus on a challenging task of naturally and controllably\ngenerating realistic and diverse HSIs from textual descriptions. From human\ncognition, the ideal generative model should correctly reason about spatial\nrelationships and interactive actions. To that end, we propose Narrator, a\nnovel relationship reasoning-based generative approach using a conditional\nvariation autoencoder for naturally controllable generation given a 3D scene\nand a textual description. Also, we model global and local spatial\nrelationships in a 3D scene and a textual description respectively based on the\nscene graph, and introduce a partlevel action mechanism to represent\ninteractions as atomic body part states. In particular, benefiting from our\nrelationship reasoning, we further propose a simple yet effective multi-human\ngeneration strategy, which is the first exploration for controllable\nmulti-human scene interaction generation. Our extensive experiments and\nperceptual studies show that Narrator can controllably generate diverse\ninteractions and significantly outperform existing works. The code and dataset\nwill be available for research purposes.",
        "authors": [
            "Haibiao Xuan",
            "Xiongzheng Li",
            "Jinsong Zhang",
            "Hongwen Zhang",
            "Yebin Liu",
            "Kun Li"
        ]
    },
    {
        "title": "Vision Relation Transformer for Unbiased Scene Graph Generation",
        "url": "http://arxiv.org/abs/2308.09472",
        "abstract": "Recent years have seen a growing interest in Scene Graph Generation (SGG), a\ncomprehensive visual scene understanding task that aims to predict entity\nrelationships using a relation encoder-decoder pipeline stacked on top of an\nobject encoder-decoder backbone. Unfortunately, current SGG methods suffer from\nan information loss regarding the entities local-level cues during the relation\nencoding process. To mitigate this, we introduce the Vision rElation\nTransfOrmer (VETO), consisting of a novel local-level entity relation encoder.\nWe further observe that many existing SGG methods claim to be unbiased, but are\nstill biased towards either head or tail classes. To overcome this bias, we\nintroduce a Mutually Exclusive ExperT (MEET) learning strategy that captures\nimportant relation features without bias towards head or tail classes.\nExperimental results on the VG and GQA datasets demonstrate that VETO + MEET\nboosts the predictive performance by up to 47 percentage over the state of the\nart while being 10 times smaller.",
        "authors": [
            "Gopika Sudhakaran",
            "Devendra Singh Dhami",
            "Kristian Kersting",
            "Stefan Roth"
        ]
    },
    {
        "title": "Scaling Data Generation in Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2307.15644",
        "abstract": "Recent research in language-guided visual navigation has demonstrated a\nsignificant demand for the diversity of traversable environments and the\nquantity of supervision for training generalizable agents. To tackle the common\ndata scarcity issue in existing vision-and-language navigation datasets, we\npropose an effective paradigm for generating large-scale data for learning,\nwhich applies 1200+ photo-realistic environments from HM3D and Gibson datasets\nand synthesizes 4.9 million instruction trajectory pairs using fully-accessible\nresources on the web. Importantly, we investigate the influence of each\ncomponent in this paradigm on the agent's performance and study how to\nadequately apply the augmented data to pre-train and fine-tune an agent. Thanks\nto our large-scale dataset, the performance of an existing agent can be pushed\nup (+11% absolute with regard to previous SoTA) to a significantly new best of\n80% single-run success rate on the R2R test split by simple imitation learning.\nThe long-lasting generalization gap between navigating in seen and unseen\nenvironments is also reduced to less than 1% (versus 8% in the previous best\nmethod). Moreover, our paradigm also facilitates different models to achieve\nnew state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous\nenvironments.",
        "authors": [
            "Zun Wang",
            "Jialu Li",
            "Yicong Hong",
            "Yi Wang",
            "Qi Wu",
            "Mohit Bansal",
            "Stephen Gould",
            "Hao Tan",
            "Yu Qiao"
        ]
    },
    {
        "title": "Better May Not Be Fairer: A Study on Subgroup Discrepancy in Image Classification",
        "url": "http://arxiv.org/abs/2212.08649",
        "abstract": "In this paper, we provide 20,000 non-trivial human annotations on popular\ndatasets as a first step to bridge gap to studying how natural semantic\nspurious features affect image classification, as prior works often study\ndatasets mixing low-level features due to limitations in accessing realistic\ndatasets. We investigate how natural background colors play a role as spurious\nfeatures by annotating the test sets of CIFAR10 and CIFAR100 into subgroups\nbased on the background color of each image. We name our datasets\n\\textbf{CIFAR10-B} and \\textbf{CIFAR100-B} and integrate them with CIFAR-Cs.\n  We find that overall human-level accuracy does not guarantee consistent\nsubgroup performances, and the phenomenon remains even on models pre-trained on\nImageNet or after data augmentation (DA). To alleviate this issue, we propose\n\\textbf{FlowAug}, a \\emph{semantic} DA that leverages decoupled semantic\nrepresentations captured by a pre-trained generative flow. Experimental results\nshow that FlowAug achieves more consistent subgroup results than other types of\nDA methods on CIFAR10/100 and on CIFAR10/100-C. Additionally, it shows better\ngeneralization performance.\n  Furthermore, we propose a generic metric, \\emph{MacroStd}, for studying model\nrobustness to spurious correlations, where we take a macro average on the\nweighted standard deviations across different classes. We show\n\\textit{MacroStd} being more predictive of better performances; per our metric,\nFlowAug demonstrates improvements on subgroup discrepancy. Although this metric\nis proposed to study our curated datasets, it applies to all datasets that have\nsubgroups or subclasses. Lastly, we also show superior out-of-distribution\nresults on CIFAR10.1.",
        "authors": [
            "Ming-Chang Chiu",
            "Pin-Yu Chen",
            "Xuezhe Ma"
        ]
    },
    {
        "title": "3D Implicit Transporter for Temporally Consistent Keypoint Discovery",
        "url": "http://arxiv.org/abs/2309.05098",
        "abstract": "Keypoint-based representation has proven advantageous in various visual and\nrobotic tasks. However, the existing 2D and 3D methods for detecting keypoints\nmainly rely on geometric consistency to achieve spatial alignment, neglecting\ntemporal consistency. To address this issue, the Transporter method was\nintroduced for 2D data, which reconstructs the target frame from the source\nframe to incorporate both spatial and temporal information. However, the direct\napplication of the Transporter to 3D point clouds is infeasible due to their\nstructural differences from 2D images. Thus, we propose the first 3D version of\nthe Transporter, which leverages hybrid 3D representation, cross attention, and\nimplicit reconstruction. We apply this new learning system on 3D articulated\nobjects and nonrigid animals (humans and rodents) and show that learned\nkeypoints are spatio-temporally consistent. Additionally, we propose a\nclosed-loop control strategy that utilizes the learned keypoints for 3D object\nmanipulation and demonstrate its superior performance. Codes are available at\nhttps://github.com/zhongcl-thu/3D-Implicit-Transporter.",
        "authors": [
            "Chengliang Zhong",
            "Yuhang Zheng",
            "Yupeng Zheng",
            "Hao Zhao",
            "Li Yi",
            "Xiaodong Mu",
            "Ling Wang",
            "Pengfei Li",
            "Guyue Zhou",
            "Chao Yang",
            "Xinliang Zhang",
            "Jian Zhao"
        ]
    },
    {
        "title": "Adaptive Rotated Convolution for Rotated Object Detection",
        "url": "http://arxiv.org/abs/2303.07820",
        "abstract": "Rotated object detection aims to identify and locate objects in images with\narbitrary orientation. In this scenario, the oriented directions of objects\nvary considerably across different images, while multiple orientations of\nobjects exist within an image. This intrinsic characteristic makes it\nchallenging for standard backbone networks to extract high-quality features of\nthese arbitrarily orientated objects. In this paper, we present Adaptive\nRotated Convolution (ARC) module to handle the aforementioned challenges. In\nour ARC module, the convolution kernels rotate adaptively to extract object\nfeatures with varying orientations in different images, and an efficient\nconditional computation mechanism is introduced to accommodate the large\norientation variations of objects within an image. The two designs work\nseamlessly in rotated object detection problem. Moreover, ARC can conveniently\nserve as a plug-and-play module in various vision backbones to boost their\nrepresentation ability to detect oriented objects accurately. Experiments on\ncommonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our\nproposed ARC module in the backbone network, the performance of multiple\npopular oriented object detectors is significantly improved (\\eg +3.03\\% mAP on\nRotated RetinaNet and +4.16\\% on CFA). Combined with the highly competitive\nmethod Oriented R-CNN, the proposed approach achieves state-of-the-art\nperformance on the DOTA dataset with 81.77\\% mAP. Code is available at\n\\url{https://github.com/LeapLabTHU/ARC}.",
        "authors": [
            "Yifan Pu",
            "Yiru Wang",
            "Zhuofan Xia",
            "Yizeng Han",
            "Yulin Wang",
            "Weihao Gan",
            "Zidong Wang",
            "Shiji Song",
            "Gao Huang"
        ]
    },
    {
        "title": "Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World",
        "url": "http://arxiv.org/abs/2303.13233",
        "abstract": "Scene Graph Generation (SGG) aims to extract <subject, predicate, object>\nrelationships in images for vision understanding. Although recent works have\nmade steady progress on SGG, they still suffer long-tail distribution issues\nthat tail-predicates are more costly to train and hard to distinguish due to a\nsmall amount of annotated data compared to frequent predicates. Existing\nre-balancing strategies try to handle it via prior rules but are still confined\nto pre-defined conditions, which are not scalable for various models and\ndatasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)\nframework, where a visually-prompted language model is learned to generate\ndiverse fine-grained predicates in a low-resource way. The proposed CaCao can\nbe applied in a plug-and-play fashion and automatically strengthen existing SGG\nto tackle the long-tailed problem. Based on that, we further introduce a novel\nEntangled cross-modal prompt approach for open-world predicate scene graph\ngeneration (Epic), where models can generalize to unseen predicates in a\nzero-shot manner. Comprehensive experiments on three benchmark datasets show\nthat CaCao consistently boosts the performance of multiple scene graph\ngeneration models in a model-agnostic way. Moreover, our Epic achieves\ncompetitive performance on open-world predicate prediction. The data and code\nfor this paper are publicly available.",
        "authors": [
            "Qifan Yu",
            "Juncheng Li",
            "Yu Wu",
            "Siliang Tang",
            "Wei Ji",
            "Yueting Zhuang"
        ]
    },
    {
        "title": "UniVTG: Towards Unified Video-Language Temporal Grounding",
        "url": "http://arxiv.org/abs/2307.16715",
        "abstract": "Video Temporal Grounding (VTG), which aims to ground target clips from videos\n(such as consecutive intervals or disjoint shots) according to custom language\nqueries (e.g., sentences or words), is key for video browsing on social media.\nMost methods in this direction develop taskspecific models that are trained\nwith type-specific labels, such as moment retrieval (time interval) and\nhighlight detection (worthiness curve), which limits their abilities to\ngeneralize to various VTG tasks and labels. In this paper, we propose to Unify\nthe diverse VTG labels and tasks, dubbed UniVTG, along three directions:\nFirstly, we revisit a wide range of VTG labels and tasks and define a unified\nformulation. Based on this, we develop data annotation schemes to create\nscalable pseudo supervision. Secondly, we develop an effective and flexible\ngrounding model capable of addressing each task and making full use of each\nlabel. Lastly, thanks to the unified framework, we are able to unlock temporal\ngrounding pretraining from large-scale diverse labels and develop stronger\ngrounding abilities e.g., zero-shot grounding. Extensive experiments on three\ntasks (moment retrieval, highlight detection and video summarization) across\nseven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights,\nTVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposed\nframework. The codes are available at https://github.com/showlab/UniVTG.",
        "authors": [
            "Kevin Qinghong Lin",
            "Pengchuan Zhang",
            "Joya Chen",
            "Shraman Pramanick",
            "Difei Gao",
            "Alex Jinpeng Wang",
            "Rui Yan",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "Disposable Transfer Learning for Selective Source Task Unlearning",
        "url": "http://arxiv.org/abs/2308.09971",
        "abstract": "Transfer learning is widely used for training deep neural networks (DNN) for\nbuilding a powerful representation. Even after the pre-trained model is adapted\nfor the target task, the representation performance of the feature extractor is\nretained to some extent. As the performance of the pre-trained model can be\nconsidered the private property of the owner, it is natural to seek the\nexclusive right of the generalized performance of the pre-trained weight. To\naddress this issue, we suggest a new paradigm of transfer learning called\ndisposable transfer learning (DTL), which disposes of only the source task\nwithout degrading the performance of the target task. To achieve knowledge\ndisposal, we propose a novel loss named Gradient Collision loss (GC loss). GC\nloss selectively unlearns the source knowledge by leading the gradient vectors\nof mini-batches in different directions. Whether the model successfully\nunlearns the source task is measured by piggyback learning accuracy (PL\naccuracy). PL accuracy estimates the vulnerability of knowledge leakage by\nretraining the scrubbed model on a subset of source data or new downstream\ndata. We demonstrate that GC loss is an effective approach to the DTL problem\nby showing that the model trained with GC loss retains the performance on the\ntarget task with a significantly reduced PL accuracy.",
        "authors": [
            "Seunghee Koh",
            "Hyounguk Shon",
            "Janghyeon Lee",
            "Hyeong Gwon Hong",
            "Junmo Kim"
        ]
    },
    {
        "title": "Grounding 3D Object Affordance from 2D Interactions in Images",
        "url": "http://arxiv.org/abs/2303.10437",
        "abstract": "Grounding 3D object affordance seeks to locate objects' ''action\npossibilities'' regions in the 3D space, which serves as a link between\nperception and operation for embodied agents. Existing studies primarily focus\non connecting visual affordances with geometry structures, e.g. relying on\nannotations to declare interactive regions of interest on the object and\nestablishing a mapping between the regions and affordances. However, the\nessence of learning object affordance is to understand how to use it, and the\nmanner that detaches interactions is limited in generalization. Normally,\nhumans possess the ability to perceive object affordances in the physical world\nthrough demonstration images or videos. Motivated by this, we introduce a novel\ntask setting: grounding 3D object affordance from 2D interactions in images,\nwhich faces the challenge of anticipating affordance through interactions of\ndifferent sources. To address this problem, we devise a novel\nInteraction-driven 3D Affordance Grounding Network (IAG), which aligns the\nregion feature of objects from different sources and models the interactive\ncontexts for 3D object affordance grounding. Besides, we collect a Point-Image\nAffordance Dataset (PIAD) to support the proposed task. Comprehensive\nexperiments on PIAD demonstrate the reliability of the proposed task and the\nsuperiority of our method. The project is available at\nhttps://github.com/yyvhang/IAGNet.",
        "authors": [
            "Yuhang Yang",
            "Wei Zhai",
            "Hongchen Luo",
            "Yang Cao",
            "Jiebo Luo",
            "Zheng-Jun Zha"
        ]
    },
    {
        "title": "Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos",
        "url": "http://arxiv.org/abs/2308.09245",
        "abstract": "Recently, the community has made tremendous progress in developing effective\nmethods for point cloud video understanding that learn from massive amounts of\nlabeled data. However, annotating point cloud videos is usually notoriously\nexpensive. Moreover, training via one or only a few traditional tasks (e.g.,\nclassification) may be insufficient to learn subtle details of the\nspatio-temporal structure existing in point cloud videos. In this paper, we\npropose a Masked Spatio-Temporal Structure Prediction (MaST-Pre) method to\ncapture the structure of point cloud videos without human annotations. MaST-Pre\nis based on spatio-temporal point-tube masking and consists of two\nself-supervised learning tasks. First, by reconstructing masked point tubes,\nour method is able to capture the appearance information of point cloud videos.\nSecond, to learn motion, we propose a temporal cardinality difference\nprediction task that estimates the change in the number of points within a\npoint tube. In this way, MaST-Pre is forced to model the spatial and temporal\nstructure in point cloud videos. Extensive experiments on MSRAction-3D,\nNTU-RGBD, NvGesture, and SHREC'17 demonstrate the effectiveness of the proposed\nmethod.",
        "authors": [
            "Zhiqiang Shen",
            "Xiaoxiao Sheng",
            "Hehe Fan",
            "Longguang Wang",
            "Yulan Guo",
            "Qiong Liu",
            "Hao Wen",
            "Xi Zhou"
        ]
    },
    {
        "title": "Hybrid Spectral Denoising Transformer with Guided Attention",
        "url": "http://arxiv.org/abs/2303.09040",
        "abstract": "In this paper, we present a Hybrid Spectral Denoising Transformer (HSDT) for\nhyperspectral image denoising. Challenges in adapting transformer for HSI arise\nfrom the capabilities to tackle existing limitations of CNN-based methods in\ncapturing the global and local spatial-spectral correlations while maintaining\nefficiency and flexibility. To address these issues, we introduce a hybrid\napproach that combines the advantages of both models with a Spatial-Spectral\nSeparable Convolution (S3Conv), Guided Spectral Self-Attention (GSSA), and\nSelf-Modulated Feed-Forward Network (SM-FFN). Our S3Conv works as a lightweight\nalternative to 3D convolution, which extracts more spatial-spectral correlated\nfeatures while keeping the flexibility to tackle HSIs with an arbitrary number\nof bands. These features are then adaptively processed by GSSA which per-forms\n3D self-attention across the spectral bands, guided by a set of learnable\nqueries that encode the spectral signatures. This not only enriches our model\nwith powerful capabilities for identifying global spectral correlations but\nalso maintains linear complexity. Moreover, our SM-FFN proposes the\nself-modulation that intensifies the activations of more informative regions,\nwhich further strengthens the aggregated features. Extensive experiments are\nconducted on various datasets under both simulated and real-world noise, and it\nshows that our HSDT significantly outperforms the existing state-of-the-art\nmethods while maintaining low computational overhead. Code is at https:\n//github.com/Zeqiang-Lai/HSDT.",
        "authors": [
            "Zeqiang Lai",
            "Chenggang Yan",
            "Ying Fu"
        ]
    },
    {
        "title": "PhysDiff: Physics-Guided Human Motion Diffusion Model",
        "url": "http://arxiv.org/abs/2212.02500",
        "abstract": "Denoising diffusion models hold great promise for generating diverse and\nrealistic human motions. However, existing motion diffusion models largely\ndisregard the laws of physics in the diffusion process and often generate\nphysically-implausible motions with pronounced artifacts such as floating, foot\nsliding, and ground penetration. This seriously impacts the quality of\ngenerated motions and limits their real-world application. To address this\nissue, we present a novel physics-guided motion diffusion model (PhysDiff),\nwhich incorporates physical constraints into the diffusion process.\nSpecifically, we propose a physics-based motion projection module that uses\nmotion imitation in a physics simulator to project the denoised motion of a\ndiffusion step to a physically-plausible motion. The projected motion is\nfurther used in the next diffusion step to guide the denoising diffusion\nprocess. Intuitively, the use of physics in our model iteratively pulls the\nmotion toward a physically-plausible space, which cannot be achieved by simple\npost-processing. Experiments on large-scale human motion datasets show that our\napproach achieves state-of-the-art motion quality and improves physical\nplausibility drastically (>78% for all datasets).",
        "authors": [
            "Ye Yuan",
            "Jiaming Song",
            "Umar Iqbal",
            "Arash Vahdat",
            "Jan Kautz"
        ]
    },
    {
        "title": "Masked Motion Predictors are Strong 3D Action Representation Learners",
        "url": "http://arxiv.org/abs/2308.07092",
        "abstract": "In 3D human action recognition, limited supervised data makes it challenging\nto fully tap into the modeling potential of powerful networks such as\ntransformers. As a result, researchers have been actively investigating\neffective self-supervised pre-training strategies. In this work, we show that\ninstead of following the prevalent pretext task to perform masked\nself-component reconstruction in human joints, explicit contextual motion\nmodeling is key to the success of learning effective feature representation for\n3D action recognition. Formally, we propose the Masked Motion Prediction (MAMP)\nframework. To be specific, the proposed MAMP takes as input the masked\nspatio-temporal skeleton sequence and predicts the corresponding temporal\nmotion of the masked human joints. Considering the high temporal redundancy of\nthe skeleton sequence, in our MAMP, the motion information also acts as an\nempirical semantic richness prior that guide the masking process, promoting\nbetter attention to semantically rich temporal regions. Extensive experiments\non NTU-60, NTU-120, and PKU-MMD datasets show that the proposed MAMP\npre-training substantially improves the performance of the adopted vanilla\ntransformer, achieving state-of-the-art results without bells and whistles. The\nsource code of our MAMP is available at https://github.com/maoyunyao/MAMP.",
        "authors": [
            "Yunyao Mao",
            "Jiajun Deng",
            "Wengang Zhou",
            "Yao Fang",
            "Wanli Ouyang",
            "Houqiang Li"
        ]
    },
    {
        "title": "SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications",
        "url": "http://arxiv.org/abs/2303.15446",
        "abstract": "Self-attention has become a defacto choice for capturing global context in\nvarious vision applications. However, its quadratic computational complexity\nwith respect to image resolution limits its use in real-time applications,\nespecially for deployment on resource-constrained mobile devices. Although\nhybrid approaches have been proposed to combine the advantages of convolutions\nand self-attention for a better speed-accuracy trade-off, the expensive matrix\nmultiplication operations in self-attention remain a bottleneck. In this work,\nwe introduce a novel efficient additive attention mechanism that effectively\nreplaces the quadratic matrix multiplication operations with linear\nelement-wise multiplications. Our design shows that the key-value interaction\ncan be replaced with a linear layer without sacrificing any accuracy. Unlike\nprevious state-of-the-art methods, our efficient formulation of self-attention\nenables its usage at all stages of the network. Using our proposed efficient\nadditive attention, we build a series of models called \"SwiftFormer\" which\nachieves state-of-the-art performance in terms of both accuracy and mobile\ninference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy\nwith only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster\ncompared to MobileViT-v2. Code: https://github.com/Amshaker/SwiftFormer",
        "authors": [
            "Abdelrahman Shaker",
            "Muhammad Maaz",
            "Hanoona Rasheed",
            "Salman Khan",
            "Ming-Hsuan Yang",
            "Fahad Shahbaz Khan"
        ]
    },
    {
        "title": "UpCycling: Semi-supervised 3D Object Detection without Sharing Raw-level Unlabeled Scenes",
        "url": "http://arxiv.org/abs/2211.11950",
        "abstract": "Semi-supervised Learning (SSL) has received increasing attention in\nautonomous driving to reduce the enormous burden of 3D annotation. In this\npaper, we propose UpCycling, a novel SSL framework for 3D object detection with\nzero additional raw-level point cloud: learning from unlabeled de-identified\nintermediate features (i.e., smashed data) to preserve privacy. Since these\nintermediate features are naturally produced by the inference pipeline, no\nadditional computation is required on autonomous vehicles. However, generating\neffective consistency loss for unlabeled feature-level scene turns out to be a\ncritical challenge. The latest SSL frameworks for 3D object detection that\nenforce consistency regularization between different augmentations of an\nunlabeled raw-point scene become detrimental when applied to intermediate\nfeatures. To solve the problem, we introduce a novel combination of hybrid\npseudo labels and feature-level Ground Truth sampling (F-GT), which safely\naugments unlabeled multi-type 3D scene features and provides high-quality\nsupervision. We implement UpCycling on two representative 3D object detection\nmodels: SECOND-IoU and PV-RCNN. Experiments on widely-used datasets (Waymo,\nKITTI, and Lyft) verify that UpCycling outperforms other augmentation methods\napplied at the feature level. In addition, while preserving privacy, UpCycling\nperforms better or comparably to the state-of-the-art methods that utilize\nraw-level unlabeled data in both domain adaptation and partial-label scenarios.",
        "authors": [
            "Sunwook Hwang",
            "Youngseok Kim",
            "Seongwon Kim",
            "Saewoong Bahk",
            "Hyung-Sin Kim"
        ]
    },
    {
        "title": "RIGID: Recurrent GAN Inversion and Editing of Real Face Videos",
        "url": "http://arxiv.org/abs/2308.06097",
        "abstract": "GAN inversion is indispensable for applying the powerful editability of GAN\nto real images. However, existing methods invert video frames individually\noften leading to undesired inconsistent results over time. In this paper, we\npropose a unified recurrent framework, named \\textbf{R}ecurrent v\\textbf{I}deo\n\\textbf{G}AN \\textbf{I}nversion and e\\textbf{D}iting (RIGID), to explicitly and\nsimultaneously enforce temporally coherent GAN inversion and facial editing of\nreal videos. Our approach models the temporal relations between current and\nprevious frames from three aspects. To enable a faithful real video\nreconstruction, we first maximize the inversion fidelity and consistency by\nlearning a temporal compensated latent code. Second, we observe incoherent\nnoises lie in the high-frequency domain that can be disentangled from the\nlatent space. Third, to remove the inconsistency after attribute manipulation,\nwe propose an \\textit{in-between frame composition constraint} such that the\narbitrary frame must be a direct composite of its neighboring frames. Our\nunified framework learns the inherent coherence between input frames in an\nend-to-end manner, and therefore it is agnostic to a specific attribute and can\nbe applied to arbitrary editing of the same video without re-training.\nExtensive experiments demonstrate that RIGID outperforms state-of-the-art\nmethods qualitatively and quantitatively in both inversion and editing tasks.\nThe deliverables can be found in \\url{https://cnnlstm.github.io/RIGID}",
        "authors": [
            "Yangyang Xu",
            "Shengfeng He",
            "Kwan-Yee K. Wong",
            "Ping Luo"
        ]
    },
    {
        "title": "PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring",
        "url": "http://arxiv.org/abs/2307.11299",
        "abstract": "Liquid perception is critical for robotic pouring tasks. It usually requires\nthe robust visual detection of flowing liquid. However, while recent works have\nshown promising results in liquid perception, they typically require labeled\ndata for model training, a process that is both time-consuming and reliant on\nhuman labor. To this end, this paper proposes a simple yet effective framework\nPourIt!, to serve as a tool for robotic pouring tasks. We design a simple data\ncollection pipeline that only needs image-level labels to reduce the reliance\non tedious pixel-wise annotations. Then, a binary classification model is\ntrained to generate Class Activation Map (CAM) that focuses on the visual\ndifference between these two kinds of collected data, i.e., the existence of\nliquid drop or not. We also devise a feature contrast strategy to improve the\nquality of the CAM, thus entirely and tightly covering the actual liquid\nregions. Then, the container pose is further utilized to facilitate the 3D\npoint cloud recovery of the detected liquid region. Finally, the\nliquid-to-container distance is calculated for visual closed-loop control of\nthe physical robot. To validate the effectiveness of our proposed method, we\nalso contribute a novel dataset for our task and name it PourIt! dataset.\nExtensive results on this dataset and physical Franka robot have shown the\nutility and effectiveness of our method in the robotic pouring tasks. Our\ndataset, code and pre-trained models will be available on the project page.",
        "authors": [
            "Haitao Lin",
            "Yanwei Fu",
            "Xiangyang Xue"
        ]
    },
    {
        "title": "Robust Mixture-of-Expert Training for Convolutional Neural Networks",
        "url": "http://arxiv.org/abs/2308.10110",
        "abstract": "Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture,\nhas demonstrated a great promise to enable high-accuracy and ultra-efficient\nmodel inference. Despite the growing popularity of MoE, little work\ninvestigated its potential to advance convolutional neural networks (CNNs),\nespecially in the plane of adversarial robustness. Since the lack of robustness\nhas become one of the main hurdles for CNNs, in this paper we ask: How to\nadversarially robustify a CNN-based MoE model? Can we robustly train it like an\nordinary CNN model? Our pilot study shows that the conventional adversarial\ntraining (AT) mechanism (developed for vanilla CNNs) no longer remains\neffective to robustify an MoE-CNN. To better understand this phenomenon, we\ndissect the robustness of an MoE-CNN into two dimensions: Robustness of routers\n(i.e., gating functions to select data-specific experts) and robustness of\nexperts (i.e., the router-guided pathways defined by the subnetworks of the\nbackbone CNN). Our analyses show that routers and experts are hard to adapt to\neach other in the vanilla AT. Thus, we propose a new router-expert alternating\nAdversarial training framework for MoE, termed AdvMoE. The effectiveness of our\nproposal is justified across 4 commonly-used CNN model architectures over 4\nbenchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustness\nimprovement over the original dense CNN, and enjoys the efficiency merit of\nsparsity-gated MoE, leading to more than 50% inference cost reduction. Codes\nare available at https://github.com/OPTML-Group/Robust-MoE-CNN.",
        "authors": [
            "Yihua Zhang",
            "Ruisi Cai",
            "Tianlong Chen",
            "Guanhua Zhang",
            "Huan Zhang",
            "Pin-Yu Chen",
            "Shiyu Chang",
            "Zhangyang Wang",
            "Sijia Liu"
        ]
    },
    {
        "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control",
        "url": "http://arxiv.org/abs/2303.17606",
        "abstract": "Neural implicit fields are powerful for representing 3D scenes and generating\nhigh-quality novel views, but it remains challenging to use such implicit\nrepresentations for creating a 3D human avatar with a specific identity and\nartistic style that can be easily animated. Our proposed method, AvatarCraft,\naddresses this challenge by using diffusion models to guide the learning of\ngeometry and texture for a neural avatar based on a single text prompt. We\ncarefully design the optimization framework of neural implicit fields,\nincluding a coarse-to-fine multi-bounding box training strategy, shape\nregularization, and diffusion-based constraints, to produce high-quality\ngeometry and texture. Additionally, we make the human avatar animatable by\ndeforming the neural implicit field with an explicit warping field that maps\nthe target human mesh to a template human mesh, both represented using\nparametric human models. This simplifies animation and reshaping of the\ngenerated avatar by controlling pose and shape parameters. Extensive\nexperiments on various text descriptions show that AvatarCraft is effective and\nrobust in creating human avatars and rendering novel views, poses, and shapes.\nOur project page is: https://avatar-craft.github.io/.",
        "authors": [
            "Ruixiang Jiang",
            "Can Wang",
            "Jingbo Zhang",
            "Menglei Chai",
            "Mingming He",
            "Dongdong Chen",
            "Jing Liao"
        ]
    },
    {
        "title": "Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?",
        "url": "http://arxiv.org/abs/2307.11978",
        "abstract": "Vision-language models such as CLIP learn a generic text-image embedding from\nlarge-scale training data. A vision-language model can be adapted to a new\nclassification task through few-shot prompt tuning. We find that such a prompt\ntuning process is highly robust to label noises. This intrigues us to study the\nkey reasons contributing to the robustness of the prompt tuning paradigm. We\nconducted extensive experiments to explore this property and find the key\nfactors are: 1) the fixed classname tokens provide a strong regularization to\nthe optimization of the model, reducing gradients induced by the noisy samples;\n2) the powerful pre-trained image-text embedding that is learned from diverse\nand generic web data provides strong prior knowledge for image classification.\nFurther, we demonstrate that noisy zero-shot predictions from CLIP can be used\nto tune its own prompt, significantly enhancing prediction accuracy in the\nunsupervised setting. The code is available at https://github.com/CEWu/PTNL.",
        "authors": [
            "Cheng-En Wu",
            "Yu Tian",
            "Haichao Yu",
            "Heng Wang",
            "Pedro Morgado",
            "Yu Hen Hu",
            "Linjie Yang"
        ]
    },
    {
        "title": "Unified Pre-Training with Pseudo Texts for Text-To-Image Person Re-Identification",
        "url": "http://arxiv.org/abs/2309.01420",
        "abstract": "The pre-training task is indispensable for the text-to-image person\nre-identification (T2I-ReID) task. However, there are two underlying\ninconsistencies between these two tasks that may impact the performance; i)\nData inconsistency. A large domain gap exists between the generic images/texts\nused in public pre-trained models and the specific person data in the T2I-ReID\ntask. This gap is especially severe for texts, as general textual data are\nusually unable to describe specific people in fine-grained detail. ii) Training\ninconsistency. The processes of pre-training of images and texts are\nindependent, despite cross-modality learning being critical to T2I-ReID. To\naddress the above issues, we present a new unified pre-training pipeline\n(UniPT) designed specifically for the T2I-ReID task. We first build a\nlarge-scale text-labeled person dataset \"LUPerson-T\", in which pseudo-textual\ndescriptions of images are automatically generated by the CLIP paradigm using a\ndivide-conquer-combine strategy. Benefiting from this dataset, we then utilize\na simple vision-and-language pre-training framework to explicitly align the\nfeature space of the image and text modalities during pre-training. In this\nway, the pre-training task and the T2I-ReID task are made consistent with each\nother on both data and training levels. Without the need for any bells and\nwhistles, our UniPT achieves competitive Rank-1 accuracy of, ie, 68.50%,\n60.09%, and 51.85% on CUHK-PEDES, ICFG-PEDES and RSTPReid, respectively. Both\nthe LUPerson-T dataset and code are available at\nhttps;//github.com/ZhiyinShao-H/UniPT.",
        "authors": [
            "Zhiyin Shao",
            "Xinyu Zhang",
            "Changxing Ding",
            "Jian Wang",
            "Jingdong Wang"
        ]
    },
    {
        "title": "Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos",
        "url": "http://arxiv.org/abs/2308.09951",
        "abstract": "Self-supervised methods have shown remarkable progress in learning high-level\nsemantics and low-level temporal correspondence. Building on these results, we\ntake one step further and explore the possibility of integrating these two\nfeatures to enhance object-centric representations. Our preliminary experiments\nindicate that query slot attention can extract different semantic components\nfrom the RGB feature map, while random sampling based slot attention can\nexploit temporal correspondence cues between frames to assist instance\nidentification. Motivated by this, we propose a novel semantic-aware masked\nslot attention on top of the fused semantic features and correspondence maps.\nIt comprises two slot attention stages with a set of shared learnable Gaussian\ndistributions. In the first stage, we use the mean vectors as slot\ninitialization to decompose potential semantics and generate semantic\nsegmentation masks through iterative attention. In the second stage, for each\nsemantics, we randomly sample slots from the corresponding Gaussian\ndistribution and perform masked feature aggregation within the semantic area to\nexploit temporal correspondence patterns for instance identification. We adopt\nsemantic- and instance-level temporal consistency as self-supervision to\nencourage temporally coherent object-centric representations. Our model\neffectively identifies multiple object instances with semantic structure,\nreaching promising results on unsupervised video object discovery. Furthermore,\nwe achieve state-of-the-art performance on dense label propagation tasks,\ndemonstrating the potential for object-centric analysis. The code is released\nat https://github.com/shvdiwnkozbw/SMTC.",
        "authors": [
            "Rui Qian",
            "Shuangrui Ding",
            "Xian Liu",
            "Dahua Lin"
        ]
    },
    {
        "title": "First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2303.13199",
        "abstract": "In Class-Incremental Learning (CIL) an image classification system is exposed\nto new classes in each learning session and must be updated incrementally.\nMethods approaching this problem have updated both the classification head and\nthe feature extractor body at each session of CIL. In this work, we develop a\nbaseline method, First Session Adaptation (FSA), that sheds light on the\nefficacy of existing CIL approaches and allows us to assess the relative\nperformance contributions from head and body adaption. FSA adapts a pre-trained\nneural network body only on the first learning session and fixes it thereafter;\na head based on linear discriminant analysis (LDA), is then placed on top of\nthe adapted body, allowing exact updates through CIL. FSA is replay-free\ni.e.~it does not memorize examples from previous sessions of continual\nlearning. To empirically motivate FSA, we first consider a diverse selection of\n22 image-classification datasets, evaluating different heads and body\nadaptation techniques in high/low-shot offline settings. We find that the LDA\nhead performs well and supports CIL out-of-the-box. We also find that\nFeaturewise Layer Modulation (FiLM) adapters are highly effective in the\nfew-shot setting, and full-body adaption in the high-shot setting. Second, we\nempirically investigate various CIL settings including high-shot CIL and\nfew-shot CIL, including settings that have previously been used in the\nliterature. We show that FSA significantly improves over the state-of-the-art\nin 15 of the 16 settings considered. FSA with FiLM adapters is especially\nperformant in the few-shot setting. These results indicate that current\napproaches to continuous body adaptation are not working as expected. Finally,\nwe propose a measure that can be applied to a set of unlabelled inputs which is\npredictive of the benefits of body adaptation.",
        "authors": [
            "Aristeidis Panos",
            "Yuriko Kobe",
            "Daniel Olmeda Reino",
            "Rahaf Aljundi",
            "Richard E. Turner"
        ]
    },
    {
        "title": "Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection",
        "url": "http://arxiv.org/abs/2307.08209",
        "abstract": "Voxel-based methods have achieved state-of-the-art performance for 3D object\ndetection in autonomous driving. However, their significant computational and\nmemory costs pose a challenge for their application to resource-constrained\nvehicles. One reason for this high resource consumption is the presence of a\nlarge number of redundant background points in Lidar point clouds, resulting in\nspatial redundancy in both 3D voxel and dense BEV map representations. To\naddress this issue, we propose an adaptive inference framework called Ada3D,\nwhich focuses on exploiting the input-level spatial redundancy. Ada3D\nadaptively filters the redundant input, guided by a lightweight importance\npredictor and the unique properties of the Lidar point cloud. Additionally, we\nutilize the BEV features' intrinsic sparsity by introducing the Sparsity\nPreserving Batch Normalization. With Ada3D, we achieve 40% reduction for 3D\nvoxels and decrease the density of 2D BEV feature maps from 100% to 20% without\nsacrificing accuracy. Ada3D reduces the model computational and memory cost by\n5x, and achieves 1.52x/1.45x end-to-end GPU latency and 1.5x/4.5x GPU peak\nmemory optimization for the 3D and 2D backbone respectively.",
        "authors": [
            "Tianchen Zhao",
            "Xuefei Ning",
            "Ke Hong",
            "Zhongyuan Qiu",
            "Pu Lu",
            "Yali Zhao",
            "Linfeng Zhang",
            "Lipu Zhou",
            "Guohao Dai",
            "Huazhong Yang",
            "Yu Wang"
        ]
    },
    {
        "title": "R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras",
        "url": "http://arxiv.org/abs/2308.14713",
        "abstract": "Dense 3D reconstruction and ego-motion estimation are key challenges in\nautonomous driving and robotics. Compared to the complex, multi-modal systems\ndeployed today, multi-camera systems provide a simpler, low-cost alternative.\nHowever, camera-based 3D reconstruction of complex dynamic scenes has proven\nextremely difficult, as existing solutions often produce incomplete or\nincoherent results. We propose R3D3, a multi-camera system for dense 3D\nreconstruction and ego-motion estimation. Our approach iterates between\ngeometric estimation that exploits spatial-temporal information from multiple\ncameras, and monocular depth refinement. We integrate multi-camera feature\ncorrelation and dense bundle adjustment operators that yield robust geometric\ndepth and pose estimates. To improve reconstruction where geometric depth is\nunreliable, e.g. for moving objects or low-textured regions, we introduce\nlearnable scene priors via a depth refinement network. We show that this design\nenables a dense, consistent 3D reconstruction of challenging, dynamic outdoor\nenvironments. Consequently, we achieve state-of-the-art dense depth prediction\non the DDAD and NuScenes benchmarks.",
        "authors": [
            "Aron Schmied",
            "Tobias Fischer",
            "Martin Danelljan",
            "Marc Pollefeys",
            "Fisher Yu"
        ]
    },
    {
        "title": "Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos",
        "url": "http://arxiv.org/abs/2308.09247",
        "abstract": "We propose a unified point cloud video self-supervised learning framework for\nobject-centric and scene-centric data. Previous methods commonly conduct\nrepresentation learning at the clip or frame level and cannot well capture\nfine-grained semantics. Instead of contrasting the representations of clips or\nframes, in this paper, we propose a unified self-supervised framework by\nconducting contrastive learning at the point level. Moreover, we introduce a\nnew pretext task by achieving semantic alignment of superpoints, which further\nfacilitates the representations to capture semantic cues at multiple scales. In\naddition, due to the high redundancy in the temporal dimension of dynamic point\nclouds, directly conducting contrastive learning at the point level usually\nleads to massive undesired negatives and insufficient modeling of positive\nrepresentations. To remedy this, we propose a selection strategy to retain\nproper negatives and make use of high-similarity samples from other instances\nas positive supplements. Extensive experiments show that our method outperforms\nsupervised counterparts on a wide range of downstream tasks and demonstrates\nthe superior transferability of the learned representations.",
        "authors": [
            "Xiaoxiao Sheng",
            "Zhiqiang Shen",
            "Gang Xiao",
            "Longguang Wang",
            "Yulan Guo",
            "Hehe Fan"
        ]
    },
    {
        "title": "Preserving Modality Structure Improves Multi-Modal Learning",
        "url": "http://arxiv.org/abs/2308.13077",
        "abstract": "Self-supervised learning on large-scale multi-modal datasets allows learning\nsemantically meaningful embeddings in a joint multi-modal representation space\nwithout relying on human annotations. These joint embeddings enable zero-shot\ncross-modal tasks like retrieval and classification. However, these methods\noften struggle to generalize well on out-of-domain data as they ignore the\nsemantic structure present in modality-specific embeddings. In this context, we\npropose a novel Semantic-Structure-Preserving Consistency approach to improve\ngeneralizability by preserving the modality-specific relationships in the joint\nembedding space. To capture modality-specific semantic relationships between\nsamples, we propose to learn multiple anchors and represent the multifaceted\nrelationship between samples with respect to their relationship with these\nanchors. To assign multiple anchors to each sample, we propose a novel\nMulti-Assignment Sinkhorn-Knopp algorithm. Our experimentation demonstrates\nthat our proposed approach learns semantically meaningful anchors in a\nself-supervised manner. Furthermore, our evaluation on MSR-VTT and YouCook2\ndatasets demonstrates that our proposed multi-anchor assignment based solution\nachieves state-of-the-art performance and generalizes to both inand\nout-of-domain datasets. Code: https://github.com/Swetha5/Multi_Sinkhorn_Knopp",
        "authors": [
            "Swetha Sirnam",
            "Mamshad Nayeem Rizve",
            "Nina Shvetsova",
            "Hilde Kuehne",
            "Mubarak Shah"
        ]
    },
    {
        "title": "Pre-training Vision Transformers with Very Limited Synthesized Images",
        "url": "http://arxiv.org/abs/2307.14710",
        "abstract": "Formula-driven supervised learning (FDSL) is a pre-training method that\nrelies on synthetic images generated from mathematical formulae such as\nfractals. Prior work on FDSL has shown that pre-training vision transformers on\nsuch synthetic datasets can yield competitive accuracy on a wide range of\ndownstream tasks. These synthetic images are categorized according to the\nparameters in the mathematical formula that generate them. In the present work,\nwe hypothesize that the process for generating different instances for the same\ncategory in FDSL, can be viewed as a form of data augmentation. We validate\nthis hypothesis by replacing the instances with data augmentation, which means\nwe only need a single image per category. Our experiments shows that this\none-instance fractal database (OFDB) performs better than the original dataset\nwhere instances were explicitly generated. We further scale up OFDB to 21,000\ncategories and show that it matches, or even surpasses, the model pre-trained\non ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is\n21k, whereas ImageNet-21k has 14M. This opens new possibilities for\npre-training vision transformers with much smaller datasets.",
        "authors": [
            "Ryo Nakamura",
            "Hirokatsu Kataoka",
            "Sora Takashima",
            "Edgar Josafat Martinez Noriega",
            "Rio Yokota",
            "Nakamasa Inoue"
        ]
    },
    {
        "title": "Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions",
        "url": "http://arxiv.org/abs/2309.10431",
        "abstract": "Robust 3D perception under corruption has become an essential task for the\nrealm of 3D vision. While current data augmentation techniques usually perform\nrandom transformations on all point cloud objects in an offline way and ignore\nthe structure of the samples, resulting in over-or-under enhancement. In this\nwork, we propose an alternative to make sample-adaptive transformations based\non the structure of the sample to cope with potential corruption via an\nauto-augmentation framework, named as AdaptPoint. Specially, we leverage a\nimitator, consisting of a Deformation Controller and a Mask Controller,\nrespectively in charge of predicting deformation parameters and producing a\nper-point mask, based on the intrinsic structural information of the input\npoint cloud, and then conduct corruption simulations on top. Then a\ndiscriminator is utilized to prevent the generation of excessive corruption\nthat deviates from the original data distribution. In addition, a\nperception-guidance feedback mechanism is incorporated to guide the generation\nof samples with appropriate difficulty level. Furthermore, to address the\npaucity of real-world corrupted point cloud, we also introduce a new dataset\nScanObjectNN-C, that exhibits greater similarity to actual data in real-world\nenvironments, especially when contrasted with preceding CAD datasets.\nExperiments show that our method achieves state-of-the-art results on multiple\ncorruption benchmarks, including ModelNet-C, our ScanObjectNN-C, and\nShapeNet-C.",
        "authors": [
            "Jie Wang",
            "Lihe Ding",
            "Tingfa Xu",
            "Shaocong Dong",
            "Xinli Xu",
            "Long Bai",
            "Jianan Li"
        ]
    },
    {
        "title": "Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding",
        "url": "http://arxiv.org/abs/2303.12326",
        "abstract": "3D GAN inversion aims to achieve high reconstruction fidelity and reasonable\n3D geometry simultaneously from a single image input. However, existing 3D GAN\ninversion methods rely on time-consuming optimization for each individual case.\nIn this work, we introduce a novel encoder-based inversion framework based on\nEG3D, one of the most widely-used 3D GAN models. We leverage the inherent\nproperties of EG3D's latent space to design a discriminator and a background\ndepth regularization. This enables us to train a geometry-aware encoder capable\nof converting the input image into corresponding latent code. Additionally, we\nexplore the feature space of EG3D and develop an adaptive refinement stage that\nimproves the representation ability of features in EG3D to enhance the recovery\nof fine-grained textural details. Finally, we propose an occlusion-aware fusion\noperation to prevent distortion in unobserved regions. Our method achieves\nimpressive results comparable to optimization-based methods while operating up\nto 500 times faster. Our framework is well-suited for applications such as\nsemantic editing.",
        "authors": [
            "Ziyang Yuan",
            "Yiming Zhu",
            "Yu Li",
            "Hongyu Liu",
            "Chun Yuan"
        ]
    },
    {
        "title": "Modality Unifying Network for Visible-Infrared Person Re-Identification",
        "url": "http://arxiv.org/abs/2309.06262",
        "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging task due\nto large cross-modality discrepancies and intra-class variations. Existing\nmethods mainly focus on learning modality-shared representations by embedding\ndifferent modalities into the same feature space. As a result, the learned\nfeature emphasizes the common patterns across modalities while suppressing\nmodality-specific and identity-aware information that is valuable for Re-ID. To\naddress these issues, we propose a novel Modality Unifying Network (MUN) to\nexplore a robust auxiliary modality for VI-ReID. First, the auxiliary modality\nis generated by combining the proposed cross-modality learner and\nintra-modality learner, which can dynamically model the modality-specific and\nmodality-shared representations to alleviate both cross-modality and\nintra-modality variations. Second, by aligning identity centres across the\nthree modalities, an identity alignment loss function is proposed to discover\nthe discriminative feature representations. Third, a modality alignment loss is\nintroduced to consistently reduce the distribution distance of visible and\ninfrared images by modality prototype modeling. Extensive experiments on\nmultiple public datasets demonstrate that the proposed method surpasses the\ncurrent state-of-the-art methods by a significant margin.",
        "authors": [
            "Hao Yu",
            "Xu Cheng",
            "Wei Peng",
            "Weihao Liu",
            "Guoying Zhao"
        ]
    },
    {
        "title": "DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer",
        "url": "http://arxiv.org/abs/2303.03755",
        "abstract": "Generating visual layouts is an essential ingredient of graphic design. The\nability to condition layout generation on a partial subset of component\nattributes is critical to real-world applications that involve user\ninteraction. Recently, diffusion models have demonstrated high-quality\ngenerative performances in various domains. However, it is unclear how to apply\ndiffusion models to the natural representation of layouts which consists of a\nmix of discrete (class) and continuous (location, size) attributes. To address\nthe conditioning layout generation problem, we introduce DLT, a joint\ndiscrete-continuous diffusion model. DLT is a transformer-based model which has\na flexible conditioning mechanism that allows for conditioning on any given\nsubset of all the layout component classes, locations, and sizes. Our method\noutperforms state-of-the-art generative models on various layout generation\ndatasets with respect to different metrics and conditioning settings.\nAdditionally, we validate the effectiveness of our proposed conditioning\nmechanism and the joint continuous-diffusion process. This joint process can be\nincorporated into a wide range of mixed discrete-continuous generative tasks.",
        "authors": [
            "Elad Levi",
            "Eli Brosh",
            "Mykola Mykhailych",
            "Meir Perez"
        ]
    },
    {
        "title": "PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels",
        "url": "http://arxiv.org/abs/2212.03462",
        "abstract": "Convolutional Neural Networks (CNNs) have demonstrated superiority in\nlearning patterns, but are sensitive to label noises and may overfit noisy\nlabels during training. The early stopping strategy averts updating CNNs during\nthe early training phase and is widely employed in the presence of noisy\nlabels. Motivated by biological findings that the amplitude spectrum (AS) and\nphase spectrum (PS) in the frequency domain play different roles in the\nanimal's vision system, we observe that PS, which captures more semantic\ninformation, can increase the robustness of DNNs to label noise, more so than\nAS can. We thus propose early stops at different times for AS and PS by\ndisentangling the features of some layer(s) into AS and PS using Discrete\nFourier Transform (DFT) during training. Our proposed Phase-AmplituDe\nDisentangLed Early Stopping (PADDLES) method is shown to be effective on both\nsynthetic and real-world label-noise datasets. PADDLES outperforms other early\nstopping methods and obtains state-of-the-art performance.",
        "authors": [
            "Huaxi Huang",
            "Hui Kang",
            "Sheng Liu",
            "Olivier Salvado",
            "Thierry Rakotoarivelo",
            "Dadong Wang",
            "Tongliang Liu"
        ]
    },
    {
        "title": "CASSPR: Cross Attention Single Scan Place Recognition",
        "url": "http://arxiv.org/abs/2211.12542",
        "abstract": "Place recognition based on point clouds (LiDAR) is an important component for\nautonomous robots or self-driving vehicles. Current SOTA performance is\nachieved on accumulated LiDAR submaps using either point-based or voxel-based\nstructures. While voxel-based approaches nicely integrate spatial context\nacross multiple scales, they do not exhibit the local precision of point-based\nmethods. As a result, existing methods struggle with fine-grained matching of\nsubtle geometric features in sparse single-shot Li- DAR scans. To overcome\nthese limitations, we propose CASSPR as a method to fuse point-based and\nvoxel-based approaches using cross attention transformers. CASSPR leverages a\nsparse voxel branch for extracting and aggregating information at lower\nresolution and a point-wise branch for obtaining fine-grained local\ninformation. CASSPR uses queries from one branch to try to match structures in\nthe other branch, ensuring that both extract self-contained descriptors of the\npoint cloud (rather than one branch dominating), but using both to inform the\noutput global descriptor of the point cloud. Extensive experiments show that\nCASSPR surpasses the state-of-the-art by a large margin on several datasets\n(Oxford RobotCar, TUM, USyd). For instance, it achieves AR@1 of 85.6% on the\nTUM dataset, surpassing the strongest prior model by ~15%. Our code is publicly\navailable.",
        "authors": [
            "Yan Xia",
            "Mariia Gladkova",
            "Rui Wang",
            "Qianyun Li",
            "Uwe Stilla",
            "Jo\u00e3o F. Henriques",
            "Daniel Cremers"
        ]
    },
    {
        "title": "DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion",
        "url": "http://arxiv.org/abs/2303.06840",
        "abstract": "Multi-modality image fusion aims to combine different modalities to produce\nfused images that retain the complementary features of each modality, such as\nfunctional highlights and texture details. To leverage strong generative priors\nand address challenges such as unstable training and lack of interpretability\nfor GAN-based generative methods, we propose a novel fusion algorithm based on\nthe denoising diffusion probabilistic model (DDPM). The fusion task is\nformulated as a conditional generation problem under the DDPM sampling\nframework, which is further divided into an unconditional generation subproblem\nand a maximum likelihood subproblem. The latter is modeled in a hierarchical\nBayesian manner with latent variables and inferred by the\nexpectation-maximization (EM) algorithm. By integrating the inference solution\ninto the diffusion sampling iteration, our method can generate high-quality\nfused images with natural image generative priors and cross-modality\ninformation from source images. Note that all we required is an unconditional\npre-trained generative model, and no fine-tuning is needed. Our extensive\nexperiments indicate that our approach yields promising fusion results in\ninfrared-visible image fusion and medical image fusion. The code is available\nat \\url{https://github.com/Zhaozixiang1228/MMIF-DDFM}.",
        "authors": [
            "Zixiang Zhao",
            "Haowen Bai",
            "Yuanzhi Zhu",
            "Jiangshe Zhang",
            "Shuang Xu",
            "Yulun Zhang",
            "Kai Zhang",
            "Deyu Meng",
            "Radu Timofte",
            "Luc Van Gool"
        ]
    },
    {
        "title": "A Unified Continual Learning Framework with General Parameter-Efficient Tuning",
        "url": "http://arxiv.org/abs/2303.10070",
        "abstract": "The \"pre-training $\\rightarrow$ downstream adaptation\" presents both new\nopportunities and challenges for Continual Learning (CL). Although the recent\nstate-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET)\nadaptation paradigm, only prompt has been explored, limiting its application to\nTransformers only. In this paper, we position prompting as one instantiation of\nPET, and propose a unified CL framework with general PET, dubbed as\nLearning-Accumulation-Ensemble (LAE). PET, e.g., using Adapter, LoRA, or\nPrefix, can adapt a pre-trained model to downstream tasks with fewer parameters\nand resources. Given a PET method, our LAE framework incorporates it for CL\nwith three novel designs. 1) Learning: the pre-trained model adapts to the new\ntask by tuning an online PET module, along with our adaptation speed\ncalibration to align different PET modules, 2) Accumulation: the task-specific\nknowledge learned by the online PET module is accumulated into an offline PET\nmodule through momentum update, 3) Ensemble: During inference, we respectively\nconstruct two experts with online/offline PET modules (which are favored by the\nnovel/historical tasks) for prediction ensemble. We show that LAE is compatible\nwith a battery of PET methods and gains strong CL capability. For example, LAE\nwith Adaptor PET surpasses the prior state-of-the-art by 1.3% and 3.6% in\nlast-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively.\nCode is available at \\url{https://github.com/gqk/LAE}.",
        "authors": [
            "Qiankun Gao",
            "Chen Zhao",
            "Yifan Sun",
            "Teng Xi",
            "Gang Zhang",
            "Bernard Ghanem",
            "Jian Zhang"
        ]
    },
    {
        "title": "Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution",
        "url": "http://arxiv.org/abs/2303.09826",
        "abstract": "Existing real-world video super-resolution (VSR) methods focus on designing a\ngeneral degradation pipeline for open-domain videos while ignoring data\nintrinsic characteristics which strongly limit their performance when applying\nto some specific domains (eg., animation videos). In this paper, we thoroughly\nexplore the characteristics of animation videos and leverage the rich priors in\nreal-world animation data for a more practical animation VSR model. In\nparticular, we propose a multi-scale Vector-Quantized Degradation model for\nanimation video Super-Resolution (VQD-SR) to decompose the local details from\nglobal structures and transfer the degradation priors in real-world animation\nvideos to a learned vector-quantized codebook for degradation modeling. A\nrich-content Real Animation Low-quality (RAL) video dataset is collected for\nextracting the priors. We further propose a data enhancement strategy for\nhigh-resolution (HR) training videos based on our observation that existing HR\nvideos are mostly collected from the Web which contains conspicuous compression\nartifacts. The proposed strategy is valid to lift the upper bound of animation\nVSR performance, regardless of the specific VSR model. Experimental results\ndemonstrate the superiority of the proposed VQD-SR over state-of-the-art\nmethods, through extensive quantitative and qualitative evaluations of the\nlatest animation video super-resolution benchmark. The code and pre-trained\nmodels can be downloaded at https://github.com/researchmm/VQD-SR.",
        "authors": [
            "Zixi Tuo",
            "Huan Yang",
            "Jianlong Fu",
            "Yujie Dun",
            "Xueming Qian"
        ]
    },
    {
        "title": "Compositional Feature Augmentation for Unbiased Scene Graph Generation",
        "url": "http://arxiv.org/abs/2308.06712",
        "abstract": "Scene Graph Generation (SGG) aims to detect all the visual relation triplets\n<sub, pred, obj> in a given image. With the emergence of various advanced\ntechniques for better utilizing both the intrinsic and extrinsic information in\neach relation triplet, SGG has achieved great progress over the recent years.\nHowever, due to the ubiquitous long-tailed predicate distributions, today's SGG\nmodels are still easily biased to the head predicates. Currently, the most\nprevalent debiasing solutions for SGG are re-balancing methods, e.g., changing\nthe distributions of original training samples. In this paper, we argue that\nall existing re-balancing strategies fail to increase the diversity of the\nrelation triplet features of each predicate, which is critical for robust SGG.\nTo this end, we propose a novel Compositional Feature Augmentation (CFA)\nstrategy, which is the first unbiased SGG work to mitigate the bias issue from\nthe perspective of increasing the diversity of triplet features. Specifically,\nwe first decompose each relation triplet feature into two components: intrinsic\nfeature and extrinsic feature, which correspond to the intrinsic\ncharacteristics and extrinsic contexts of a relation triplet, respectively.\nThen, we design two different feature augmentation modules to enrich the\nfeature diversity of original relation triplets by replacing or mixing up\neither their intrinsic or extrinsic features from other samples. Due to its\nmodel-agnostic nature, CFA can be seamlessly incorporated into various SGG\nframeworks. Extensive ablations have shown that CFA achieves a new\nstate-of-the-art performance on the trade-off between different metrics.",
        "authors": [
            "Lin Li",
            "Guikun Chen",
            "Jun Xiao",
            "Yi Yang",
            "Chunping Wang",
            "Long Chen"
        ]
    },
    {
        "title": "Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network",
        "url": "http://arxiv.org/abs/2304.01198",
        "abstract": "Recently, the open-vocabulary semantic segmentation problem has attracted\nincreasing attention and the best performing methods are based on two-stream\nnetworks: one stream for proposal mask generation and the other for segment\nclassification using a pretrained visual-language model. However, existing\ntwo-stream methods require passing a great number of (up to a hundred) image\ncrops into the visual-language model, which is highly inefficient. To address\nthe problem, we propose a network that only needs a single pass through the\nvisual-language model for each input image. Specifically, we first propose a\nnovel network adaptation approach, termed patch severance, to restrict the\nharmful interference between the patch embeddings in the pre-trained visual\nencoder. We then propose classification anchor learning to encourage the\nnetwork to spatially focus on more discriminative features for classification.\nExtensive experiments demonstrate that the proposed method achieves outstanding\nperformance, surpassing state-of-the-art methods while being 4 to 7 times\nfaster at inference. Code: https://github.com/CongHan0808/DeOP.git",
        "authors": [
            "Cong Han",
            "Yujie Zhong",
            "Dengjie Li",
            "Kai Han",
            "Lin Ma"
        ]
    },
    {
        "title": "Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation",
        "url": "http://arxiv.org/abs/2309.00216",
        "abstract": "Facial sketch synthesis (FSS) aims to generate a vivid sketch portrait from a\ngiven facial photo. Existing FSS methods merely rely on 2D representations of\nfacial semantic or appearance. However, professional human artists usually use\noutlines or shadings to covey 3D geometry. Thus facial 3D geometry (e.g. depth\nmap) is extremely important for FSS. Besides, different artists may use diverse\ndrawing techniques and create multiple styles of sketches; but the style is\nglobally consistent in a sketch. Inspired by such observations, in this paper,\nwe propose a novel Human-Inspired Dynamic Adaptation (HIDA) method. Specially,\nwe propose to dynamically modulate neuron activations based on a joint\nconsideration of both facial 3D geometry and 2D appearance, as well as globally\nconsistent style control. Besides, we use deformable convolutions at\ncoarse-scales to align deep features, for generating abstract and distinct\noutlines. Experiments show that HIDA can generate high-quality sketches in\nmultiple styles, and significantly outperforms previous methods, over a large\nrange of challenging faces. Besides, HIDA allows precise style control of the\nsynthesized sketch, and generalizes well to natural scenes and other artistic\nstyles. Our code and results have been released online at:\nhttps://github.com/AiArt-HDU/HIDA.",
        "authors": [
            "Fei Gao",
            "Yifan Zhu",
            "Chang Jiang",
            "Nannan Wang"
        ]
    },
    {
        "title": "Calibrating Panoramic Depth Estimation for Practical Localization and Mapping",
        "url": "http://arxiv.org/abs/2308.14005",
        "abstract": "The absolute depth values of surrounding environments provide crucial cues\nfor various assistive technologies, such as localization, navigation, and 3D\nstructure estimation. We propose that accurate depth estimated from panoramic\nimages can serve as a powerful and light-weight input for a wide range of\ndownstream tasks requiring 3D information. While panoramic images can easily\ncapture the surrounding context from commodity devices, the estimated depth\nshares the limitations of conventional image-based depth estimation; the\nperformance deteriorates under large domain shifts and the absolute values are\nstill ambiguous to infer from 2D observations. By taking advantage of the\nholistic view, we mitigate such effects in a self-supervised way and fine-tune\nthe network with geometric consistency during the test phase. Specifically, we\nconstruct a 3D point cloud from the current depth prediction and project the\npoint cloud at various viewpoints or apply stretches on the current input image\nto generate synthetic panoramas. Then we minimize the discrepancy of the 3D\nstructure estimated from synthetic images without collecting additional data.\nWe empirically evaluate our method in robot navigation and map-free\nlocalization where our method shows large performance enhancements. Our\ncalibration method can therefore widen the applicability under various external\nconditions, serving as a key component for practical panorama-based machine\nvision systems. Code is available through the following link:\n\\url{https://github.com/82magnolia/panoramic-depth-calibration}.",
        "authors": [
            "Junho Kim",
            "Eun Sun Lee",
            "Young Min Kim"
        ]
    },
    {
        "title": "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability",
        "url": "http://arxiv.org/abs/2308.09306",
        "abstract": "Recently, large-scale diffusion models, e.g., Stable diffusion and DallE2,\nhave shown remarkable results on image synthesis. On the other hand,\nlarge-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are\ncompetent for various downstream tasks by learning to align vision and language\nembeddings. In this paper, we explore the possibility of jointly modeling\ngeneration and discrimination. Specifically, we propose DiffDis to unify the\ncross-modal generative and discriminative pretraining into one single framework\nunder the diffusion process. DiffDis first formulates the image-text\ndiscriminative problem as a generative diffusion process of the text embedding\nfrom the text encoder conditioned on the image. Then, we propose a novel\ndual-stream network architecture, which fuses the noisy text embedding with the\nknowledge of latent images from different scales for image-text discriminative\nlearning. Moreover, the generative and discriminative tasks can efficiently\nshare the image-branch network structure in the multi-modality model.\nBenefiting from diffusion-based unified training, DiffDis achieves both better\ngeneration ability and cross-modal semantic alignment in one architecture.\nExperimental results show that DiffDis outperforms single-task models on both\nthe image generation and the image-text discriminative tasks, e.g., 1.65%\nimprovement on average accuracy of zero-shot classification over 12 datasets\nand 2.42 improvement on FID of zero-shot image synthesis.",
        "authors": [
            "Runhui Huang",
            "Jianhua Han",
            "Guansong Lu",
            "Xiaodan Liang",
            "Yihan Zeng",
            "Wei Zhang",
            "Hang Xu"
        ]
    },
    {
        "title": "View Consistent Purification for Accurate Cross-View Localization",
        "url": "http://arxiv.org/abs/2308.08110",
        "abstract": "This paper proposes a fine-grained self-localization method for outdoor\nrobotics that utilizes a flexible number of onboard cameras and readily\naccessible satellite images. The proposed method addresses limitations in\nexisting cross-view localization methods that struggle to handle noise sources\nsuch as moving objects and seasonal variations. It is the first sparse\nvisual-only method that enhances perception in dynamic environments by\ndetecting view-consistent key points and their corresponding deep features from\nground and satellite views, while removing off-the-ground objects and\nestablishing homography transformation between the two views. Moreover, the\nproposed method incorporates a spatial embedding approach that leverages camera\nintrinsic and extrinsic information to reduce the ambiguity of purely visual\nmatching, leading to improved feature matching and overall pose estimation\naccuracy. The method exhibits strong generalization and is robust to\nenvironmental changes, requiring only geo-poses as ground truth. Extensive\nexperiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that\nour proposed method outperforms existing state-of-the-art methods, achieving\nmedian spatial accuracy errors below $0.5$ meters along the lateral and\nlongitudinal directions, and a median orientation accuracy error below 2\ndegrees.",
        "authors": [
            "Shan Wang",
            "Yanhao Zhang",
            "Akhil Perincherry",
            "Ankit Vora",
            "Hongdong Li"
        ]
    },
    {
        "title": "Efficient Video Action Detection with Token Dropout and Context Refinement",
        "url": "http://arxiv.org/abs/2304.08451",
        "abstract": "Streaming video clips with large-scale video tokens impede vision\ntransformers (ViTs) for efficient recognition, especially in video action\ndetection where sufficient spatiotemporal representations are required for\nprecise actor identification. In this work, we propose an end-to-end framework\nfor efficient video action detection (EVAD) based on vanilla ViTs. Our EVAD\nconsists of two specialized designs for video action detection. First, we\npropose a spatiotemporal token dropout from a keyframe-centric perspective. In\na video clip, we maintain all tokens from its keyframe, preserve tokens\nrelevant to actor motions from other frames, and drop out the remaining tokens\nin this clip. Second, we refine scene context by leveraging remaining tokens\nfor better recognizing actor identities. The region of interest (RoI) in our\naction detector is expanded into temporal domain. The captured spatiotemporal\nactor identity representations are refined via scene context in a decoder with\nthe attention mechanism. These two designs make our EVAD efficient while\nmaintaining accuracy, which is validated on three benchmark datasets (i.e.,\nAVA, UCF101-24, JHMDB). Compared to the vanilla ViT backbone, our EVAD reduces\nthe overall GFLOPs by 43% and improves real-time inference speed by 40% with no\nperformance degradation. Moreover, even at similar computational costs, our\nEVAD can improve the performance by 1.1 mAP with higher resolution inputs. Code\nis available at https://github.com/MCG-NJU/EVAD.",
        "authors": [
            "Lei Chen",
            "Zhan Tong",
            "Yibing Song",
            "Gangshan Wu",
            "Limin Wang"
        ]
    },
    {
        "title": "LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment",
        "url": "http://arxiv.org/abs/2308.01686",
        "abstract": "3D panoptic segmentation is a challenging perception task that requires both\nsemantic segmentation and instance segmentation. In this task, we notice that\nimages could provide rich texture, color, and discriminative information, which\ncan complement LiDAR data for evident performance improvement, but their fusion\nremains a challenging problem. To this end, we propose LCPS, the first\nLiDAR-Camera Panoptic Segmentation network. In our approach, we conduct\nLiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel\nAlignment (ACPA) module that calibrates the coordinate misalignment caused by\nasynchronous problems between sensors; 2) a Semantic-Aware Region Alignment\n(SARA) module that extends the one-to-one point-pixel mapping to one-to-many\nsemantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that\nintegrates both geometric and semantic fusion information for the entire point\ncloud. Our fusion strategy improves about 6.9% PQ performance over the\nLiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative\nexperiments further demonstrate the effectiveness of our novel framework. The\ncode will be released at https://github.com/zhangzw12319/lcps.git.",
        "authors": [
            "Zhiwei Zhang",
            "Zhizhong Zhang",
            "Qian Yu",
            "Ran Yi",
            "Yuan Xie",
            "Lizhuang Ma"
        ]
    },
    {
        "title": "GrowCLIP: Data-Aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-Training",
        "url": "http://arxiv.org/abs/2308.11331",
        "abstract": "Cross-modal pre-training has shown impressive performance on a wide range of\ndownstream tasks, benefiting from massive image-text pairs collected from the\nInternet. In practice, online data are growing constantly, highlighting the\nimportance of the ability of pre-trained model to learn from data that is\ncontinuously growing. Existing works on cross-modal pre-training mainly focus\non training a network with fixed architecture. However, it is impractical to\nlimit the model capacity when considering the continuously growing nature of\npre-training data in real-world applications. On the other hand, it is\nimportant to utilize the knowledge in the current model to obtain efficient\ntraining and better performance. To address the above issues, in this paper, we\npropose GrowCLIP, a data-driven automatic model growing algorithm for\ncontrastive language-image pre-training with continuous image-text pairs as\ninput. Specially, we adopt a dynamic growth space and seek out the optimal\narchitecture at each growth step to adapt to online learning scenarios. And the\nshared encoder is proposed in our growth space to enhance the degree of\ncross-modal fusion. Besides, we explore the effect of growth in different\ndimensions, which could provide future references for the design of cross-modal\nmodel architecture. Finally, we employ parameter inheriting with momentum (PIM)\nto maintain the previous knowledge and address the issue of the local minimum\ndilemma. Compared with the existing methods, GrowCLIP improves 2.3% average\ntop-1 accuracy on zero-shot image classification of 9 downstream tasks. As for\nzero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text\nrecall on Flickr30K dataset.",
        "authors": [
            "Xinchi Deng",
            "Han Shi",
            "Runhui Huang",
            "Changlin Li",
            "Hang Xu",
            "Jianhua Han",
            "James Kwok",
            "Shen Zhao",
            "Wei Zhang",
            "Xiaodan Liang"
        ]
    },
    {
        "title": "Identity-Consistent Aggregation for Video Object Detection",
        "url": "http://arxiv.org/abs/2308.07737",
        "abstract": "In Video Object Detection (VID), a common practice is to leverage the rich\ntemporal contexts from the video to enhance the object representations in each\nframe. Existing methods treat the temporal contexts obtained from different\nobjects indiscriminately and ignore their different identities. While\nintuitively, aggregating local views of the same object in different frames may\nfacilitate a better understanding of the object. Thus, in this paper, we aim to\nenable the model to focus on the identity-consistent temporal contexts of each\nobject to obtain more comprehensive object representations and handle the rapid\nobject appearance variations such as occlusion, motion blur, etc. However,\nrealizing this goal on top of existing VID models faces low-efficiency problems\ndue to their redundant region proposals and nonparallel frame-wise prediction\nmanner. To aid this, we propose ClipVID, a VID model equipped with\nIdentity-Consistent Aggregation (ICA) layers specifically designed for mining\nfine-grained and identity-consistent temporal contexts. It effectively reduces\nthe redundancies through the set prediction strategy, making the ICA layers\nvery efficient and further allowing us to design an architecture that makes\nparallel clip-wise predictions for the whole video clip. Extensive experimental\nresults demonstrate the superiority of our method: a state-of-the-art (SOTA)\nperformance (84.7% mAP) on the ImageNet VID dataset while running at a speed\nabout 7x faster (39.3 fps) than previous SOTAs.",
        "authors": [
            "Chaorui Deng",
            "Da Chen",
            "Qi Wu"
        ]
    },
    {
        "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models",
        "url": "http://arxiv.org/abs/2305.06077",
        "abstract": "Following the remarkable success of diffusion models on image generation,\nrecent works have also demonstrated their impressive ability to address a\nnumber of inverse problems in an unsupervised way, by properly constraining the\nsampling process based on a conditioning input. Motivated by this, in this\npaper, we present the first approach to use diffusion models as a prior for\nhighly accurate 3D facial BRDF reconstruction from a single image. We start by\nleveraging a high-quality UV dataset of facial reflectance (diffuse and\nspecular albedo and normals), which we render under varying illumination\nsettings to simulate natural RGB textures and, then, train an unconditional\ndiffusion model on concatenated pairs of rendered textures and reflectance\ncomponents. At test time, we fit a 3D morphable model to the given image and\nunwrap the face in a partial UV texture. By sampling from the diffusion model,\nwhile retaining the observed texture part intact, the model inpaints not only\nthe self-occluded areas but also the unknown reflectance components, in a\nsingle sequence of denoising steps. In contrast to existing methods, we\ndirectly acquire the observed texture from the input image, thus, resulting in\nmore faithful and consistent reflectance estimation. Through a series of\nqualitative and quantitative comparisons, we demonstrate superior performance\nin both texture completion as well as reflectance reconstruction tasks.",
        "authors": [
            "Foivos Paraperas Papantoniou",
            "Alexandros Lattas",
            "Stylianos Moschoglou",
            "Stefanos Zafeiriou"
        ]
    },
    {
        "title": "Fcaformer: Forward Cross Attention in Hybrid Vision Transformer",
        "url": "http://arxiv.org/abs/2211.07198",
        "abstract": "Currently, one main research line in designing a more efficient vision\ntransformer is reducing the computational cost of self attention modules by\nadopting sparse attention or using local attention windows. In contrast, we\npropose a different approach that aims to improve the performance of\ntransformer-based architectures by densifying the attention pattern.\nSpecifically, we proposed forward cross attention for hybrid vision transformer\n(FcaFormer), where tokens from previous blocks in the same stage are secondary\nused. To achieve this, the FcaFormer leverages two innovative components:\nlearnable scale factors (LSFs) and a token merge and enhancement module (TME).\nThe LSFs enable efficient processing of cross tokens, while the TME generates\nrepresentative cross tokens. By integrating these components, the proposed\nFcaFormer enhances the interactions of tokens across blocks with potentially\ndifferent semantics, and encourages more information flows to the lower levels.\nBased on the forward cross attention (Fca), we have designed a series of\nFcaFormer models that achieve the best trade-off between model size,\ncomputational cost, memory cost, and accuracy. For example, without the need\nfor knowledge distillation to strengthen training, our FcaFormer achieves 83.1%\ntop-1 accuracy on Imagenet with only 16.3 million parameters and about 3.6\nbillion MACs. This saves almost half of the parameters and a few computational\ncosts while achieving 0.7% higher accuracy compared to distilled\nEfficientFormer.",
        "authors": [
            "Haokui Zhang",
            "Wenze Hu",
            "Xiaoyu Wang"
        ]
    },
    {
        "title": "Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition",
        "url": "http://arxiv.org/abs/2212.04761",
        "abstract": "Skeleton-based action recognition has attracted considerable attention due to\nits compact representation of the human body's skeletal sructure. Many recent\nmethods have achieved remarkable performance using graph convolutional networks\n(GCNs) and convolutional neural networks (CNNs), which extract spatial and\ntemporal features, respectively. Although spatial and temporal dependencies in\nthe human skeleton have been explored separately, spatio-temporal dependency is\nrarely considered. In this paper, we propose the Spatio-Temporal Curve Network\n(STC-Net) to effectively leverage the spatio-temporal dependency of the human\nskeleton. Our proposed network consists of two novel elements: 1) The\nSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph\nConvolution (DK-GC). The STC module dynamically adjusts the receptive field by\nidentifying meaningful node connections between every adjacent frame and\ngenerating spatio-temporal curves based on the identified node connections,\nproviding an adaptive spatio-temporal coverage. In addition, we propose DK-GC\nto consider long-range dependencies, which results in a large receptive field\nwithout any additional parameters by applying an extended kernel to the given\nadjacency matrices of the graph. Our STC-Net combines these two modules and\nachieves state-of-the-art performance on four skeleton-based action recognition\nbenchmarks.",
        "authors": [
            "Jungho Lee",
            "Minhyeok Lee",
            "Suhwan Cho",
            "Sungmin Woo",
            "Sungjun Jang",
            "Sangyoun Lee"
        ]
    },
    {
        "title": "Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification",
        "url": "http://arxiv.org/abs/2308.11901",
        "abstract": "We present a novel unsupervised domain adaption method for person\nre-identification (reID) that generalizes a model trained on a labeled source\ndomain to an unlabeled target domain. We introduce a camera-driven curriculum\nlearning (CaCL) framework that leverages camera labels of person images to\ntransfer knowledge from source to target domains progressively. To this end, we\ndivide target domain dataset into multiple subsets based on the camera labels,\nand initially train our model with a single subset (i.e., images captured by a\nsingle camera). We then gradually exploit more subsets for training, according\nto a curriculum sequence obtained with a camera-driven scheduling rule. The\nscheduler considers maximum mean discrepancies (MMD) between each subset and\nthe source domain dataset, such that the subset closer to the source domain is\nexploited earlier within the curriculum. For each curriculum sequence, we\ngenerate pseudo labels of person images in a target domain to train a reID\nmodel in a supervised way. We have observed that the pseudo labels are highly\nbiased toward cameras, suggesting that person images obtained from the same\ncamera are likely to have the same pseudo labels, even for different IDs. To\naddress the camera bias problem, we also introduce a camera-diversity (CD) loss\nencouraging person images of the same pseudo label, but captured across various\ncameras, to involve more for discriminative feature learning, providing person\nrepresentations robust to inter-camera variations. Experimental results on\nstandard benchmarks, including real-to-real and synthetic-to-real scenarios,\ndemonstrate the effectiveness of our framework.",
        "authors": [
            "Geon Lee",
            "Sanghoon Lee",
            "Dohyung Kim",
            "Younghoon Shin",
            "Yongsang Yoon",
            "Bumsub Ham"
        ]
    },
    {
        "title": "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation",
        "url": "http://arxiv.org/abs/2308.15226",
        "abstract": "There has been a growing interest in developing multimodal machine\ntranslation (MMT) systems that enhance neural machine translation (NMT) with\nvisual knowledge. This problem setup involves using images as auxiliary\ninformation during training, and more recently, eliminating their use during\ninference. Towards this end, previous works face a challenge in training\npowerful MMT models from scratch due to the scarcity of annotated multilingual\nvision-language data, especially for low-resource languages. Simultaneously,\nthere has been an influx of multilingual pre-trained models for NMT and\nmultimodal pre-trained models for vision-language tasks, primarily in English,\nwhich have shown exceptional generalisation ability. However, these are not\ndirectly applicable to MMT since they do not provide aligned multimodal\nmultilingual features for generative tasks. To alleviate this issue, instead of\ndesigning complex modules for MMT, we propose CLIPTrans, which simply adapts\nthe independently pre-trained multimodal M-CLIP and the multilingual mBART. In\norder to align their embedding spaces, mBART is conditioned on the M-CLIP\nfeatures by a prefix sequence generated through a lightweight mapping network.\nWe train this in a two-stage pipeline which warms up the model with image\ncaptioning before the actual translation task. Through experiments, we\ndemonstrate the merits of this framework and consequently push forward the\nstate-of-the-art across standard benchmarks by an average of +2.67 BLEU. The\ncode can be found at www.github.com/devaansh100/CLIPTrans.",
        "authors": [
            "Devaansh Gupta",
            "Siddhant Kharbanda",
            "Jiawei Zhou",
            "Wanhua Li",
            "Hanspeter Pfister",
            "Donglai Wei"
        ]
    },
    {
        "title": "SGAligner: 3D Scene Alignment with Scene Graphs",
        "url": "http://arxiv.org/abs/2304.14880",
        "abstract": "Building 3D scene graphs has recently emerged as a topic in scene\nrepresentation for several embodied AI applications to represent the world in a\nstructured and rich manner. With their increased use in solving downstream\ntasks (eg, navigation and room rearrangement), can we leverage and recycle them\nfor creating 3D maps of environments, a pivotal step in agent operation? We\nfocus on the fundamental problem of aligning pairs of 3D scene graphs whose\noverlap can range from zero to partial and can contain arbitrary changes. We\npropose SGAligner, the first method for aligning pairs of 3D scene graphs that\nis robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and\nchanges in the environment). We get inspired by multi-modality knowledge graphs\nand use contrastive learning to learn a joint, multi-modal embedding space. We\nevaluate on the 3RScan dataset and further showcase that our method can be used\nfor estimating the transformation between pairs of 3D scenes. Since benchmarks\nfor these tasks are missing, we create them on this dataset. The code,\nbenchmark, and trained models are available on the project website.",
        "authors": [
            "Sayan Deb Sarkar",
            "Ondrej Miksik",
            "Marc Pollefeys",
            "Daniel Barath",
            "Iro Armeni"
        ]
    },
    {
        "title": "Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer",
        "url": "http://arxiv.org/abs/2212.03434",
        "abstract": "The long-standing theory that a colour-naming system evolves under dual\npressure of efficient communication and perceptual mechanism is supported by\nmore and more linguistic studies, including analysing four decades of\ndiachronic data from the Nafaanra language. This inspires us to explore whether\nmachine learning could evolve and discover a similar colour-naming system via\noptimising the communication efficiency represented by high-level recognition\nperformance. Here, we propose a novel colour quantisation transformer,\nCQFormer, that quantises colour space while maintaining the accuracy of machine\nrecognition on the quantised images. Given an RGB image, Annotation Branch maps\nit into an index map before generating the quantised image with a colour\npalette; meanwhile the Palette Branch utilises a key-point detection way to\nfind proper colours in the palette among the whole colour space. By interacting\nwith colour annotation, CQFormer is able to balance both the machine vision\naccuracy and colour perceptual structure such as distinct and stable colour\ndistribution for discovered colour system. Very interestingly, we even observe\nthe consistent evolution pattern between our artificial colour system and basic\ncolour terms across human languages. Besides, our colour quantisation method\nalso offers an efficient quantisation method that effectively compresses the\nimage storage while maintaining high performance in high-level recognition\ntasks such as classification and detection. Extensive experiments demonstrate\nthe superior performance of our method with extremely low bit-rate colours,\nshowing potential to integrate into quantisation network to quantities from\nimage to network activation. The source code is available at\nhttps://github.com/ryeocthiv/CQFormer",
        "authors": [
            "Shenghan Su",
            "Lin Gu",
            "Yue Yang",
            "Zenghui Zhang",
            "Tatsuya Harada"
        ]
    },
    {
        "title": "FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2306.11046",
        "abstract": "Existing skeleton-based action recognition methods typically follow a\ncentralized learning paradigm, which can pose privacy concerns when exposing\nhuman-related videos. Federated Learning (FL) has attracted much attention due\nto its outstanding advantages in privacy-preserving. However, directly applying\nFL approaches to skeleton videos suffers from unstable training. In this paper,\nwe investigate and discover that the heterogeneous human topology graph\nstructure is the crucial factor hindering training stability. To address this\nlimitation, we pioneer a novel Federated Skeleton-based Action Recognition\n(FSAR) paradigm, which enables the construction of a globally generalized model\nwithout accessing local sensitive data. Specifically, we introduce an Adaptive\nTopology Structure (ATS), separating generalization and personalization by\nlearning a domain-invariant topology shared across clients and a\ndomain-specific topology decoupled from global model aggregation.Furthermore,\nwe explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancy\nbetween clients and server caused by distinct updating patterns through\naligning shallow block-wise motion features. Extensive experiments on multiple\ndatasets demonstrate that FSAR outperforms state-of-the-art FL-based methods\nwhile inherently protecting privacy.",
        "authors": [
            "Jingwen Guo",
            "Hong Liu",
            "Shitong Sun",
            "Tianyu Guo",
            "Min Zhang",
            "Chenyang Si"
        ]
    },
    {
        "title": "Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation",
        "url": "http://arxiv.org/abs/2309.13700",
        "abstract": "Although convolutional neural networks (CNNs) have been proposed to remove\nadverse weather conditions in single images using a single set of pre-trained\nweights, they fail to restore weather videos due to the absence of temporal\ninformation. Furthermore, existing methods for removing adverse weather\nconditions (e.g., rain, fog, and snow) from videos can only handle one type of\nadverse weather. In this work, we propose the first framework for restoring\nvideos from all adverse weather conditions by developing a video\nadverse-weather-component suppression network (ViWS-Net). To achieve this, we\nfirst devise a weather-agnostic video transformer encoder with multiple\ntransformer stages. Moreover, we design a long short-term temporal modeling\nmechanism for weather messenger to early fuse input adjacent video frames and\nlearn weather-specific information. We further introduce a weather\ndiscriminator with gradient reversion, to maintain the weather-invariant common\ninformation and suppress the weather-specific information in pixel features, by\nadversarially predicting weather types. Finally, we develop a messenger-driven\nvideo transformer decoder to retrieve the residual weather-specific feature,\nwhich is spatiotemporally aggregated with hierarchical pixel features and\nrefined to predict the clean target frame of input videos. Experimental\nresults, on benchmark datasets and real-world weather videos, demonstrate that\nour ViWS-Net outperforms current state-of-the-art methods in terms of restoring\nvideos degraded by any weather condition.",
        "authors": [
            "Yijun Yang",
            "Angelica I. Aviles-Rivero",
            "Huazhu Fu",
            "Ye Liu",
            "Weiming Wang",
            "Lei Zhu"
        ]
    },
    {
        "title": "Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond",
        "url": "http://arxiv.org/abs/2308.14753",
        "abstract": "Visual similarities discovery (VSD) is an important task with broad\ne-commerce applications. Given an image of a certain object, the goal of VSD is\nto retrieve images of different objects with high perceptual visual similarity.\nAlthough being a highly addressed problem, the evaluation of proposed methods\nfor VSD is often based on a proxy of an identification-retrieval task,\nevaluating the ability of a model to retrieve different images of the same\nobject. We posit that evaluating VSD methods based on identification tasks is\nlimited, and faithful evaluation must rely on expert annotations. In this\npaper, we introduce the first large-scale fashion visual similarity benchmark\ndataset, consisting of more than 110K expert-annotated image pairs. Besides\nthis major contribution, we share insight from the challenges we faced while\ncurating this dataset. Based on these insights, we propose a novel and\nefficient labeling procedure that can be applied to any dataset. Our analysis\nexamines its limitations and inductive biases, and based on these findings, we\npropose metrics to mitigate those limitations. Though our primary focus lies on\nvisual similarity, the methodologies we present have broader applications for\ndiscovering and evaluating perceptual similarity across various domains.",
        "authors": [
            "Oren Barkan",
            "Tal Reiss",
            "Jonathan Weill",
            "Ori Katz",
            "Roy Hirsch",
            "Itzik Malkiel",
            "Noam Koenigstein"
        ]
    },
    {
        "title": "Part-Aware Transformer for Generalizable Person Re-identification",
        "url": "http://arxiv.org/abs/2308.03322",
        "abstract": "Domain generalization person re-identification (DG-ReID) aims to train a\nmodel on source domains and generalize well on unseen domains. Vision\nTransformer usually yields better generalization ability than common CNN\nnetworks under distribution shifts. However, Transformer-based ReID models\ninevitably over-fit to domain-specific biases due to the supervised learning\nstrategy on the source domain. We observe that while the global images of\ndifferent IDs should have different features, their similar local parts (e.g.,\nblack backpack) are not bounded by this constraint. Motivated by this, we\npropose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by\ndesigning a proxy task, named Cross-ID Similarity Learning (CSL), to mine local\nvisual information shared by different IDs. This proxy task allows the model to\nlearn generic features because it only cares about the visual similarity of the\nparts regardless of the ID labels, thus alleviating the side effect of\ndomain-specific biases. Based on the local similarity obtained in CSL, a\nPart-guided Self-Distillation (PSD) is proposed to further improve the\ngeneralization of global features. Our method achieves state-of-the-art\nperformance under most DG ReID settings. Under the Market$\\to$Duke setting, our\nmethod exceeds state-of-the-art by 10.9% and 12.8% in Rank1 and mAP,\nrespectively. The code is available at\nhttps://github.com/liyuke65535/Part-Aware-Transformer.",
        "authors": [
            "Hao Ni",
            "Yuke Li",
            "Lianli Gao",
            "Heng Tao Shen",
            "Jingkuan Song"
        ]
    },
    {
        "title": "Panoramas from Photons",
        "url": "http://arxiv.org/abs/2309.03811",
        "abstract": "Scene reconstruction in the presence of high-speed motion and low\nillumination is important in many applications such as augmented and virtual\nreality, drone navigation, and autonomous robotics. Traditional motion\nestimation techniques fail in such conditions, suffering from too much blur in\nthe presence of high-speed motion and strong noise in low-light conditions.\nSingle-photon cameras have recently emerged as a promising technology capable\nof capturing hundreds of thousands of photon frames per second thanks to their\nhigh speed and extreme sensitivity. Unfortunately, traditional computer vision\ntechniques are not well suited for dealing with the binary-valued photon data\ncaptured by these cameras because these are corrupted by extreme Poisson noise.\nHere we present a method capable of estimating extreme scene motion under\nchallenging conditions, such as low light or high dynamic range, from a\nsequence of high-speed image frames such as those captured by a single-photon\ncamera. Our method relies on iteratively improving a motion estimate by\ngrouping and aggregating frames after-the-fact, in a stratified manner. We\ndemonstrate the creation of high-quality panoramas under fast motion and\nextremely low light, and super-resolution results using a custom single-photon\ncamera prototype. For code and supplemental material see our\n$\\href{https://wisionlab.com/project/panoramas-from-photons/}{\\text{project\nwebpage}}$.",
        "authors": [
            "Sacha Jungerman",
            "Atul Ingle",
            "Mohit Gupta"
        ]
    },
    {
        "title": "Global Adaptation Meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation",
        "url": "http://arxiv.org/abs/2303.16456",
        "abstract": "When applying a pre-trained 2D-to-3D human pose lifting model to a target\nunseen dataset, large performance degradation is commonly encountered due to\ndomain shift issues. We observe that the degradation is caused by two factors:\n1) the large distribution gap over global positions of poses between the source\nand target datasets due to variant camera parameters and settings, and 2) the\ndeficient diversity of local structures of poses in training. To this end, we\ncombine \\textbf{global adaptation} and \\textbf{local generalization} in\n\\textit{PoseDA}, a simple yet effective framework of unsupervised domain\nadaptation for 3D human pose estimation. Specifically, global adaptation aims\nto align global positions of poses from the source domain to the target domain\nwith a proposed global position alignment (GPA) module. And local\ngeneralization is designed to enhance the diversity of 2D-3D pose mapping with\na local pose augmentation (LPA) module. These modules bring significant\nperformance improvement without introducing additional learnable parameters. In\naddition, we propose local pose augmentation (LPA) to enhance the diversity of\n3D poses following an adversarial training scheme consisting of 1) a\naugmentation generator that generates the parameters of pre-defined pose\ntransformations and 2) an anchor discriminator to ensure the reality and\nquality of the augmented data. Our approach can be applicable to almost all\n2D-3D lifting models. \\textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHP\nunder a cross-dataset evaluation setup, improving upon the previous\nstate-of-the-art method by 10.2\\%.",
        "authors": [
            "Wenhao Chai",
            "Zhongyu Jiang",
            "Jenq-Neng Hwang",
            "Gaoang Wang"
        ]
    },
    {
        "title": "Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation",
        "url": "http://arxiv.org/abs/2308.06015",
        "abstract": "Deep neural networks are vulnerable to universal adversarial perturbation\n(UAP), an instance-agnostic perturbation capable of fooling the target model\nfor most samples. Compared to instance-specific adversarial examples, UAP is\nmore challenging as it needs to generalize across various samples and models.\nIn this paper, we examine the serious dilemma of UAP generation methods from a\ngeneralization perspective -- the gradient vanishing problem using small-batch\nstochastic gradient optimization and the local optima problem using large-batch\noptimization. To address these problems, we propose a simple and effective\nmethod called Stochastic Gradient Aggregation (SGA), which alleviates the\ngradient vanishing and escapes from poor local optima at the same time.\nSpecifically, SGA employs the small-batch training to perform multiple\niterations of inner pre-search. Then, all the inner gradients are aggregated as\na one-step gradient estimation to enhance the gradient stability and reduce\nquantization errors. Extensive experiments on the standard ImageNet dataset\ndemonstrate that our method significantly enhances the generalization ability\nof UAP and outperforms other state-of-the-art methods. The code is available at\nhttps://github.com/liuxuannan/Stochastic-Gradient-Aggregation.",
        "authors": [
            "Xuannan Liu",
            "Yaoyao Zhong",
            "Yuhang Zhang",
            "Lixiong Qin",
            "Weihong Deng"
        ]
    },
    {
        "title": "DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image",
        "url": "http://arxiv.org/abs/2309.12594",
        "abstract": "Accurate 3D shape abstraction from a single 2D image is a long-standing\nproblem in computer vision and graphics. By leveraging a set of primitives to\nrepresent the target shape, recent methods have achieved promising results.\nHowever, these methods either use a relatively large number of primitives or\nlack geometric flexibility due to the limited expressibility of the primitives.\nIn this paper, we propose a novel bi-channel Transformer architecture,\nintegrated with parameterized deformable models, termed DeFormer, to\nsimultaneously estimate the global and local deformations of primitives. In\nthis way, DeFormer can abstract complex object shapes while using a small\nnumber of primitives which offer a broader geometry coverage and finer details.\nThen, we introduce a force-driven dynamic fitting and a cycle-consistent\nre-projection loss to optimize the primitive parameters. Extensive experiments\non ShapeNet across various settings show that DeFormer achieves better\nreconstruction accuracy over the state-of-the-art, and visualizes with\nconsistent semantic correspondences for improved interpretability.",
        "authors": [
            "Di Liu",
            "Xiang Yu",
            "Meng Ye",
            "Qilong Zhangli",
            "Zhuowei Li",
            "Zhixing Zhang",
            "Dimitris N. Metaxas"
        ]
    },
    {
        "title": "Cross-view Semantic Alignment for Livestreaming Product Recognition",
        "url": "http://arxiv.org/abs/2308.04912",
        "abstract": "Live commerce is the act of selling products online through live streaming.\nThe customer's diverse demands for online products introduce more challenges to\nLivestreaming Product Recognition. Previous works have primarily focused on\nfashion clothing data or utilize single-modal input, which does not reflect the\nreal-world scenario where multimodal data from various categories are present.\nIn this paper, we present LPR4M, a large-scale multimodal dataset that covers\n34 categories, comprises 3 modalities (image, video, and text), and is 50x\nlarger than the largest publicly available dataset. LPR4M contains diverse\nvideos and noise modality pairs while exhibiting a long-tailed distribution,\nresembling real-world problems. Moreover, a cRoss-vIew semantiC alignmEnt\n(RICE) model is proposed to learn discriminative instance features from the\nimage and video views of the products. This is achieved through instance-level\ncontrastive learning and cross-view patch-level feature propagation. A novel\nPatch Feature Reconstruction loss is proposed to penalize the semantic\nmisalignment between cross-view patches. Extensive experiments demonstrate the\neffectiveness of RICE and provide insights into the importance of dataset\ndiversity and expressivity. The dataset and code are available at\nhttps://github.com/adxcreative/RICE",
        "authors": [
            "Wenjie Yang",
            "Yiyi Chen",
            "Yan Li",
            "Yanhua Cheng",
            "Xudong Liu",
            "Quan Chen",
            "Han Li"
        ]
    },
    {
        "title": "Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction",
        "url": "http://arxiv.org/abs/2308.10694",
        "abstract": "We tackle the problem of estimating a Manhattan frame, i.e. three orthogonal\nvanishing points, and the unknown focal length of the camera, leveraging a\nprior vertical direction. The direction can come from an Inertial Measurement\nUnit that is a standard component of recent consumer devices, e.g.,\nsmartphones. We provide an exhaustive analysis of minimal line configurations\nand derive two new 2-line solvers, one of which does not suffer from\nsingularities affecting existing solvers. Additionally, we design a new\nnon-minimal method, running on an arbitrary number of lines, to boost the\nperformance in local optimization. Combining all solvers in a hybrid robust\nestimator, our method achieves increased accuracy even with a rough prior.\nExperiments on synthetic and real-world datasets demonstrate the superior\naccuracy of our method compared to the state of the art, while having\ncomparable runtimes. We further demonstrate the applicability of our solvers\nfor relative rotation estimation. The code is available at\nhttps://github.com/cvg/VP-Estimation-with-Prior-Gravity.",
        "authors": [
            "R\u00e9mi Pautrat",
            "Shaohui Liu",
            "Petr Hruby",
            "Marc Pollefeys",
            "Daniel Barath"
        ]
    },
    {
        "title": "MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception",
        "url": "http://arxiv.org/abs/2211.10593",
        "abstract": "This paper proposes an efficient multi-camera to Bird's-Eye-View (BEV) view\ntransformation method for 3D perception, dubbed MatrixVT. Existing view\ntransformers either suffer from poor transformation efficiency or rely on\ndevice-specific operators, hindering the broad application of BEV models. In\ncontrast, our method generates BEV features efficiently with only convolutions\nand matrix multiplications (MatMul). Specifically, we propose describing the\nBEV feature as the MatMul of image feature and a sparse Feature Transporting\nMatrix (FTM). A Prime Extraction module is then introduced to compress the\ndimension of image features and reduce FTM's sparsity. Moreover, we propose the\nRing \\& Ray Decomposition to replace the FTM with two matrices and reformulate\nour pipeline to reduce calculation further. Compared to existing methods,\nMatrixVT enjoys a faster speed and less memory footprint while remaining\ndeploy-friendly. Extensive experiments on the nuScenes benchmark demonstrate\nthat our method is highly efficient but obtains results on par with the SOTA\nmethod in object detection and map segmentation tasks",
        "authors": [
            "Hongyu Zhou",
            "Zheng Ge",
            "Zeming Li",
            "Xiangyu Zhang"
        ]
    },
    {
        "title": "Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need",
        "url": "http://arxiv.org/abs/2303.15256",
        "abstract": "Self-Supervised Learning (SSL) has emerged as the solution of choice to learn\ntransferable representations from unlabeled data. However, SSL requires to\nbuild samples that are known to be semantically akin, i.e. positive views.\nRequiring such knowledge is the main limitation of SSL and is often tackled by\nad-hoc strategies e.g. applying known data-augmentations to the same input. In\nthis work, we formalize and generalize this principle through Positive Active\nLearning (PAL) where an oracle queries semantic relationships between samples.\nPAL achieves three main objectives. First, it unveils a theoretically grounded\nlearning framework beyond SSL, based on similarity graphs, that can be extended\nto tackle supervised and semi-supervised learning depending on the employed\noracle. Second, it provides a consistent algorithm to embed a priori knowledge,\ne.g. some observed labels, into any SSL losses without any change in the\ntraining pipeline. Third, it provides a proper active learning framework\nyielding low-cost solutions to annotate datasets, arguably bringing the gap\nbetween theory and practice of active learning that is based on\nsimple-to-answer-by-non-experts queries of semantic relationships between\ninputs.",
        "authors": [
            "Vivien Cabannes",
            "Leon Bottou",
            "Yann Lecun",
            "Randall Balestriero"
        ]
    },
    {
        "title": "Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning",
        "url": "http://arxiv.org/abs/2303.08566",
        "abstract": "Visual Parameter-Efficient Fine-Tuning (PEFT) has become a powerful\nalternative for full fine-tuning so as to adapt pre-trained vision models to\ndownstream tasks, which only tunes a small number of parameters while freezing\nthe vast majority ones to ease storage burden and optimization difficulty.\nHowever, existing PEFT methods introduce trainable parameters to the same\npositions across different tasks depending solely on human heuristics and\nneglect the domain gaps. To this end, we study where to introduce and how to\nallocate trainable parameters by proposing a novel Sensitivity-aware visual\nParameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates\ntrainable parameters to task-specific important positions given a desired\ntunable parameter budget. Specifically, our SPT first quickly identifies the\nsensitive parameters that require tuning for a given task in a data-dependent\nway. Next, our SPT further boosts the representational capability for the\nweight matrices whose number of sensitive parameters exceeds a pre-defined\nthreshold by utilizing existing structured tuning methods, e.g., LoRA [23] or\nAdapter [22], to replace directly tuning the selected sensitive parameters\n(unstructured tuning) under the budget. Extensive experiments on a wide range\nof downstream recognition tasks show that our SPT is complementary to the\nexisting PEFT methods and largely boosts their performance, e.g., SPT improves\nAdapter with supervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean\nTop-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks,\nrespectively. Source code is at https://github.com/ziplab/SPT",
        "authors": [
            "Haoyu He",
            "Jianfei Cai",
            "Jing Zhang",
            "Dacheng Tao",
            "Bohan Zhuang"
        ]
    },
    {
        "title": "Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events",
        "url": "http://arxiv.org/abs/2308.09383",
        "abstract": "Recognizing objects from sparse and noisy events becomes extremely difficult\nwhen paired images and category labels do not exist. In this paper, we study\nlabel-free event-based object recognition where category labels and paired\nimages are not available. To this end, we propose a joint formulation of object\nrecognition and image reconstruction in a complementary manner. Our method\nfirst reconstructs images from events and performs object recognition through\nContrastive Language-Image Pre-training (CLIP), enabling better recognition\nthrough a rich context of images. Since the category information is essential\nin reconstructing images, we propose category-guided attraction loss and\ncategory-agnostic repulsion loss to bridge the textual features of predicted\ncategories and the visual features of reconstructed images using CLIP.\nMoreover, we introduce a reliable data sampling strategy and local-global\nreconstruction consistency to boost joint learning of two tasks. To enhance the\naccuracy of prediction and quality of reconstruction, we also propose a\nprototype-based approach using unpaired images. Extensive experiments\ndemonstrate the superiority of our method and its extensibility for zero-shot\nobject recognition. Our project code is available at\n\\url{https://github.com/Chohoonhee/Ev-LaFOR}.",
        "authors": [
            "Hoonhee Cho",
            "Hyeonseong Kim",
            "Yujeong Chae",
            "Kuk-Jin Yoon"
        ]
    },
    {
        "title": "Weakly-supervised 3D Pose Transfer with Keypoints",
        "url": "http://arxiv.org/abs/2307.13459",
        "abstract": "The main challenges of 3D pose transfer are: 1) Lack of paired training data\nwith different characters performing the same pose; 2) Disentangling pose and\nshape information from the target mesh; 3) Difficulty in applying to meshes\nwith different topologies. We thus propose a novel weakly-supervised\nkeypoint-based framework to overcome these difficulties. Specifically, we use a\ntopology-agnostic keypoint detector with inverse kinematics to compute\ntransformations between the source and target meshes. Our method only requires\nsupervision on the keypoints, can be applied to meshes with different\ntopologies and is shape-invariant for the target which allows extraction of\npose-only information from the target meshes without transferring shape\ninformation. We further design a cycle reconstruction to perform\nself-supervised pose transfer without the need for ground truth deformed mesh\nwith the same pose and shape as the target and source, respectively. We\nevaluate our approach on benchmark human and animal datasets, where we achieve\nsuperior performance compared to the state-of-the-art unsupervised approaches\nand even comparable performance with the fully supervised approaches. We test\non the more challenging Mixamo dataset to verify our approach's ability in\nhandling meshes with different topologies and complex clothes. Cross-dataset\nevaluation further shows the strong generalization ability of our approach.",
        "authors": [
            "Jinnan Chen",
            "Chen Li",
            "Gim Hee Lee"
        ]
    },
    {
        "title": "Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement",
        "url": "http://arxiv.org/abs/2304.01195",
        "abstract": "The popularity of Contrastive Language-Image Pre-training (CLIP) has\npropelled its application to diverse downstream vision tasks. To improve its\ncapacity on downstream tasks, few-shot learning has become a widely-adopted\ntechnique. However, existing methods either exhibit limited performance or\nsuffer from excessive learnable parameters. In this paper, we propose APE, an\nAdaptive Prior rEfinement method for CLIP's pre-trained knowledge, which\nachieves superior accuracy with high computational efficiency. Via a prior\nrefinement module, we analyze the inter-class disparity in the downstream data\nand decouple the domain-specific knowledge from the CLIP-extracted cache model.\nOn top of that, we introduce two model variants, a training-free APE and a\ntraining-required APE-T. We explore the trilateral affinities between the test\nimage, prior cache model, and textual representations, and only enable a\nlightweight category-residual module to be trained. For the average accuracy\nover 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively\noutperform the second-best by +1.59% and +1.99% under 16 shots with x30 less\nlearnable parameters.",
        "authors": [
            "Xiangyang Zhu",
            "Renrui Zhang",
            "Bowei He",
            "Aojun Zhou",
            "Dong Wang",
            "Bin Zhao",
            "Peng Gao"
        ]
    },
    {
        "title": "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone",
        "url": "http://arxiv.org/abs/2307.05463",
        "abstract": "Video-language pre-training (VLP) has become increasingly important due to\nits ability to generalize to various vision and language tasks. However,\nexisting egocentric VLP frameworks utilize separate video and language encoders\nand learn task-specific cross-modal information only during fine-tuning,\nlimiting the development of a unified system. In this work, we introduce the\nsecond generation of egocentric video-language pre-training (EgoVLPv2), a\nsignificant improvement from the previous generation, by incorporating\ncross-modal fusion directly into the video and language backbones. EgoVLPv2\nlearns strong video-text representation during pre-training and reuses the\ncross-modal attention modules to support different downstream tasks in a\nflexible and efficient manner, reducing fine-tuning costs. Moreover, our\nproposed fusion in the backbone strategy is more lightweight and\ncompute-efficient than stacking additional fusion-specific layers. Extensive\nexperiments on a wide range of VL tasks demonstrate the effectiveness of\nEgoVLPv2 by achieving consistent state-of-the-art performance over strong\nbaselines across all downstream. Our project page can be found at\nhttps://shramanpramanick.github.io/EgoVLPv2/.",
        "authors": [
            "Shraman Pramanick",
            "Yale Song",
            "Sayan Nag",
            "Kevin Qinghong Lin",
            "Hardik Shah",
            "Mike Zheng Shou",
            "Rama Chellappa",
            "Pengchuan Zhang"
        ]
    },
    {
        "title": "On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement",
        "url": "http://arxiv.org/abs/2307.12027",
        "abstract": "Several recent studies advocate the use of spectral discriminators, which\nevaluate the Fourier spectra of images for generative modeling. However, the\neffectiveness of the spectral discriminators is not well interpreted yet. We\ntackle this issue by examining the spectral discriminators in the context of\nperceptual image super-resolution (i.e., GAN-based SR), as SR image quality is\nsusceptible to spectral changes. Our analyses reveal that the spectral\ndiscriminator indeed performs better than the ordinary (a.k.a. spatial)\ndiscriminator in identifying the differences in the high-frequency range;\nhowever, the spatial discriminator holds an advantage in the low-frequency\nrange. Thus, we suggest that the spectral and spatial discriminators shall be\nused simultaneously. Moreover, we improve the spectral discriminators by first\ncalculating the patch-wise Fourier spectrum and then aggregating the spectra by\nTransformer. We verify the effectiveness of the proposed method twofold. On the\none hand, thanks to the additional spectral discriminator, our obtained SR\nimages have their spectra better aligned to those of the real images, which\nleads to a better PD tradeoff. On the other hand, our ensembled discriminator\npredicts the perceptual quality more accurately, as evidenced in the\nno-reference image quality assessment task.",
        "authors": [
            "Xin Luo",
            "Yunan Zhu",
            "Shunxin Xu",
            "Dong Liu"
        ]
    },
    {
        "title": "Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2308.06777",
        "abstract": "Semi-supervised learning is attracting blooming attention, due to its success\nin combining unlabeled data. To mitigate potentially incorrect pseudo labels,\nrecent frameworks mostly set a fixed confidence threshold to discard uncertain\nsamples. This practice ensures high-quality pseudo labels, but incurs a\nrelatively low utilization of the whole unlabeled set. In this work, our key\ninsight is that these uncertain samples can be turned into certain ones, as\nlong as the confusion classes for the top-1 class are detected and removed.\nInvoked by this, we propose a novel method dubbed ShrinkMatch to learn\nuncertain samples. For each uncertain sample, it adaptively seeks a shrunk\nclass space, which merely contains the original top-1 class, as well as\nremaining less likely classes. Since the confusion ones are removed in this\nspace, the re-calculated top-1 confidence can satisfy the pre-defined\nthreshold. We then impose a consistency regularization between a pair of\nstrongly and weakly augmented samples in the shrunk space to strive for\ndiscriminative representations. Furthermore, considering the varied reliability\namong uncertain samples and the gradually improved model during training, we\ncorrespondingly design two reweighting principles for our uncertain loss. Our\nmethod exhibits impressive performance on widely adopted benchmarks. Code is\navailable at https://github.com/LiheYoung/ShrinkMatch.",
        "authors": [
            "Lihe Yang",
            "Zhen Zhao",
            "Lei Qi",
            "Yu Qiao",
            "Yinghuan Shi",
            "Hengshuang Zhao"
        ]
    },
    {
        "title": "Deep Equilibrium Object Detection",
        "url": "http://arxiv.org/abs/2308.09564",
        "abstract": "Query-based object detectors directly decode image features into object\ninstances with a set of learnable queries. These query vectors are\nprogressively refined to stable meaningful representations through a sequence\nof decoder layers, and then used to directly predict object locations and\ncategories with simple FFN heads. In this paper, we present a new query-based\nobject detector (DEQDet) by designing a deep equilibrium decoder. Our DEQ\ndecoder models the query vector refinement as the fixed point solving of an\n{implicit} layer and is equivalent to applying {infinite} steps of refinement.\nTo be more specific to object decoding, we use a two-step unrolled equilibrium\nequation to explicitly capture the query vector refinement. Accordingly, we are\nable to incorporate refinement awareness into the DEQ training with the inexact\ngradient back-propagation (RAG). In addition, to stabilize the training of our\nDEQDet and improve its generalization ability, we devise the deep supervision\nscheme on the optimization path of DEQ with refinement-aware\nperturbation~(RAP). Our experiments demonstrate DEQDet converges faster,\nconsumes less memory, and achieves better results than the baseline counterpart\n(AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries\nachieves the $49.5$ mAP and $33.0$ AP$_s$ on the MS COCO benchmark under\n$2\\times$ training scheme (24 epochs).",
        "authors": [
            "Shuai Wang",
            "Yao Teng",
            "Limin Wang"
        ]
    },
    {
        "title": "Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation",
        "url": "http://arxiv.org/abs/2303.11579",
        "abstract": "In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method with\nJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposed\nfor probabilistic 3D human pose estimation. On the one hand, D3DP generates\nmultiple possible 3D pose hypotheses for a single 2D observation. It gradually\ndiffuses the ground truth 3D poses to a random distribution, and learns a\ndenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.\nThe proposed D3DP is compatible with existing 3D pose estimators and supports\nusers to balance efficiency and accuracy during inference through two\ncustomizable parameters. On the other hand, JPMA is proposed to assemble\nmultiple hypotheses generated by D3DP into a single 3D pose for practical use.\nIt reprojects 3D pose hypotheses to the 2D camera plane, selects the best\nhypothesis joint-by-joint based on the reprojection errors, and combines the\nselected joints into the final pose. The proposed JPMA conducts aggregation at\nthe joint level and makes use of the 2D prior information, both of which have\nbeen overlooked by previous approaches. Extensive experiments on Human3.6M and\nMPI-INF-3DHP datasets show that our method outperforms the state-of-the-art\ndeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Code\nis available at https://github.com/paTRICK-swk/D3DP.",
        "authors": [
            "Wenkang Shan",
            "Zhenhua Liu",
            "Xinfeng Zhang",
            "Zhao Wang",
            "Kai Han",
            "Shanshe Wang",
            "Siwei Ma",
            "Wen Gao"
        ]
    },
    {
        "title": "SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training",
        "url": "http://arxiv.org/abs/2211.11446",
        "abstract": "Video-language pre-training is crucial for learning powerful multi-modal\nrepresentation. However, it typically requires a massive amount of computation.\nIn this paper, we develop SMAUG, an efficient pre-training framework for\nvideo-language models. The foundation component in SMAUG is masked\nautoencoders. Different from prior works which only mask textual inputs, our\nmasking strategy considers both visual and textual modalities, providing a\nbetter cross-modal alignment and saving more pre-training costs. On top of\nthat, we introduce a space-time token sparsification module, which leverages\ncontext information to further select only \"important\" spatial regions and\ntemporal frames for pre-training. Coupling all these designs allows our method\nto enjoy both competitive performances on text-to-video retrieval and video\nquestion answering tasks, and much less pre-training costs by 1.9X or more. For\nexample, our SMAUG only needs about 50 NVIDIA A6000 GPU hours for pre-training\nto attain competitive performances on these two video-language tasks across six\npopular benchmarks.",
        "authors": [
            "Yuanze Lin",
            "Chen Wei",
            "Huiyu Wang",
            "Alan Yuille",
            "Cihang Xie"
        ]
    },
    {
        "title": "Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction",
        "url": "http://arxiv.org/abs/2306.08330",
        "abstract": "Survival prediction is a complicated ordinal regression task that aims to\npredict the ranking risk of death, which generally benefits from the\nintegration of histology and genomic data. Despite the progress in joint\nlearning from pathology and genomics, existing methods still suffer from\nchallenging issues: 1) Due to the large size of pathological images, it is\ndifficult to effectively represent the gigapixel whole slide images (WSIs). 2)\nInteractions within tumor microenvironment (TME) in histology are essential for\nsurvival analysis. Although current approaches attempt to model these\ninteractions via co-attention between histology and genomic data, they focus on\nonly dense local similarity across modalities, which fails to capture global\nconsistency between potential structures, i.e. TME-related interactions of\nhistology and co-expression of genomic data. To address these challenges, we\npropose a Multimodal Optimal Transport-based Co-Attention Transformer framework\nwith global structure consistency, in which optimal transport (OT) is applied\nto match patches of a WSI and genes embeddings for selecting informative\npatches to represent the gigapixel WSI. More importantly, OT-based co-attention\nprovides a global awareness to effectively capture structural interactions\nwithin TME for survival prediction. To overcome high computational complexity\nof OT, we propose a robust and efficient implementation over micro-batch of WSI\npatches by approximating the original OT with unbalanced mini-batch OT.\nExtensive experiments show the superiority of our method on five benchmark\ndatasets compared to the state-of-the-art methods. The code is released.",
        "authors": [
            "Yingxue Xu",
            "Hao Chen"
        ]
    },
    {
        "title": "Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples",
        "url": "http://arxiv.org/abs/2303.16270",
        "abstract": "Federated learning is a popular collaborative learning approach that enables\nclients to train a global model without sharing their local data. Vertical\nfederated learning (VFL) deals with scenarios in which the data on clients have\ndifferent feature spaces but share some overlapping samples. Existing VFL\napproaches suffer from high communication costs and cannot deal efficiently\nwith limited overlapping samples commonly seen in the real world. We propose a\npractical vertical federated learning (VFL) framework called \\textbf{one-shot\nVFL} that can solve the communication bottleneck and the problem of limited\noverlapping samples simultaneously based on semi-supervised learning. We also\npropose \\textbf{few-shot VFL} to improve the accuracy further with just one\nmore communication round between the server and the clients. In our proposed\nframework, the clients only need to communicate with the server once or only a\nfew times. We evaluate the proposed VFL framework on both image and tabular\ndatasets. Our methods can improve the accuracy by more than 46.5\\% and reduce\nthe communication cost by more than 330$\\times$ compared with state-of-the-art\nVFL methods when evaluated on CIFAR-10. Our code will be made publicly\navailable at \\url{https://nvidia.github.io/NVFlare/research/one-shot-vfl}.",
        "authors": [
            "Jingwei Sun",
            "Ziyue Xu",
            "Dong Yang",
            "Vishwesh Nath",
            "Wenqi Li",
            "Can Zhao",
            "Daguang Xu",
            "Yiran Chen",
            "Holger R. Roth"
        ]
    },
    {
        "title": "On the Audio-visual Synchronization for Lip-to-Speech Synthesis",
        "url": "http://arxiv.org/abs/2303.00502",
        "abstract": "Most lip-to-speech (LTS) synthesis models are trained and evaluated under the\nassumption that the audio-video pairs in the dataset are perfectly\nsynchronized. In this work, we show that the commonly used audio-visual\ndatasets, such as GRID, TCD-TIMIT, and Lip2Wav, can have data asynchrony\nissues. Training lip-to-speech with such datasets may further cause the model\nasynchrony issue -- that is, the generated speech and the input video are out\nof sync. To address these asynchrony issues, we propose a synchronized\nlip-to-speech (SLTS) model with an automatic synchronization mechanism (ASM) to\ncorrect data asynchrony and penalize model asynchrony. We further demonstrate\nthe limitation of the commonly adopted evaluation metrics for LTS with\nasynchronous test data and introduce an audio alignment frontend before the\nmetrics sensitive to time alignment for better evaluation. We compare our\nmethod with state-of-the-art approaches on conventional and time-aligned\nmetrics to show the benefits of synchronization training.",
        "authors": [
            "Zhe Niu",
            "Brian Mak"
        ]
    },
    {
        "title": "BallGAN: 3D-aware Image Synthesis with a Spherical Background",
        "url": "http://arxiv.org/abs/2301.09091",
        "abstract": "3D-aware GANs aim to synthesize realistic 3D scenes such that they can be\nrendered in arbitrary perspectives to produce images. Although previous methods\nproduce realistic images, they suffer from unstable training or degenerate\nsolutions where the 3D geometry is unnatural. We hypothesize that the 3D\ngeometry is underdetermined due to the insufficient constraint, i.e., being\nclassified as real image to the discriminator is not enough. To solve this\nproblem, we propose to approximate the background as a spherical surface and\nrepresent a scene as a union of the foreground placed in the sphere and the\nthin spherical background. It reduces the degree of freedom in the background\nfield. Accordingly, we modify the volume rendering equation and incorporate\ndedicated constraints to design a novel 3D-aware GAN framework named BallGAN.\nBallGAN has multiple advantages as follows. 1) It produces more reasonable 3D\ngeometry; the images of a scene across different viewpoints have better\nphotometric consistency and fidelity than the state-of-the-art methods. 2) The\ntraining becomes much more stable. 3) The foreground can be separately rendered\non top of different arbitrary backgrounds.",
        "authors": [
            "Minjung Shin",
            "Yunji Seo",
            "Jeongmin Bae",
            "Young Sun Choi",
            "Hyunsu Kim",
            "Hyeran Byun",
            "Youngjung Uh"
        ]
    },
    {
        "title": "Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models",
        "url": "http://arxiv.org/abs/2303.08010",
        "abstract": "Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.",
        "authors": [
            "Guoxuan Xia",
            "Christos-Savvas Bouganis"
        ]
    },
    {
        "title": "AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism",
        "url": "http://arxiv.org/abs/2309.00796",
        "abstract": "Generating 3D human motion based on textual descriptions has been a research\nfocus in recent years. It requires the generated motion to be diverse, natural,\nand conform to the textual description. Due to the complex spatio-temporal\nnature of human motion and the difficulty in learning the cross-modal\nrelationship between text and motion, text-driven motion generation is still a\nchallenging problem. To address these issues, we propose \\textbf{AttT2M}, a\ntwo-stage method with multi-perspective attention mechanism: \\textbf{body-part\nattention} and \\textbf{global-local motion-text attention}. The former focuses\non the motion embedding perspective, which means introducing a body-part\nspatio-temporal encoder into VQ-VAE to learn a more expressive discrete latent\nspace. The latter is from the cross-modal perspective, which is used to learn\nthe sentence-level and word-level motion-text cross-modal relationship. The\ntext-driven motion is finally generated with a generative transformer.\nExtensive experiments conducted on HumanML3D and KIT-ML demonstrate that our\nmethod outperforms the current state-of-the-art works in terms of qualitative\nand quantitative evaluation, and achieve fine-grained synthesis and\naction2motion. Our code is in https://github.com/ZcyMonkey/AttT2M",
        "authors": [
            "Chongyang Zhong",
            "Lei Hu",
            "Zihao Zhang",
            "Shihong Xia"
        ]
    },
    {
        "title": "A Theory of Topological Derivatives for Inverse Rendering of Geometry",
        "url": "http://arxiv.org/abs/2308.09865",
        "abstract": "We introduce a theoretical framework for differentiable surface evolution\nthat allows discrete topology changes through the use of topological\nderivatives for variational optimization of image functionals. While prior\nmethods for inverse rendering of geometry rely on silhouette gradients for\ntopology changes, such signals are sparse. In contrast, our theory derives\ntopological derivatives that relate the introduction of vanishing holes and\nphases to changes in image intensity. As a result, we enable differentiable\nshape perturbations in the form of hole or phase nucleation. We validate the\nproposed theory with optimization of closed curves in 2D and surfaces in 3D to\nlend insights into limitations of current methods and enable improved\napplications such as image vectorization, vector-graphics generation from text\nprompts, single-image reconstruction of shape ambigrams and multi-view 3D\nreconstruction.",
        "authors": [
            "Ishit Mehta",
            "Manmohan Chandraker",
            "Ravi Ramamoorthi"
        ]
    },
    {
        "title": "Canonical Factors for Hybrid Neural Fields",
        "url": "http://arxiv.org/abs/2308.15461",
        "abstract": "Factored feature volumes offer a simple way to build more compact, efficient,\nand intepretable neural fields, but also introduce biases that are not\nnecessarily beneficial for real-world data. In this work, we (1) characterize\nthe undesirable biases that these architectures have for axis-aligned signals\n-- they can lead to radiance field reconstruction differences of as high as 2\nPSNR -- and (2) explore how learning a set of canonicalizing transformations\ncan improve representations by removing these biases. We prove in a\ntwo-dimensional model problem that simultaneously learning these\ntransformations together with scene appearance succeeds with drastically\nimproved efficiency. We validate the resulting architectures, which we call\nTILTED, using image, signed distance, and radiance field reconstruction tasks,\nwhere we observe improvements across quality, robustness, compactness, and\nruntime. Results demonstrate that TILTED can enable capabilities comparable to\nbaselines that are 2x larger, while highlighting weaknesses of neural field\nevaluation procedures.",
        "authors": [
            "Brent Yi",
            "Weijia Zeng",
            "Sam Buchanan",
            "Yi Ma"
        ]
    },
    {
        "title": "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation",
        "url": "http://arxiv.org/abs/2301.00805",
        "abstract": "In this work, we focus on open vocabulary instance segmentation to expand a\nsegmentation model to classify and segment instance-level novel categories.\nPrevious approaches have relied on massive caption datasets and complex\npipelines to establish one-to-one mappings between image regions and words in\ncaptions. However, such methods build noisy supervision by matching non-visible\nwords to image regions, such as adjectives and verbs. Meanwhile, context words\nare also important for inferring the existence of novel objects as they show\nhigh inter-correlations with novel categories. To overcome these limitations,\nwe devise a joint \\textbf{Caption Grounding and Generation (CGG)} framework,\nwhich incorporates a novel grounding loss that only focuses on matching object\nnouns to improve learning efficiency. We also introduce a caption generation\nhead that enables additional supervision and contextual modeling as a\ncomplementation to the grounding loss. Our analysis and results demonstrate\nthat grounding and generation components complement each other, significantly\nenhancing the segmentation performance for novel classes. Experiments on the\nCOCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS)\nand Open Set Panoptic Segmentation (OSPS) demonstrate the superiority of the\nCGG. Specifically, CGG achieves a substantial improvement of 6.8% mAP for novel\nclasses without extra data on the OVIS task and 15% PQ improvements for novel\nclasses on the OSPS benchmark.",
        "authors": [
            "Jianzong Wu",
            "Xiangtai Li",
            "Henghui Ding",
            "Xia Li",
            "Guangliang Cheng",
            "Yunhai Tong",
            "Chen Change Loy"
        ]
    },
    {
        "title": "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces",
        "url": "http://arxiv.org/abs/2303.06146",
        "abstract": "Recent advances in face manipulation using StyleGAN have produced impressive\nresults. However, StyleGAN is inherently limited to cropped aligned faces at a\nfixed image resolution it is pre-trained on. In this paper, we propose a simple\nand effective solution to this limitation by using dilated convolutions to\nrescale the receptive fields of shallow layers in StyleGAN, without altering\nany model parameters. This allows fixed-size small features at shallow layers\nto be extended into larger ones that can accommodate variable resolutions,\nmaking them more robust in characterizing unaligned faces. To enable real face\ninversion and manipulation, we introduce a corresponding encoder that provides\nthe first-layer feature of the extended StyleGAN in addition to the latent\nstyle code. We validate the effectiveness of our method using unaligned face\ninputs of various resolutions in a diverse set of face manipulation tasks,\nincluding facial attribute editing, super-resolution, sketch/mask-to-face\ntranslation, and face toonification.",
        "authors": [
            "Shuai Yang",
            "Liming Jiang",
            "Ziwei Liu",
            "Chen Change Loy"
        ]
    },
    {
        "title": "DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition",
        "url": "http://arxiv.org/abs/2303.14953",
        "abstract": "Gait recognition is a biometric technology that recognizes the identity of\nhumans through their walking patterns. Compared with other biometric\ntechnologies, gait recognition is more difficult to disguise and can be applied\nto the condition of long-distance without the cooperation of subjects. Thus, it\nhas unique potential and wide application for crime prevention and social\nsecurity. At present, most gait recognition methods directly extract features\nfrom the video frames to establish representations. However, these\narchitectures learn representations from different features equally but do not\npay enough attention to dynamic features, which refers to a representation of\ndynamic parts of silhouettes over time (e.g. legs). Since dynamic parts of the\nhuman body are more informative than other parts (e.g. bags) during walking, in\nthis paper, we propose a novel and high-performance framework named DyGait.\nThis is the first framework on gait recognition that is designed to focus on\nthe extraction of dynamic features. Specifically, to take full advantage of the\ndynamic information, we propose a Dynamic Augmentation Module (DAM), which can\nautomatically establish spatial-temporal feature representations of the dynamic\nparts of the human body. The experimental results show that our DyGait network\noutperforms other state-of-the-art gait recognition methods. It achieves an\naverage Rank-1 accuracy of 71.4% on the GREW dataset, 66.3% on the Gait3D\ndataset, 98.4% on the CASIA-B dataset and 98.3% on the OU-MVLP dataset.",
        "authors": [
            "Ming Wang",
            "Xianda Guo",
            "Beibei Lin",
            "Tian Yang",
            "Zheng Zhu",
            "Lincheng Li",
            "Shunli Zhang",
            "Xin Yu"
        ]
    },
    {
        "title": "When Do Curricula Work in Federated Learning?",
        "url": "http://arxiv.org/abs/2212.12712",
        "abstract": "An oft-cited open problem of federated learning is the existence of data\nheterogeneity at the clients. One pathway to understanding the drastic accuracy\ndrop in federated learning is by scrutinizing the behavior of the clients' deep\nmodels on data with different levels of \"difficulty\", which has been left\nunaddressed. In this paper, we investigate a different and rarely studied\ndimension of FL: ordered learning. Specifically, we aim to investigate how\nordered learning principles can contribute to alleviating the heterogeneity\neffects in FL. We present theoretical analysis and conduct extensive empirical\nstudies on the efficacy of orderings spanning three kinds of learning:\ncurriculum, anti-curriculum, and random curriculum. We find that curriculum\nlearning largely alleviates non-IIDness. Interestingly, the more disparate the\ndata distributions across clients the more they benefit from ordered learning.\nWe provide analysis explaining this phenomenon, specifically indicating how\ncurriculum training appears to make the objective landscape progressively less\nconvex, suggesting fast converging iterations at the beginning of the training\nprocedure. We derive quantitative results of convergence for both convex and\nnonconvex objectives by modeling the curriculum training on federated devices\nas local SGD with locally biased stochastic gradients. Also, inspired by\nordered learning, we propose a novel client selection technique that benefits\nfrom the real-world disparity in the clients. Our proposed approach to client\nselection has a synergic effect when applied together with ordered learning in\nFL.",
        "authors": [
            "Saeed Vahidian",
            "Sreevatsank Kadaveru",
            "Woonjoon Baek",
            "Weijia Wang",
            "Vyacheslav Kungurtsev",
            "Chen Chen",
            "Mubarak Shah",
            "Bill Lin"
        ]
    },
    {
        "title": "GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds",
        "url": "http://arxiv.org/abs/2308.13175",
        "abstract": "Learning implicit representations has been a widely used solution for surface\nreconstruction from 3D point clouds. The latest methods infer a distance or\noccupancy field by overfitting a neural network on a single point cloud.\nHowever, these methods suffer from a slow inference due to the slow convergence\nof neural networks and the extensive calculation of distances to surface\npoints, which limits them to small scale points. To resolve the scalability\nissue in surface reconstruction, we propose GridPull to improve the efficiency\nof learning implicit representations from large scale point clouds. Our novelty\nlies in the fast inference of a discrete distance field defined on grids\nwithout using any neural components. To remedy the lack of continuousness\nbrought by neural networks, we introduce a loss function to encourage\ncontinuous distances and consistent gradients in the field during pulling\nqueries onto the surface in grids near to the surface. We use uniform grids for\na fast grid search to localize sampled queries, and organize surface points in\na tree structure to speed up the calculation of distances to the surface. We do\nnot rely on learning priors or normal supervision during optimization, and\nachieve superiority over the latest methods in terms of complexity and\naccuracy. We evaluate our method on shape and scene benchmarks, and report\nnumerical and visual comparisons with the latest methods to justify our\neffectiveness and superiority. The code is available at\nhttps://github.com/chenchao15/GridPull.",
        "authors": [
            "Chao Chen",
            "Yu-Shen Liu",
            "Zhizhong Han"
        ]
    },
    {
        "title": "Audio-Visual Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2308.11073",
        "abstract": "In this paper, we introduce audio-visual class-incremental learning, a\nclass-incremental learning scenario for audio-visual video recognition. We\ndemonstrate that joint audio-visual modeling can improve class-incremental\nlearning, but current methods fail to preserve semantic similarity between\naudio and visual features as incremental step grows. Furthermore, we observe\nthat audio-visual correlations learned in previous tasks can be forgotten as\nincremental steps progress, leading to poor performance. To overcome these\nchallenges, we propose AV-CIL, which incorporates Dual-Audio-Visual Similarity\nConstraint (D-AVSC) to maintain both instance-aware and class-aware semantic\nsimilarity between audio-visual modalities and Visual Attention Distillation\n(VAD) to retain previously learned audio-guided visual attentive ability. We\ncreate three audio-visual class-incremental datasets, AVE-Class-Incremental\n(AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and\nVGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and\nVGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and\nVS100-CI demonstrate that AV-CIL significantly outperforms existing\nclass-incremental learning methods in audio-visual class-incremental learning.\nCode and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.",
        "authors": [
            "Weiguo Pian",
            "Shentong Mo",
            "Yunhui Guo",
            "Yapeng Tian"
        ]
    },
    {
        "title": "GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding",
        "url": "http://arxiv.org/abs/2303.11325",
        "abstract": "Multi-view camera-based 3D detection is a challenging problem in computer\nvision. Recent works leverage a pretrained LiDAR detection model to transfer\nknowledge to a camera-based student network. However, we argue that there is a\nmajor domain gap between the LiDAR BEV features and the camera-based BEV\nfeatures, as they have different characteristics and are derived from different\nsources. In this paper, we propose Geometry Enhanced Masked Image Modeling\n(GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune\nparadigm for improving the multi-view camera-based 3D detection. GeoMIM is a\nmulti-camera vision transformer with Cross-View Attention (CVA) blocks that\nuses LiDAR BEV features encoded by the pretrained BEV model as learning\ntargets. During pretraining, GeoMIM's decoder has a semantic branch completing\ndense perspective-view features and the other geometry branch reconstructing\ndense perspective-view depth maps. The depth branch is designed to be\ncamera-aware by inputting the camera's parameters for better transfer\ncapability. Extensive results demonstrate that GeoMIM outperforms existing\nmethods on nuScenes benchmark, achieving state-of-the-art performance for\ncamera-based 3D object detection and 3D segmentation. Code and pretrained\nmodels are available at https://github.com/Sense-X/GeoMIM.",
        "authors": [
            "Jihao Liu",
            "Tai Wang",
            "Boxiao Liu",
            "Qihang Zhang",
            "Yu Liu",
            "Hongsheng Li"
        ]
    },
    {
        "title": "Towards Viewpoint-Invariant Visual Recognition via Adversarial Training",
        "url": "http://arxiv.org/abs/2307.10235",
        "abstract": "Visual recognition models are not invariant to viewpoint changes in the 3D\nworld, as different viewing directions can dramatically affect the predictions\ngiven the same object. Although many efforts have been devoted to making neural\nnetworks invariant to 2D image translations and rotations, viewpoint invariance\nis rarely investigated. As most models process images in the perspective view,\nit is challenging to impose invariance to 3D viewpoint changes based only on 2D\ninputs. Motivated by the success of adversarial training in promoting model\nrobustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) to\nimprove viewpoint robustness of common image classifiers. By regarding\nviewpoint transformation as an attack, VIAT is formulated as a minimax\noptimization problem, where the inner maximization characterizes diverse\nadversarial viewpoints by learning a Gaussian mixture distribution based on a\nnew attack GMVFool, while the outer minimization trains a viewpoint-invariant\nclassifier by minimizing the expected loss over the worst-case adversarial\nviewpoint distributions. To further improve the generalization performance, a\ndistribution sharing strategy is introduced leveraging the transferability of\nadversarial viewpoints across objects. Experiments validate the effectiveness\nof VIAT in improving the viewpoint robustness of various image classifiers\nbased on the diversity of adversarial viewpoints generated by GMVFool.",
        "authors": [
            "Shouwei Ruan",
            "Yinpeng Dong",
            "Hang Su",
            "Jianteng Peng",
            "Ning Chen",
            "Xingxing Wei"
        ]
    },
    {
        "title": "Helping Hands: An Object-Aware Ego-Centric Video Recognition Model",
        "url": "http://arxiv.org/abs/2308.07918",
        "abstract": "We introduce an object-aware decoder for improving the performance of\nspatio-temporal representations on ego-centric videos. The key idea is to\nenhance object-awareness during training by tasking the model to predict hand\npositions, object positions, and the semantic label of the objects using paired\ncaptions when available. At inference time the model only requires RGB frames\nas inputs, and is able to track and ground objects (although it has not been\ntrained explicitly for this). We demonstrate the performance of the\nobject-aware representations learnt by our model, by: (i) evaluating it for\nstrong transfer, i.e. through zero-shot testing, on a number of downstream\nvideo-text retrieval and classification benchmarks; and (ii) by using the\nrepresentations learned as input for long-term video understanding tasks (e.g.\nEpisodic Memory in Ego4D). In all cases the performance improves over the state\nof the art -- even compared to networks trained with far larger batch sizes. We\nalso show that by using noisy image-level detection as pseudo-labels in\ntraining, the model learns to provide better bounding boxes using video\nconsistency, as well as grounding the words in the associated text\ndescriptions. Overall, we show that the model can act as a drop-in replacement\nfor an ego-centric video model to improve performance through visual-text\ngrounding.",
        "authors": [
            "Chuhan Zhang",
            "Ankush Gupta",
            "Andrew Zisserman"
        ]
    },
    {
        "title": "RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation",
        "url": "http://arxiv.org/abs/2309.09301",
        "abstract": "The current interacting hand (IH) datasets are relatively simplistic in terms\nof background and texture, with hand joints being annotated by a machine\nannotator, which may result in inaccuracies, and the diversity of pose\ndistribution is limited. However, the variability of background, pose\ndistribution, and texture can greatly influence the generalization ability.\nTherefore, we present a large-scale synthetic dataset RenderIH for interacting\nhands with accurate and diverse pose annotations. The dataset contains 1M\nphoto-realistic images with varied backgrounds, perspectives, and hand\ntextures. To generate natural and diverse interacting poses, we propose a new\npose optimization algorithm. Additionally, for better pose estimation accuracy,\nwe introduce a transformer-based pose estimation network, TransHand, to\nleverage the correlation between interacting hands and verify the effectiveness\nof RenderIH in improving results. Our dataset is model-agnostic and can improve\nmore accuracy of any hand pose estimation method in comparison to other real or\nsynthetic datasets. Experiments have shown that pretraining on our synthetic\ndata can significantly decrease the error from 6.76mm to 5.79mm, and our\nTranshand surpasses contemporary methods. Our dataset and code are available at\nhttps://github.com/adwardlee/RenderIH.",
        "authors": [
            "Lijun Li",
            "Linrui Tian",
            "Xindi Zhang",
            "Qi Wang",
            "Bang Zhang",
            "Mengyuan Liu",
            "Chen Chen"
        ]
    },
    {
        "title": "Multi-Metrics Adaptively Identifies Backdoors in Federated Learning",
        "url": "http://arxiv.org/abs/2303.06601",
        "abstract": "The decentralized and privacy-preserving nature of federated learning (FL)\nmakes it vulnerable to backdoor attacks aiming to manipulate the behavior of\nthe resulting model on specific adversary-chosen inputs. However, most existing\ndefenses based on statistical differences take effect only against specific\nattacks, especially when the malicious gradients are similar to benign ones or\nthe data are highly non-independent and identically distributed (non-IID). In\nthis paper, we revisit the distance-based defense methods and discover that i)\nEuclidean distance becomes meaningless in high dimensions and ii) malicious\ngradients with diverse characteristics cannot be identified by a single metric.\nTo this end, we present a simple yet effective defense strategy with\nmulti-metrics and dynamic weighting to identify backdoors adaptively.\nFurthermore, our novel defense has no reliance on predefined assumptions over\nattack settings or data distributions and little impact on benign performance.\nTo evaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on different datasets under various attack settings, where our\nmethod achieves the best defensive performance. For instance, we achieve the\nlowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showing\nsignificant superiority over previous defenses. The results also demonstrate\nthat our method can be well-adapted to a wide range of non-IID degrees without\nsacrificing the benign performance.",
        "authors": [
            "Siquan Huang",
            "Yijiang Li",
            "Chong Chen",
            "Leyu Shi",
            "Ying Gao"
        ]
    },
    {
        "title": "DETRDistill: A Universal Knowledge Distillation Framework for DETR-families",
        "url": "http://arxiv.org/abs/2211.10156",
        "abstract": "Transformer-based detectors (DETRs) are becoming popular for their simple\nframework, but the large model size and heavy time consumption hinder their\ndeployment in the real world. While knowledge distillation (KD) can be an\nappealing technique to compress giant detectors into small ones for comparable\ndetection performance and low inference cost. Since DETRs formulate object\ndetection as a set prediction problem, existing KD methods designed for classic\nconvolution-based detectors may not be directly applicable. In this paper, we\npropose DETRDistill, a novel knowledge distillation method dedicated to\nDETR-families. Specifically, we first design a Hungarian-matching logits\ndistillation to encourage the student model to have the exact predictions as\nthat of teacher DETRs. Next, we propose a target-aware feature distillation to\nhelp the student model learn from the object-centric features of the teacher\nmodel. Finally, in order to improve the convergence rate of the student DETR,\nwe introduce a query-prior assignment distillation to speed up the student\nmodel learning from well-trained queries and stable assignment of the teacher\nmodel. Extensive experimental results on the COCO dataset validate the\neffectiveness of our approach. Notably, DETRDistill consistently improves\nvarious DETRs by more than 2.0 mAP, even surpassing their teacher models.",
        "authors": [
            "Jiahao Chang",
            "Shuo Wang",
            "Haiming Xu",
            "Zehui Chen",
            "Chenhongyi Yang",
            "Feng Zhao"
        ]
    },
    {
        "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
        "url": "http://arxiv.org/abs/2307.16525",
        "abstract": "Image-to-text generation aims to describe images using natural language.\nRecently, zero-shot image captioning based on pre-trained vision-language\nmodels (VLMs) and large language models (LLMs) has made significant progress.\nHowever, we have observed and empirically demonstrated that these methods are\nsusceptible to modality bias induced by LLMs and tend to generate descriptions\ncontaining objects (entities) that do not actually exist in the image but\nfrequently appear during training (i.e., object hallucination). In this paper,\nwe propose ViECap, a transferable decoding model that leverages entity-aware\ndecoding to generate descriptions in both seen and unseen scenarios. ViECap\nincorporates entity-aware hard prompts to guide LLMs' attention toward the\nvisual entities present in the image, enabling coherent caption generation\nacross diverse scenes. With entity-aware hard prompts, ViECap is capable of\nmaintaining performance when transferring from in-domain to out-of-domain\nscenarios. Extensive experiments demonstrate that ViECap sets a new\nstate-of-the-art cross-domain (transferable) captioning and performs\ncompetitively in-domain captioning compared to previous VLMs-based zero-shot\nmethods. Our code is available at: https://github.com/FeiElysia/ViECap",
        "authors": [
            "Junjie Fei",
            "Teng Wang",
            "Jinrui Zhang",
            "Zhenyu He",
            "Chengjie Wang",
            "Feng Zheng"
        ]
    },
    {
        "title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model",
        "url": "http://arxiv.org/abs/2304.01116",
        "abstract": "3D human motion generation is crucial for creative industry. Recent advances\nrely on generative models with domain knowledge for text-driven motion\ngeneration, leading to substantial progress in capturing common motions.\nHowever, the performance on more diverse motions remains unsatisfactory. In\nthis work, we propose ReMoDiffuse, a diffusion-model-based motion generation\nframework that integrates a retrieval mechanism to refine the denoising\nprocess. ReMoDiffuse enhances the generalizability and diversity of text-driven\nmotion generation with three key designs: 1) Hybrid Retrieval finds appropriate\nreferences from the database in terms of both semantic and kinematic\nsimilarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval\nknowledge, adapting to the difference between retrieved samples and the target\nmotion sequence. 3) Condition Mixture better utilizes the retrieval database\nduring inference, overcoming the scale sensitivity in classifier-free guidance.\nExtensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art\nmethods by balancing both text-motion consistency and motion quality,\nespecially for more diverse motion generation.",
        "authors": [
            "Mingyuan Zhang",
            "Xinying Guo",
            "Liang Pan",
            "Zhongang Cai",
            "Fangzhou Hong",
            "Huirong Li",
            "Lei Yang",
            "Ziwei Liu"
        ]
    },
    {
        "title": "Towards Multi-Layered 3D Garments Animation",
        "url": "http://arxiv.org/abs/2305.10418",
        "abstract": "Mimicking realistic dynamics in 3D garment animations is a challenging task\ndue to the complex nature of multi-layered garments and the variety of outer\nforces involved. Existing approaches mostly focus on single-layered garments\ndriven by only human bodies and struggle to handle general scenarios. In this\npaper, we propose a novel data-driven method, called LayersNet, to model\ngarment-level animations as particle-wise interactions in a micro physics\nsystem. We improve simulation efficiency by representing garments as\npatch-level particles in a two-level structural hierarchy. Moreover, we\nintroduce a novel Rotation Equivalent Transformation that leverages the\nrotation invariance and additivity of physics systems to better model outer\nforces. To verify the effectiveness of our approach and bridge the gap between\nexperimental environments and real-world scenarios, we introduce a new\nchallenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900\ndifferent combinations of multi-layered garments driven by both human bodies\nand randomly sampled wind. Our experiments show that LayersNet achieves\nsuperior performance both quantitatively and qualitatively. We will make the\ndataset and code publicly available at\nhttps://mmlab-ntu.github.io/project/layersnet/index.html .",
        "authors": [
            "Yidi Shao",
            "Chen Change Loy",
            "Bo Dai"
        ]
    },
    {
        "title": "LiveHand: Real-time and Photorealistic Neural Hand Rendering",
        "url": "http://arxiv.org/abs/2302.07672",
        "abstract": "The human hand is the main medium through which we interact with our\nsurroundings, making its digitization an important problem. While there are\nseveral works modeling the geometry of hands, little attention has been paid to\ncapturing photo-realistic appearance. Moreover, for applications in extended\nreality and gaming, real-time rendering is critical. We present the first\nneural-implicit approach to photo-realistically render hands in real-time. This\nis a challenging problem as hands are textured and undergo strong articulations\nwith pose-dependent effects. However, we show that this aim is achievable\nthrough our carefully designed method. This includes training on a\nlow-resolution rendering of a neural radiance field, together with a\n3D-consistent super-resolution module and mesh-guided sampling and space\ncanonicalization. We demonstrate a novel application of perceptual loss on the\nimage space, which is critical for learning details accurately. We also show a\nlive demo where we photo-realistically render the human hand in real-time for\nthe first time, while also modeling pose- and view-dependent appearance\neffects. We ablate all our design choices and show that they optimize for\nrendering speed and quality. Video results and our code can be accessed from\nhttps://vcai.mpi-inf.mpg.de/projects/LiveHand/",
        "authors": [
            "Akshay Mundra",
            "Mallikarjun B R",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
        ]
    },
    {
        "title": "Advancing Referring Expression Segmentation Beyond Single Image",
        "url": "http://arxiv.org/abs/2305.12452",
        "abstract": "Referring Expression Segmentation (RES) is a widely explored multi-modal\ntask, which endeavors to segment the pre-existing object within a single image\nwith a given linguistic expression. However, in broader real-world scenarios,\nit is not always possible to determine if the described object exists in a\nspecific image. Typically, we have a collection of images, some of which may\ncontain the described objects. The current RES setting curbs its practicality\nin such situations. To overcome this limitation, we propose a more realistic\nand general setting, named Group-wise Referring Expression Segmentation (GRES),\nwhich expands RES to a collection of related images, allowing the described\nobjects to be present in a subset of input images. To support this new setting,\nwe introduce an elaborately compiled dataset named Grouped Referring Dataset\n(GRD), containing complete group-wise annotations of target objects described\nby given expressions. We also present a baseline method named Grouped Referring\nSegmenter (GRSer), which explicitly captures the language-vision and\nintra-group vision-vision interactions to achieve state-of-the-art results on\nthe proposed GRES and related tasks, such as Co-Salient Object Detection and\nRES. Our dataset and codes will be publicly released in\nhttps://github.com/yixuan730/group-res.",
        "authors": [
            "Yixuan Wu",
            "Zhao Zhang",
            "Xie Chi",
            "Feng Zhu",
            "Rui Zhao"
        ]
    },
    {
        "title": "LogicSeg: Parsing Visual Semantics with Neural Logic Learning and Reasoning",
        "url": "http://arxiv.org/abs/2309.13556",
        "abstract": "Current high-performance semantic segmentation models are purely data-driven\nsub-symbolic approaches and blind to the structured nature of the visual world.\nThis is in stark contrast to human cognition which abstracts visual perceptions\nat multiple levels and conducts symbolic reasoning with such structured\nabstraction. To fill these fundamental gaps, we devise LOGICSEG, a holistic\nvisual semantic parser that integrates neural inductive learning and logic\nreasoning with both rich data and symbolic knowledge. In particular, the\nsemantic concepts of interest are structured as a hierarchy, from which a set\nof constraints are derived for describing the symbolic relations and formalized\nas first-order logic rules. After fuzzy logic-based continuous relaxation,\nlogical formulae are grounded onto data and neural computational graphs, hence\nenabling logic-induced network training. During inference, logical constraints\nare packaged into an iterative process and injected into the network in a form\nof several matrix multiplications, so as to achieve hierarchy-coherent\nprediction with logic reasoning. These designs together make LOGICSEG a general\nand compact neural-logic machine that is readily integrated into existing\nsegmentation models. Extensive experiments over four datasets with various\nsegmentation models and backbones verify the effectiveness and generality of\nLOGICSEG. We believe this study opens a new avenue for visual semantic parsing.",
        "authors": [
            "Liulei Li",
            "Wenguan Wang",
            "Yi Yang"
        ]
    },
    {
        "title": "The Devil is in the Upsampling: Architectural Decisions Made Simpler for Denoising with Deep Image Prior",
        "url": "http://arxiv.org/abs/2304.11409",
        "abstract": "Deep Image Prior (DIP) shows that some network architectures naturally bias\ntowards smooth images and resist noises, a phenomenon known as spectral bias.\nImage denoising is an immediate application of this property. Although DIP has\nremoved the requirement of large training sets, it still presents two practical\nchallenges for denoising: architectural design and noise-fitting, which are\noften intertwined. Existing methods mostly handcraft or search for the\narchitecture from a large design space, due to the lack of understanding on how\nthe architectural choice corresponds to the image. In this study, we analyze\nfrom a frequency perspective to demonstrate that the unlearnt upsampling is the\nmain driving force behind the denoising phenomenon in DIP. This finding then\nleads to strategies for estimating a suitable architecture for every image\nwithout a laborious search. Extensive experiments show that the estimated\narchitectures denoise and preserve the textural details better than current\nmethods with up to 95% fewer parameters. The under-parameterized nature also\nmakes them especially robust to a higher level of noise.",
        "authors": [
            "Yilin Liu",
            "Jiang Li",
            "Yunkui Pang",
            "Dong Nie",
            "Pew-thian Yap"
        ]
    },
    {
        "title": "Texture Learning Domain Randomization for Domain Generalized Segmentation",
        "url": "http://arxiv.org/abs/2303.11546",
        "abstract": "Deep Neural Networks (DNNs)-based semantic segmentation models trained on a\nsource domain often struggle to generalize to unseen target domains, i.e., a\ndomain gap problem. Texture often contributes to the domain gap, making DNNs\nvulnerable to domain shift because they are prone to be texture-biased.\nExisting Domain Generalized Semantic Segmentation (DGSS) methods have\nalleviated the domain gap problem by guiding models to prioritize shape over\ntexture. On the other hand, shape and texture are two prominent and\ncomplementary cues in semantic segmentation. This paper argues that leveraging\ntexture is crucial for improving performance in DGSS. Specifically, we propose\na novel framework, coined Texture Learning Domain Randomization (TLDR). TLDR\nincludes two novel losses to effectively enhance texture learning in DGSS: (1)\na texture regularization loss to prevent overfitting to source domain textures\nby using texture features from an ImageNet pre-trained model and (2) a texture\ngeneralization loss that utilizes random style images to learn diverse texture\nrepresentations in a self-supervised manner. Extensive experimental results\ndemonstrate the superiority of the proposed TLDR; e.g., TLDR achieves 46.5 mIoU\non GTA-to-Cityscapes using ResNet-50, which improves the prior state-of-the-art\nmethod by 1.9 mIoU. The source code is available at\nhttps://github.com/ssssshwan/TLDR.",
        "authors": [
            "Sunghwan Kim",
            "Dae-hwan Kim",
            "Hoseong Kim"
        ]
    },
    {
        "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
        "url": "http://arxiv.org/abs/2308.03685",
        "abstract": "Recent advances in foundation models present new opportunities for\ninterpretable visual recognition -- one can first query Large Language Models\n(LLMs) to obtain a set of attributes that describe each class, then apply\nvision-language models to classify images via these attributes. Pioneering work\nshows that querying thousands of attributes can achieve performance competitive\nwith image features. However, our further investigation on 8 datasets reveals\nthat LLM-generated attributes in a large quantity perform almost the same as\nrandom words. This surprising finding suggests that significant noise may be\npresent in these attributes. We hypothesize that there exist subsets of\nattributes that can maintain the classification performance with much smaller\nsizes, and propose a novel learning-to-search method to discover those concise\nsets of attributes. As a result, on the CUB dataset, our method achieves\nperformance close to that of massive LLM-generated attributes (e.g., 10k\nattributes for CUB), yet using only 32 attributes in total to distinguish 200\nbird species. Furthermore, our new paradigm demonstrates several additional\nbenefits: higher interpretability and interactivity for humans, and the ability\nto summarize knowledge for a recognition task.",
        "authors": [
            "An Yan",
            "Yu Wang",
            "Yiwu Zhong",
            "Chengyu Dong",
            "Zexue He",
            "Yujie Lu",
            "William Wang",
            "Jingbo Shang",
            "Julian McAuley"
        ]
    },
    {
        "title": "Learning Unified Decompositional and Compositional NeRF for Editable Novel View Synthesis",
        "url": "http://arxiv.org/abs/2308.02840",
        "abstract": "Implicit neural representations have shown powerful capacity in modeling\nreal-world 3D scenes, offering superior performance in novel view synthesis. In\nthis paper, we target a more challenging scenario, i.e., joint scene novel view\nsynthesis and editing based on implicit neural scene representations.\nState-of-the-art methods in this direction typically consider building separate\nnetworks for these two tasks (i.e., view synthesis and editing). Thus, the\nmodeling of interactions and correlations between these two tasks is very\nlimited, which, however, is critical for learning high-quality scene\nrepresentations. To tackle this problem, in this paper, we propose a unified\nNeural Radiance Field (NeRF) framework to effectively perform joint scene\ndecomposition and composition for modeling real-world scenes. The decomposition\naims at learning disentangled 3D representations of different objects and the\nbackground, allowing for scene editing, while scene composition models an\nentire scene representation for novel view synthesis. Specifically, with a\ntwo-stage NeRF framework, we learn a coarse stage for predicting a global\nradiance field as guidance for point sampling, and in the second fine-grained\nstage, we perform scene decomposition by a novel one-hot object radiance field\nregularization module and a pseudo supervision via inpainting to handle\nambiguous background regions occluded by objects. The decomposed object-level\nradiance fields are further composed by using activations from the\ndecomposition module. Extensive quantitative and qualitative results show the\neffectiveness of our method for scene decomposition and composition,\noutperforming state-of-the-art methods for both novel-view synthesis and\nediting tasks.",
        "authors": [
            "Yuxin Wang",
            "Wayne Wu",
            "Dan Xu"
        ]
    },
    {
        "title": "Label-Noise Learning with Intrinsically Long-Tailed Data",
        "url": "http://arxiv.org/abs/2208.09833",
        "abstract": "Label noise is one of the key factors that lead to the poor generalization of\ndeep learning models. Existing label-noise learning methods usually assume that\nthe ground-truth classes of the training data are balanced. However, the\nreal-world data is often imbalanced, leading to the inconsistency between\nobserved and intrinsic class distribution with label noises. In this case, it\nis hard to distinguish clean samples from noisy samples on the intrinsic tail\nclasses with the unknown intrinsic class distribution. In this paper, we\npropose a learning framework for label-noise learning with intrinsically\nlong-tailed data. Specifically, we propose two-stage bi-dimensional sample\nselection (TABASCO) to better separate clean samples from noisy samples,\nespecially for the tail classes. TABASCO consists of two new separation metrics\nthat complement each other to compensate for the limitation of using a single\nmetric in sample separation. Extensive experiments on benchmarks demonstrate\nthe effectiveness of our method. Our code is available at\nhttps://github.com/Wakings/TABASCO.",
        "authors": [
            "Yang Lu",
            "Yiliang Zhang",
            "Bo Han",
            "Yiu-ming Cheung",
            "Hanzi Wang"
        ]
    },
    {
        "title": "SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes",
        "url": "http://arxiv.org/abs/2211.11296",
        "abstract": "Modern deepfake detectors have achieved encouraging results, when training\nand test images are drawn from the same data collection. However, when these\ndetectors are applied to images produced with unknown deepfake-generation\ntechniques, considerable performance degradations are commonly observed. In\nthis paper, we propose a novel deepfake detector, called SeeABLE, that\nformalizes the detection problem as a (one-class) out-of-distribution detection\ntask and generalizes better to unseen deepfakes. Specifically, SeeABLE first\ngenerates local image perturbations (referred to as soft-discrepancies) and\nthen pushes the perturbed faces towards predefined prototypes using a novel\nregression-based bounded contrastive loss. To strengthen the generalization\nperformance of SeeABLE to unknown deepfake types, we generate a rich set of\nsoft discrepancies and train the detector: (i) to localize, which part of the\nface was modified, and (ii) to identify the alteration type. To demonstrate the\ncapabilities of SeeABLE, we perform rigorous experiments on several widely-used\ndeepfake datasets and show that our model convincingly outperforms competing\nstate-of-the-art detectors, while exhibiting highly encouraging generalization\ncapabilities.",
        "authors": [
            "Nicolas Larue",
            "Ngoc-Son Vu",
            "Vitomir Struc",
            "Peter Peer",
            "Vassilis Christophides"
        ]
    },
    {
        "title": "Semi-Supervised Learning via Weight-Aware Distillation under Class Distribution Mismatch",
        "url": "http://arxiv.org/abs/2308.11874",
        "abstract": "Semi-Supervised Learning (SSL) under class distribution mismatch aims to\ntackle a challenging problem wherein unlabeled data contain lots of unknown\ncategories unseen in the labeled ones. In such mismatch scenarios, traditional\nSSL suffers severe performance damage due to the harmful invasion of the\ninstances with unknown categories into the target classifier. In this study, by\nstrict mathematical reasoning, we reveal that the SSL error under class\ndistribution mismatch is composed of pseudo-labeling error and invasion error,\nboth of which jointly bound the SSL population risk. To alleviate the SSL\nerror, we propose a robust SSL framework called Weight-Aware Distillation (WAD)\nthat, by weights, selectively transfers knowledge beneficial to the target task\nfrom unsupervised contrastive representation to the target classifier.\nSpecifically, WAD captures adaptive weights and high-quality pseudo labels to\ntarget instances by exploring point mutual information (PMI) in representation\nspace to maximize the role of unlabeled data and filter unknown categories.\nTheoretically, we prove that WAD has a tight upper bound of population risk\nunder class distribution mismatch. Experimentally, extensive results\ndemonstrate that WAD outperforms five state-of-the-art SSL approaches and one\nstandard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and an\nartificial cross-dataset. The code is available at\nhttps://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.",
        "authors": [
            "Pan Du",
            "Suyun Zhao",
            "Zisen Sheng",
            "Cuiping Li",
            "Hong Chen"
        ]
    },
    {
        "title": "ELFNet: Evidential Local-global Fusion for Stereo Matching",
        "url": "http://arxiv.org/abs/2308.00728",
        "abstract": "Although existing stereo matching models have achieved continuous\nimprovement, they often face issues related to trustworthiness due to the\nabsence of uncertainty estimation. Additionally, effectively leveraging\nmulti-scale and multi-view knowledge of stereo pairs remains unexplored. In\nthis paper, we introduce the \\textbf{E}vidential \\textbf{L}ocal-global\n\\textbf{F}usion (ELF) framework for stereo matching, which endows both\nuncertainty estimation and confidence-aware fusion with trustworthy heads.\nInstead of predicting the disparity map alone, our model estimates an\nevidential-based disparity considering both aleatoric and epistemic\nuncertainties. With the normal inverse-Gamma distribution as a bridge, the\nproposed framework realizes intra evidential fusion of multi-level predictions\nand inter evidential fusion between cost-volume-based and transformer-based\nstereo matching. Extensive experimental results show that the proposed\nframework exploits multi-view information effectively and achieves\nstate-of-the-art overall performance both on accuracy and cross-domain\ngeneralization.\n  The codes are available at https://github.com/jimmy19991222/ELFNet.",
        "authors": [
            "Jieming Lou",
            "Weide Liu",
            "Zhuo Chen",
            "Fayao Liu",
            "Jun Cheng"
        ]
    },
    {
        "title": "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers",
        "url": "http://arxiv.org/abs/2210.11006",
        "abstract": "Click-based interactive image segmentation aims at extracting objects with a\nlimited user clicking. A hierarchical backbone is the de-facto architecture for\ncurrent methods. Recently, the plain, non-hierarchical Vision Transformer (ViT)\nhas emerged as a competitive backbone for dense prediction tasks. This design\nallows the original ViT to be a foundation model that can be finetuned for\ndownstream tasks without redesigning a hierarchical backbone for pretraining.\nAlthough this design is simple and has been proven effective, it has not yet\nbeen explored for interactive image segmentation. To fill this gap, we propose\nSimpleClick, the first interactive segmentation method that leverages a plain\nbackbone. Based on the plain backbone, we introduce a symmetric patch embedding\nlayer that encodes clicks into the backbone with minor modifications to the\nbackbone itself. With the plain backbone pretrained as a masked autoencoder\n(MAE), SimpleClick achieves state-of-the-art performance. Remarkably, our\nmethod achieves 4.15 NoC@90 on SBD, improving 21.8% over the previous best\nresult. Extensive evaluation on medical images demonstrates the\ngeneralizability of our method. We further develop an extremely tiny ViT\nbackbone for SimpleClick and provide a detailed computational analysis,\nhighlighting its suitability as a practical annotation tool.",
        "authors": [
            "Qin Liu",
            "Zhenlin Xu",
            "Gedas Bertasius",
            "Marc Niethammer"
        ]
    },
    {
        "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
        "url": "http://arxiv.org/abs/2309.05438",
        "abstract": "This paper introduces the first two pixel retrieval benchmarks. Pixel\nretrieval is segmented instance retrieval. Like semantic segmentation extends\nclassification to the pixel level, pixel retrieval is an extension of image\nretrieval and offers information about which pixels are related to the query\nobject. In addition to retrieving images for the given query, it helps users\nquickly identify the query object in true positive images and exclude false\npositive images by denoting the correlated pixels. Our user study results show\npixel-level annotation can significantly improve the user experience.\n  Compared with semantic and instance segmentation, pixel retrieval requires a\nfine-grained recognition capability for variable-granularity targets. To this\nend, we propose pixel retrieval benchmarks named PROxford and PRParis, which\nare based on the widely used image retrieval datasets, ROxford and RParis.\nThree professional annotators label 5,942 images with two rounds of\ndouble-checking and refinement. Furthermore, we conduct extensive experiments\nand analysis on the SOTA methods in image search, image matching, detection,\nsegmentation, and dense matching using our pixel retrieval benchmarks. Results\nshow that the pixel retrieval task is challenging to these approaches and\ndistinctive from existing problems, suggesting that further research can\nadvance the content-based pixel-retrieval and thus user search experience. The\ndatasets can be downloaded from\n\\href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{this\nlink}.",
        "authors": [
            "Guoyuan An",
            "Woo Jae Kim",
            "Saelyne Yang",
            "Rong Li",
            "Yuchi Huo",
            "Sung-Eui Yoon"
        ]
    },
    {
        "title": "Rethinking Range View Representation for LiDAR Segmentation",
        "url": "http://arxiv.org/abs/2303.05367",
        "abstract": "LiDAR segmentation is crucial for autonomous driving perception. Recent\ntrends favor point- or voxel-based methods as they often yield better\nperformance than the traditional range view representation. In this work, we\nunveil several key factors in building powerful range view models. We observe\nthat the \"many-to-one\" mapping, semantic incoherence, and shape deformation are\npossible impediments against effective learning from range view projections. We\npresent RangeFormer -- a full-cycle framework comprising novel designs across\nnetwork architecture, data augmentation, and post-processing -- that better\nhandles the learning and processing of LiDAR point clouds from the range view.\nWe further introduce a Scalable Training from Range view (STR) strategy that\ntrains on arbitrary low-resolution 2D range images, while still maintaining\nsatisfactory 3D segmentation accuracy. We show that, for the first time, a\nrange view method is able to surpass the point, voxel, and multi-view fusion\ncounterparts in the competing LiDAR semantic and panoptic segmentation\nbenchmarks, i.e., SemanticKITTI, nuScenes, and ScribbleKITTI.",
        "authors": [
            "Lingdong Kong",
            "Youquan Liu",
            "Runnan Chen",
            "Yuexin Ma",
            "Xinge Zhu",
            "Yikang Li",
            "Yuenan Hou",
            "Yu Qiao",
            "Ziwei Liu"
        ]
    },
    {
        "title": "Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization",
        "url": "http://arxiv.org/abs/2207.11209",
        "abstract": "Instance segmentation on point clouds is crucially important for 3D scene\nunderstanding. Most SOTAs adopt distance clustering, which is typically\neffective but does not perform well in segmenting adjacent objects with the\nsame semantic label (especially when they share neighboring points). Due to the\nuneven distribution of offset points, these existing methods can hardly cluster\nall instance points. To this end, we design a novel divide-and-conquer strategy\nnamed PBNet that binarizes each point and clusters them separately to segment\ninstances. Our binary clustering divides offset instance points into two\ncategories: high and low density points (HPs vs. LPs). Adjacent objects can be\nclearly separated by removing LPs, and then be completed and refined by\nassigning LPs via a neighbor voting method. To suppress potential\nover-segmentation, we propose to construct local scenes with the weight mask\nfor each instance. As a plug-in, the proposed binary clustering can replace\ntraditional distance clustering and lead to consistent performance gains on\nmany mainstream baselines. A series of experiments on ScanNetV2 and S3DIS\ndatasets indicate the superiority of our model. In particular, PBNet ranks\nfirst on the ScanNetV2 official benchmark challenge, achieving the highest mAP.\nCode will be available publicly at https://github.com/weiguangzhao/PBNet.",
        "authors": [
            "Weiguang Zhao",
            "Yuyao Yan",
            "Chaolong Yang",
            "Jianan Ye",
            "Xi Yang",
            "Kaizhu Huang"
        ]
    },
    {
        "title": "BANSAC: A Dynamic BAyesian Network for Adaptive SAmple Consensus",
        "url": "http://arxiv.org/abs/2309.08690",
        "abstract": "RANSAC-based algorithms are the standard techniques for robust estimation in\ncomputer vision. These algorithms are iterative and computationally expensive;\nthey alternate between random sampling of data, computing hypotheses, and\nrunning inlier counting. Many authors tried different approaches to improve\nefficiency. One of the major improvements is having a guided sampling, letting\nthe RANSAC cycle stop sooner. This paper presents a new adaptive sampling\nprocess for RANSAC. Previous methods either assume no prior information about\nthe inlier/outlier classification of data points or use some previously\ncomputed scores in the sampling. In this paper, we derive a dynamic Bayesian\nnetwork that updates individual data points' inlier scores while iterating\nRANSAC. At each iteration, we apply weighted sampling using the updated scores.\nOur method works with or without prior data point scorings. In addition, we use\nthe updated inlier/outlier scoring for deriving a new stopping criterion for\nthe RANSAC loop. We test our method in multiple real-world datasets for several\napplications and obtain state-of-the-art results. Our method outperforms the\nbaselines in accuracy while needing less computational time.",
        "authors": [
            "Valter Piedade",
            "Pedro Miraldo"
        ]
    },
    {
        "title": "Read-only Prompt Optimization for Vision-Language Few-shot Learning",
        "url": "http://arxiv.org/abs/2308.14960",
        "abstract": "In recent years, prompt tuning has proven effective in adapting pre-trained\nvision-language models to downstream tasks. These methods aim to adapt the\npre-trained models by introducing learnable prompts while keeping pre-trained\nweights frozen. However, learnable prompts can affect the internal\nrepresentation within the self-attention module, which may negatively impact\nperformance variance and generalization, especially in data-deficient settings.\nTo address these issues, we propose a novel approach, Read-only Prompt\nOptimization (RPO). RPO leverages masked attention to prevent the internal\nrepresentation shift in the pre-trained model. Further, to facilitate the\noptimization of RPO, the read-only prompts are initialized based on special\ntokens of the pre-trained model. Our extensive experiments demonstrate that RPO\noutperforms CLIP and CoCoOp in base-to-new generalization and domain\ngeneralization while displaying better robustness. Also, the proposed method\nachieves better generalization on extremely data-deficient settings, while\nimproving parameter efficiency and computational overhead. Code is available at\nhttps://github.com/mlvlab/RPO.",
        "authors": [
            "Dongjun Lee",
            "Seokwon Song",
            "Jihee Suh",
            "Joonmyung Choi",
            "Sanghyeok Lee",
            "Hyunwoo J. Kim"
        ]
    },
    {
        "title": "EgoTV: Egocentric Task Verification from Natural Language Task Descriptions",
        "url": "http://arxiv.org/abs/2303.16975",
        "abstract": "To enable progress towards egocentric agents capable of understanding\neveryday tasks specified in natural language, we propose a benchmark and a\nsynthetic dataset called Egocentric Task Verification (EgoTV). The goal in\nEgoTV is to verify the execution of tasks from egocentric videos based on the\nnatural language description of these tasks. EgoTV contains pairs of videos and\ntheir task descriptions for multi-step tasks -- these tasks contain multiple\nsub-task decompositions, state changes, object interactions, and sub-task\nordering constraints. In addition, EgoTV also provides abstracted task\ndescriptions that contain only partial details about ways to accomplish a task.\nConsequently, EgoTV requires causal, temporal, and compositional reasoning of\nvideo and language modalities, which is missing in existing datasets. We also\nfind that existing vision-language models struggle at such all round reasoning\nneeded for task verification in EgoTV. Inspired by the needs of EgoTV, we\npropose a novel Neuro-Symbolic Grounding (NSG) approach that leverages symbolic\nrepresentations to capture the compositional and temporal structure of tasks.\nWe demonstrate NSG's capability towards task tracking and verification on our\nEgoTV dataset and a real-world dataset derived from CrossTask (CTV). We\nopen-source the EgoTV and CTV datasets and the NSG model for future research on\negocentric assistive agents.",
        "authors": [
            "Rishi Hazra",
            "Brian Chen",
            "Akshara Rai",
            "Nitin Kamra",
            "Ruta Desai"
        ]
    },
    {
        "title": "Benchmarking Low-Shot Robustness to Natural Distribution Shifts",
        "url": "http://arxiv.org/abs/2304.11263",
        "abstract": "Robustness to natural distribution shifts has seen remarkable progress thanks\nto recent pre-training strategies combined with better fine-tuning methods.\nHowever, such fine-tuning assumes access to large amounts of labelled data, and\nthe extent to which the observations hold when the amount of training data is\nnot as high remains unknown. We address this gap by performing the first\nin-depth study of robustness to various natural distribution shifts in\ndifferent low-shot regimes: spanning datasets, architectures, pre-trained\ninitializations, and state-of-the-art robustness interventions. Most\nimportantly, we find that there is no single model of choice that is often more\nrobust than others, and existing interventions can fail to improve robustness\non some datasets even if they do so in the full-shot regime. We hope that our\nwork will motivate the community to focus on this problem of practical\nimportance.",
        "authors": [
            "Aaditya Singh",
            "Kartik Sarangmath",
            "Prithvijit Chattopadhyay",
            "Judy Hoffman"
        ]
    },
    {
        "title": "StageInteractor: Query-based Object Detector with Cross-stage Interaction",
        "url": "http://arxiv.org/abs/2304.04978",
        "abstract": "Previous object detectors make predictions based on dense grid points or\nnumerous preset anchors. Most of these detectors are trained with one-to-many\nlabel assignment strategies. On the contrary, recent query-based object\ndetectors depend on a sparse set of learnable queries and a series of decoder\nlayers. The one-to-one label assignment is independently applied on each layer\nfor the deep supervision during training. Despite the great success of\nquery-based object detection, however, this one-to-one label assignment\nstrategy demands the detectors to have strong fine-grained discrimination and\nmodeling capacity. To solve the above problems, in this paper, we propose a new\nquery-based object detector with cross-stage interaction, coined as\nStageInteractor. During the forward propagation, we come up with an efficient\nway to improve this modeling ability by reusing dynamic operators with\nlightweight adapters. As for the label assignment, a cross-stage label assigner\nis applied subsequent to the one-to-one label assignment. With this assigner,\nthe training target class labels are gathered across stages and then\nreallocated to proper predictions at each decoder layer. On MS COCO benchmark,\nour model improves the baseline by 2.2 AP, and achieves 44.8 AP with ResNet-50\nas backbone, 100 queries and 12 training epochs. With longer training time and\n300 queries, StageInteractor achieves 51.1 AP and 52.2 AP with ResNeXt-101-DCN\nand Swin-S, respectively.",
        "authors": [
            "Yao Teng",
            "Haisong Liu",
            "Sheng Guo",
            "Limin Wang"
        ]
    },
    {
        "title": "DeLiRa: Self-Supervised Depth, Light, and Radiance Fields",
        "url": "http://arxiv.org/abs/2304.02797",
        "abstract": "Differentiable volumetric rendering is a powerful paradigm for 3D\nreconstruction and novel view synthesis. However, standard volume rendering\napproaches struggle with degenerate geometries in the case of limited viewpoint\ndiversity, a common scenario in robotics applications. In this work, we propose\nto use the multi-view photometric objective from the self-supervised depth\nestimation literature as a geometric regularizer for volumetric rendering,\nsignificantly improving novel view synthesis without requiring additional\ninformation. Building upon this insight, we explore the explicit modeling of\nscene geometry using a generalist Transformer, jointly learning a radiance\nfield as well as depth and light fields with a set of shared latent codes. We\ndemonstrate that sharing geometric information across tasks is mutually\nbeneficial, leading to improvements over single-task learning without an\nincrease in network complexity. Our DeLiRa architecture achieves\nstate-of-the-art results on the ScanNet benchmark, enabling high quality\nvolumetric rendering as well as real-time novel view and depth synthesis in the\nlimited viewpoint diversity setting.",
        "authors": [
            "Vitor Guizilini",
            "Igor Vasiljevic",
            "Jiading Fang",
            "Rares Ambrus",
            "Sergey Zakharov",
            "Vincent Sitzmann",
            "Adrien Gaidon"
        ]
    },
    {
        "title": "Pix2Video: Video Editing using Image Diffusion",
        "url": "http://arxiv.org/abs/2303.12688",
        "abstract": "Image diffusion models, trained on massive image collections, have emerged as\nthe most versatile image generator model in terms of quality and diversity.\nThey support inverting real images and conditional (e.g., text) generation,\nmaking them attractive for high-quality image editing applications. We\ninvestigate how to use such pre-trained image models for text-guided video\nediting. The critical challenge is to achieve the target edits while still\npreserving the content of the source video. Our method works in two simple\nsteps: first, we use a pre-trained structure-guided (e.g., depth) image\ndiffusion model to perform text-guided edits on an anchor frame; then, in the\nkey step, we progressively propagate the changes to the future frames via\nself-attention feature injection to adapt the core denoising step of the\ndiffusion model. We then consolidate the changes by adjusting the latent code\nfor the frame before continuing the process. Our approach is training-free and\ngeneralizes to a wide range of edits. We demonstrate the effectiveness of the\napproach by extensive experimentation and compare it against four different\nprior and parallel efforts (on ArXiv). We demonstrate that realistic\ntext-guided video edits are possible, without any compute-intensive\npreprocessing or video-specific finetuning.",
        "authors": [
            "Duygu Ceylan",
            "Chun-Hao Paul Huang",
            "Niloy J. Mitra"
        ]
    },
    {
        "title": "DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting",
        "url": "http://arxiv.org/abs/2307.12972",
        "abstract": "In this paper, we propose a new operator, called 3D DeFormable Attention\n(DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D image\nfeatures into a unified 3D space for 3D object detection. Existing feature\nlifting approaches, such as Lift-Splat-based and 2D attention-based, either use\nestimated depth to get pseudo LiDAR features and then splat them to a 3D space,\nwhich is a one-pass operation without feature refinement, or ignore depth and\nlift features by 2D attention mechanisms, which achieve finer semantics while\nsuffering from a depth ambiguity problem. In contrast, our DFA3D-based method\nfirst leverages the estimated depth to expand each view's 2D feature map to 3D\nand then utilizes DFA3D to aggregate features from the expanded 3D feature\nmaps. With the help of DFA3D, the depth ambiguity problem can be effectively\nalleviated from the root, and the lifted features can be progressively refined\nlayer by layer, thanks to the Transformer-like architecture. In addition, we\npropose a mathematically equivalent implementation of DFA3D which can\nsignificantly improve its memory efficiency and computational speed. We\nintegrate DFA3D into several methods that use 2D attention-based feature\nlifting with only a few modifications in code and evaluate on the nuScenes\ndataset. The experiment results show a consistent improvement of +1.41\\% mAP on\naverage, and up to +15.1\\% mAP improvement when high-quality depth information\nis available, demonstrating the superiority, applicability, and huge potential\nof DFA3D. The code is available at\nhttps://github.com/IDEA-Research/3D-deformable-attention.git.",
        "authors": [
            "Hongyang Li",
            "Hao Zhang",
            "Zhaoyang Zeng",
            "Shilong Liu",
            "Feng Li",
            "Tianhe Ren",
            "Lei Zhang"
        ]
    },
    {
        "title": "Holistic Geometric Feature Learning for Structured Reconstruction",
        "url": "http://arxiv.org/abs/2309.09622",
        "abstract": "The inference of topological principles is a key problem in structured\nreconstruction. We observe that wrongly predicted topological relationships are\noften incurred by the lack of holistic geometry clues in low-level features.\nInspired by the fact that massive signals can be compactly described with\nfrequency analysis, we experimentally explore the efficiency and tendency of\nlearning structure geometry in the frequency domain. Accordingly, we propose a\nfrequency-domain feature learning strategy (F-Learn) to fuse scattered\ngeometric fragments holistically for topology-intact structure reasoning.\nBenefiting from the parsimonious design, the F-Learn strategy can be easily\ndeployed into a deep reconstructor with a lightweight model modification.\nExperiments demonstrate that the F-Learn strategy can effectively introduce\nstructure awareness into geometric primitive detection and topology inference,\nbringing significant performance improvement to final structured\nreconstruction. Code and pre-trained models are available at\nhttps://github.com/Geo-Tell/F-Learn.",
        "authors": [
            "Ziqiong Lu",
            "Linxi Huan",
            "Qiyuan Ma",
            "Xianwei Zheng"
        ]
    },
    {
        "title": "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing",
        "url": "http://arxiv.org/abs/2303.09535",
        "abstract": "The diffusion-based generative models have achieved remarkable success in\ntext-based image generation. However, since it contains enormous randomness in\ngeneration progress, it is still challenging to apply such models for\nreal-world visual content editing, especially in videos. In this paper, we\npropose FateZero, a zero-shot text-based editing method on real-world videos\nwithout per-prompt training or use-specific mask. To edit videos consistently,\nwe propose several techniques based on the pre-trained models. Firstly, in\ncontrast to the straightforward DDIM inversion technique, our approach captures\nintermediate attention maps during inversion, which effectively retain both\nstructural and motion information. These maps are directly fused in the editing\nprocess rather than generated during denoising. To further minimize semantic\nleakage of the source video, we then fuse self-attentions with a blending mask\nobtained by cross-attention features from the source prompt. Furthermore, we\nhave implemented a reform of the self-attention mechanism in denoising UNet by\nintroducing spatial-temporal attention to ensure frame consistency. Yet\nsuccinct, our method is the first one to show the ability of zero-shot\ntext-driven video style and local attribute editing from the trained\ntext-to-image model. We also have a better zero-shot shape-aware editing\nability based on the text-to-video model. Extensive experiments demonstrate our\nsuperior temporal consistency and editing capability than previous works.",
        "authors": [
            "Chenyang Qi",
            "Xiaodong Cun",
            "Yong Zhang",
            "Chenyang Lei",
            "Xintao Wang",
            "Ying Shan",
            "Qifeng Chen"
        ]
    },
    {
        "title": "LMR: A Large-Scale Multi-Reference Dataset for Reference-Based Super-Resolution",
        "url": "http://arxiv.org/abs/2303.04970",
        "abstract": "It is widely agreed that reference-based super-resolution (RefSR) achieves\nsuperior results by referring to similar high quality images, compared to\nsingle image super-resolution (SISR). Intuitively, the more references, the\nbetter performance. However, previous RefSR methods have all focused on\nsingle-reference image training, while multiple reference images are often\navailable in testing or practical applications. The root cause of such\ntraining-testing mismatch is the absence of publicly available multi-reference\nSR training datasets, which greatly hinders research efforts on multi-reference\nsuper-resolution. To this end, we construct a large-scale, multi-reference\nsuper-resolution dataset, named LMR. It contains 112,142 groups of 300x300\ntraining images, which is 10x of the existing largest RefSR dataset. The image\nsize is also much larger. More importantly, each group is equipped with 5\nreference images with different similarity levels. Furthermore, we propose a\nnew baseline method for multi-reference super-resolution: MRefSR, including a\nMulti-Reference Attention Module (MAM) for feature fusion of an arbitrary\nnumber of reference images, and a Spatial Aware Filtering Module (SAFM) for the\nfused feature selection. The proposed MRefSR achieves significant improvements\nover state-of-the-art approaches on both quantitative and qualitative\nevaluations. Our code and data would be made available soon.",
        "authors": [
            "Lin Zhang",
            "Xin Li",
            "Dongliang He",
            "Errui Ding",
            "Zhaoxiang Zhang"
        ]
    },
    {
        "title": "Neural Implicit Surface Evolution",
        "url": "http://arxiv.org/abs/2201.09636",
        "abstract": "This work investigates the use of smooth neural networks for modeling dynamic\nvariations of implicit surfaces under the level set equation (LSE). For this,\nit extends the representation of neural implicit surfaces to the space-time\n$\\mathbb{R}^3\\times \\mathbb{R}$, which opens up mechanisms for continuous\ngeometric transformations. Examples include evolving an initial surface towards\ngeneral vector fields, smoothing and sharpening using the mean curvature\nequation, and interpolations of initial conditions.\n  The network training considers two constraints. A data term is responsible\nfor fitting the initial condition to the corresponding time instant, usually\n$\\mathbb{R}^3 \\times \\{0\\}$. Then, a LSE term forces the network to approximate\nthe underlying geometric evolution given by the LSE, without any supervision.\nThe network can also be initialized based on previously trained initial\nconditions, resulting in faster convergence compared to the standard approach.",
        "authors": [
            "Tiago Novello",
            "Vinicius da Silva",
            "Guilherme Schardong",
            "Luiz Schirmer",
            "Helio Lopes",
            "Luiz Velho"
        ]
    },
    {
        "title": "Distribution-Aligned Diffusion for Human Mesh Recovery",
        "url": "http://arxiv.org/abs/2308.13369",
        "abstract": "Recovering a 3D human mesh from a single RGB image is a challenging task due\nto depth ambiguity and self-occlusion, resulting in a high degree of\nuncertainty. Meanwhile, diffusion models have recently seen much success in\ngenerating high-quality outputs by progressively denoising noisy inputs.\nInspired by their capability, we explore a diffusion-based approach for human\nmesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which\nframes mesh recovery as a reverse diffusion process. We also propose a\nDistribution Alignment Technique (DAT) that infuses prior distribution\ninformation into the mesh distribution diffusion process, and provides useful\nprior knowledge to facilitate the mesh recovery task. Our method achieves\nstate-of-the-art performance on three widely used datasets. Project page:\nhttps://gongjia0208.github.io/HMDiff/.",
        "authors": [
            "Lin Geng Foo",
            "Jia Gong",
            "Hossein Rahmani",
            "Jun Liu"
        ]
    },
    {
        "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
        "url": "http://arxiv.org/abs/2306.09346",
        "abstract": "Do different neural networks, trained for various vision tasks, share some\ncommon representations? In this paper, we demonstrate the existence of common\nfeatures we call \"Rosetta Neurons\" across a range of models with different\narchitectures, different tasks (generative and discriminative), and different\ntypes of supervision (class-supervised, text-supervised, self-supervised). We\npresent an algorithm for mining a dictionary of Rosetta Neurons across several\npopular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE,\nCLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that\ncertain visual concepts and structures are inherently embedded in the natural\nworld and can be learned by different models regardless of the specific task or\narchitecture, and without the use of semantic labels. We can visualize shared\nconcepts directly due to generative models included in our analysis. The\nRosetta Neurons facilitate model-to-model translation enabling various\ninversion-based manipulations, including cross-class alignments, shifting,\nzooming, and more, without the need for specialized training.",
        "authors": [
            "Amil Dravid",
            "Yossi Gandelsman",
            "Alexei A. Efros",
            "Assaf Shocher"
        ]
    },
    {
        "title": "Segment Anything",
        "url": "http://arxiv.org/abs/2304.02643",
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and\ndataset for image segmentation. Using our efficient model in a data collection\nloop, we built the largest segmentation dataset to date (by far), with over 1\nbillion masks on 11M licensed and privacy respecting images. The model is\ndesigned and trained to be promptable, so it can transfer zero-shot to new\nimage distributions and tasks. We evaluate its capabilities on numerous tasks\nand find that its zero-shot performance is impressive -- often competitive with\nor even superior to prior fully supervised results. We are releasing the\nSegment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and\n11M images at https://segment-anything.com to foster research into foundation\nmodels for computer vision.",
        "authors": [
            "Alexander Kirillov",
            "Eric Mintun",
            "Nikhila Ravi",
            "Hanzi Mao",
            "Chloe Rolland",
            "Laura Gustafson",
            "Tete Xiao",
            "Spencer Whitehead",
            "Alexander C. Berg",
            "Wan-Yen Lo",
            "Piotr Doll\u00e1r",
            "Ross Girshick"
        ]
    },
    {
        "title": "Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization",
        "url": "http://arxiv.org/abs/2303.11003",
        "abstract": "We propose a self-supervised method for learning motion-focused video\nrepresentations. Existing approaches minimize distances between temporally\naugmented videos, which maintain high spatial similarity. We instead propose to\nlearn similarities between videos with identical local motion dynamics but an\notherwise different appearance. We do so by adding synthetic motion\ntrajectories to videos which we refer to as tubelets. By simulating different\ntubelet motions and applying transformations, such as scaling and rotation, we\nintroduce motion patterns beyond what is present in the pretraining data. This\nallows us to learn a video representation that is remarkably data efficient:\nour approach maintains performance when using only 25\\% of the pretraining\nvideos. Experiments on 10 diverse downstream settings demonstrate our\ncompetitive performance and generalizability to new domains and fine-grained\nactions.",
        "authors": [
            "Fida Mohammad Thoker",
            "Hazel Doughty",
            "Cees Snoek"
        ]
    },
    {
        "title": "360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking",
        "url": "http://arxiv.org/abs/2307.14630",
        "abstract": "360{\\deg} images can provide an omnidirectional field of view which is\nimportant for stable and long-term scene perception. In this paper, we explore\n360{\\deg} images for visual object tracking and perceive new challenges caused\nby large distortion, stitching artifacts, and other unique attributes of\n360{\\deg} images. To alleviate these problems, we take advantage of novel\nrepresentations of target localization, i.e., bounding field-of-view, and then\nintroduce a general 360 tracking framework that can adopt typical trackers for\nomnidirectional tracking. More importantly, we propose a new large-scale\nomnidirectional tracking benchmark dataset, 360VOT, in order to facilitate\nfuture research. 360VOT contains 120 sequences with up to 113K high-resolution\nframes in equirectangular projection. The tracking targets cover 32 categories\nin diverse scenarios. Moreover, we provide 4 types of unbiased ground truth,\nincluding (rotated) bounding boxes and (rotated) bounding field-of-views, as\nwell as new metrics tailored for 360{\\deg} images which allow for the accurate\nevaluation of omnidirectional tracking performance. Finally, we extensively\nevaluated 20 state-of-the-art visual trackers and provided a new baseline for\nfuture comparisons. Homepage: https://360vot.hkustvgd.com",
        "authors": [
            "Huajian Huang",
            "Yinzhe Xu",
            "Yingshu Chen",
            "Sai-Kit Yeung"
        ]
    },
    {
        "title": "Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training",
        "url": "http://arxiv.org/abs/2307.07909",
        "abstract": "We introduce DualMind, a generalist agent designed to tackle various\ndecision-making tasks that addresses challenges posed by current methods, such\nas overfitting behaviors and dependence on task-specific fine-tuning. DualMind\nuses a novel \"Dual-phase\" training strategy that emulates how humans learn to\nact in the world. The model first learns fundamental common knowledge through a\nself-supervised objective tailored for control tasks and then learns how to\nmake decisions based on different contexts through imitating behaviors\nconditioned on given prompts. DualMind can handle tasks across domains, scenes,\nand embodiments using just a single set of model weights and can execute\nzero-shot prompting without requiring task-specific fine-tuning. We evaluate\nDualMind on MetaWorld and Habitat through extensive experiments and demonstrate\nits superior generalizability compared to previous techniques, outperforming\nother generalist agents by over 50$\\%$ and 70$\\%$ on Habitat and MetaWorld,\nrespectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at\na 90$\\%$ success rate.",
        "authors": [
            "Yao Wei",
            "Yanchao Sun",
            "Ruijie Zheng",
            "Sai Vemprala",
            "Rogerio Bonatti",
            "Shuhang Chen",
            "Ratnesh Madaan",
            "Zhongjie Ba",
            "Ashish Kapoor",
            "Shuang Ma"
        ]
    },
    {
        "title": "Generalizing Event-Based Motion Deblurring in Real-World Scenarios",
        "url": "http://arxiv.org/abs/2308.05932",
        "abstract": "Event-based motion deblurring has shown promising results by exploiting\nlow-latency events. However, current approaches are limited in their practical\nusage, as they assume the same spatial resolution of inputs and specific\nblurriness distributions. This work addresses these limitations and aims to\ngeneralize the performance of event-based deblurring in real-world scenarios.\nWe propose a scale-aware network that allows flexible input spatial scales and\nenables learning from different temporal scales of motion blur. A two-stage\nself-supervised learning scheme is then developed to fit real-world data\ndistribution. By utilizing the relativity of blurriness, our approach\nefficiently ensures the restored brightness and structure of latent images and\nfurther generalizes deblurring performance to handle varying spatial and\ntemporal scales of motion blur in a self-distillation manner. Our method is\nextensively evaluated, demonstrating remarkable performance, and we also\nintroduce a real-world dataset consisting of multi-scale blurry frames and\nevents to facilitate research in event-based deblurring.",
        "authors": [
            "Xiang Zhang",
            "Lei Yu",
            "Wen Yang",
            "Jianzhuang Liu",
            "Gui-Song Xia"
        ]
    },
    {
        "title": "Handwritten and Printed Text Segmentation: A Signature Case Study",
        "url": "http://arxiv.org/abs/2307.07887",
        "abstract": "While analyzing scanned documents, handwritten text can overlap with printed\ntext. This overlap causes difficulties during the optical character recognition\n(OCR) and digitization process of documents, and subsequently, hurts downstream\nNLP tasks. Prior research either focuses solely on the binary classification of\nhandwritten text or performs a three-class segmentation of the document, i.e.,\nrecognition of handwritten, printed, and background pixels. This approach\nresults in the assignment of overlapping handwritten and printed pixels to only\none of the classes, and thus, they are not accounted for in the other class.\nThus, in this research, we develop novel approaches to address the challenges\nof handwritten and printed text segmentation. Our objective is to recover text\nfrom different classes in their entirety, especially enhancing the segmentation\nperformance on overlapping sections. To support this task, we introduce a new\ndataset, SignaTR6K, collected from real legal documents, as well as a new model\narchitecture for the handwritten and printed text segmentation task. Our best\nconfiguration outperforms prior work on two different datasets by 17.9% and\n7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the\nfollowing link: https://forms.office.com/r/2a5RDg7cAY.",
        "authors": [
            "Sina Gholamian",
            "Ali Vahdat"
        ]
    },
    {
        "title": "LERF: Language Embedded Radiance Fields",
        "url": "http://arxiv.org/abs/2303.09553",
        "abstract": "Humans describe the physical world using natural language to refer to\nspecific 3D locations based on a vast range of properties: visual appearance,\nsemantics, abstract associations, or actionable affordances. In this work we\npropose Language Embedded Radiance Fields (LERFs), a method for grounding\nlanguage embeddings from off-the-shelf models like CLIP into NeRF, which enable\nthese types of open-ended language queries in 3D. LERF learns a dense,\nmulti-scale language field inside NeRF by volume rendering CLIP embeddings\nalong training rays, supervising these embeddings across training views to\nprovide multi-view consistency and smooth the underlying language field. After\noptimization, LERF can extract 3D relevancy maps for a broad range of language\nprompts interactively in real-time, which has potential use cases in robotics,\nunderstanding vision-language models, and interacting with 3D scenes. LERF\nenables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings\nwithout relying on region proposals or masks, supporting long-tail\nopen-vocabulary queries hierarchically across the volume. The project website\ncan be found at https://lerf.io .",
        "authors": [
            "Justin Kerr",
            "Chung Min Kim",
            "Ken Goldberg",
            "Angjoo Kanazawa",
            "Matthew Tancik"
        ]
    },
    {
        "title": "DomainAdaptor: A Novel Approach to Test-time Adaptation",
        "url": "http://arxiv.org/abs/2308.10297",
        "abstract": "To deal with the domain shift between training and test samples, current\nmethods have primarily focused on learning generalizable features during\ntraining and ignore the specificity of unseen samples that are also critical\nduring the test. In this paper, we investigate a more challenging task that\naims to adapt a trained CNN model to unseen domains during the test. To\nmaximumly mine the information in the test data, we propose a unified method\ncalled DomainAdaptor for the test-time adaptation, which consists of an\nAdaMixBN module and a Generalized Entropy Minimization (GEM) loss.\nSpecifically, AdaMixBN addresses the domain shift by adaptively fusing training\nand test statistics in the normalization layer via a dynamic mixture\ncoefficient and a statistic transformation operation. To further enhance the\nadaptation ability of AdaMixBN, we design a GEM loss that extends the Entropy\nMinimization loss to better exploit the information in the test data. Extensive\nexperiments show that DomainAdaptor consistently outperforms the\nstate-of-the-art methods on four benchmarks. Furthermore, our method brings\nmore remarkable improvement against existing methods on the few-data unseen\ndomain. The code is available at https://github.com/koncle/DomainAdaptor.",
        "authors": [
            "Jian Zhang",
            "Lei Qi",
            "Yinghuan Shi",
            "Yang Gao"
        ]
    },
    {
        "title": "Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground",
        "url": "http://arxiv.org/abs/2211.12883",
        "abstract": "In video action recognition, shortcut static features can interfere with the\nlearning of motion features, resulting in poor out-of-distribution (OOD)\ngeneralization. The video background is clearly a source of static bias, but\nthe video foreground, such as the clothing of the actor, can also provide\nstatic bias. In this paper, we empirically verify the existence of foreground\nstatic bias by creating test videos with conflicting signals from the static\nand moving portions of the video. To tackle this issue, we propose a simple yet\neffective technique, StillMix, to learn robust action representations.\nSpecifically, StillMix identifies bias-inducing video frames using a 2D\nreference network and mixes them with videos for training, serving as effective\nbias suppression even when we cannot explicitly extract the source of bias\nwithin each video frame or enumerate types of bias. Finally, to precisely\nevaluate static bias, we synthesize two new benchmarks, SCUBA for static cues\nin the background, and SCUFO for static cues in the foreground. With extensive\nexperiments, we demonstrate that StillMix mitigates both types of static bias\nand improves video representations for downstream applications. Code is\navailable at https://github.com/lihaoxin05/StillMix.",
        "authors": [
            "Haoxin Li",
            "Yuan Liu",
            "Hanwang Zhang",
            "Boyang Li"
        ]
    },
    {
        "title": "RbA: Segmenting Unknown Regions Rejected by All",
        "url": "http://arxiv.org/abs/2211.14293",
        "abstract": "Standard semantic segmentation models owe their success to curated datasets\nwith a fixed set of semantic categories, without contemplating the possibility\nof identifying unknown objects from novel categories. Existing methods in\noutlier detection suffer from a lack of smoothness and objectness in their\npredictions, due to limitations of the per-pixel classification paradigm.\nFurthermore, additional training for detecting outliers harms the performance\nof known classes. In this paper, we explore another paradigm with region-level\nclassification to better segment unknown objects. We show that the object\nqueries in mask classification tend to behave like one \\vs all classifiers.\nBased on this finding, we propose a novel outlier scoring function called RbA\nby defining the event of being an outlier as being rejected by all known\nclasses. Our extensive experiments show that mask classification improves the\nperformance of the existing outlier detection methods, and the best results are\nachieved with the proposed RbA. We also propose an objective to optimize RbA\nusing minimal outlier supervision. Further fine-tuning with outliers improves\nthe unknown performance, and unlike previous methods, it does not degrade the\ninlier performance.",
        "authors": [
            "Nazir Nayal",
            "M\u0131sra Yavuz",
            "Jo\u00e3o F. Henriques",
            "Fatma G\u00fcney"
        ]
    },
    {
        "title": "CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution",
        "url": "http://arxiv.org/abs/2303.16242",
        "abstract": "Medical image arbitrary-scale super-resolution (MIASSR) has recently gained\nwidespread attention, aiming to super sample medical volumes at arbitrary\nscales via a single model. However, existing MIASSR methods face two major\nlimitations: (i) reliance on high-resolution (HR) volumes and (ii) limited\ngeneralization ability, which restricts their application in various scenarios.\nTo overcome these limitations, we propose Cube-based Neural Radiance Field\n(CuNeRF), a zero-shot MIASSR framework that can yield medical images at\narbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR\nmethods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF\nfocuses on building a coordinate-intensity continuous representation from LR\nvolumes without the need for HR references. This is achieved by the proposed\ndifferentiable modules: including cube-based sampling, isotropic volume\nrendering, and cube-based hierarchical rendering. Through extensive experiments\non magnetic resource imaging (MRI) and computed tomography (CT) modalities, we\ndemonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF\nyields better visual verisimilitude and reduces aliasing artifacts at various\nupsampling factors. Moreover, our CuNeRF does not need any LR-HR training\npairs, which is more flexible and easier to be used than others. Our code will\nbe publicly available soon.",
        "authors": [
            "Zixuan Chen",
            "Jian-Huang Lai",
            "Lingxiao Yang",
            "Xiaohua Xie"
        ]
    },
    {
        "title": "Beyond Object Recognition: A New Benchmark towards Object Concept Learning",
        "url": "http://arxiv.org/abs/2212.02710",
        "abstract": "Understanding objects is a central building block of artificial intelligence,\nespecially for embodied AI. Even though object recognition excels with deep\nlearning, current machines still struggle to learn higher-level knowledge,\ne.g., what attributes an object has, and what can we do with an object. In this\nwork, we propose a challenging Object Concept Learning (OCL) task to push the\nenvelope of object understanding. It requires machines to reason out object\naffordances and simultaneously give the reason: what attributes make an object\npossesses these affordances. To support OCL, we build a densely annotated\nknowledge base including extensive labels for three levels of object concept\n(category, attribute, affordance), and the causal relations of three levels. By\nanalyzing the causal structure of OCL, we present a baseline, Object Concept\nReasoning Network (OCRN). It leverages causal intervention and concept\ninstantiation to infer the three levels following their causal relations. In\nexperiments, OCRN effectively infers the object knowledge while following the\ncausalities well. Our data and code are available at https://mvig-rhos.com/ocl.",
        "authors": [
            "Yong-Lu Li",
            "Yue Xu",
            "Xinyu Xu",
            "Xiaohan Mao",
            "Yuan Yao",
            "Siqi Liu",
            "Cewu Lu"
        ]
    },
    {
        "title": "Towards Open-Vocabulary Video Instance Segmentation",
        "url": "http://arxiv.org/abs/2304.01715",
        "abstract": "Video Instance Segmentation (VIS) aims at segmenting and categorizing objects\nin videos from a closed set of training categories, lacking the generalization\nability to handle novel categories in real-world videos. To address this\nlimitation, we make the following three contributions. First, we introduce the\nnovel task of Open-Vocabulary Video Instance Segmentation, which aims to\nsimultaneously segment, track, and classify objects in videos from open-set\ncategories, including novel categories unseen during training. Second, to\nbenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance\nSegmentation dataset (LV-VIS), that contains well-annotated objects from 1,196\ndiverse categories, significantly surpassing the category size of existing\ndatasets by more than one order of magnitude. Third, we propose an efficient\nMemory-Induced Transformer architecture, OV2Seg, to first achieve\nOpen-Vocabulary VIS in an end-to-end manner with near real-time inference\nspeed. Extensive experiments on LV-VIS and four existing VIS datasets\ndemonstrate the strong zero-shot generalization ability of OV2Seg on novel\ncategories. The dataset and code are released here\nhttps://github.com/haochenheheda/LVVIS.",
        "authors": [
            "Haochen Wang",
            "Cilin Yan",
            "Shuai Wang",
            "Xiaolong Jiang",
            "XU Tang",
            "Yao Hu",
            "Weidi Xie",
            "Efstratios Gavves"
        ]
    },
    {
        "title": "EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding",
        "url": "http://arxiv.org/abs/2309.08816",
        "abstract": "Object understanding in egocentric visual data is arguably a fundamental\nresearch topic in egocentric vision. However, existing object datasets are\neither non-egocentric or have limitations in object categories, visual content,\nand annotation granularities. In this work, we introduce EgoObjects, a\nlarge-scale egocentric dataset for fine-grained object understanding. Its Pilot\nversion contains over 9K videos collected by 250 participants from 50+\ncountries using 4 wearable devices, and over 650K object annotations from 368\nobject categories. Unlike prior datasets containing only object category\nlabels, EgoObjects also annotates each object with an instance-level\nidentifier, and includes over 14K unique object instances. EgoObjects was\ndesigned to capture the same object under diverse background complexities,\nsurrounding objects, distance, lighting and camera motion. In parallel to the\ndata collection, we conducted data annotation by developing a multi-stage\nfederated annotation process to accommodate the growing nature of the dataset.\nTo bootstrap the research on EgoObjects, we present a suite of 4 benchmark\ntasks around the egocentric object understanding, including a novel instance\nlevel- and the classical category level object detection. Moreover, we also\nintroduce 2 novel continual learning object detection tasks. The dataset and\nAPI are available at https://github.com/facebookresearch/EgoObjects.",
        "authors": [
            "Chenchen Zhu",
            "Fanyi Xiao",
            "Andres Alvarado",
            "Yasmine Babaei",
            "Jiabo Hu",
            "Hichem El-Mohri",
            "Sean Chang Culatana",
            "Roshan Sumbaly",
            "Zhicheng Yan"
        ]
    },
    {
        "title": "What Can Simple Arithmetic Operations Do for Temporal Modeling?",
        "url": "http://arxiv.org/abs/2307.08908",
        "abstract": "Temporal modeling plays a crucial role in understanding video content. To\ntackle this problem, previous studies built complicated temporal relations\nthrough time sequence thanks to the development of computationally powerful\ndevices. In this work, we explore the potential of four simple arithmetic\noperations for temporal modeling. Specifically, we first capture auxiliary\ntemporal cues by computing addition, subtraction, multiplication, and division\nbetween pairs of extracted frame features. Then, we extract corresponding\nfeatures from these cues to benefit the original temporal-irrespective domain.\nWe term such a simple pipeline as an Arithmetic Temporal Module (ATM), which\noperates on the stem of a visual backbone with a plug-and-play style. We\nconduct comprehensive ablation studies on the instantiation of ATMs and\ndemonstrate that this module provides powerful temporal modeling capability at\na low computational cost. Moreover, the ATM is compatible with both CNNs- and\nViTs-based architectures. Our results show that ATM achieves superior\nperformance over several popular video benchmarks. Specifically, on\nSomething-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%,\n74.6%, and 89.4% respectively. The code is available at\nhttps://github.com/whwu95/ATM.",
        "authors": [
            "Wenhao Wu",
            "Yuxin Song",
            "Zhun Sun",
            "Jingdong Wang",
            "Chang Xu",
            "Wanli Ouyang"
        ]
    },
    {
        "title": "Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction",
        "url": "http://arxiv.org/abs/2308.10820",
        "abstract": "Hyperspectral Image (HSI) reconstruction has made gratifying progress with\nthe deep unfolding framework by formulating the problem into a data module and\na prior module. Nevertheless, existing methods still face the problem of\ninsufficient matching with HSI data. The issues lie in three aspects: 1) fixed\ngradient descent step in the data module while the degradation of HSI is\nagnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3)\nstage interaction ignoring the differences in features at different stages. To\naddress these issues, in this work, we propose a Pixel Adaptive Deep Unfolding\nTransformer (PADUT) for HSI reconstruction. In the data module, a pixel\nadaptive descent step is employed to focus on pixel-level agnostic degradation.\nIn the prior module, we introduce the Non-local Spectral Transformer (NST) to\nemphasize the 3D characteristics of HSI for recovering. Moreover, inspired by\nthe diverse expression of features in different stages and depths, the stage\ninteraction is improved by the Fast Fourier Transform (FFT). Experimental\nresults on both simulated and real scenes exhibit the superior performance of\nour method compared to state-of-the-art HSI reconstruction methods. The code is\nreleased at: https://github.com/MyuLi/PADUT.",
        "authors": [
            "Miaoyu Li",
            "Ying Fu",
            "Ji Liu",
            "Yulun Zhang"
        ]
    },
    {
        "title": "BiViT: Extremely Compressed Binary Vision Transformers",
        "url": "http://arxiv.org/abs/2211.07091",
        "abstract": "Model binarization can significantly compress model size, reduce energy\nconsumption, and accelerate inference through efficient bit-wise operations.\nAlthough binarizing convolutional neural networks have been extensively\nstudied, there is little work on exploring binarization of vision Transformers\nwhich underpin most recent breakthroughs in visual recognition. To this end, we\npropose to solve two fundamental challenges to push the horizon of Binary\nVision Transformers (BiViT). First, the traditional binary method does not take\nthe long-tailed distribution of softmax attention into consideration, bringing\nlarge binarization errors in the attention module. To solve this, we propose\nSoftmax-aware Binarization, which dynamically adapts to the data distribution\nand reduces the error caused by binarization. Second, to better preserve the\ninformation of the pretrained model and restore accuracy, we propose a\nCross-layer Binarization scheme that decouples the binarization of\nself-attention and multi-layer perceptrons (MLPs), and Parameterized Weight\nScales which introduce learnable scaling factors for weight binarization.\nOverall, our method performs favorably against state-of-the-arts by 19.8% on\nthe TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6%\nTop-1 accuracy over Swin-S model. Additionally, on COCO object detection, our\nmethod achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN\nframework.",
        "authors": [
            "Yefei He",
            "Zhenyu Lou",
            "Luoming Zhang",
            "Jing Liu",
            "Weijia Wu",
            "Hong Zhou",
            "Bohan Zhuang"
        ]
    },
    {
        "title": "Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF",
        "url": "http://arxiv.org/abs/2307.15333",
        "abstract": "The explicit neural radiance field (NeRF) has gained considerable interest\nfor its efficient training and fast inference capabilities, making it a\npromising direction such as virtual reality and gaming. In particular,\nPlenOctree (POT)[1], an explicit hierarchical multi-scale octree\nrepresentation, has emerged as a structural and influential framework. However,\nPOT's fixed structure for direct optimization is sub-optimal as the scene\ncomplexity evolves continuously with updates to cached color and density,\nnecessitating refining the sampling distribution to capture signal complexity\naccordingly. To address this issue, we propose the dynamic PlenOctree DOT,\nwhich adaptively refines the sample distribution to adjust to changing scene\ncomplexity. Specifically, DOT proposes a concise yet novel hierarchical feature\nfusion strategy during the iterative rendering process. Firstly, it identifies\nthe regions of interest through training signals to ensure adaptive and\nefficient refinement. Next, rather than directly filtering out valueless nodes,\nDOT introduces the sampling and pruning operations for octrees to aggregate\nfeatures, enabling rapid parameter learning. Compared with POT, our DOT\noutperforms it by enhancing visual quality, reducing over $55.15$/$68.84\\%$\nparameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\\&$\nTemples, respectively. Project homepage:https://vlislab22.github.io/DOT.\n  [1] Yu, Alex, et al. \"Plenoctrees for real-time rendering of neural radiance\nfields.\" Proceedings of the IEEE/CVF International Conference on Computer\nVision. 2021.",
        "authors": [
            "Haotian Bai",
            "Yiqi Lin",
            "Yize Chen",
            "Lin Wang"
        ]
    },
    {
        "title": "Scene Matters: Model-based Deep Video Compression",
        "url": "http://arxiv.org/abs/2303.04557",
        "abstract": "Video compression has always been a popular research area, where many\ntraditional and deep video compression methods have been proposed. These\nmethods typically rely on signal prediction theory to enhance compression\nperformance by designing high efficient intra and inter prediction strategies\nand compressing video frames one by one. In this paper, we propose a novel\nmodel-based video compression (MVC) framework that regards scenes as the\nfundamental units for video sequences. Our proposed MVC directly models the\nintensity variation of the entire video sequence in one scene, seeking\nnon-redundant representations instead of reducing redundancy through\nspatio-temporal predictions. To achieve this, we employ implicit neural\nrepresentation as our basic modeling architecture. To improve the efficiency of\nvideo modeling, we first propose context-related spatial positional embedding\nand frequency domain supervision in spatial context enhancement. For temporal\ncorrelation capturing, we design the scene flow constrain mechanism and\ntemporal contrastive loss. Extensive experimental results demonstrate that our\nmethod achieves up to a 20\\% bitrate reduction compared to the latest video\ncoding standard H.266 and is more efficient in decoding than existing video\ncoding strategies.",
        "authors": [
            "Lv Tang",
            "Xinfeng Zhang",
            "Gai Zhang",
            "Xiaoqi Ma"
        ]
    },
    {
        "title": "Tree-Structured Shading Decomposition",
        "url": "http://arxiv.org/abs/2309.07122",
        "abstract": "We study inferring a tree-structured representation from a single image for\nobject shading. Prior work typically uses the parametric or measured\nrepresentation to model shading, which is neither interpretable nor easily\neditable. We propose using the shade tree representation, which combines basic\nshading nodes and compositing methods to factorize object surface shading. The\nshade tree representation enables novice users who are unfamiliar with the\nphysical shading process to edit object shading in an efficient and intuitive\nmanner. A main challenge in inferring the shade tree is that the inference\nproblem involves both the discrete tree structure and the continuous parameters\nof the tree nodes. We propose a hybrid approach to address this issue. We\nintroduce an auto-regressive inference model to generate a rough estimation of\nthe tree structure and node parameters, and then we fine-tune the inferred\nshade tree through an optimization algorithm. We show experiments on synthetic\nimages, captured reflectance, real images, and non-realistic vector drawings,\nallowing downstream applications such as material editing, vectorized shading,\nand relighting. Project website: https://chen-geng.com/inv-shade-trees",
        "authors": [
            "Chen Geng",
            "Hong-Xing Yu",
            "Sharon Zhang",
            "Maneesh Agrawala",
            "Jiajun Wu"
        ]
    },
    {
        "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones",
        "url": "http://arxiv.org/abs/2211.09703",
        "abstract": "The superior performance of modern deep networks usually comes with a costly\ntraining procedure. This paper presents a new curriculum learning approach for\nthe efficient training of visual backbones (e.g., vision Transformers). Our\nwork is inspired by the inherent learning dynamics of deep networks: we\nexperimentally show that at an earlier training stage, the model mainly learns\nto recognize some 'easier-to-learn' discriminative patterns within each\nexample, e.g., the lower-frequency components of images and the original\ninformation before data augmentation. Driven by this phenomenon, we propose a\ncurriculum where the model always leverages all the training data at each\nepoch, while the curriculum starts with only exposing the 'easier-to-learn'\npatterns of each example, and introduces gradually more difficult patterns. To\nimplement this idea, we 1) introduce a cropping operation in the Fourier\nspectrum of the inputs, which enables the model to learn from only the\nlower-frequency components efficiently, 2) demonstrate that exposing the\nfeatures of original images amounts to adopting weaker data augmentation, and\n3) integrate 1) and 2) and design a curriculum learning schedule with a\ngreedy-search algorithm. The resulting approach, EfficientTrain, is simple,\ngeneral, yet surprisingly effective. As an off-the-shelf method, it reduces the\nwall-time training cost of a wide variety of popular models (e.g., ResNet,\nConvNeXt, DeiT, PVT, Swin, and CSWin) by >1.5x on ImageNet-1K/22K without\nsacrificing accuracy. It is also effective for self-supervised learning (e.g.,\nMAE). Code is available at https://github.com/LeapLabTHU/EfficientTrain.",
        "authors": [
            "Yulin Wang",
            "Yang Yue",
            "Rui Lu",
            "Tianjiao Liu",
            "Zhao Zhong",
            "Shiji Song",
            "Gao Huang"
        ]
    },
    {
        "title": "Simulating Fluids in Real-World Still Images",
        "url": "http://arxiv.org/abs/2204.11335",
        "abstract": "In this work, we tackle the problem of real-world fluid animation from a\nstill image. The key of our system is a surface-based layered representation\nderiving from video decomposition, where the scene is decoupled into a surface\nfluid layer and an impervious background layer with corresponding\ntransparencies to characterize the composition of the two layers. The animated\nvideo can be produced by warping only the surface fluid layer according to the\nestimation of fluid motions and recombining it with the background. In\naddition, we introduce surface-only fluid simulation, a $2.5D$ fluid\ncalculation version, as a replacement for motion estimation. Specifically, we\nleverage the triangular mesh based on a monocular depth estimator to represent\nthe fluid surface layer and simulate the motion in the physics-based framework\nwith the inspiration of the classic theory of the hybrid Lagrangian-Eulerian\nmethod, along with a learnable network so as to adapt to complex real-world\nimage textures. We demonstrate the effectiveness of the proposed system through\ncomparison with existing methods in both standard objective metrics and\nsubjective ranking scores. Extensive experiments not only indicate our method's\ncompetitive performance for common fluid scenes but also better robustness and\nreasonability under complex transparent fluid scenarios. Moreover, as the\nproposed surface-based layer representation and surface-only fluid simulation\nnaturally disentangle the scene, interactive editing such as adding objects to\nthe river and texture replacing could be easily achieved with realistic\nresults.",
        "authors": [
            "Siming Fan",
            "Jingtan Piao",
            "Chen Qian",
            "Kwan-Yee Lin",
            "Hongsheng Li"
        ]
    },
    {
        "title": "SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data",
        "url": "http://arxiv.org/abs/2308.05410",
        "abstract": "This paper proposes a new method to infer keypoints from arbitrary object\ncategories in practical scenarios where point cloud data (PCD) are noisy,\ndown-sampled and arbitrarily rotated. Our proposed model adheres to the\nfollowing principles: i) keypoints inference is fully unsupervised (no\nannotation given), ii) keypoints position error should be low and resilient to\nPCD perturbations (robustness), iii) keypoints should not change their indexes\nfor the intra-class objects (semantic coherence), iv) keypoints should be close\nto or proximal to PCD surface (compactness). We achieve these desiderata by\nproposing a new self-supervised training strategy for keypoints estimation that\ndoes not assume any a priori knowledge of the object class, and a model\narchitecture with coupled auxiliary losses that promotes the desired keypoints\nproperties. We compare the keypoints estimated by the proposed approach with\nthose of the state-of-the-art unsupervised approaches. The experiments show\nthat our approach outperforms by estimating keypoints with improved coverage\n(+9.41%) while being semantically consistent (+4.66%) that best characterizes\nthe object's 3D shape for downstream tasks. Code and data are available at:\nhttps://github.com/IITPAVIS/SC3K",
        "authors": [
            "Mohammad Zohaib",
            "Alessio Del Bue"
        ]
    },
    {
        "title": "IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis",
        "url": "http://arxiv.org/abs/2210.00647",
        "abstract": "Existing inverse rendering combined with neural rendering methods can only\nperform editable novel view synthesis on object-specific scenes, while we\npresent intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce\nintrinsic decomposition into the NeRF-based neural rendering method and can\nextend its application to room-scale scenes. Since intrinsic decomposition is a\nfundamentally under-constrained inverse problem, we propose a novel\ndistance-aware point sampling and adaptive reflectance iterative clustering\noptimization method, which enables IntrinsicNeRF with traditional intrinsic\ndecomposition constraints to be trained in an unsupervised manner, resulting in\nmulti-view consistent intrinsic decomposition results. To cope with the problem\nthat different adjacent instances of similar reflectance in a scene are\nincorrectly clustered together, we further propose a hierarchical clustering\nmethod with coarse-to-fine optimization to obtain a fast hierarchical indexing\nrepresentation. It supports compelling real-time augmented applications such as\nrecoloring and illumination variation. Extensive experiments and editing\nsamples on both object-specific/room-scale scenes and synthetic/real-word data\ndemonstrate that we can obtain consistent intrinsic decomposition results and\nhigh-fidelity novel view synthesis even for challenging sequences.",
        "authors": [
            "Weicai Ye",
            "Shuo Chen",
            "Chong Bao",
            "Hujun Bao",
            "Marc Pollefeys",
            "Zhaopeng Cui",
            "Guofeng Zhang"
        ]
    },
    {
        "title": "Segmenting Known Objects and Unseen Unknowns without Prior Knowledge",
        "url": "http://arxiv.org/abs/2209.05407",
        "abstract": "Panoptic segmentation methods assign a known class to each pixel given in\ninput. Even for state-of-the-art approaches, this inevitably enforces decisions\nthat systematically lead to wrong predictions for objects outside the training\ncategories. However, robustness against out-of-distribution samples and corner\ncases is crucial in safety-critical settings to avoid dangerous consequences.\nSince real-world datasets cannot contain enough data points to adequately\nsample the long tail of the underlying distribution, models must be able to\ndeal with unseen and unknown scenarios as well. Previous methods targeted this\nby re-identifying already-seen unlabeled objects. In this work, we propose the\nnecessary step to extend segmentation with a new setting which we term holistic\nsegmentation. Holistic segmentation aims to identify and separate objects of\nunseen, unknown categories into instances without any prior knowledge about\nthem while performing panoptic segmentation of known classes. We tackle this\nnew problem with U3HS, which finds unknowns as highly uncertain regions and\nclusters their corresponding instance-aware embeddings into individual objects.\nBy doing so, for the first time in panoptic segmentation with unknown objects,\nour U3HS is trained without unknown categories, reducing assumptions and\nleaving the settings as unconstrained as in real-life scenarios. Extensive\nexperiments on public data from MS COCO, Cityscapes, and Lost&Found demonstrate\nthe effectiveness of U3HS for this new, challenging, and assumptions-free\nsetting called holistic segmentation. Project page:\nhttps://holisticseg.github.io.",
        "authors": [
            "Stefano Gasperini",
            "Alvaro Marcos-Ramiro",
            "Michael Schmidt",
            "Nassir Navab",
            "Benjamin Busam",
            "Federico Tombari"
        ]
    },
    {
        "title": "A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.12574",
        "abstract": "In this paper, we strive to answer the question \"how to collaboratively learn\nconvolutional neural network (CNN)-based and vision transformer (ViT)-based\nmodels by selecting and exchanging the reliable knowledge between them for\nsemantic segmentation?\" Accordingly, we propose an online knowledge\ndistillation (KD) framework that can simultaneously learn compact yet effective\nCNN-based and ViT-based models with two key technical breakthroughs to take\nfull advantage of CNNs and ViT while compensating their limitations. Firstly,\nwe propose heterogeneous feature distillation (HFD) to improve students'\nconsistency in low-layer feature space by mimicking heterogeneous features\nbetween CNNs and ViT. Secondly, to facilitate the two students to learn\nreliable knowledge from each other, we propose bidirectional selective\ndistillation (BSD) that can dynamically transfer selective knowledge. This is\nachieved by 1) region-wise BSD determining the directions of knowledge\ntransferred between the corresponding regions in the feature space and 2)\npixel-wise BSD discerning which of the prediction knowledge to be transferred\nin the logit space. Extensive experiments on three benchmark datasets\ndemonstrate that our proposed framework outperforms the state-of-the-art online\ndistillation methods by a large margin, and shows its efficacy in learning\ncollaboratively between ViT-based and CNN-based models.",
        "authors": [
            "Jinjing Zhu",
            "Yunhao Luo",
            "Xu Zheng",
            "Hao Wang",
            "Lin Wang"
        ]
    },
    {
        "title": "CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.15942",
        "abstract": "Most nighttime semantic segmentation studies are based on domain adaptation\napproaches and image input. However, limited by the low dynamic range of\nconventional cameras, images fail to capture structural details and boundary\ninformation in low-light conditions. Event cameras, as a new form of vision\nsensors, are complementary to conventional cameras with their high dynamic\nrange. To this end, we propose a novel unsupervised Cross-Modality Domain\nAdaptation (CMDA) framework to leverage multi-modality (Images and Events)\ninformation for nighttime semantic segmentation, with only labels on daytime\nimages. In CMDA, we design the Image Motion-Extractor to extract motion\ninformation and the Image Content-Extractor to extract content information from\nimages, in order to bridge the gap between different modalities (Images to\nEvents) and domains (Day to Night). Besides, we introduce the first image-event\nnighttime semantic segmentation dataset. Extensive experiments on both the\npublic image dataset and the proposed image-event dataset demonstrate the\neffectiveness of our proposed approach. We open-source our code, models, and\ndataset at https://github.com/XiaRho/CMDA.",
        "authors": [
            "Ruihao Xia",
            "Chaoqiang Zhao",
            "Meng Zheng",
            "Ziyan Wu",
            "Qiyu Sun",
            "Yang Tang"
        ]
    },
    {
        "title": "Dynamic Residual Classifier for Class Incremental Learning",
        "url": "http://arxiv.org/abs/2308.13305",
        "abstract": "The rehearsal strategy is widely used to alleviate the catastrophic\nforgetting problem in class incremental learning (CIL) by preserving limited\nexemplars from previous tasks. With imbalanced sample numbers between old and\nnew classes, the classifier learning can be biased. Existing CIL methods\nexploit the long-tailed (LT) recognition techniques, e.g., the adjusted losses\nand the data re-sampling methods, to handle the data imbalance issue within\neach increment task. In this work, the dynamic nature of data imbalance in CIL\nis shown and a novel Dynamic Residual Classifier (DRC) is proposed to handle\nthis challenging scenario. Specifically, DRC is built upon a recent advance\nresidual classifier with the branch layer merging to handle the model-growing\nproblem. Moreover, DRC is compatible with different CIL pipelines and\nsubstantially improves them. Combining DRC with the model adaptation and fusion\n(MAF) pipeline, this method achieves state-of-the-art results on both the\nconventional CIL and the LT-CIL benchmarks. Extensive experiments are also\nconducted for a detailed analysis. The code is publicly available.",
        "authors": [
            "Xiuwei Chen",
            "Xiaobin Chang"
        ]
    },
    {
        "title": "Diverse Inpainting and Editing with GAN Inversion",
        "url": "http://arxiv.org/abs/2307.15033",
        "abstract": "Recent inversion methods have shown that real images can be inverted into\nStyleGAN's latent space and numerous edits can be achieved on those images\nthanks to the semantically rich feature representations of well-trained GAN\nmodels. However, extensive research has also shown that image inversion is\nchallenging due to the trade-off between high-fidelity reconstruction and\neditability. In this paper, we tackle an even more difficult task, inverting\nerased images into GAN's latent space for realistic inpaintings and editings.\nFurthermore, by augmenting inverted latent codes with different latent samples,\nwe achieve diverse inpaintings. Specifically, we propose to learn an encoder\nand mixing network to combine encoded features from erased images with\nStyleGAN's mapped features from random samples. To encourage the mixing network\nto utilize both inputs, we train the networks with generated data via a novel\nset-up. We also utilize higher-rate features to prevent color inconsistencies\nbetween the inpainted and unerased parts. We run extensive experiments and\ncompare our method with state-of-the-art inversion and inpainting methods.\nQualitative metrics and visual comparisons show significant improvements.",
        "authors": [
            "Ahmet Burak Yildirim",
            "Hamza Pehlivan",
            "Bahri Batuhan Bilecen",
            "Aysegul Dundar"
        ]
    },
    {
        "title": "InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion",
        "url": "http://arxiv.org/abs/2308.16905",
        "abstract": "This paper addresses a novel task of anticipating 3D human-object\ninteractions (HOIs). Most existing research on HOI synthesis lacks\ncomprehensive whole-body interactions with dynamic objects, e.g., often limited\nto manipulating small or static objects. Our task is significantly more\nchallenging, as it requires modeling dynamic objects with various shapes,\ncapturing whole-body motion, and ensuring physically valid interactions. To\nthis end, we propose InterDiff, a framework comprising two key steps: (i)\ninteraction diffusion, where we leverage a diffusion model to encode the\ndistribution of future human-object interactions; (ii) interaction correction,\nwhere we introduce a physics-informed predictor to correct denoised HOIs in a\ndiffusion step. Our key insight is to inject prior knowledge that the\ninteractions under reference with respect to contact points follow a simple\npattern and are easily predictable. Experiments on multiple human-object\ninteraction datasets demonstrate the effectiveness of our method for this task,\ncapable of producing realistic, vivid, and remarkably long-term 3D HOI\npredictions.",
        "authors": [
            "Sirui Xu",
            "Zhengyuan Li",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ]
    },
    {
        "title": "DiFaReli: Diffusion Face Relighting",
        "url": "http://arxiv.org/abs/2304.09479",
        "abstract": "We present a novel approach to single-view face relighting in the wild.\nHandling non-diffuse effects, such as global illumination or cast shadows, has\nlong been a challenge in face relighting. Prior work often assumes Lambertian\nsurfaces, simplified lighting models or involves estimating 3D shape, albedo,\nor a shadow map. This estimation, however, is error-prone and requires many\ntraining examples with lighting ground truth to generalize well. Our work\nbypasses the need for accurate estimation of intrinsic components and can be\ntrained solely on 2D images without any light stage data, multi-view images, or\nlighting ground truth. Our key idea is to leverage a conditional diffusion\nimplicit model (DDIM) for decoding a disentangled light encoding along with\nother encodings related to 3D shape and facial identity inferred from\noff-the-shelf estimators. We also propose a novel conditioning technique that\neases the modeling of the complex interaction between light and geometry by\nusing a rendered shading reference to spatially modulate the DDIM. We achieve\nstate-of-the-art performance on standard benchmark Multi-PIE and can\nphotorealistically relight in-the-wild images. Please visit our page:\nhttps://diffusion-face-relighting.github.io",
        "authors": [
            "Puntawat Ponglertnapakorn",
            "Nontawat Tritrong",
            "Supasorn Suwajanakorn"
        ]
    },
    {
        "title": "Localizing Object-Level Shape Variations with Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2303.11306",
        "abstract": "Text-to-image models give rise to workflows which often begin with an\nexploration step, where users sift through a large collection of generated\nimages. The global nature of the text-to-image generation process prevents\nusers from narrowing their exploration to a particular object in the image. In\nthis paper, we present a technique to generate a collection of images that\ndepicts variations in the shape of a specific object, enabling an object-level\nshape exploration process. Creating plausible variations is challenging as it\nrequires control over the shape of the generated object while respecting its\nsemantics. A particular challenge when generating object variations is\naccurately localizing the manipulation applied over the object's shape. We\nintroduce a prompt-mixing technique that switches between prompts along the\ndenoising process to attain a variety of shape choices. To localize the\nimage-space operation, we present two techniques that use the self-attention\nlayers in conjunction with the cross-attention layers. Moreover, we show that\nthese localization techniques are general and effective beyond the scope of\ngenerating object variations. Extensive results and comparisons demonstrate the\neffectiveness of our method in generating object variations, and the competence\nof our localization techniques.",
        "authors": [
            "Or Patashnik",
            "Daniel Garibi",
            "Idan Azuri",
            "Hadar Averbuch-Elor",
            "Daniel Cohen-Or"
        ]
    },
    {
        "title": "Curvature-Aware Training for Coordinate Networks",
        "url": "http://arxiv.org/abs/2305.08552",
        "abstract": "Coordinate networks are widely used in computer vision due to their ability\nto represent signals as compressed, continuous entities. However, training\nthese networks with first-order optimizers can be slow, hindering their use in\nreal-time applications. Recent works have opted for shallow voxel-based\nrepresentations to achieve faster training, but this sacrifices memory\nefficiency. This work proposes a solution that leverages second-order\noptimization methods to significantly reduce training times for coordinate\nnetworks while maintaining their compressibility. Experiments demonstrate the\neffectiveness of this approach on various signal modalities, such as audio,\nimages, videos, shape reconstruction, and neural radiance fields.",
        "authors": [
            "Hemanth Saratchandran",
            "Shin-Fang Chng",
            "Sameera Ramasinghe",
            "Lachlan MacDonald",
            "Simon Lucey"
        ]
    },
    {
        "title": "Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement",
        "url": "http://arxiv.org/abs/2307.09362",
        "abstract": "Most prior semantic segmentation methods have been developed for day-time\nscenes, while typically underperforming in night-time scenes due to\ninsufficient and complicated lighting conditions. In this work, we tackle this\nchallenge by proposing a novel night-time semantic segmentation paradigm, i.e.,\ndisentangle then parse (DTP). DTP explicitly disentangles night-time images\ninto light-invariant reflectance and light-specific illumination components and\nthen recognizes semantics based on their adaptive fusion. Concretely, the\nproposed DTP comprises two key components: 1) Instead of processing\nlighting-entangled features as in prior works, our Semantic-Oriented\nDisentanglement (SOD) framework enables the extraction of reflectance component\nwithout being impeded by lighting, allowing the network to consistently\nrecognize the semantics under cover of varying and complicated lighting\nconditions. 2) Based on the observation that the illumination component can\nserve as a cue for some semantically confused regions, we further introduce an\nIllumination-Aware Parser (IAParser) to explicitly learn the correlation\nbetween semantics and lighting, and aggregate the illumination features to\nyield more precise predictions. Extensive experiments on the night-time\nsegmentation task with various settings demonstrate that DTP significantly\noutperforms state-of-the-art methods. Furthermore, with negligible additional\nparameters, DTP can be directly used to benefit existing day-time methods for\nnight-time segmentation.",
        "authors": [
            "Zhixiang Wei",
            "Lin Chen",
            "Tao Tu",
            "Huaian Chen",
            "Pengyang Ling",
            "Yi Jin"
        ]
    },
    {
        "title": "ToonTalker: Cross-Domain Face Reenactment",
        "url": "http://arxiv.org/abs/2308.12866",
        "abstract": "We target cross-domain face reenactment in this paper, i.e., driving a\ncartoon image with the video of a real person and vice versa. Recently, many\nworks have focused on one-shot talking face generation to drive a portrait with\na real video, i.e., within-domain reenactment. Straightforwardly applying those\nmethods to cross-domain animation will cause inaccurate expression transfer,\nblur effects, and even apparent artifacts due to the domain shift between\ncartoon and real faces. Only a few works attempt to settle cross-domain face\nreenactment. The most related work AnimeCeleb requires constructing a dataset\nwith pose vector and cartoon image pairs by animating 3D characters, which\nmakes it inapplicable anymore if no paired data is available. In this paper, we\npropose a novel method for cross-domain reenactment without paired data.\nSpecifically, we propose a transformer-based framework to align the motions\nfrom different domains into a common latent space where motion transfer is\nconducted via latent code addition. Two domain-specific motion encoders and two\nlearnable motion base memories are used to capture domain properties. A source\nquery transformer and a driving one are exploited to project domain-specific\nmotion to the canonical space. The edited motion is projected back to the\ndomain of the source with a transformer. Moreover, since no paired data is\nprovided, we propose a novel cross-domain training scheme using data from two\ndomains with the designed analogy constraint. Besides, we contribute a cartoon\ndataset in Disney style. Extensive evaluations demonstrate the superiority of\nour method over competing methods.",
        "authors": [
            "Yuan Gong",
            "Yong Zhang",
            "Xiaodong Cun",
            "Fei Yin",
            "Yanbo Fan",
            "Xuan Wang",
            "Baoyuan Wu",
            "Yujiu Yang"
        ]
    },
    {
        "title": "LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition",
        "url": "http://arxiv.org/abs/2308.12774",
        "abstract": "The diversity in length constitutes a significant characteristic of text. Due\nto the long-tail distribution of text lengths, most existing methods for scene\ntext recognition (STR) only work well on short or seen-length text, lacking the\ncapability of recognizing longer text or performing length extrapolation. This\nis a crucial issue, since the lengths of the text to be recognized are usually\nnot given in advance in real-world applications, but it has not been adequately\ninvestigated in previous works. Therefore, we propose in this paper a method\ncalled Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the\nlimitation regarding the robustness to various text lengths. Specifically, a\nNeighbor Decoder is proposed to obtain accurate character attention maps with\nthe assistance of a novel neighbor matrix regardless of the text lengths.\nBesides, a Feature Enhancement Module is devised to model the long-range\ndependency with low computation cost, which is able to perform iterations with\nthe neighbor decoder to enhance the feature map progressively. To the best of\nour knowledge, we are the first to achieve effective length-insensitive scene\ntext recognition. Extensive experiments demonstrate that the proposed LISTER\nalgorithm exhibits obvious superiority on long text recognition and the ability\nfor length extrapolation, while comparing favourably with the previous\nstate-of-the-art methods on standard benchmarks for STR (mainly short text).",
        "authors": [
            "Changxu Cheng",
            "Peng Wang",
            "Cheng Da",
            "Qi Zheng",
            "Cong Yao"
        ]
    },
    {
        "title": "Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery",
        "url": "http://arxiv.org/abs/2307.10943",
        "abstract": "Recent advances in deep learning have significantly improved the performance\nof various computer vision applications. However, discovering novel categories\nin an incremental learning scenario remains a challenging problem due to the\nlack of prior knowledge about the number and nature of new categories. Existing\nmethods for novel category discovery are limited by their reliance on labeled\ndatasets and prior knowledge about the number of novel categories and the\nproportion of novel samples in the batch. To address the limitations and more\naccurately reflect real-world scenarios, in this paper, we propose a novel\nunsupervised class incremental learning approach for discovering novel\ncategories on unlabeled sets without prior knowledge. The proposed method\nfine-tunes the feature extractor and proxy anchors on labeled sets, then splits\nsamples into old and novel categories and clusters on the unlabeled dataset.\nFurthermore, the proxy anchors-based exemplar generates representative category\nvectors to mitigate catastrophic forgetting. Experimental results demonstrate\nthat our proposed approach outperforms the state-of-the-art methods on\nfine-grained datasets under real-world scenarios.",
        "authors": [
            "Hyungmin Kim",
            "Sungho Suh",
            "Daehwan Kim",
            "Daun Jeong",
            "Hansang Cho",
            "Junmo Kim"
        ]
    },
    {
        "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
        "url": "http://arxiv.org/abs/2309.03406",
        "abstract": "Pre-trained vision-language models (VLMs) have shown impressive performance\non various downstream tasks by utilizing knowledge learned from large data. In\ngeneral, the performance of VLMs on target tasks can be further improved by\nprompt tuning, which adds context to the input image or text. By leveraging\ndata from target tasks, various prompt-tuning methods have been studied in the\nliterature. A key to prompt tuning is the feature space alignment between two\nmodalities via learnable vectors with model parameters fixed. We observed that\nthe alignment becomes more effective when embeddings of each modality are\n`well-arranged' in the latent space. Inspired by this observation, we proposed\ndistribution-aware prompt tuning (DAPT) for vision-language models, which is\nsimple yet effective. Specifically, the prompts are learned by maximizing\ninter-dispersion, the distance between classes, as well as minimizing the\nintra-dispersion measured by the distance between embeddings from the same\nclass. Our extensive experiments on 11 benchmark datasets demonstrate that our\nmethod significantly improves generalizability. The code is available at\nhttps://github.com/mlvlab/DAPT.",
        "authors": [
            "Eulrang Cho",
            "Jooyeon Kim",
            "Hyunwoo J. Kim"
        ]
    },
    {
        "title": "FBLNet: FeedBack Loop Network for Driver Attention Prediction",
        "url": "http://arxiv.org/abs/2212.02096",
        "abstract": "The problem of predicting driver attention from the driving perspective is\ngaining increasing research focus due to its remarkable significance for\nautonomous driving and assisted driving systems. The driving experience is\nextremely important for safe driving,a skilled driver is able to effortlessly\npredict oncoming danger (before it becomes salient) based on the driving\nexperience and quickly pay attention to the corresponding zones.However, the\nnonobjective driving experience is difficult to model, so a mechanism\nsimulating the driver experience accumulation procedure is absent in existing\nmethods, and the current methods usually follow the technique line of saliency\nprediction methods to predict driver attention. In this paper, we propose a\nFeedBack Loop Network (FBLNet), which attempts to model the driving experience\naccumulation procedure. By over-and-over iterations, FBLNet generates the\nincremental knowledge that carries rich historically-accumulative and long-term\ntemporal information. The incremental knowledge in our model is like the\ndriving experience of humans. Under the guidance of the incremental knowledge,\nour model fuses the CNN feature and Transformer feature that are extracted from\nthe input image to predict driver attention. Our model exhibits a solid\nadvantage over existing methods, achieving an outstanding performance\nimprovement on two driver attention benchmark datasets.",
        "authors": [
            "Yilong Chen",
            "Zhixiong Nan",
            "Tao Xiang"
        ]
    },
    {
        "title": "Source-free Domain Adaptive Human Pose Estimation",
        "url": "http://arxiv.org/abs/2308.03202",
        "abstract": "Human Pose Estimation (HPE) is widely used in various fields, including\nmotion analysis, healthcare, and virtual reality. However, the great expenses\nof labeled real-world datasets present a significant challenge for HPE. To\novercome this, one approach is to train HPE models on synthetic datasets and\nthen perform domain adaptation (DA) on real-world data. Unfortunately, existing\nDA methods for HPE neglect data privacy and security by using both source and\ntarget data in the adaptation process. To this end, we propose a new task,\nnamed source-free domain adaptive HPE, which aims to address the challenges of\ncross-domain learning of HPE without access to source data during the\nadaptation process. We further propose a novel framework that consists of three\nmodels: source model, intermediate model, and target model, which explores the\ntask from both source-protect and target-relevant perspectives. The\nsource-protect module preserves source information more effectively while\nresisting noise, and the target-relevant module reduces the sparsity of spatial\nrepresentations by building a novel spatial probability space, and\npose-specific contrastive learning and information maximization are proposed on\nthe basis of this space. Comprehensive experiments on several domain adaptive\nHPE benchmarks show that the proposed method outperforms existing approaches by\na considerable margin. The codes are available at\nhttps://github.com/davidpengucf/SFDAHPE.",
        "authors": [
            "Qucheng Peng",
            "Ce Zheng",
            "Chen Chen"
        ]
    },
    {
        "title": "DOT: A Distillation-Oriented Trainer",
        "url": "http://arxiv.org/abs/2307.08436",
        "abstract": "Knowledge distillation transfers knowledge from a large model to a small one\nvia task and distillation losses. In this paper, we observe a trade-off between\ntask and distillation losses, i.e., introducing distillation loss limits the\nconvergence of task loss. We believe that the trade-off results from the\ninsufficient optimization of distillation loss. The reason is: The teacher has\na lower task loss than the student, and a lower distillation loss drives the\nstudent more similar to the teacher, then a better-converged task loss could be\nobtained. To break the trade-off, we propose the Distillation-Oriented Trainer\n(DOT). DOT separately considers gradients of task and distillation losses, then\napplies a larger momentum to distillation loss to accelerate its optimization.\nWe empirically prove that DOT breaks the trade-off, i.e., both losses are\nsufficiently optimized. Extensive experiments validate the superiority of DOT.\nNotably, DOT achieves a +2.59% accuracy improvement on ImageNet-1k for the\nResNet50-MobileNetV1 pair. Conclusively, DOT greatly benefits the student's\noptimization properties in terms of loss convergence and model generalization.\nCode will be made publicly available.",
        "authors": [
            "Borui Zhao",
            "Quan Cui",
            "Renjie Song",
            "Jiajun Liang"
        ]
    },
    {
        "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
        "url": "http://arxiv.org/abs/2303.13873",
        "abstract": "Automatic 3D content creation has achieved rapid progress recently due to the\navailability of pre-trained, large language models and image diffusion models,\nforming the emerging topic of text-to-3D content creation. Existing text-to-3D\nmethods commonly use implicit scene representations, which couple the geometry\nand appearance via volume rendering and are suboptimal in terms of recovering\nfiner geometries and achieving photorealistic rendering; consequently, they are\nless effective for generating high-quality 3D assets. In this work, we propose\na new method of Fantasia3D for high-quality text-to-3D content creation. Key to\nFantasia3D is the disentangled modeling and learning of geometry and\nappearance. For geometry learning, we rely on a hybrid scene representation,\nand propose to encode surface normal extracted from the representation as the\ninput of the image diffusion model. For appearance modeling, we introduce the\nspatially varying bidirectional reflectance distribution function (BRDF) into\nthe text-to-3D task, and learn the surface material for photorealistic\nrendering of the generated surface. Our disentangled framework is more\ncompatible with popular graphics engines, supporting relighting, editing, and\nphysical simulation of the generated 3D assets. We conduct thorough experiments\nthat show the advantages of our method over existing ones under different\ntext-to-3D task settings. Project page and source codes:\nhttps://fantasia3d.github.io/.",
        "authors": [
            "Rui Chen",
            "Yongwei Chen",
            "Ningxin Jiao",
            "Kui Jia"
        ]
    },
    {
        "title": "MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models",
        "url": "http://arxiv.org/abs/2303.13126",
        "abstract": "The advent of open-source AI communities has produced a cornucopia of\npowerful text-guided diffusion models that are trained on various datasets.\nWhile few explorations have been conducted on ensembling such models to combine\ntheir strengths. In this work, we propose a simple yet effective method called\nSaliency-aware Noise Blending (SNB) that can empower the fused text-guided\ndiffusion models to achieve more controllable generation. Specifically, we\nexperimentally find that the responses of classifier-free guidance are highly\nrelated to the saliency of generated images. Thus we propose to trust different\nmodels in their areas of expertise by blending the predicted noises of two\ndiffusion models in a saliency-aware manner. SNB is training-free and can be\ncompleted within a DDIM sampling process. Additionally, it can automatically\nalign the semantics of two noise spaces without requiring additional\nannotations such as masks. Extensive experiments show the impressive\neffectiveness of SNB in various applications. Project page is available at\nhttps://magicfusion.github.io/.",
        "authors": [
            "Jing Zhao",
            "Heliang Zheng",
            "Chaoyue Wang",
            "Long Lan",
            "Wenjing Yang"
        ]
    },
    {
        "title": "UCF: Uncovering Common Features for Generalizable Deepfake Detection",
        "url": "http://arxiv.org/abs/2304.13949",
        "abstract": "Deepfake detection remains a challenging task due to the difficulty of\ngeneralizing to new types of forgeries. This problem primarily stems from the\noverfitting of existing detection methods to forgery-irrelevant features and\nmethod-specific patterns. The latter has been rarely studied and not well\naddressed by previous works. This paper presents a novel approach to address\nthe two types of overfitting issues by uncovering common forgery features.\nSpecifically, we first propose a disentanglement framework that decomposes\nimage information into three distinct components: forgery-irrelevant,\nmethod-specific forgery, and common forgery features. To ensure the decoupling\nof method-specific and common forgery features, a multi-task learning strategy\nis employed, including a multi-class classification that predicts the category\nof the forgery method and a binary classification that distinguishes the real\nfrom the fake. Additionally, a conditional decoder is designed to utilize\nforgery features as a condition along with forgery-irrelevant features to\ngenerate reconstructed images. Furthermore, a contrastive regularization\ntechnique is proposed to encourage the disentanglement of the common and\nspecific forgery features. Ultimately, we only utilize the common forgery\nfeatures for the purpose of generalizable deepfake detection. Extensive\nevaluations demonstrate that our framework can perform superior generalization\nthan current state-of-the-art methods.",
        "authors": [
            "Zhiyuan Yan",
            "Yong Zhang",
            "Yanbo Fan",
            "Baoyuan Wu"
        ]
    },
    {
        "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression",
        "url": "http://arxiv.org/abs/2308.10141",
        "abstract": "Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent\nyears, from room-based to object-based and indoor to outdoor. The REVERIE\n(Remote Embodied Referring Expression) is interesting since it only provides\nhigh-level instructions to the agent, which are closer to human commands in\npractice. Nevertheless, this poses more challenges than other VLN tasks since\nit requires agents to infer a navigation plan only based on a short\ninstruction. Large Language Models (LLMs) show great potential in robot action\nplanning by providing proper prompts. Still, this strategy has not been\nexplored under the REVERIE settings. There are several new challenges. For\nexample, the LLM should be environment-aware so that the navigation plan can be\nadjusted based on the current visual observation. Moreover, the LLM planned\nactions should be adaptable to the much larger and more complex REVERIE\nenvironment. This paper proposes a March-in-Chat (MiC) model that can talk to\nthe LLM on the fly and plan dynamically based on a newly proposed\nRoom-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the\nprevious state-of-the-art by large margins by SPL and RGSPL metrics on the\nREVERIE benchmark.",
        "authors": [
            "Yanyuan Qiao",
            "Yuankai Qi",
            "Zheng Yu",
            "Jing Liu",
            "Qi Wu"
        ]
    },
    {
        "title": "Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation",
        "url": "http://arxiv.org/abs/2303.11851",
        "abstract": "Cross-View Geo-Localisation is still a challenging task where additional\nmodules, specific pre-processing or zooming strategies are necessary to\ndetermine accurate positions of images. Since different views have different\ngeometries, pre-processing like polar transformation helps to merge them.\nHowever, this results in distorted images which then have to be rectified.\nAdding hard negatives to the training batch could improve the overall\nperformance but with the default loss functions in geo-localisation it is\ndifficult to include them. In this article, we present a simplified but\neffective architecture based on contrastive learning with symmetric InfoNCE\nloss that outperforms current state-of-the-art results. Our framework consists\nof a narrow training pipeline that eliminates the need of using aggregation\nmodules, avoids further pre-processing steps and even increases the\ngeneralisation capability of the model to unknown regions. We introduce two\ntypes of sampling strategies for hard negatives. The first explicitly exploits\ngeographically neighboring locations to provide a good starting point. The\nsecond leverages the visual similarity between the image embeddings in order to\nmine hard negative samples. Our work shows excellent performance on common\ncross-view datasets like CVUSA, CVACT, University-1652 and VIGOR. A comparison\nbetween cross-area and same-area settings demonstrate the good generalisation\ncapability of our model.",
        "authors": [
            "Fabian Deuser",
            "Konrad Habel",
            "Norbert Oswald"
        ]
    },
    {
        "title": "LIMITR: Leveraging Local Information for Medical Image-Text Representation",
        "url": "http://arxiv.org/abs/2303.11755",
        "abstract": "Medical imaging analysis plays a critical role in the diagnosis and treatment\nof various medical conditions. This paper focuses on chest X-ray images and\ntheir corresponding radiological reports. It presents a new model that learns a\njoint X-ray image & report representation. The model is based on a novel\nalignment scheme between the visual data and the text, which takes into account\nboth local and global information. Furthermore, the model integrates\ndomain-specific information of two types -- lateral images and the consistent\nvisual structure of chest images. Our representation is shown to benefit three\ntypes of retrieval tasks: text-image retrieval, class-based retrieval, and\nphrase-grounding.",
        "authors": [
            "Gefen Dawidowicz",
            "Elad Hirsch",
            "Ayellet Tal"
        ]
    },
    {
        "title": "Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation",
        "url": "http://arxiv.org/abs/2309.14241",
        "abstract": "Contemporary domain adaptation offers a practical solution for achieving\ncross-domain transfer of semantic segmentation between labeled source data and\nunlabeled target data. These solutions have gained significant popularity;\nhowever, they require the model to be retrained when the test environment\nchanges. This can result in unbearable costs in certain applications due to the\ntime-consuming training process and concerns regarding data privacy. One-shot\ndomain adaptation methods attempt to overcome these challenges by transferring\nthe pre-trained source model to the target domain using only one target data.\nDespite this, the referring style transfer module still faces issues with\ncomputation cost and over-fitting problems. To address this problem, we propose\na novel framework called Informative Data Mining (IDM) that enables efficient\none-shot domain adaptation for semantic segmentation. Specifically, IDM\nprovides an uncertainty-based selection criterion to identify the most\ninformative samples, which facilitates quick adaptation and reduces redundant\ntraining. We then perform a model adaptation method using these selected\nsamples, which includes patch-wise mixing and prototype-based information\nmaximization to update the model. This approach effectively enhances adaptation\nand mitigates the overfitting problem. In general, we provide empirical\nevidence of the effectiveness and efficiency of IDM. Our approach outperforms\nexisting methods and achieves a new state-of-the-art one-shot performance of\n56.7\\%/55.4\\% on the GTA5/SYNTHIA to Cityscapes adaptation tasks, respectively.\nThe code will be released at \\url{https://github.com/yxiwang/IDM}.",
        "authors": [
            "Yuxi Wang",
            "Jian Liang",
            "Jun Xiao",
            "Shuqi Mei",
            "Yuran Yang",
            "Zhaoxiang Zhang"
        ]
    },
    {
        "title": "Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors",
        "url": "http://arxiv.org/abs/2307.10667",
        "abstract": "As the physical size of recent CMOS image sensors (CIS) gets smaller, the\nlatest mobile cameras are adopting unique non-Bayer color filter array (CFA)\npatterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units with\nadjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFA\nthanks to their changeable pixel-bin sizes for different light conditions but\nmay introduce visual artifacts during demosaicing due to their inherent pixel\npattern structures and sensor hardware characteristics. Previous demosaicing\nmethods have primarily focused on Bayer CFA, necessitating distinct\nreconstruction methods for non-Bayer patterned CIS with various CFA modes under\ndifferent lighting conditions. In this work, we propose an efficient unified\ndemosaicing method that can be applied to both conventional Bayer RAW and\nvarious non-Bayer CFAs' RAW data in different operation modes. Our Knowledge\nLearning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizes\nCFA-adaptive filters for only 1% key filters in the network for each CFA, but\nstill manages to effectively demosaic all the CFAs, yielding comparable\nperformance to the large-scale models. Furthermore, by employing meta-learning\nduring inference (KLAP-M), our model is able to eliminate unknown\nsensor-generic artifacts in real RAW data, effectively bridging the gap between\nsynthetic images and real sensor RAW. Our KLAP and KLAP-M methods achieved\nstate-of-the-art demosaicing performance in both synthetic and real RAW data of\nBayer and non-Bayer CFAs.",
        "authors": [
            "Haechang Lee",
            "Dongwon Park",
            "Wongi Jeong",
            "Kijeong Kim",
            "Hyunwoo Je",
            "Dongil Ryu",
            "Se Young Chun"
        ]
    },
    {
        "title": "Householder Projector for Unsupervised Latent Semantics Discovery",
        "url": "http://arxiv.org/abs/2307.08012",
        "abstract": "Generative Adversarial Networks (GANs), especially the recent style-based\ngenerators (StyleGANs), have versatile semantics in the structured latent\nspace. Latent semantics discovery methods emerge to move around the latent code\nsuch that only one factor varies during the traversal. Recently, an\nunsupervised method proposed a promising direction to directly use the\neigenvectors of the projection matrix that maps latent codes to features as the\ninterpretable directions. However, one overlooked fact is that the projection\nmatrix is non-orthogonal and the number of eigenvectors is too large. The\nnon-orthogonality would entangle semantic attributes in the top few\neigenvectors, and the large dimensionality might result in meaningless\nvariations among the directions even if the matrix is orthogonal. To avoid\nthese issues, we propose Householder Projector, a flexible and general low-rank\northogonal matrix representation based on Householder transformations, to\nparameterize the projection matrix. The orthogonality guarantees that the\neigenvectors correspond to disentangled interpretable semantics, while the\nlow-rank property encourages that each identified direction has meaningful\nvariations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and\nevaluate the models on several benchmarks. Within only $1\\%$ of the original\ntraining steps for fine-tuning, our projector helps StyleGANs to discover more\ndisentangled and precise semantic attributes without sacrificing image\nfidelity.",
        "authors": [
            "Yue Song",
            "Jichao Zhang",
            "Nicu Sebe",
            "Wei Wang"
        ]
    },
    {
        "title": "Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution",
        "url": "http://arxiv.org/abs/2302.13800",
        "abstract": "Although numerous solutions have been proposed for image super-resolution,\nthey are usually incompatible with low-power devices with many computational\nand memory constraints. In this paper, we address this problem by proposing a\nsimple yet effective deep network to solve image super-resolution efficiently.\nIn detail, we develop a spatially-adaptive feature modulation (SAFM) mechanism\nupon a vision transformer (ViT)-like block. Within it, we first apply the SAFM\nblock over input features to dynamically select representative feature\nrepresentations. As the SAFM block processes the input features from a\nlong-range perspective, we further introduce a convolutional channel mixer\n(CCM) to simultaneously extract local contextual information and perform\nchannel mixing. Extensive experimental results show that the proposed method is\n$3\\times$ smaller than state-of-the-art efficient SR methods, e.g., IMDN, in\nterms of the network parameters and requires less computational cost while\nachieving comparable performance. The code is available at\nhttps://github.com/sunny2109/SAFMN.",
        "authors": [
            "Long Sun",
            "Jiangxin Dong",
            "Jinhui Tang",
            "Jinshan Pan"
        ]
    },
    {
        "title": "Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches",
        "url": "http://arxiv.org/abs/2308.06776",
        "abstract": "Deep learning methods have shown remarkable performance in image denoising,\nparticularly when trained on large-scale paired datasets. However, acquiring\nsuch paired datasets for real-world scenarios poses a significant challenge.\nAlthough unsupervised approaches based on generative adversarial networks offer\na promising solution for denoising without paired datasets, they are difficult\nin surpassing the performance limitations of conventional GAN-based\nunsupervised frameworks without significantly modifying existing structures or\nincreasing the computational complexity of denoisers. To address this problem,\nwe propose a SC strategy for multiple denoisers. This strategy can achieve\nsignificant performance improvement without increasing the inference complexity\nof the GAN-based denoising framework. Its basic idea is to iteratively replace\nthe previous less powerful denoiser in the filter-guided noise extraction\nmodule with the current powerful denoiser. This process generates better\nsynthetic clean-noisy image pairs, leading to a more powerful denoiser for the\nnext iteration. This baseline ensures the stability and effectiveness of the\ntraining network. The experimental results demonstrate the superiority of our\nmethod over state-of-the-art unsupervised methods.",
        "authors": [
            "Xin Lin",
            "Chao Ren",
            "Xiao Liu",
            "Jie Huang",
            "Yinjie Lei"
        ]
    },
    {
        "title": "Bayesian Optimization Meets Self-Distillation",
        "url": "http://arxiv.org/abs/2304.12666",
        "abstract": "Bayesian optimization (BO) has contributed greatly to improving model\nperformance by suggesting promising hyperparameter configurations iteratively\nbased on observations from multiple training trials. However, only partial\nknowledge (i.e., the measured performances of trained models and their\nhyperparameter configurations) from previous trials is transferred. On the\nother hand, Self-Distillation (SD) only transfers partial knowledge learned by\nthe task model itself. To fully leverage the various knowledge gained from all\ntraining trials, we propose the BOSS framework, which combines BO and SD. BOSS\nsuggests promising hyperparameter configurations through BO and carefully\nselects pre-trained models from previous trials for SD, which are otherwise\nabandoned in the conventional BO process. BOSS achieves significantly better\nperformance than both BO and SD in a wide range of tasks including general\nimage classification, learning with noisy labels, semi-supervised learning, and\nmedical image analysis tasks.",
        "authors": [
            "HyunJae Lee",
            "Heon Song",
            "Hyeonsoo Lee",
            "Gi-hyeon Lee",
            "Suyeong Park",
            "Donggeun Yoo"
        ]
    },
    {
        "title": "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier",
        "url": "http://arxiv.org/abs/2303.10058",
        "abstract": "Data heterogeneity is an inherent challenge that hinders the performance of\nfederated learning (FL). Recent studies have identified the biased classifiers\nof local models as the key bottleneck. Previous attempts have used classifier\ncalibration after FL training, but this approach falls short in improving the\npoor feature representations caused by training-time classifier biases.\nResolving the classifier bias dilemma in FL requires a full understanding of\nthe mechanisms behind the classifier. Recent advances in neural collapse have\nshown that the classifiers and feature prototypes under perfect training\nscenarios collapse into an optimal structure called simplex equiangular tight\nframe (ETF). Building on this neural collapse insight, we propose a solution to\nthe FL's classifier bias problem by utilizing a synthetic and fixed ETF\nclassifier during training. The optimal classifier structure enables all\nclients to learn unified and optimal feature representations even under\nextremely heterogeneous data. We devise several effective modules to better\nadapt the ETF structure in FL, achieving both high generalization and\npersonalization. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet.",
        "authors": [
            "Zexi Li",
            "Xinyi Shang",
            "Rui He",
            "Tao Lin",
            "Chao Wu"
        ]
    },
    {
        "title": "Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time",
        "url": "http://arxiv.org/abs/2309.14022",
        "abstract": "We present a video decomposition method that facilitates layer-based editing\nof videos with spatiotemporally varying lighting and motion effects. Our neural\nmodel decomposes an input video into multiple layered representations, each\ncomprising a 2D texture map, a mask for the original video, and a\nmultiplicative residual characterizing the spatiotemporal variations in\nlighting conditions. A single edit on the texture maps can be propagated to the\ncorresponding locations in the entire video frames while preserving other\ncontents' consistencies. Our method efficiently learns the layer-based neural\nrepresentations of a 1080p video in 25s per frame via coordinate hashing and\nallows real-time rendering of the edited result at 71 fps on a single GPU.\nQualitatively, we run our method on various videos to show its effectiveness in\ngenerating high-quality editing effects. Quantitatively, we propose to adopt\nfeature-tracking evaluation metrics for objectively assessing the consistency\nof video editing. Project page: https://lightbulb12294.github.io/hashing-nvd/",
        "authors": [
            "Cheng-Hung Chan",
            "Cheng-Yang Yuan",
            "Cheng Sun",
            "Hwann-Tzong Chen"
        ]
    },
    {
        "title": "DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer",
        "url": "http://arxiv.org/abs/2304.06668",
        "abstract": "Most state-of-the-art instance segmentation methods rely on large amounts of\npixel-precise ground-truth annotations for training, which are expensive to\ncreate. Interactive segmentation networks help generate such annotations based\non an image and the corresponding user interactions such as clicks. Existing\nmethods for this task can only process a single instance at a time and each\nuser interaction requires a full forward pass through the entire deep network.\nWe introduce a more efficient approach, called DynaMITe, in which we represent\nuser interactions as spatio-temporal queries to a Transformer decoder with a\npotential to segment multiple object instances in a single iteration. Our\narchitecture also alleviates any need to re-compute image features during\nrefinement, and requires fewer interactions for segmenting multiple instances\nin a single image when compared to other methods. DynaMITe achieves\nstate-of-the-art results on multiple existing interactive segmentation\nbenchmarks, and also on the new multi-instance benchmark that we propose in\nthis paper.",
        "authors": [
            "Amit Kumar Rana",
            "Sabarinath Mahadevan",
            "Alexander Hermans",
            "Bastian Leibe"
        ]
    },
    {
        "title": "FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation",
        "url": "http://arxiv.org/abs/2205.14900",
        "abstract": "Federated Learning (FL) is a decentralized learning paradigm, in which\nmultiple clients collaboratively train deep learning models without\ncentralizing their local data, and hence preserve data privacy. Real-world\napplications usually involve a distribution shift across the datasets of the\ndifferent clients, which hurts the generalization ability of the clients to\nunseen samples from their respective data distributions. In this work, we\naddress the recently proposed feature shift problem where the clients have\ndifferent feature distributions, while the label distribution is the same. We\npropose Federated Representation Augmentation (FRAug) to tackle this practical\nand challenging problem. Our approach generates synthetic client-specific\nsamples in the embedding space to augment the usually small client datasets.\nFor that, we train a shared generative model to fuse the clients knowledge\nlearned from their different feature distributions. This generator synthesizes\nclient-agnostic embeddings, which are then locally transformed into\nclient-specific embeddings by Representation Transformation Networks (RTNets).\nBy transferring knowledge across the clients, the generated embeddings act as a\nregularizer for the client models and reduce overfitting to the local original\ndatasets, hence improving generalization. Our empirical evaluation on public\nbenchmarks and a real-world medical dataset demonstrates the effectiveness of\nthe proposed method, which substantially outperforms the current\nstate-of-the-art FL methods for non-IID features, including PartialFed and\nFedBN.",
        "authors": [
            "Haokun Chen",
            "Ahmed Frikha",
            "Denis Krompass",
            "Jindong Gu",
            "Volker Tresp"
        ]
    },
    {
        "title": "OmnimatteRF: Robust Omnimatte with 3D Background Modeling",
        "url": "http://arxiv.org/abs/2309.07749",
        "abstract": "Video matting has broad applications, from adding interesting effects to\ncasually captured movies to assisting video production professionals. Matting\nwith associated effects such as shadows and reflections has also attracted\nincreasing research activity, and methods like Omnimatte have been proposed to\nseparate dynamic foreground objects of interest into their own layers. However,\nprior works represent video backgrounds as 2D image layers, limiting their\ncapacity to express more complicated scenes, thus hindering application to\nreal-world videos. In this paper, we propose a novel video matting method,\nOmnimatteRF, that combines dynamic 2D foreground layers and a 3D background\nmodel. The 2D layers preserve the details of the subjects, while the 3D\nbackground robustly reconstructs scenes in real-world videos. Extensive\nexperiments demonstrate that our method reconstructs scenes with better quality\non various videos.",
        "authors": [
            "Geng Lin",
            "Chen Gao",
            "Jia-Bin Huang",
            "Changil Kim",
            "Yipeng Wang",
            "Matthias Zwicker",
            "Ayush Saraf"
        ]
    },
    {
        "title": "Self-supervised Image Denoising with Downsampled Invariance Loss and Conditional Blind-Spot Network",
        "url": "http://arxiv.org/abs/2304.09507",
        "abstract": "There have been many image denoisers using deep neural networks, which\noutperform conventional model-based methods by large margins. Recently,\nself-supervised methods have attracted attention because constructing a large\nreal noise dataset for supervised training is an enormous burden. The most\nrepresentative self-supervised denoisers are based on blind-spot networks,\nwhich exclude the receptive field's center pixel. However, excluding any input\npixel is abandoning some information, especially when the input pixel at the\ncorresponding output position is excluded. In addition, a standard blind-spot\nnetwork fails to reduce real camera noise due to the pixel-wise correlation of\nnoise, though it successfully removes independently distributed synthetic\nnoise. Hence, to realize a more practical denoiser, we propose a novel\nself-supervised training framework that can remove real noise. For this, we\nderive the theoretic upper bound of a supervised loss where the network is\nguided by the downsampled blinded output. Also, we design a conditional\nblind-spot network (C-BSN), which selectively controls the blindness of the\nnetwork to use the center pixel information. Furthermore, we exploit a random\nsubsampler to decorrelate noise spatially, making the C-BSN free of visual\nartifacts that were often seen in downsample-based methods. Extensive\nexperiments show that the proposed C-BSN achieves state-of-the-art performance\non real-world datasets as a self-supervised denoiser and shows qualitatively\npleasing results without any post-processing or refinement.",
        "authors": [
            "Yeong Il Jang",
            "Keuntek Lee",
            "Gu Yong Park",
            "Seyun Kim",
            "Nam Ik Cho"
        ]
    },
    {
        "title": "Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation",
        "url": "http://arxiv.org/abs/2303.13399",
        "abstract": "Interactive segmentation enables users to segment as needed by providing cues\nof objects, which introduces human-computer interaction for many fields, such\nas image editing and medical image analysis. Typically, massive and expansive\npixel-level annotations are spent to train deep models by object-oriented\ninteractions with manually labeled object masks. In this work, we reveal that\ninformative interactions can be made by simulation with semantic-consistent yet\ndiverse region exploration in an unsupervised paradigm. Concretely, we\nintroduce a Multi-granularity Interaction Simulation (MIS) approach to open up\na promising direction for unsupervised interactive segmentation. Drawing on the\nhigh-quality dense features produced by recent self-supervised models, we\npropose to gradually merge patches or regions with similar features to form\nmore extensive regions and thus, every merged region serves as a\nsemantic-meaningful multi-granularity proposal. By randomly sampling these\nproposals and simulating possible interactions based on them, we provide\nmeaningful interaction at multiple granularities to teach the model to\nunderstand interactions. Our MIS significantly outperforms non-deep learning\nunsupervised methods and is even comparable with some previous deep-supervised\nmethods without any annotation.",
        "authors": [
            "Kehan Li",
            "Yian Zhao",
            "Zhennan Wang",
            "Zesen Cheng",
            "Peng Jin",
            "Xiangyang Ji",
            "Li Yuan",
            "Chang Liu",
            "Jie Chen"
        ]
    },
    {
        "title": "RecursiveDet: End-to-End Region-Based Recursive Object Detection",
        "url": "http://arxiv.org/abs/2307.13619",
        "abstract": "End-to-end region-based object detectors like Sparse R-CNN usually have\nmultiple cascade bounding box decoding stages, which refine the current\npredictions according to their previous results. Model parameters within each\nstage are independent, evolving a huge cost. In this paper, we find the general\nsetting of decoding stages is actually redundant. By simply sharing parameters\nand making a recursive decoder, the detector already obtains a significant\nimprovement. The recursive decoder can be further enhanced by positional\nencoding (PE) of the proposal box, which makes it aware of the exact locations\nand sizes of input bounding boxes, thus becoming adaptive to proposals from\ndifferent stages during the recursion. Moreover, we also design\ncenterness-based PE to distinguish the RoI feature element and dynamic\nconvolution kernels at different positions within the bounding box. To validate\nthe effectiveness of the proposed method, we conduct intensive ablations and\nbuild the full model on three recent mainstream region-based detectors. The\nRecusiveDet is able to achieve obvious performance boosts with even fewer model\nparameters and slightly increased computation cost. Codes are available at\nhttps://github.com/bravezzzzzz/RecursiveDet.",
        "authors": [
            "Jing Zhao",
            "Li Sun",
            "Qingli Li"
        ]
    },
    {
        "title": "Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration",
        "url": "http://arxiv.org/abs/2309.11103",
        "abstract": "Personalized federated learning (PFL) reduces the impact of non-independent\nand identically distributed (non-IID) data among clients by allowing each\nclient to train a personalized model when collaborating with others. A key\nquestion in PFL is to decide which parameters of a client should be localized\nor shared with others. In current mainstream approaches, all layers that are\nsensitive to non-IID data (such as classifier layers) are generally\npersonalized. The reasoning behind this approach is understandable, as\nlocalizing parameters that are easily influenced by non-IID data can prevent\nthe potential negative effect of collaboration. However, we believe that this\napproach is too conservative for collaboration. For example, for a certain\nclient, even if its parameters are easily influenced by non-IID data, it can\nstill benefit by sharing these parameters with clients having similar data\ndistribution. This observation emphasizes the importance of considering not\nonly the sensitivity to non-IID data but also the similarity of data\ndistribution when determining which parameters should be localized in PFL. This\npaper introduces a novel guideline for client collaboration in PFL. Unlike\nexisting approaches that prohibit all collaboration of sensitive parameters,\nour guideline allows clients to share more parameters with others, leading to\nimproved model performance. Additionally, we propose a new PFL method named\nFedCAC, which employs a quantitative metric to evaluate each parameter's\nsensitivity to non-IID data and carefully selects collaborators based on this\nevaluation. Experimental results demonstrate that FedCAC enables clients to\nshare more parameters with others, resulting in superior performance compared\nto state-of-the-art methods, particularly in scenarios where clients have\ndiverse distributions.",
        "authors": [
            "Xinghao Wu",
            "Xuefeng Liu",
            "Jianwei Niu",
            "Guogang Zhu",
            "Shaojie Tang"
        ]
    },
    {
        "title": "ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution",
        "url": "http://arxiv.org/abs/2307.14010",
        "abstract": "Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a\nhigh-resolution hyperspectral image from a low-resolution observation. However,\nthe prevailing CNN-based approaches have shown limitations in building\nlong-range dependencies and capturing interaction information between spectral\nfeatures. This results in inadequate utilization of spectral information and\nartifacts after upsampling. To address this issue, we propose ESSAformer, an\nESSA attention-embedded Transformer network for single-HSI-SR with an iterative\nrefining structure. Specifically, we first introduce a robust and\nspectral-friendly similarity metric, \\ie, the spectral correlation coefficient\nof the spectrum (SCC), to replace the original attention matrix and\nincorporates inductive biases into the model to facilitate training. Built upon\nit, we further utilize the kernelizable attention technique with theoretical\nsupport to form a novel efficient SCC-kernel-based self-attention (ESSA) and\nreduce attention computation to linear complexity. ESSA enlarges the receptive\nfield for features after upsampling without bringing much computation and\nallows the model to effectively utilize spatial-spectral information from\ndifferent scales, resulting in the generation of more natural high-resolution\nimages. Without the need for pretraining on large-scale datasets, our\nexperiments demonstrate ESSA's effectiveness in both visual quality and\nquantitative results.",
        "authors": [
            "Mingjin Zhang",
            "Chi Zhang",
            "Qiming Zhang",
            "Jie Guo",
            "Xinbo Gao",
            "Jing Zhang"
        ]
    },
    {
        "title": "Generative Action Description Prompts for Skeleton-based Action Recognition",
        "url": "http://arxiv.org/abs/2208.05318",
        "abstract": "Skeleton-based action recognition has recently received considerable\nattention. Current approaches to skeleton-based action recognition are\ntypically formulated as one-hot classification tasks and do not fully exploit\nthe semantic relations between actions. For example, \"make victory sign\" and\n\"thumb up\" are two actions of hand gestures, whose major difference lies in the\nmovement of hands. This information is agnostic from the categorical one-hot\nencoding of action classes but could be unveiled from the action description.\nTherefore, utilizing action description in training could potentially benefit\nrepresentation learning. In this work, we propose a Generative\nAction-description Prompts (GAP) approach for skeleton-based action\nrecognition. More specifically, we employ a pre-trained large-scale language\nmodel as the knowledge engine to automatically generate text descriptions for\nbody parts movements of actions, and propose a multi-modal training scheme by\nutilizing the text encoder to generate feature vectors for different body parts\nand supervise the skeleton encoder for action representation learning.\nExperiments show that our proposed GAP method achieves noticeable improvements\nover various baseline models without extra computation cost at inference. GAP\nachieves new state-of-the-arts on popular skeleton-based action recognition\nbenchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is\navailable at https://github.com/MartinXM/GAP.",
        "authors": [
            "Wangmeng Xiang",
            "Chao Li",
            "Yuxuan Zhou",
            "Biao Wang",
            "Lei Zhang"
        ]
    },
    {
        "title": "DISeR: Designing Imaging Systems with Reinforcement Learning",
        "url": "http://arxiv.org/abs/2309.13851",
        "abstract": "Imaging systems consist of cameras to encode visual information about the\nworld and perception models to interpret this encoding. Cameras contain (1)\nillumination sources, (2) optical elements, and (3) sensors, while perception\nmodels use (4) algorithms. Directly searching over all combinations of these\nfour building blocks to design an imaging system is challenging due to the size\nof the search space. Moreover, cameras and perception models are often designed\nindependently, leading to sub-optimal task performance. In this paper, we\nformulate these four building blocks of imaging systems as a context-free\ngrammar (CFG), which can be automatically searched over with a learned camera\ndesigner to jointly optimize the imaging system with task-specific perception\nmodels. By transforming the CFG to a state-action space, we then show how the\ncamera designer can be implemented with reinforcement learning to intelligently\nsearch over the combinatorial space of possible imaging system configurations.\nWe demonstrate our approach on two tasks, depth estimation and camera rig\ndesign for autonomous vehicles, showing that our method yields rigs that\noutperform industry-wide standards. We believe that our proposed approach is an\nimportant step towards automating imaging system design.",
        "authors": [
            "Tzofi Klinghoffer",
            "Kushagra Tiwary",
            "Nikhil Behari",
            "Bhavya Agrawalla",
            "Ramesh Raskar"
        ]
    },
    {
        "title": "The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-shot Image Generation",
        "url": "http://arxiv.org/abs/2211.12347",
        "abstract": "Few-shot image generation is a challenging task since it aims to generate\ndiverse new images for an unseen category with only a few images. Existing\nmethods suffer from the trade-off between the quality and diversity of\ngenerated images. To tackle this problem, we propose Hyperbolic Attribute\nEditing~(HAE), a simple yet effective method. Unlike other methods that work in\nEuclidean space, HAE captures the hierarchy among images using data from seen\ncategories in hyperbolic space. Given a well-trained HAE, images of unseen\ncategories can be generated by moving the latent code of a given image toward\nany meaningful directions in the Poincar\\'e disk with a fixing radius. Most\nimportantly, the hyperbolic space allows us to control the semantic diversity\nof the generated images by setting different radii in the disk. Extensive\nexperiments and visualizations demonstrate that HAE is capable of not only\ngenerating images with promising quality and diversity using limited data but\nachieving a highly controllable and interpretable editing process.",
        "authors": [
            "Lingxiao Li",
            "Yi Zhang",
            "Shuhui Wang"
        ]
    },
    {
        "title": "FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level Gradient Calibration",
        "url": "http://arxiv.org/abs/2307.16617",
        "abstract": "Multi-modality fusion and multi-task learning are becoming trendy in 3D\nautonomous driving scenario, considering robust prediction and computation\nbudget. However, naively extending the existing framework to the domain of\nmulti-modality multi-task learning remains ineffective and even poisonous due\nto the notorious modality bias and task conflict. Previous works manually\ncoordinate the learning framework with empirical knowledge, which may lead to\nsub-optima. To mitigate the issue, we propose a novel yet simple multi-level\ngradient calibration learning framework across tasks and modalities during\noptimization. Specifically, the gradients, produced by the task heads and used\nto update the shared backbone, will be calibrated at the backbone's last layer\nto alleviate the task conflict. Before the calibrated gradients are further\npropagated to the modality branches of the backbone, their magnitudes will be\ncalibrated again to the same level, ensuring the downstream tasks pay balanced\nattention to different modalities. Experiments on large-scale benchmark\nnuScenes demonstrate the effectiveness of the proposed method, eg, an absolute\n14.4% mIoU improvement on map segmentation and 1.4% mAP improvement on 3D\ndetection, advancing the application of 3D autonomous driving in the domain of\nmulti-modality fusion and multi-task learning. We also discuss the links\nbetween modalities and tasks.",
        "authors": [
            "Zhijian Huang",
            "Sihao Lin",
            "Guiyu Liu",
            "Mukun Luo",
            "Chaoqiang Ye",
            "Hang Xu",
            "Xiaojun Chang",
            "Xiaodan Liang"
        ]
    },
    {
        "title": "Transparent Shape from a Single View Polarization Image",
        "url": "http://arxiv.org/abs/2204.06331",
        "abstract": "This paper presents a learning-based method for transparent surface\nestimation from a single view polarization image. Existing shape from\npolarization(SfP) methods have the difficulty in estimating transparent shape\nsince the inherent transmission interference heavily reduces the reliability of\nphysics-based prior. To address this challenge, we propose the concept of\nphysics-based prior, which is inspired by the characteristic that the\ntransmission component in the polarization image has more noise than\nreflection. The confidence is used to determine the contribution of the\ninterfered physics-based prior. Then, we build a network(TransSfP) with\nmulti-branch architecture to avoid the destruction of relationships between\ndifferent hierarchical inputs. To train and test our method, we construct a\ndataset for transparent shape from polarization with paired polarization images\nand ground-truth normal maps. Extensive experiments and comparisons demonstrate\nthe superior accuracy of our method.",
        "authors": [
            "Mingqi Shao",
            "Chongkun Xia",
            "Zhendong Yang",
            "Junnan Huang",
            "Xueqian Wang"
        ]
    },
    {
        "title": "DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving",
        "url": "http://arxiv.org/abs/2308.00398",
        "abstract": "End-to-end autonomous driving aims to build a fully differentiable system\nthat takes raw sensor data as inputs and directly outputs the planned\ntrajectory or control signals of the ego vehicle. State-of-the-art methods\nusually follow the `Teacher-Student' paradigm. The Teacher model uses\nprivileged information (ground-truth states of surrounding agents and map\nelements) to learn the driving strategy. The student model only has access to\nraw sensor data and conducts behavior cloning on the data collected by the\nteacher model. By eliminating the noise of the perception part during planning\nlearning, state-of-the-art works could achieve better performance with\nsignificantly less data compared to those coupled ones.\n  However, under the current Teacher-Student paradigm, the student model still\nneeds to learn a planning head from scratch, which could be challenging due to\nthe redundant and noisy nature of raw sensor inputs and the casual confusion\nissue of behavior cloning. In this work, we aim to explore the possibility of\ndirectly adopting the strong teacher model to conduct planning while letting\nthe student model focus more on the perception part. We find that even equipped\nwith a SOTA perception model, directly letting the student model learn the\nrequired inputs of the teacher model leads to poor driving performance, which\ncomes from the large distribution gap between predicted privileged inputs and\nthe ground-truth.\n  To this end, we propose DriveAdapter, which employs adapters with the feature\nalignment objective function between the student (perception) and teacher\n(planning) modules. Additionally, since the pure learning-based teacher model\nitself is imperfect and occasionally breaks safety rules, we propose a method\nof action-guided feature learning with a mask for those imperfect teacher\nfeatures to further inject the priors of hand-crafted rules into the learning\nprocess.",
        "authors": [
            "Xiaosong Jia",
            "Yulu Gao",
            "Li Chen",
            "Junchi Yan",
            "Patrick Langechuan Liu",
            "Hongyang Li"
        ]
    },
    {
        "title": "Local Context-Aware Active Domain Adaptation",
        "url": "http://arxiv.org/abs/2208.12856",
        "abstract": "Active Domain Adaptation (ADA) queries the labels of a small number of\nselected target samples to help adapting a model from a source domain to a\ntarget domain. The local context of queried data is important, especially when\nthe domain gap is large. However, this has not been fully explored by existing\nADA works. In this paper, we propose a Local context-aware ADA framework, named\nLADA, to address this issue. To select informative target samples, we devise a\nnovel criterion based on the local inconsistency of model predictions. Since\nthe labeling budget is usually small, fine-tuning model on only queried data\ncan be inefficient. We progressively augment labeled target data with the\nconfident neighbors in a class-balanced manner. Experiments validate that the\nproposed criterion chooses more informative target samples than existing active\nselection strategies. Furthermore, our full method clearly surpasses recent ADA\narts on various benchmarks. Code is available at https://github.com/tsun/LADA.",
        "authors": [
            "Tao Sun",
            "Cheng Lu",
            "Haibin Ling"
        ]
    },
    {
        "title": "Deep Incubation: Training Large Models by Divide-and-Conquering",
        "url": "http://arxiv.org/abs/2212.04129",
        "abstract": "Recent years have witnessed a remarkable success of large deep learning\nmodels. However, training these models is challenging due to high computational\ncosts, painfully slow convergence, and overfitting issues. In this paper, we\npresent Deep Incubation, a novel approach that enables the efficient and\neffective training of large models by dividing them into smaller sub-modules\nthat can be trained separately and assembled seamlessly. A key challenge for\nimplementing this idea is to ensure the compatibility of the independently\ntrained sub-modules. To address this issue, we first introduce a global, shared\nmeta model, which is leveraged to implicitly link all the modules together, and\ncan be designed as an extremely small network with negligible computational\noverhead. Then we propose a module incubation algorithm, which trains each\nsub-module to replace the corresponding component of the meta model and\naccomplish a given learning task. Despite the simplicity, our approach\neffectively encourages each sub-module to be aware of its role in the target\nlarge model, such that the finally-learned sub-modules can collaborate with\neach other smoothly after being assembled. Empirically, our method outperforms\nend-to-end (E2E) training in terms of both final accuracy and training\nefficiency. For example, on top of ViT-Huge, it improves the accuracy by 2.7%\non ImageNet or achieves similar performance with 4x less training time.\nNotably, the gains are significant for downstream tasks as well (e.g., object\ndetection and image segmentation on COCO and ADE20K). Code is available at\nhttps://github.com/LeapLabTHU/Deep-Incubation.",
        "authors": [
            "Zanlin Ni",
            "Yulin Wang",
            "Jiangwei Yu",
            "Haojun Jiang",
            "Yue Cao",
            "Gao Huang"
        ]
    },
    {
        "title": "Downscaled Representation Matters: Improving Image Rescaling with Collaborative Downscaled Images",
        "url": "http://arxiv.org/abs/2211.10643",
        "abstract": "Deep networks have achieved great success in image rescaling (IR) task that\nseeks to learn the optimal downscaled representations, i.e., low-resolution\n(LR) images, to reconstruct the original high-resolution (HR) images. Compared\nwith super-resolution methods that consider a fixed downscaling scheme, e.g.,\nbicubic, IR often achieves significantly better reconstruction performance\nthanks to the learned downscaled representations. This highlights the\nimportance of a good downscaled representation in image reconstruction tasks.\nExisting IR methods mainly learn the downscaled representation by jointly\noptimizing the downscaling and upscaling models. Unlike them, we seek to\nimprove the downscaled representation through a different and more direct way:\noptimizing the downscaled image itself instead of the down-/upscaling models.\nSpecifically, we propose a collaborative downscaling scheme that directly\ngenerates the collaborative LR examples by descending the gradient w.r.t. the\nreconstruction loss on them to benefit the IR process. Furthermore, since LR\nimages are downscaled from the corresponding HR images, one can also improve\nthe downscaled representation if we have a better representation in the HR\ndomain. Inspired by this, we propose a Hierarchical Collaborative Downscaling\n(HCD) method that performs gradient descent in both HR and LR domains to\nimprove the downscaled representations. Extensive experiments show that our HCD\nsignificantly improves the reconstruction performance both quantitatively and\nqualitatively. Moreover, we also highlight the flexibility of our HCD since it\ncan generalize well across diverse IR models.",
        "authors": [
            "Bingna Xu",
            "Yong Guo",
            "Luoqian Jiang",
            "Mianjie Yu",
            "Jian Chen"
        ]
    },
    {
        "title": "Detection Transformer with Stable Matching",
        "url": "http://arxiv.org/abs/2304.04742",
        "abstract": "This paper is concerned with the matching stability problem across different\ndecoder layers in DEtection TRansformers (DETR). We point out that the unstable\nmatching in DETR is caused by a multi-optimization path problem, which is\nhighlighted by the one-to-one matching design in DETR. To address this problem,\nwe show that the most important design is to use and only use positional\nmetrics (like IOU) to supervise classification scores of positive examples.\nUnder the principle, we propose two simple yet effective modifications by\nintegrating positional metrics to DETR's classification loss and matching cost,\nnamed position-supervised loss and position-modulated cost. We verify our\nmethods on several DETR variants. Our methods show consistent improvements over\nbaselines. By integrating our methods with DINO, we achieve 50.4 and 51.5 AP on\nthe COCO detection benchmark using ResNet-50 backbones under 12 epochs and 24\nepochs training settings, achieving a new record under the same setting. We\nachieve 63.8 AP on COCO detection test-dev with a Swin-Large backbone. Our code\nwill be made available at https://github.com/IDEA-Research/Stable-DINO.",
        "authors": [
            "Shilong Liu",
            "Tianhe Ren",
            "Jiayu Chen",
            "Zhaoyang Zeng",
            "Hao Zhang",
            "Feng Li",
            "Hongyang Li",
            "Jun Huang",
            "Hang Su",
            "Jun Zhu",
            "Lei Zhang"
        ]
    },
    {
        "title": "Story Visualization by Online Text Augmentation with Context Memory",
        "url": "http://arxiv.org/abs/2308.07575",
        "abstract": "Story visualization (SV) is a challenging text-to-image generation task for\nthe difficulty of not only rendering visual details from the text descriptions\nbut also encoding a long-term context across multiple sentences. While prior\nefforts mostly focus on generating a semantically relevant image for each\nsentence, encoding a context spread across the given paragraph to generate\ncontextually convincing images (e.g., with a correct character or with a proper\nbackground of the scene) remains a challenge. To this end, we propose a novel\nmemory architecture for the Bi-directional Transformer framework with an online\ntext augmentation that generates multiple pseudo-descriptions as supplementary\nsupervision during training for better generalization to the language variation\nat inference. In extensive experiments on the two popular SV benchmarks, i.e.,\nthe Pororo-SV and Flintstones-SV, the proposed method significantly outperforms\nthe state of the arts in various metrics including FID, character F1, frame\naccuracy, BLEU-2/3, and R-precision with similar or less computational\ncomplexity.",
        "authors": [
            "Daechul Ahn",
            "Daneul Kim",
            "Gwangmo Song",
            "Seung Hwan Kim",
            "Honglak Lee",
            "Dongyeop Kang",
            "Jonghyun Choi"
        ]
    },
    {
        "title": "All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction",
        "url": "http://arxiv.org/abs/2303.09417",
        "abstract": "Nearest neighbour based methods have proved to be one of the most successful\nself-supervised learning (SSL) approaches due to their high generalization\ncapabilities. However, their computational efficiency decreases when more than\none neighbour is used. In this paper, we propose a novel contrastive SSL\napproach, which we call All4One, that reduces the distance between neighbour\nrepresentations using ''centroids'' created through a self-attention mechanism.\nWe use a Centroid Contrasting objective along with single Neighbour Contrasting\nand Feature Contrasting objectives. Centroids help in learning contextual\ninformation from multiple neighbours whereas the neighbour contrast enables\nlearning representations directly from the neighbours and the feature contrast\nallows learning representations unique to the features. This combination\nenables All4One to outperform popular instance discrimination approaches by\nmore than 1% on linear classification evaluation for popular benchmark datasets\nand obtains state-of-the-art (SoTA) results. Finally, we show that All4One is\nrobust towards embedding dimensionalities and augmentations, surpassing NNCLR\nand Barlow Twins by more than 5% on low dimensionality and weak augmentation\nsettings. The source code would be made available soon.",
        "authors": [
            "Imanol G. Estepa",
            "Ignacio Saras\u00faa",
            "Bhalaji Nagarajan",
            "Petia Radeva"
        ]
    },
    {
        "title": "Contrastive Pseudo Learning for Open-World DeepFake Attribution",
        "url": "http://arxiv.org/abs/2309.11132",
        "abstract": "The challenge in sourcing attribution for forgery faces has gained widespread\nattention due to the rapid development of generative techniques. While many\nrecent works have taken essential steps on GAN-generated faces, more\nthreatening attacks related to identity swapping or expression transferring are\nstill overlooked. And the forgery traces hidden in unknown attacks from the\nopen-world unlabeled faces still remain under-explored. To push the related\nfrontier research, we introduce a new benchmark called Open-World DeepFake\nAttribution (OW-DFA), which aims to evaluate attribution performance against\nvarious types of fake faces under open-world scenarios. Meanwhile, we propose a\nnovel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task\nthrough 1) introducing a Global-Local Voting module to guide the feature\nalignment of forged faces with different manipulated regions, 2) designing a\nConfidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused\nby similar methods in unlabeled set. In addition, we extend the CPL framework\nwith a multi-stage paradigm that leverages pre-train technique and iterative\nlearning to further enhance traceability performance. Extensive experiments\nverify the superiority of our proposed method on the OW-DFA and also\ndemonstrate the interpretability of deepfake attribution task and its impact on\nimproving the security of deepfake detection area.",
        "authors": [
            "Zhimin Sun",
            "Shen Chen",
            "Taiping Yao",
            "Bangjie Yin",
            "Ran Yi",
            "Shouhong Ding",
            "Lizhuang Ma"
        ]
    },
    {
        "title": "SimNP: Learning Self-Similarity Priors Between Neural Points",
        "url": "http://arxiv.org/abs/2309.03809",
        "abstract": "Existing neural field representations for 3D object reconstruction either (1)\nutilize object-level representations, but suffer from low-quality details due\nto conditioning on a global latent code, or (2) are able to perfectly\nreconstruct the observations, but fail to utilize object-level prior knowledge\nto infer unobserved regions. We present SimNP, a method to learn category-level\nself-similarities, which combines the advantages of both worlds by connecting\nneural point radiance fields with a category-level self-similarity\nrepresentation. Our contribution is two-fold. (1) We design the first neural\npoint representation on a category level by utilizing the concept of coherent\npoint clouds. The resulting neural point radiance fields store a high level of\ndetail for locally supported object regions. (2) We learn how information is\nshared between neural points in an unconstrained and unsupervised fashion,\nwhich allows to derive unobserved regions of an object during the\nreconstruction process from given observations. We show that SimNP is able to\noutperform previous methods in reconstructing symmetric unseen object regions,\nsurpassing methods that build upon category-level or pixel-aligned radiance\nfields, while providing semantic correspondences between instances",
        "authors": [
            "Christopher Wewer",
            "Eddy Ilg",
            "Bernt Schiele",
            "Jan Eric Lenssen"
        ]
    },
    {
        "title": "ACLS: Adaptive and Conditional Label Smoothing for Network Calibration",
        "url": "http://arxiv.org/abs/2308.11911",
        "abstract": "We address the problem of network calibration adjusting miscalibrated\nconfidences of deep neural networks. Many approaches to network calibration\nadopt a regularization-based method that exploits a regularization term to\nsmooth the miscalibrated confidences. Although these approaches have shown the\neffectiveness on calibrating the networks, there is still a lack of\nunderstanding on the underlying principles of regularization in terms of\nnetwork calibration. We present in this paper an in-depth analysis of existing\nregularization-based methods, providing a better understanding on how they\naffect to network calibration. Specifically, we have observed that 1) the\nregularization-based methods can be interpreted as variants of label smoothing,\nand 2) they do not always behave desirably. Based on the analysis, we introduce\na novel loss function, dubbed ACLS, that unifies the merits of existing\nregularization methods, while avoiding the limitations. We show extensive\nexperimental results for image classification and semantic segmentation on\nstandard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL\nVOC, demonstrating the effectiveness of our loss function.",
        "authors": [
            "Hyekang Park",
            "Jongyoun Noh",
            "Youngmin Oh",
            "Donghyeon Baek",
            "Bumsub Ham"
        ]
    },
    {
        "title": "Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks",
        "url": "http://arxiv.org/abs/2212.05946",
        "abstract": "Part-prototype networks (e.g., ProtoPNet, ProtoTree, and ProtoPool) have\nattracted broad research interest for their intrinsic interpretability and\ncomparable accuracy to non-interpretable counterparts. However, recent works\nfind that the interpretability from prototypes is fragile, due to the semantic\ngap between the similarities in the feature space and that in the input space.\nIn this work, we strive to address this challenge by making the first attempt\nto quantitatively and objectively evaluate the interpretability of the\npart-prototype networks. Specifically, we propose two evaluation metrics,\ntermed as consistency score and stability score, to evaluate the explanation\nconsistency across images and the explanation robustness against perturbations,\nrespectively, both of which are essential for explanations taken into practice.\nFurthermore, we propose an elaborated part-prototype network with a\nshallow-deep feature alignment (SDFA) module and a score aggregation (SA)\nmodule to improve the interpretability of prototypes. We conduct systematical\nevaluation experiments and provide substantial discussions to uncover the\ninterpretability of existing part-prototype networks. Experiments on three\nbenchmarks across nine architectures demonstrate that our model achieves\nsignificantly superior performance to the state of the art, in both the\naccuracy and interpretability. Our code is available at\nhttps://github.com/hqhQAQ/EvalProtoPNet.",
        "authors": [
            "Qihan Huang",
            "Mengqi Xue",
            "Wenqi Huang",
            "Haofei Zhang",
            "Jie Song",
            "Yongcheng Jing",
            "Mingli Song"
        ]
    },
    {
        "title": "Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning",
        "url": "http://arxiv.org/abs/2307.07250",
        "abstract": "Adversarial examples derived from deliberately crafted perturbations on\nvisual inputs can easily harm decision process of deep neural networks. To\nprevent potential threats, various adversarial training-based defense methods\nhave grown rapidly and become a de facto standard approach for robustness.\nDespite recent competitive achievements, we observe that adversarial\nvulnerability varies across targets and certain vulnerabilities remain\nprevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with\ndeeper architectures and advanced defense methods. To address this issue, in\nthis paper, we introduce a causal approach called Adversarial Double Machine\nLearning (ADML), which allows us to quantify the degree of adversarial\nvulnerability for network predictions and capture the effect of treatments on\noutcome of interests. ADML can directly estimate causal parameter of\nadversarial perturbations per se and mitigate negative effects that can\npotentially damage robustness, bridging a causal perspective into the\nadversarial vulnerability. Through extensive experiments on various CNN and\nTransformer architectures, we corroborate that ADML improves adversarial\nrobustness with large margins and relieve the empirical observation.",
        "authors": [
            "Byung-Kwan Lee",
            "Junho Kim",
            "Yong Man Ro"
        ]
    },
    {
        "title": "Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.01045",
        "abstract": "Vision transformers have achieved leading performance on various visual tasks\nyet still suffer from high computational complexity. The situation deteriorates\nin dense prediction tasks like semantic segmentation, as high-resolution inputs\nand outputs usually imply more tokens involved in computations. Directly\nremoving the less attentive tokens has been discussed for the image\nclassification task but can not be extended to semantic segmentation since a\ndense prediction is required for every patch. To this end, this work introduces\na Dynamic Token Pruning (DToP) method based on the early exit of tokens for\nsemantic segmentation. Motivated by the coarse-to-fine segmentation process by\nhumans, we naturally split the widely adopted auxiliary-loss-based network\narchitecture into several stages, where each auxiliary block grades every\ntoken's difficulty level. We can finalize the prediction of easy tokens in\nadvance without completing the entire forward pass. Moreover, we keep $k$\nhighest confidence tokens for each semantic category to uphold the\nrepresentative context information. Thus, computational complexity will change\nwith the difficulty of the input, akin to the way humans do segmentation.\nExperiments suggest that the proposed DToP architecture reduces on average\n$20\\% - 35\\%$ of computational cost for current semantic segmentation methods\nbased on plain vision transformers without accuracy degradation.",
        "authors": [
            "Quan Tang",
            "Bowen Zhang",
            "Jiajun Liu",
            "Fagui Liu",
            "Yifan Liu"
        ]
    },
    {
        "title": "Shape Anchor Guided Holistic Indoor Scene Understanding",
        "url": "http://arxiv.org/abs/2309.11133",
        "abstract": "This paper proposes a shape anchor guided learning strategy (AncLearn) for\nrobust holistic indoor scene understanding. We observe that the search space\nconstructed by current methods for proposal feature grouping and instance point\nsampling often introduces massive noise to instance detection and mesh\nreconstruction. Accordingly, we develop AncLearn to generate anchors that\ndynamically fit instance surfaces to (i) unmix noise and target-related\nfeatures for offering reliable proposals at the detection stage, and (ii)\nreduce outliers in object point sampling for directly providing well-structured\ngeometry priors without segmentation during reconstruction. We embed AncLearn\ninto a reconstruction-from-detection learning system (AncRec) to generate\nhigh-quality semantic scene models in a purely instance-oriented manner.\nExperiments conducted on the challenging ScanNetv2 dataset demonstrate that our\nshape anchor-based method consistently achieves state-of-the-art performance in\nterms of 3D object detection, layout estimation, and shape reconstruction. The\ncode will be available at https://github.com/Geo-Tell/AncRec.",
        "authors": [
            "Mingyue Dong",
            "Linxi Huan",
            "Hanjiang Xiong",
            "Shuhan Shen",
            "Xianwei Zheng"
        ]
    },
    {
        "title": "Knowledge-Aware Federated Active Learning with Non-IID Data",
        "url": "http://arxiv.org/abs/2211.13579",
        "abstract": "Federated learning enables multiple decentralized clients to learn\ncollaboratively without sharing the local training data. However, the expensive\nannotation cost to acquire data labels on local clients remains an obstacle in\nutilizing local data. In this paper, we propose a federated active learning\nparadigm to efficiently learn a global model with limited annotation budget\nwhile protecting data privacy in a decentralized learning way. The main\nchallenge faced by federated active learning is the mismatch between the active\nsampling goal of the global model on the server and that of the asynchronous\nlocal clients. This becomes even more significant when data is distributed\nnon-IID across local clients. To address the aforementioned challenge, we\npropose Knowledge-Aware Federated Active Learning (KAFAL), which consists of\nKnowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory\nFederated Update (KCFU). KSAS is a novel active sampling method tailored for\nthe federated active learning problem. It deals with the mismatch challenge by\nsampling actively based on the discrepancies between local and global models.\nKSAS intensifies specialized knowledge in local clients, ensuring the sampled\ndata to be informative for both the local clients and the global model. KCFU,\nin the meantime, deals with the client heterogeneity caused by limited data and\nnon-IID data distributions. It compensates for each client's ability in weak\nclasses by the assistance of the global model. Extensive experiments and\nanalyses are conducted to show the superiority of KSAS over the\nstate-of-the-art active learning methods and the efficiency of KCFU under the\nfederated active learning framework.",
        "authors": [
            "Yu-Tong Cao",
            "Ye Shi",
            "Baosheng Yu",
            "Jingya Wang",
            "Dacheng Tao"
        ]
    },
    {
        "title": "PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs",
        "url": "http://arxiv.org/abs/2308.05744",
        "abstract": "In this paper, we develop a new method to automatically convert 2D line\ndrawings from three orthographic views into 3D CAD models. Existing methods for\nthis problem reconstruct 3D models by back-projecting the 2D observations into\n3D space while maintaining explicit correspondence between the input and\noutput. Such methods are sensitive to errors and noises in the input, thus\noften fail in practice where the input drawings created by human designers are\nimperfect. To overcome this difficulty, we leverage the attention mechanism in\na Transformer-based sequence generation model to learn flexible mappings\nbetween the input and output. Further, we design shape programs which are\nsuitable for generating the objects of interest to boost the reconstruction\naccuracy and facilitate CAD modeling applications. Experiments on a new\nbenchmark dataset show that our method significantly outperforms existing ones\nwhen the inputs are noisy or incomplete.",
        "authors": [
            "Wentao Hu",
            "Jia Zheng",
            "Zixin Zhang",
            "Xiaojun Yuan",
            "Jian Yin",
            "Zihan Zhou"
        ]
    },
    {
        "title": "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-efficient Fine-Tuning",
        "url": "http://arxiv.org/abs/2304.06648",
        "abstract": "Diffusion models have proven to be highly effective in generating\nhigh-quality images. However, adapting large pre-trained diffusion models to\nnew domains remains an open challenge, which is critical for real-world\napplications. This paper proposes DiffFit, a parameter-efficient strategy to\nfine-tune large pre-trained diffusion models that enable fast adaptation to new\ndomains. DiffFit is embarrassingly simple that only fine-tunes the bias term\nand newly-added scaling factors in specific layers, yet resulting in\nsignificant training speed-up and reduced model storage costs. Compared with\nfull fine-tuning, DiffFit achieves 2$\\times$ training speed-up and only needs\nto store approximately 0.12\\% of the total model parameters. Intuitive\ntheoretical analysis has been provided to justify the efficacy of scaling\nfactors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior\nor competitive performances compared to the full fine-tuning while being more\nefficient. Remarkably, we show that DiffFit can adapt a pre-trained\nlow-resolution generative model to a high-resolution one by adding minimal\ncost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of\n3.02 on ImageNet 512$\\times$512 benchmark by fine-tuning only 25 epochs from a\npublic pre-trained ImageNet 256$\\times$256 checkpoint while being 30$\\times$\nmore training efficient than the closest competitor.",
        "authors": [
            "Enze Xie",
            "Lewei Yao",
            "Han Shi",
            "Zhili Liu",
            "Daquan Zhou",
            "Zhaoqiang Liu",
            "Jiawei Li",
            "Zhenguo Li"
        ]
    },
    {
        "title": "NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes",
        "url": "http://arxiv.org/abs/2308.12967",
        "abstract": "Recent implicit neural representations have shown great results for novel\nview synthesis. However, existing methods require expensive per-scene\noptimization from many views hence limiting their application to real-world\nunbounded urban settings where the objects of interest or backgrounds are\nobserved from very few views. To mitigate this challenge, we introduce a new\napproach called NeO 360, Neural fields for sparse view synthesis of outdoor\nscenes. NeO 360 is a generalizable method that reconstructs 360{\\deg} scenes\nfrom a single or a few posed RGB images. The essence of our approach is in\ncapturing the distribution of complex real-world outdoor 3D scenes and using a\nhybrid image-conditional triplanar representation that can be queried from any\nworld point. Our representation combines the best of both voxel-based and\nbird's-eye-view (BEV) representations and is more effective and expressive than\neach. NeO 360's representation allows us to learn from a large collection of\nunbounded 3D scenes while offering generalizability to new views and novel\nscenes from as few as a single image during inference. We demonstrate our\napproach on the proposed challenging 360{\\deg} unbounded dataset, called NeRDS\n360, and show that NeO 360 outperforms state-of-the-art generalizable methods\nfor novel view synthesis while also offering editing and composition\ncapabilities. Project page:\nhttps://zubair-irshad.github.io/projects/neo360.html",
        "authors": [
            "Muhammad Zubair Irshad",
            "Sergey Zakharov",
            "Katherine Liu",
            "Vitor Guizilini",
            "Thomas Kollar",
            "Adrien Gaidon",
            "Zsolt Kira",
            "Rares Ambrus"
        ]
    },
    {
        "title": "UnLoc: A Unified Framework for Video Localization Tasks",
        "url": "http://arxiv.org/abs/2308.11062",
        "abstract": "While large-scale image-text pretrained models such as CLIP have been used\nfor multiple video-level tasks on trimmed videos, their use for temporal\nlocalization in untrimmed videos is still a relatively unexplored task. We\ndesign a new approach for this called UnLoc, which uses pretrained image and\ntext towers, and feeds tokens to a video-text fusion model. The output of the\nfusion module are then used to construct a feature pyramid in which each level\nconnects to a head to predict a per-frame relevancy score and start/end time\ndisplacements. Unlike previous works, our architecture enables Moment\nRetrieval, Temporal Localization, and Action Segmentation with a single stage\nmodel, without the need for action proposals, motion based pretrained features\nor representation masking. Unlike specialized models, we achieve state of the\nart results on all three different localization tasks with a unified approach.\nCode will be available at: \\url{https://github.com/google-research/scenic}.",
        "authors": [
            "Shen Yan",
            "Xuehan Xiong",
            "Arsha Nagrani",
            "Anurag Arnab",
            "Zhonghao Wang",
            "Weina Ge",
            "David Ross",
            "Cordelia Schmid"
        ]
    },
    {
        "title": "QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.10515",
        "abstract": "Multi-view 3D detection based on BEV (bird-eye-view) has recently achieved\nsignificant improvements. However, the huge memory consumption of\nstate-of-the-art models makes it hard to deploy them on vehicles, and the\nnon-trivial latency will affect the real-time perception of streaming\napplications. Despite the wide application of quantization to lighten models,\nwe show in our paper that directly applying quantization in BEV tasks will 1)\nmake the training unstable, and 2) lead to intolerable performance degradation.\nTo solve these issues, our method QD-BEV enables a novel view-guided\ndistillation (VGD) objective, which can stabilize the quantization-aware\ntraining (QAT) while enhancing the model performance by leveraging both image\nfeatures and BEV features. Our experiments show that QD-BEV achieves similar or\neven better accuracy than previous methods with significant efficiency gains.\nOn the nuScenes datasets, the 4-bit weight and 6-bit activation quantized\nQD-BEV-Tiny model achieves 37.2% NDS with only 15.8 MB model size,\noutperforming BevFormer-Tiny by 1.8% with an 8x model compression. On the Small\nand Base variants, QD-BEV models also perform superbly and achieve 47.9% NDS\n(28.2 MB) and 50.9% NDS (32.9 MB), respectively.",
        "authors": [
            "Yifan Zhang",
            "Zhen Dong",
            "Huanrui Yang",
            "Ming Lu",
            "Cheng-Ching Tseng",
            "Yuan Du",
            "Kurt Keutzer",
            "Li Du",
            "Shanghang Zhang"
        ]
    },
    {
        "title": "Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction",
        "url": "http://arxiv.org/abs/2308.08824",
        "abstract": "Safety-critical applications such as autonomous vehicles and social robots\nrequire fast computation and accurate probability density estimation on\ntrajectory prediction. To address both requirements, this paper presents a new\nnormalizing flow-based trajectory prediction model named FlowChain. FlowChain\nis a stack of conditional continuously-indexed flows (CIFs) that are expressive\nand allow analytical probability density computation. This analytical\ncomputation is faster than the generative models that need additional\napproximations such as kernel density estimation. Moreover, FlowChain is more\naccurate than the Gaussian mixture-based models due to fewer assumptions on the\nestimated density. FlowChain also allows a rapid update of estimated\nprobability densities. This update is achieved by adopting the \\textit{newest\nobserved position} and reusing the flow transformations and its\nlog-det-jacobians that represent the \\textit{motion trend}. This update is\ncompleted in less than one millisecond because this reuse greatly omits the\ncomputational cost. Experimental results showed our FlowChain achieved\nstate-of-the-art trajectory prediction accuracy compared to previous methods.\nFurthermore, our FlowChain demonstrated superiority in the accuracy and speed\nof density estimation. Our code is available at\n\\url{https://github.com/meaten/FlowChain-ICCV2023}",
        "authors": [
            "Takahiro Maeda",
            "Norimichi Ukita"
        ]
    },
    {
        "title": "CLIPascene: Scene Sketching with Different Types and Levels of Abstraction",
        "url": "http://arxiv.org/abs/2211.17256",
        "abstract": "In this paper, we present a method for converting a given scene image into a\nsketch using different types and multiple levels of abstraction. We distinguish\nbetween two types of abstraction. The first considers the fidelity of the\nsketch, varying its representation from a more precise portrayal of the input\nto a looser depiction. The second is defined by the visual simplicity of the\nsketch, moving from a detailed depiction to a sparse sketch. Using an explicit\ndisentanglement into two abstraction axes -- and multiple levels for each one\n-- provides users additional control over selecting the desired sketch based on\ntheir personal goals and preferences. To form a sketch at a given level of\nfidelity and simplification, we train two MLP networks. The first network\nlearns the desired placement of strokes, while the second network learns to\ngradually remove strokes from the sketch without harming its recognizability\nand semantics. Our approach is able to generate sketches of complex scenes\nincluding those with complex backgrounds (e.g., natural and urban settings) and\nsubjects (e.g., animals and people) while depicting gradual abstractions of the\ninput scene in terms of fidelity and simplicity.",
        "authors": [
            "Yael Vinker",
            "Yuval Alaluf",
            "Daniel Cohen-Or",
            "Ariel Shamir"
        ]
    },
    {
        "title": "Vision Grid Transformer for Document Layout Analysis",
        "url": "http://arxiv.org/abs/2308.14978",
        "abstract": "Document pre-trained models and grid-based models have proven to be very\neffective on various tasks in Document AI. However, for the document layout\nanalysis (DLA) task, existing document pre-trained models, even those\npre-trained in a multi-modal fashion, usually rely on either textual features\nor visual features. Grid-based models for DLA are multi-modality but largely\nneglect the effect of pre-training. To fully leverage multi-modal information\nand exploit pre-training techniques to learn better representation for DLA, in\nthis paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid\nTransformer (GiT) is proposed and pre-trained for 2D token-level and\nsegment-level semantic understanding. Furthermore, a new dataset named D$^4$LA,\nwhich is so far the most diverse and detailed manually-annotated benchmark for\ndocument layout analysis, is curated and released. Experiment results have\nillustrated that the proposed VGT model achieves new state-of-the-art results\non DLA tasks, e.g. PubLayNet ($95.7\\%$$\\rightarrow$$96.2\\%$), DocBank\n($79.6\\%$$\\rightarrow$$84.1\\%$), and D$^4$LA ($67.7\\%$$\\rightarrow$$68.8\\%$).\nThe code and models as well as the D$^4$LA dataset will be made publicly\navailable ~\\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}.",
        "authors": [
            "Cheng Da",
            "Chuwei Luo",
            "Qi Zheng",
            "Cong Yao"
        ]
    },
    {
        "title": "Multi-Directional Subspace Editing in Style-Space",
        "url": "http://arxiv.org/abs/2211.11825",
        "abstract": "This paper describes a new technique for finding disentangled semantic\ndirections in the latent space of StyleGAN. Our method identifies meaningful\northogonal subspaces that allow editing of one human face attribute, while\nminimizing undesired changes in other attributes. Our model is capable of\nediting a single attribute in multiple directions, resulting in a range of\npossible generated images. We compare our scheme with three state-of-the-art\nmodels and show that our method outperforms them in terms of face editing and\ndisentanglement capabilities. Additionally, we suggest quantitative measures\nfor evaluating attribute separation and disentanglement, and exhibit the\nsuperiority of our model with respect to those measures.",
        "authors": [
            "Chen Naveh",
            "Yacov Hel-Or"
        ]
    },
    {
        "title": "Adaptive Superpixel for Active Learning in Semantic Segmentation",
        "url": "http://arxiv.org/abs/2303.16817",
        "abstract": "Learning semantic segmentation requires pixel-wise annotations, which can be\ntime-consuming and expensive. To reduce the annotation cost, we propose a\nsuperpixel-based active learning (AL) framework, which collects a dominant\nlabel per superpixel instead. To be specific, it consists of adaptive\nsuperpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,\nwe adaptively merge neighboring pixels of similar learned features into\nsuperpixels. We then query a selected subset of these superpixels using an\nacquisition function assuming no uniform superpixel size. This approach is more\nefficient than existing methods, which rely only on innate features such as RGB\ncolor and assume uniform superpixel sizes. Obtaining a dominant label per\nsuperpixel drastically reduces annotators' burden as it requires fewer clicks.\nHowever, it inevitably introduces noisy annotations due to mismatches between\nsuperpixel and ground truth segmentation. To address this issue, we further\ndevise a sieving mechanism that identifies and excludes potentially noisy\nannotations from learning. Our experiments on both Cityscapes and PASCAL VOC\ndatasets demonstrate the efficacy of adaptive superpixel and sieving\nmechanisms.",
        "authors": [
            "Hoyoung Kim",
            "Minhyeon Oh",
            "Sehyun Hwang",
            "Suha Kwak",
            "Jungseul Ok"
        ]
    },
    {
        "title": "Parametric Information Maximization for Generalized Category Discovery",
        "url": "http://arxiv.org/abs/2212.00334",
        "abstract": "We introduce a Parametric Information Maximization (PIM) model for the\nGeneralized Category Discovery (GCD) problem. Specifically, we propose a\nbi-level optimization formulation, which explores a parameterized family of\nobjective functions, each evaluating a weighted mutual information between the\nfeatures and the latent labels, subject to supervision constraints from the\nlabeled samples. Our formulation mitigates the class-balance bias encoded in\nstandard information maximization approaches, thereby handling effectively both\nshort-tailed and long-tailed data sets. We report extensive experiments and\ncomparisons demonstrating that our PIM model consistently sets new\nstate-of-the-art performances in GCD across six different datasets, more so\nwhen dealing with challenging fine-grained problems.",
        "authors": [
            "Florent Chiaroni",
            "Jose Dolz",
            "Ziko Imtiaz Masud",
            "Amar Mitiche",
            "Ismail Ben Ayed"
        ]
    },
    {
        "title": "Convex Decomposition of Indoor Scenes",
        "url": "http://arxiv.org/abs/2307.04246",
        "abstract": "We describe a method to parse a complex, cluttered indoor scene into\nprimitives which offer a parsimonious abstraction of scene structure. Our\nprimitives are simple convexes. Our method uses a learned regression procedure\nto parse a scene into a fixed number of convexes from RGBD input, and can\noptionally accept segmentations to improve the decomposition. The result is\nthen polished with a descent method which adjusts the convexes to produce a\nvery good fit, and greedily removes superfluous primitives. Because the entire\nscene is parsed, we can evaluate using traditional depth, normal, and\nsegmentation error metrics. Our evaluation procedure demonstrates that the\nerror from our primitive representation is comparable to that of predicting\ndepth from a single image.",
        "authors": [
            "Vaibhav Vavilala",
            "David Forsyth"
        ]
    },
    {
        "title": "Toward Unsupervised Realistic Visual Question Answering",
        "url": "http://arxiv.org/abs/2303.05068",
        "abstract": "The problem of realistic VQA (RVQA), where a model has to reject unanswerable\nquestions (UQs) and answer answerable ones (AQs), is studied. We first point\nout 2 drawbacks in current RVQA research, where (1) datasets contain too many\nunchallenging UQs and (2) a large number of annotated UQs are required for\ntraining. To resolve the first drawback, we propose a new testing dataset,\nRGQA, which combines AQs from an existing VQA dataset with around 29K\nhuman-annotated UQs. These UQs consist of both fine-grained and coarse-grained\nimage-question pairs generated with 2 approaches: CLIP-based and\nPerturbation-based. To address the second drawback, we introduce an\nunsupervised training approach. This combines pseudo UQs obtained by randomly\npairing images and questions, with an RoI Mixup procedure to generate more\nfine-grained pseudo UQs, and model ensembling to regularize model confidence.\nExperiments show that using pseudo UQs significantly outperforms RVQA\nbaselines. RoI Mixup and model ensembling further increase the gain. Finally,\nhuman evaluation reveals a performance gap between humans and models, showing\nthat more RVQA research is needed.",
        "authors": [
            "Yuwei Zhang",
            "Chih-Hui Ho",
            "Nuno Vasconcelos"
        ]
    },
    {
        "title": "A Generalist Framework for Panoptic Segmentation of Images and Videos",
        "url": "http://arxiv.org/abs/2210.06366",
        "abstract": "Panoptic segmentation assigns semantic and instance ID labels to every pixel\nof an image. As permutations of instance IDs are also valid solutions, the task\nrequires learning of high-dimensional one-to-many mapping. As a result,\nstate-of-the-art approaches use customized architectures and task-specific loss\nfunctions. We formulate panoptic segmentation as a discrete data generation\nproblem, without relying on inductive bias of the task. A diffusion model is\nproposed to model panoptic masks, with a simple architecture and generic loss\nfunction. By simply adding past predictions as a conditioning signal, our\nmethod is capable of modeling video (in a streaming setting) and thereby learns\nto track object instances automatically. With extensive experiments, we\ndemonstrate that our simple approach can perform competitively to\nstate-of-the-art specialist methods in similar settings.",
        "authors": [
            "Ting Chen",
            "Lala Li",
            "Saurabh Saxena",
            "Geoffrey Hinton",
            "David J. Fleet"
        ]
    },
    {
        "title": "Few Shot Font Generation Via Transferring Similarity Guided Global Style and Quantization Local Style",
        "url": "http://arxiv.org/abs/2309.00827",
        "abstract": "Automatic few-shot font generation (AFFG), aiming at generating new fonts\nwith only a few glyph references, reduces the labor cost of manually designing\nfonts. However, the traditional AFFG paradigm of style-content disentanglement\ncannot capture the diverse local details of different fonts. So, many\ncomponent-based approaches are proposed to tackle this problem. The issue with\ncomponent-based approaches is that they usually require special pre-defined\nglyph components, e.g., strokes and radicals, which is infeasible for AFFG of\ndifferent languages. In this paper, we present a novel font generation approach\nby aggregating styles from character similarity-guided global features and\nstylized component-level representations. We calculate the similarity scores of\nthe target character and the referenced samples by measuring the distance along\nthe corresponding channels from the content features, and assigning them as the\nweights for aggregating the global style features. To better capture the local\nstyles, a cross-attention-based style transfer module is adopted to transfer\nthe styles of reference glyphs to the components, where the components are\nself-learned discrete latent codes through vector quantization without manual\ndefinition. With these designs, our AFFG method could obtain a complete set of\ncomponent-level style representations, and also control the global glyph\ncharacteristics. The experimental results reflect the effectiveness and\ngeneralization of the proposed method on different linguistic scripts, and also\nshow its superiority when compared with other state-of-the-art methods. The\nsource code can be found at https://github.com/awei669/VQ-Font.",
        "authors": [
            "Wei Pan",
            "Anna Zhu",
            "Xinyu Zhou",
            "Brian Kenji Iwana",
            "Shilin Li"
        ]
    },
    {
        "title": "Differentiable Transportation Pruning",
        "url": "http://arxiv.org/abs/2307.08483",
        "abstract": "Deep learning algorithms are increasingly employed at the edge. However, edge\ndevices are resource constrained and thus require efficient deployment of deep\nneural networks. Pruning methods are a key tool for edge deployment as they can\nimprove storage, compute, memory bandwidth, and energy usage. In this paper we\npropose a novel accurate pruning technique that allows precise control over the\noutput network size. Our method uses an efficient optimal transportation scheme\nwhich we make end-to-end differentiable and which automatically tunes the\nexploration-exploitation behavior of the algorithm to find accurate sparse\nsub-networks. We show that our method achieves state-of-the-art performance\ncompared to previous pruning methods on 3 different datasets, using 5 different\nmodels, across a wide range of pruning ratios, and with two types of sparsity\nbudgets and pruning granularities.",
        "authors": [
            "Yunqiang Li",
            "Jan C. van Gemert",
            "Torsten Hoefler",
            "Bert Moons",
            "Evangelos Eleftheriou",
            "Bram-Ernst Verhoef"
        ]
    },
    {
        "title": "Physics-Driven Turbulence Image Restoration with Stochastic Refinement",
        "url": "http://arxiv.org/abs/2307.10603",
        "abstract": "Image distortion by atmospheric turbulence is a stochastic degradation, which\nis a critical problem in long-range optical imaging systems. A number of\nresearch has been conducted during the past decades, including model-based and\nemerging deep-learning solutions with the help of synthetic data. Although fast\nand physics-grounded simulation tools have been introduced to help the\ndeep-learning models adapt to real-world turbulence conditions recently, the\ntraining of such models only relies on the synthetic data and ground truth\npairs. This paper proposes the Physics-integrated Restoration Network (PiRN) to\nbring the physics-based simulator directly into the training process to help\nthe network to disentangle the stochasticity from the degradation and the\nunderlying image. Furthermore, to overcome the ``average effect\" introduced by\ndeterministic models and the domain gap between the synthetic and real-world\ndegradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) to\nboost its perceptual quality. Overall, our PiRN and PiRN-SR improve the\ngeneralization to real-world unknown turbulence conditions and provide a\nstate-of-the-art restoration in both pixel-wise accuracy and perceptual\nquality. Our codes are available at \\url{https://github.com/VITA-Group/PiRN}.",
        "authors": [
            "Ajay Jaiswal",
            "Xingguang Zhang",
            "Stanley H. Chan",
            "Zhangyang Wang"
        ]
    },
    {
        "title": "Scale-Aware Modulation Meet Transformer",
        "url": "http://arxiv.org/abs/2307.08579",
        "abstract": "This paper presents a new vision Transformer, Scale-Aware Modulation\nTransformer (SMT), that can handle various downstream tasks efficiently by\ncombining the convolutional network and vision Transformer. The proposed\nScale-Aware Modulation (SAM) in the SMT includes two primary novel designs.\nFirstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can\ncapture multi-scale features and expand the receptive field. Secondly, we\npropose the Scale-Aware Aggregation (SAA) module, which is lightweight but\neffective, enabling information fusion across different heads. By leveraging\nthese two modules, convolutional modulation is further enhanced. Furthermore,\nin contrast to prior works that utilized modulations throughout all stages to\nbuild an attention-free network, we propose an Evolutionary Hybrid Network\n(EHN), which can effectively simulate the shift from capturing local to global\ndependencies as the network becomes deeper, resulting in superior performance.\nExtensive experiments demonstrate that SMT significantly outperforms existing\nstate-of-the-art models across a wide range of visual tasks. Specifically, SMT\nwith 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1\naccuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in\n224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with\nresolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN,\nthe SMT base trained with 1x and 3x schedule outperforms the Swin Transformer\ncounterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation\nwith UPerNet, the SMT base test at single- and multi-scale surpasses Swin by\n2.0 and 1.1 mIoU respectively on the ADE20K.",
        "authors": [
            "Weifeng Lin",
            "Ziheng Wu",
            "Jiayu Chen",
            "Jun Huang",
            "Lianwen Jin"
        ]
    },
    {
        "title": "Large Selective Kernel Network for Remote Sensing Object Detection",
        "url": "http://arxiv.org/abs/2303.09030",
        "abstract": "Recent research on remote sensing object detection has largely focused on\nimproving the representation of oriented bounding boxes but has overlooked the\nunique prior knowledge presented in remote sensing scenarios. Such prior\nknowledge can be useful because tiny remote sensing objects may be mistakenly\ndetected without referencing a sufficiently long-range context, and the\nlong-range context required by different types of objects can vary. In this\npaper, we take these priors into account and propose the Large Selective Kernel\nNetwork (LSKNet). LSKNet can dynamically adjust its large spatial receptive\nfield to better model the ranging context of various objects in remote sensing\nscenarios. To the best of our knowledge, this is the first time that large and\nselective kernel mechanisms have been explored in the field of remote sensing\nobject detection. Without bells and whistles, LSKNet sets new state-of-the-art\nscores on standard benchmarks, i.e., HRSC2016 (98.46\\% mAP), DOTA-v1.0 (81.85\\%\nmAP) and FAIR1M-v1.0 (47.87\\% mAP). Based on a similar technique, we rank 2nd\nplace in 2022 the Greater Bay Area International Algorithm Competition. Code is\navailable at https://github.com/zcablii/Large-Selective-Kernel-Network.",
        "authors": [
            "Yuxuan Li",
            "Qibin Hou",
            "Zhaohui Zheng",
            "Ming-Ming Cheng",
            "Jian Yang",
            "Xiang Li"
        ]
    },
    {
        "title": "PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View",
        "url": "http://arxiv.org/abs/2307.13756",
        "abstract": "3D plane recovery from a single image can usually be divided into several\nsubtasks of plane detection, segmentation, parameter estimation and possibly\ndepth estimation. Previous works tend to solve this task by either extending\nthe RCNN-based segmentation network or the dense pixel embedding-based\nclustering framework. However, none of them tried to integrate above related\nsubtasks into a unified framework but treat them separately and sequentially,\nwhich we suspect is potentially a main source of performance limitation for\nexisting approaches. Motivated by this finding and the success of query-based\nlearning in enriching reasoning among semantic entities, in this paper, we\npropose PlaneRecTR, a Transformer-based architecture, which for the first time\nunifies all subtasks related to single-view plane recovery with a single\ncompact model. Extensive quantitative and qualitative experiments demonstrate\nthat our proposed unified learning achieves mutual benefits across subtasks,\nobtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane\ndatasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.",
        "authors": [
            "Jingjia Shi",
            "Shuaifeng Zhi",
            "Kai Xu"
        ]
    },
    {
        "title": "EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting",
        "url": "http://arxiv.org/abs/2307.09306",
        "abstract": "Capturing high-dimensional social interactions and feasible futures is\nessential for predicting trajectories. To address this complex nature, several\nattempts have been devoted to reducing the dimensionality of the output\nvariables via parametric curve fitting such as the B\\'ezier curve and B-spline\nfunction. However, these functions, which originate in computer graphics\nfields, are not suitable to account for socially acceptable human dynamics. In\nthis paper, we present EigenTrajectory ($\\mathbb{ET}$), a trajectory prediction\napproach that uses a novel trajectory descriptor to form a compact space, known\nhere as $\\mathbb{ET}$ space, in place of Euclidean space, for representing\npedestrian movements. We first reduce the complexity of the trajectory\ndescriptor via a low-rank approximation. We transform the pedestrians' history\npaths into our $\\mathbb{ET}$ space represented by spatio-temporal principle\ncomponents, and feed them into off-the-shelf trajectory forecasting models. The\ninputs and outputs of the models as well as social interactions are all\ngathered and aggregated in the corresponding $\\mathbb{ET}$ space. Lastly, we\npropose a trajectory anchor-based refinement method to cover all possible\nfutures in the proposed $\\mathbb{ET}$ space. Extensive experiments demonstrate\nthat our EigenTrajectory predictor can significantly improve both the\nprediction accuracy and reliability of existing trajectory forecasting models\non public benchmarks, indicating that the proposed descriptor is suited to\nrepresent pedestrian behaviors. Code is publicly available at\nhttps://github.com/inhwanbae/EigenTrajectory .",
        "authors": [
            "Inhwan Bae",
            "Jean Oh",
            "Hae-Gon Jeon"
        ]
    },
    {
        "title": "SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets",
        "url": "http://arxiv.org/abs/2308.11880",
        "abstract": "Scene understanding using multi-modal data is necessary in many applications,\ne.g., autonomous navigation. To achieve this in a variety of situations,\nexisting models must be able to adapt to shifting data distributions without\narduous data annotation. Current approaches assume that the source data is\navailable during adaptation and that the source consists of paired multi-modal\ndata. Both these assumptions may be problematic for many applications. Source\ndata may not be available due to privacy, security, or economic concerns.\nAssuming the existence of paired multi-modal data for training also entails\nsignificant data collection costs and fails to take advantage of widely\navailable freely distributed pre-trained uni-modal models. In this work, we\nrelax both of these assumptions by addressing the problem of adapting a set of\nmodels trained independently on uni-modal data to a target domain consisting of\nunlabeled multi-modal data, without having access to the original source\ndataset. Our proposed approach solves this problem through a switching\nframework which automatically chooses between two complementary methods of\ncross-modal pseudo-label fusion -- agreement filtering and entropy weighting --\nbased on the estimated domain gap. We demonstrate our work on the semantic\nsegmentation problem. Experiments across seven challenging adaptation scenarios\nverify the efficacy of our approach, achieving results comparable to, and in\nsome cases outperforming, methods which assume access to source data. Our\nmethod achieves an improvement in mIoU of up to 12% over competing baselines.\nOur code is publicly available at https://github.com/csimo005/SUMMIT.",
        "authors": [
            "Cody Simons",
            "Dripta S. Raychaudhuri",
            "Sk Miraj Ahmed",
            "Suya You",
            "Konstantinos Karydis",
            "Amit K. Roy-Chowdhury"
        ]
    },
    {
        "title": "Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection",
        "url": "http://arxiv.org/abs/2308.11441",
        "abstract": "Latest methods represent shapes with open surfaces using unsigned distance\nfunctions (UDFs). They train neural networks to learn UDFs and reconstruct\nsurfaces with the gradients around the zero level set of the UDF. However, the\ndifferential networks struggle from learning the zero level set where the UDF\nis not differentiable, which leads to large errors on unsigned distances and\ngradients around the zero level set, resulting in highly fragmented and\ndiscontinuous surfaces. To resolve this problem, we propose to learn a more\ncontinuous zero level set in UDFs with level set projections. Our insight is to\nguide the learning of zero level set using the rest non-zero level sets via a\nprojection procedure. Our idea is inspired from the observations that the\nnon-zero level sets are much smoother and more continuous than the zero level\nset. We pull the non-zero level sets onto the zero level set with gradient\nconstraints which align gradients over different level sets and correct\nunsigned distance errors on the zero level set, leading to a smoother and more\ncontinuous unsigned distance field. We conduct comprehensive experiments in\nsurface reconstruction for point clouds, real scans or depth maps, and further\nexplore the performance in unsupervised point cloud upsampling and unsupervised\npoint normal estimation with the learned UDF, which demonstrate our non-trivial\nimprovements over the state-of-the-art methods. Code is available at\nhttps://github.com/junshengzhou/LevelSetUDF .",
        "authors": [
            "Junsheng Zhou",
            "Baorui Ma",
            "Shujuan Li",
            "Yu-Shen Liu",
            "Zhizhong Han"
        ]
    },
    {
        "title": "To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.15063",
        "abstract": "The goal of Online Domain Adaptation for semantic segmentation is to handle\nunforeseeable domain changes that occur during deployment, like sudden weather\nevents. However, the high computational costs associated with brute-force\nadaptation make this paradigm unfeasible for real-world applications. In this\npaper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training\nframework for real-time domain adaptation. Our approach includes a\nhardware-aware back-propagation orchestration agent (HAMT) and a dedicated\ndomain-shift detector that enables active control over when and how the model\nis adapted (LT). Thanks to these advancements, our approach is capable of\nperforming semantic segmentation while simultaneously adapting at more than\n29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and\nspeed trade-off is demonstrated on OnDA and SHIFT benchmarks through\nexperimental results.",
        "authors": [
            "Marc Botet Colomer",
            "Pier Luigi Dovesi",
            "Theodoros Panagiotakopoulos",
            "Joao Frederico Carvalho",
            "Linus H\u00e4renstam-Nielsen",
            "Hossein Azizpour",
            "Hedvig Kjellstr\u00f6m",
            "Daniel Cremers",
            "Matteo Poggi"
        ]
    },
    {
        "title": "Hidden Biases of End-to-End Driving Models",
        "url": "http://arxiv.org/abs/2306.07957",
        "abstract": "End-to-end driving systems have recently made rapid progress, in particular\non CARLA. Independent of their major contribution, they introduce changes to\nminor system components. Consequently, the source of improvements is unclear.\nWe identify two biases that recur in nearly all state-of-the-art methods and\nare critical for the observed progress on CARLA: (1) lateral recovery via a\nstrong inductive bias towards target point following, and (2) longitudinal\naveraging of multimodal waypoint predictions for slowing down. We investigate\nthe drawbacks of these biases and identify principled alternatives. By\nincorporating our insights, we develop TF++, a simple end-to-end method that\nranks first on the Longest6 and LAV benchmarks, gaining 11 driving score over\nthe best prior work on Longest6.",
        "authors": [
            "Bernhard Jaeger",
            "Kashyap Chitta",
            "Andreas Geiger"
        ]
    },
    {
        "title": "Strivec: Sparse Tri-Vector Radiance Fields",
        "url": "http://arxiv.org/abs/2307.13226",
        "abstract": "We propose Strivec, a novel neural representation that models a 3D scene as a\nradiance field with sparsely distributed and compactly factorized local tensor\nfeature grids. Our approach leverages tensor decomposition, following the\nrecent work TensoRF, to model the tensor grids. In contrast to TensoRF which\nuses a global tensor and focuses on their vector-matrix decomposition, we\npropose to utilize a cloud of local tensors and apply the classic\nCANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple\nvectors that express local feature distributions along spatial axes and\ncompactly encode a local neural field. We also apply multi-scale tensor grids\nto discover the geometry and appearance commonalities and exploit spatial\ncoherence with the tri-vector factorization at multiple local scales. The final\nradiance field properties are regressed by aggregating neural features from\nmultiple local tensors across all scales. Our tri-vector tensors are sparsely\ndistributed around the actual scene surface, discovered by a fast coarse\nreconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our\nmodel can achieve better rendering quality while using significantly fewer\nparameters than previous methods, including TensoRF and Instant-NGP.",
        "authors": [
            "Quankai Gao",
            "Qiangeng Xu",
            "Hao Su",
            "Ulrich Neumann",
            "Zexiang Xu"
        ]
    },
    {
        "title": "Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering",
        "url": "http://arxiv.org/abs/2304.10075",
        "abstract": "The rendering scheme in neural radiance field (NeRF) is effective in\nrendering a pixel by casting a ray into the scene. However, NeRF yields blurred\nrendering results when the training images are captured at non-uniform scales,\nand produces aliasing artifacts if the test images are taken in distant views.\nTo address this issue, Mip-NeRF proposes a multiscale representation as a\nconical frustum to encode scale information. Nevertheless, this approach is\nonly suitable for offline rendering since it relies on integrated positional\nencoding (IPE) to query a multilayer perceptron (MLP). To overcome this\nlimitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale\nrepresentation with a deferred architecture for real-time anti-aliasing\nrendering. Our approach includes a density Mip-VoG for scene geometry and a\nfeature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes\nscene scale using the level of detail (LOD) derived from ray differentials and\nuses quadrilinear interpolation to map a queried 3D location to its features\nand density from two neighboring downsampled voxel grids. To our knowledge, our\napproach is the first to offer multiscale training and real-time anti-aliasing\nrendering simultaneously. We conducted experiments on multiscale datasets, and\nthe results show that our approach outperforms state-of-the-art real-time\nrendering baselines.",
        "authors": [
            "Dongting Hu",
            "Zhenkai Zhang",
            "Tingbo Hou",
            "Tongliang Liu",
            "Huan Fu",
            "Mingming Gong"
        ]
    },
    {
        "title": "Tracking without Label: Unsupervised Multiple Object Tracking via Contrastive Similarity Learning",
        "url": "http://arxiv.org/abs/2309.00942",
        "abstract": "Unsupervised learning is a challenging task due to the lack of labels.\nMultiple Object Tracking (MOT), which inevitably suffers from mutual object\ninterference, occlusion, etc., is even more difficult without label\nsupervision. In this paper, we explore the latent consistency of sample\nfeatures across video frames and propose an Unsupervised Contrastive Similarity\nLearning method, named UCSL, including three contrast modules: self-contrast,\ncross-contrast, and ambiguity contrast. Specifically, i) self-contrast uses\nintra-frame direct and inter-frame indirect contrast to obtain discriminative\nrepresentations by maximizing self-similarity. ii) Cross-contrast aligns cross-\nand continuous-frame matching results, mitigating the persistent negative\neffect caused by object occlusion. And iii) ambiguity contrast matches\nambiguous objects with each other to further increase the certainty of\nsubsequent object association through an implicit manner. On existing\nbenchmarks, our method outperforms the existing unsupervised methods using only\nlimited help from ReID head, and even provides higher accuracy than lots of\nfully supervised methods.",
        "authors": [
            "Sha Meng",
            "Dian Shao",
            "Jiacheng Guo",
            "Shan Gao"
        ]
    },
    {
        "title": "Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection",
        "url": "http://arxiv.org/abs/2307.13529",
        "abstract": "Human-Object Interaction (HOI) detection is a challenging computer vision\ntask that requires visual models to address the complex interactive\nrelationship between humans and objects and predict HOI triplets. Despite the\nchallenges posed by the numerous interaction combinations, they also offer\nopportunities for multimodal learning of visual texts. In this paper, we\npresent a systematic and unified framework (RmLR) that enhances HOI detection\nby incorporating structured text knowledge. Firstly, we qualitatively and\nquantitatively analyze the loss of interaction information in the two-stage HOI\ndetector and propose a re-mining strategy to generate more comprehensive visual\nrepresentation.Secondly, we design more fine-grained sentence- and word-level\nalignment and knowledge transfer strategies to effectively address the\nmany-to-many matching problem between multiple interactions and multiple\ntexts.These strategies alleviate the matching confusion problem that arises\nwhen multiple interactions occur simultaneously, thereby improving the\neffectiveness of the alignment process. Finally, HOI reasoning by visual\nfeatures augmented with textual knowledge substantially improves the\nunderstanding of interactions. Experimental results illustrate the\neffectiveness of our approach, where state-of-the-art performance is achieved\non public benchmarks. We further analyze the effects of different components of\nour approach to provide insights into its efficacy.",
        "authors": [
            "Yichao Cao",
            "Qingfei Tang",
            "Feng Yang",
            "Xiu Su",
            "Shan You",
            "Xiaobo Lu",
            "Chang Xu"
        ]
    },
    {
        "title": "Strata-NeRF : Neural Radiance Fields for Stratified Scenes",
        "url": "http://arxiv.org/abs/2308.10337",
        "abstract": "Neural Radiance Field (NeRF) approaches learn the underlying 3D\nrepresentation of a scene and generate photo-realistic novel views with high\nfidelity. However, most proposed settings concentrate on modelling a single\nobject or a single level of a scene. However, in the real world, we may capture\na scene at multiple levels, resulting in a layered capture. For example,\ntourists usually capture a monument's exterior structure before capturing the\ninner structure. Modelling such scenes in 3D with seamless switching between\nlevels can drastically improve immersive experiences. However, most existing\ntechniques struggle in modelling such scenes. We propose Strata-NeRF, a single\nneural radiance field that implicitly captures a scene with multiple levels.\nStrata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)\nlatent representations which allow sudden changes in scene structure. We\nevaluate the effectiveness of our approach in multi-layered synthetic dataset\ncomprising diverse scenes and then further validate its generalization on the\nreal-world RealEstate10K dataset. We find that Strata-NeRF effectively captures\nstratified scenes, minimizes artifacts, and synthesizes high-fidelity views\ncompared to existing approaches.",
        "authors": [
            "Ankit Dhiman",
            "Srinath R",
            "Harsh Rangwani",
            "Rishubh Parihar",
            "Lokesh R Boregowda",
            "Srinath Sridhar",
            "R Venkatesh Babu"
        ]
    },
    {
        "title": "StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model",
        "url": "http://arxiv.org/abs/2303.09268",
        "abstract": "Despite the progress made in the style transfer task, most previous work\nfocus on transferring only relatively simple features like color or texture,\nwhile missing more abstract concepts such as overall art expression or\npainter-specific traits. However, these abstract semantics can be captured by\nmodels like DALL-E or CLIP, which have been trained using huge datasets of\nimages and textual documents. In this paper, we propose StylerDALLE, a style\ntransfer method that exploits both of these models and uses natural language to\ndescribe abstract art styles. Specifically, we formulate the language-guided\nstyle transfer task as a non-autoregressive token sequence translation, i.e.,\nfrom input content image to output stylized image, in the discrete latent space\nof a large-scale pretrained vector-quantized tokenizer, e.g., the discrete\nvariational auto-encoder (dVAE) of DALL-E. To incorporate style information, we\npropose a Reinforcement Learning strategy with CLIP-based language supervision\nthat ensures stylization and content preservation simultaneously. Experimental\nresults demonstrate the superiority of our method, which can effectively\ntransfer art styles using language instructions at different granularities.\nCode is available at https://github.com/zipengxuc/StylerDALLE.",
        "authors": [
            "Zipeng Xu",
            "Enver Sangineto",
            "Nicu Sebe"
        ]
    },
    {
        "title": "3D-aware Blending with Generative NeRFs",
        "url": "http://arxiv.org/abs/2302.06608",
        "abstract": "Image blending aims to combine multiple images seamlessly. It remains\nchallenging for existing 2D-based methods, especially when input images are\nmisaligned due to differences in 3D camera poses and object shapes. To tackle\nthese issues, we propose a 3D-aware blending method using generative Neural\nRadiance Fields (NeRF), including two key components: 3D-aware alignment and\n3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of\nthe reference image with respect to generative NeRFs and then perform 3D local\nalignment for each part. To further leverage 3D information of the generative\nNeRF, we propose 3D-aware blending that directly blends images on the NeRF's\nlatent representation space, rather than raw pixel space. Collectively, our\nmethod outperforms existing 2D baselines, as validated by extensive\nquantitative and qualitative evaluations with FFHQ and AFHQ-Cat.",
        "authors": [
            "Hyunsu Kim",
            "Gayoung Lee",
            "Yunjey Choi",
            "Jin-Hwa Kim",
            "Jun-Yan Zhu"
        ]
    },
    {
        "title": "Multi-Modal Gated Mixture of Local-to-Global Experts for Dynamic Image Fusion",
        "url": "http://arxiv.org/abs/2302.01392",
        "abstract": "Infrared and visible image fusion aims to integrate comprehensive information\nfrom multiple sources to achieve superior performances on various practical\ntasks, such as detection, over that of a single modality. However, most\nexisting methods directly combined the texture details and object contrast of\ndifferent modalities, ignoring the dynamic changes in reality, which diminishes\nthe visible texture in good lighting conditions and the infrared contrast in\nlow lighting conditions. To fill this gap, we propose a dynamic image fusion\nframework with a multi-modal gated mixture of local-to-global experts, termed\nMoE-Fusion, to dynamically extract effective and comprehensive information from\nthe respective modalities. Our model consists of a Mixture of Local Experts\n(MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The\nMoLE performs specialized learning of multi-modal local features, prompting the\nfused images to retain the local information in a sample-adaptive manner, while\nthe MoGE focuses on the global information that complements the fused image\nwith overall texture detail and contrast. Extensive experiments show that our\nMoE-Fusion outperforms state-of-the-art methods in preserving multi-modal image\ntexture and contrast through the local-to-global dynamic learning paradigm, and\nalso achieves superior performance on detection tasks. Our code will be\navailable: https://github.com/SunYM2020/MoE-Fusion.",
        "authors": [
            "Yiming Sun",
            "Bing Cao",
            "Pengfei Zhu",
            "Qinghua Hu"
        ]
    },
    {
        "title": "Deep Image Harmonization with Learnable Augmentation",
        "url": "http://arxiv.org/abs/2308.00376",
        "abstract": "The goal of image harmonization is adjusting the foreground appearance in a\ncomposite image to make the whole image harmonious. To construct paired\ntraining images, existing datasets adopt different ways to adjust the\nillumination statistics of foregrounds of real images to produce synthetic\ncomposite images. However, different datasets have considerable domain gap and\nthe performances on small-scale datasets are limited by insufficient training\ndata. In this work, we explore learnable augmentation to enrich the\nillumination diversity of small-scale datasets for better harmonization\nperformance. In particular, our designed SYthetic COmposite Network (SycoNet)\ntakes in a real image with foreground mask and a random vector to learn\nsuitable color transformation, which is applied to the foreground of this real\nimage to produce a synthetic composite image. Comprehensive experiments\ndemonstrate the effectiveness of our proposed learnable augmentation for image\nharmonization. The code of SycoNet is released at\nhttps://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization.",
        "authors": [
            "Li Niu",
            "Junyan Cao",
            "Wenyan Cong",
            "Liqing Zhang"
        ]
    },
    {
        "title": "DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds",
        "url": "http://arxiv.org/abs/2308.04383",
        "abstract": "Point clouds are naturally sparse, while image pixels are dense. The\ninconsistency limits feature fusion from both modalities for point-wise scene\nflow estimation. Previous methods rarely predict scene flow from the entire\npoint clouds of the scene with one-time inference due to the memory\ninefficiency and heavy overhead from distance calculation and sorting involved\nin commonly used farthest point sampling, KNN, and ball query algorithms for\nlocal feature aggregation. To mitigate these issues in scene flow learning, we\nregularize raw points to a dense format by storing 3D coordinates in 2D grids.\nUnlike the sampling operation commonly used in existing works, the dense 2D\nrepresentation 1) preserves most points in the given scene, 2) brings in a\nsignificant boost of efficiency, and 3) eliminates the density gap between\npoints and pixels, allowing us to perform effective feature fusion. We also\npresent a novel warping projection technique to alleviate the information loss\nproblem resulting from the fact that multiple points could be mapped into one\ngrid during projection when computing cost volume. Sufficient experiments\ndemonstrate the efficiency and effectiveness of our method, outperforming the\nprior-arts on the FlyingThings3D and KITTI dataset.",
        "authors": [
            "Chensheng Peng",
            "Guangming Wang",
            "Xian Wan Lo",
            "Xinrui Wu",
            "Chenfeng Xu",
            "Masayoshi Tomizuka",
            "Wei Zhan",
            "Hesheng Wang"
        ]
    },
    {
        "title": "High-Resolution Document Shadow Removal via A Large-Scale Real-World Dataset and A Frequency-Aware Shadow Erasing Net",
        "url": "http://arxiv.org/abs/2308.14221",
        "abstract": "Shadows often occur when we capture the documents with casual equipment,\nwhich influences the visual quality and readability of the digital copies.\nDifferent from the algorithms for natural shadow removal, the algorithms in\ndocument shadow removal need to preserve the details of fonts and figures in\nhigh-resolution input. Previous works ignore this problem and remove the\nshadows via approximate attention and small datasets, which might not work in\nreal-world situations. We handle high-resolution document shadow removal\ndirectly via a larger-scale real-world dataset and a carefully designed\nfrequency-aware network. As for the dataset, we acquire over 7k couples of\nhigh-resolution (2462 x 3699) images of real-world document pairs with various\nsamples under different lighting circumstances, which is 10 times larger than\nexisting datasets. As for the design of the network, we decouple the\nhigh-resolution images in the frequency domain, where the low-frequency details\nand high-frequency boundaries can be effectively learned via the carefully\ndesigned network structure. Powered by our network and dataset, the proposed\nmethod clearly shows a better performance than previous methods in terms of\nvisual quality and numerical results. The code, models, and dataset are\navailable at: https://github.com/CXH-Research/DocShadow-SD7K",
        "authors": [
            "Zinuo Li",
            "Xuhang Chen",
            "Chi-Man Pun",
            "Xiaodong Cun"
        ]
    },
    {
        "title": "Scalable Diffusion Models with Transformers",
        "url": "http://arxiv.org/abs/2212.09748",
        "abstract": "We explore a new class of diffusion models based on the transformer\narchitecture. We train latent diffusion models of images, replacing the\ncommonly-used U-Net backbone with a transformer that operates on latent\npatches. We analyze the scalability of our Diffusion Transformers (DiTs)\nthrough the lens of forward pass complexity as measured by Gflops. We find that\nDiTs with higher Gflops -- through increased transformer depth/width or\nincreased number of input tokens -- consistently have lower FID. In addition to\npossessing good scalability properties, our largest DiT-XL/2 models outperform\nall prior diffusion models on the class-conditional ImageNet 512x512 and\n256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
        "authors": [
            "William Peebles",
            "Saining Xie"
        ]
    },
    {
        "title": "From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels",
        "url": "http://arxiv.org/abs/2303.13005",
        "abstract": "Knowledge Distillation (KD) uses the teacher's prediction logits as soft\nlabels to guide the student, while self-KD does not need a real teacher to\nrequire the soft labels. This work unifies the formulations of the two tasks by\ndecomposing and reorganizing the generic KD loss into a Normalized KD (NKD)\nloss and customized soft labels for both target class (image's category) and\nnon-target classes named Universal Self-Knowledge Distillation (USKD). We\ndecompose the KD loss and find the non-target loss from it forces the student's\nnon-target logits to match the teacher's, but the sum of the two non-target\nlogits is different, preventing them from being identical. NKD normalizes the\nnon-target logits to equalize their sum. It can be generally used for KD and\nself-KD to better use the soft labels for distillation loss. USKD generates\ncustomized soft labels for both target and non-target classes without a\nteacher. It smooths the target logit of the student as the soft target label\nand uses the rank of the intermediate feature to generate the soft non-target\nlabels with Zipf's law. For KD with teachers, our NKD achieves state-of-the-art\nperformance on CIFAR-100 and ImageNet datasets, boosting the ImageNet Top-1\naccuracy of ResNet18 from 69.90% to 71.96% with a ResNet-34 teacher. For\nself-KD without teachers, USKD is the first self-KD method that can be\neffectively applied to both CNN and ViT models with negligible additional time\nand memory cost, resulting in new state-of-the-art results, such as 1.17% and\n0.55% accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively. Our\ncodes are available at https://github.com/yzd-v/cls_KD.",
        "authors": [
            "Zhendong Yang",
            "Ailing Zeng",
            "Zhe Li",
            "Tianke Zhang",
            "Chun Yuan",
            "Yu Li"
        ]
    },
    {
        "title": "SILT: Shadow-Aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels",
        "url": "http://arxiv.org/abs/2308.12064",
        "abstract": "Existing shadow detection datasets often contain missing or mislabeled\nshadows, which can hinder the performance of deep learning models trained\ndirectly on such data. To address this issue, we propose SILT, the Shadow-aware\nIterative Label Tuning framework, which explicitly considers noise in shadow\nlabels and trains the deep model in a self-training manner. Specifically, we\nincorporate strong data augmentations with shadow counterfeiting to help the\nnetwork better recognize non-shadow regions and alleviate overfitting. We also\ndevise a simple yet effective label tuning strategy with global-local fusion\nand shadow-aware filtering to encourage the network to make significant\nrefinements on the noisy labels. We evaluate the performance of SILT by\nrelabeling the test set of the SBU dataset and conducting various experiments.\nOur results show that even a simple U-Net trained with SILT can outperform all\nstate-of-the-art methods by a large margin. When trained on SBU / UCF / ISTD,\nour network can successfully reduce the Balanced Error Rate by 25.2% / 36.9% /\n21.3% over the best state-of-the-art method.",
        "authors": [
            "Han Yang",
            "Tianyu Wang",
            "Xiaowei Hu",
            "Chi-Wing Fu"
        ]
    },
    {
        "title": "Implicit Autoencoder for Point-Cloud Self-Supervised Representation Learning",
        "url": "http://arxiv.org/abs/2201.00785",
        "abstract": "This paper advocates the use of implicit surface representation in\nautoencoder-based self-supervised 3D representation learning. The most popular\nand accessible 3D representation, i.e., point clouds, involves discrete samples\nof the underlying continuous 3D surface. This discretization process introduces\nsampling variations on the 3D shape, making it challenging to develop\ntransferable knowledge of the true 3D geometry. In the standard autoencoding\nparadigm, the encoder is compelled to encode not only the 3D geometry but also\ninformation on the specific discrete sampling of the 3D shape into the latent\ncode. This is because the point cloud reconstructed by the decoder is\nconsidered unacceptable unless there is a perfect mapping between the original\nand the reconstructed point clouds. This paper introduces the Implicit\nAutoEncoder (IAE), a simple yet effective method that addresses the sampling\nvariation issue by replacing the commonly-used point-cloud decoder with an\nimplicit decoder. The implicit decoder reconstructs a continuous representation\nof the 3D shape, independent of the imperfections in the discrete samples.\nExtensive experiments demonstrate that the proposed IAE achieves\nstate-of-the-art performance across various self-supervised learning\nbenchmarks.",
        "authors": [
            "Siming Yan",
            "Zhenpei Yang",
            "Haoxiang Li",
            "Chen Song",
            "Li Guan",
            "Hao Kang",
            "Gang Hua",
            "Qixing Huang"
        ]
    },
    {
        "title": "Grounded Image Text Matching with Mismatched Relation Reasoning",
        "url": "http://arxiv.org/abs/2308.01236",
        "abstract": "This paper introduces Grounded Image Text Matching with Mismatched Relation\n(GITM-MR), a novel visual-linguistic joint task that evaluates the relation\nunderstanding capabilities of transformer-based pre-trained models. GITM-MR\nrequires a model to first determine if an expression describes an image, then\nlocalize referred objects or ground the mismatched parts of the text. We\nprovide a benchmark for evaluating pre-trained models on this task, with a\nfocus on the challenging settings of limited data and out-of-distribution\nsentence lengths. Our evaluation demonstrates that pre-trained models lack data\nefficiency and length generalization ability. To address this, we propose the\nRelation-sensitive Correspondence Reasoning Network (RCRN), which incorporates\nrelation-aware reasoning via bi-directional message propagation guided by\nlanguage structure. RCRN can be interpreted as a modular program and delivers\nstrong performance in both length generalization and data efficiency.",
        "authors": [
            "Yu Wu",
            "Yana Wei",
            "Haozhe Wang",
            "Yongfei Liu",
            "Sibei Yang",
            "Xuming He"
        ]
    },
    {
        "title": "BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion",
        "url": "http://arxiv.org/abs/2307.10816",
        "abstract": "Recent text-to-image diffusion models have demonstrated an astonishing\ncapacity to generate high-quality images. However, researchers mainly studied\nthe way of synthesizing images with only text prompts. While some works have\nexplored using other modalities as conditions, considerable paired data, e.g.,\nbox/mask-image pairs, and fine-tuning time are required for nurturing models.\nAs such paired data is time-consuming and labor-intensive to acquire and\nrestricted to a closed set, this potentially becomes the bottleneck for\napplications in an open world. This paper focuses on the simplest form of\nuser-provided conditions, e.g., box or scribble. To mitigate the aforementioned\nproblem, we propose a training-free method to control objects and contexts in\nthe synthesized images adhering to the given spatial conditions. Specifically,\nthree spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,\nare designed and seamlessly integrated into the denoising step of diffusion\nmodels, requiring no additional training and massive annotated layout data.\nExtensive experimental results demonstrate that the proposed constraints can\ncontrol what and where to present in the images while retaining the ability of\nDiffusion models to synthesize with high fidelity and diverse concept coverage.\nThe code is publicly available at https://github.com/showlab/BoxDiff.",
        "authors": [
            "Jinheng Xie",
            "Yuexiang Li",
            "Yawen Huang",
            "Haozhe Liu",
            "Wentian Zhang",
            "Yefeng Zheng",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "Generalizing Neural Human Fitting to Unseen Poses With Articulated SE(3) Equivariance",
        "url": "http://arxiv.org/abs/2304.10528",
        "abstract": "We address the problem of fitting a parametric human body model (SMPL) to\npoint cloud data. Optimization-based methods require careful initialization and\nare prone to becoming trapped in local optima. Learning-based methods address\nthis but do not generalize well when the input pose is far from those seen\nduring training. For rigid point clouds, remarkable generalization has been\nachieved by leveraging SE(3)-equivariant networks, but these methods do not\nwork on articulated objects. In this work we extend this idea to human bodies\nand propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for\nSMPL model estimation from point clouds. Specifically, we learn a part\ndetection network by leveraging local SO(3) invariance, and regress shape and\npose using articulated SE(3) shape-invariant and pose-equivariant networks, all\ntrained end-to-end. Our novel pose regression module leverages the\npermutation-equivariant property of self-attention layers to preserve\nrotational equivariance. Experimental results show that ArtEq generalizes to\nposes not seen during training, outperforming state-of-the-art methods by ~44%\nin terms of body reconstruction accuracy, without requiring an optimization\nrefinement step. Furthermore, ArtEq is three orders of magnitude faster during\ninference than prior work and has 97.3% fewer parameters. The code and model\nare available for research purposes at https://arteq.is.tue.mpg.de.",
        "authors": [
            "Haiwen Feng",
            "Peter Kulits",
            "Shichen Liu",
            "Michael J. Black",
            "Victoria Abrevaya"
        ]
    },
    {
        "title": "Theoretical and Numerical Analysis of 3D Reconstruction Using Point and Line Incidences",
        "url": "http://arxiv.org/abs/2303.13593",
        "abstract": "We study the joint image of lines incident to points, meaning the set of\nimage tuples obtained from fixed cameras observing a varying 3D point-line\nincidence. We prove a formula for the number of complex critical points of the\ntriangulation problem that aims to compute a 3D point-line incidence from noisy\nimages. Our formula works for an arbitrary number of images and measures the\nintrinsic difficulty of this triangulation. Additionally, we conduct numerical\nexperiments using homotopy continuation methods, comparing different approaches\nof triangulation of such incidences. In our setup, exploiting the incidence\nrelations gives both a faster point reconstruction and in three views more\naccurate.",
        "authors": [
            "Felix Rydell",
            "Elima Shehu",
            "Angelica Torres"
        ]
    },
    {
        "title": "Leaping Into Memories: Space-Time Deep Feature Synthesis",
        "url": "http://arxiv.org/abs/2303.09941",
        "abstract": "The success of deep learning models has led to their adaptation and adoption\nby prominent video understanding methods. The majority of these approaches\nencode features in a joint space-time modality for which the inner workings and\nlearned representations are difficult to visually interpret. We propose LEArned\nPreconscious Synthesis (LEAPS), an architecture-independent method for\nsynthesizing videos from the internal spatiotemporal representations of models.\nUsing a stimulus video and a target class, we prime a fixed space-time model\nand iteratively optimize a video initialized with random noise. Additional\nregularizers are used to improve the feature diversity of the synthesized\nvideos alongside the cross-frame temporal coherence of motions. We\nquantitatively and qualitatively evaluate the applicability of LEAPS by\ninverting a range of spatiotemporal convolutional and attention-based\narchitectures trained on Kinetics-400, which to the best of our knowledge has\nnot been previously accomplished.",
        "authors": [
            "Alexandros Stergiou",
            "Nikos Deligiannis"
        ]
    },
    {
        "title": "Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation",
        "url": "http://arxiv.org/abs/2308.01194",
        "abstract": "Learning a policy with great generalization to unseen environments remains\nchallenging but critical in visual reinforcement learning. Despite the success\nof augmentation combination in the supervised learning generalization, naively\napplying it to visual RL algorithms may damage the training efficiency,\nsuffering from serve performance degradation. In this paper, we first conduct\nqualitative analysis and illuminate the main causes: (i) high-variance gradient\nmagnitudes and (ii) gradient conflicts existed in various augmentation methods.\nTo alleviate these issues, we propose a general policy gradient optimization\nframework, named Conflict-aware Gradient Agreement Augmentation (CG2A), and\nbetter integrate augmentation combination into visual RL algorithms to address\nthe generalization bias. In particular, CG2A develops a Gradient Agreement\nSolver to adaptively balance the varying gradient magnitudes, and introduces a\nSoft Gradient Surgery strategy to alleviate the gradient conflicts. Extensive\nexperiments demonstrate that CG2A significantly improves the generalization\nperformance and sample efficiency of visual RL algorithms.",
        "authors": [
            "Siao Liu",
            "Zhaoyu Chen",
            "Yang Liu",
            "Yuzheng Wang",
            "Dingkang Yang",
            "Zhile Zhao",
            "Ziqing Zhou",
            "Xie Yi",
            "Wei Li",
            "Wenqiang Zhang",
            "Zhongxue Gan"
        ]
    },
    {
        "title": "Graph Matching with Bi-level Noisy Correspondence",
        "url": "http://arxiv.org/abs/2212.04085",
        "abstract": "In this paper, we study a novel and widely existing problem in graph matching\n(GM), namely, Bi-level Noisy Correspondence (BNC), which refers to node-level\nnoisy correspondence (NNC) and edge-level noisy correspondence (ENC). In brief,\non the one hand, due to the poor recognizability and viewpoint differences\nbetween images, it is inevitable to inaccurately annotate some keypoints with\noffset and confusion, leading to the mismatch between two associated nodes,\ni.e., NNC. On the other hand, the noisy node-to-node correspondence will\nfurther contaminate the edge-to-edge correspondence, thus leading to ENC. For\nthe BNC challenge, we propose a novel method termed Contrastive Matching with\nMomentum Distillation. Specifically, the proposed method is with a robust\nquadratic contrastive loss which enjoys the following merits: i) better\nexploring the node-to-node and edge-to-edge correlations through a GM\ncustomized quadratic contrastive learning paradigm; ii) adaptively penalizing\nthe noisy assignments based on the confidence estimated by the momentum\nteacher. Extensive experiments on three real-world datasets show the robustness\nof our model compared with 12 competitive baselines. The code is available at\nhttps://github.com/XLearning-SCU/2023-ICCV-COMMON.",
        "authors": [
            "Yijie Lin",
            "Mouxing Yang",
            "Jun Yu",
            "Peng Hu",
            "Changqing Zhang",
            "Xi Peng"
        ]
    },
    {
        "title": "InfiniCity: Infinite-Scale City Synthesis",
        "url": "http://arxiv.org/abs/2301.09637",
        "abstract": "Toward infinite-scale 3D city synthesis, we propose a novel framework,\nInfiniCity, which constructs and renders an unconstrainedly large and\n3D-grounded environment from random noises. InfiniCity decomposes the seemingly\nimpractical task into three feasible modules, taking advantage of both 2D and\n3D data. First, an infinite-pixel image synthesis module generates\narbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel\ncompletion module lifts the generated 2D map to 3D octrees. Finally, a\nvoxel-based neural rendering module texturizes the voxels and renders 2D\nimages. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city\nenvironments, and allow flexible and interactive editing from users. We\nquantitatively and qualitatively demonstrate the efficacy of the proposed\nframework. Project page: https://hubert0527.github.io/infinicity/",
        "authors": [
            "Chieh Hubert Lin",
            "Hsin-Ying Lee",
            "Willi Menapace",
            "Menglei Chai",
            "Aliaksandr Siarohin",
            "Ming-Hsuan Yang",
            "Sergey Tulyakov"
        ]
    },
    {
        "title": "OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception",
        "url": "http://arxiv.org/abs/2303.03991",
        "abstract": "Semantic occupancy perception is essential for autonomous driving, as\nautomated vehicles require a fine-grained perception of the 3D urban\nstructures. However, existing relevant benchmarks lack diversity in urban\nscenes, and they only evaluate front-view predictions. Towards a comprehensive\nbenchmarking of surrounding perception algorithms, we propose OpenOccupancy,\nwhich is the first surrounding semantic occupancy perception benchmark. In the\nOpenOccupancy benchmark, we extend the large-scale nuScenes dataset with dense\nsemantic occupancy annotations. Previous annotations rely on LiDAR points\nsuperimposition, where some occupancy labels are missed due to sparse LiDAR\nchannels. To mitigate the problem, we introduce the Augmenting And Purifying\n(AAP) pipeline to ~2x densify the annotations, where ~4000 human hours are\ninvolved in the labeling process. Besides, camera-based, LiDAR-based and\nmulti-modal baselines are established for the OpenOccupancy benchmark.\nFurthermore, considering the complexity of surrounding occupancy perception\nlies in the computational burden of high-resolution 3D predictions, we propose\nthe Cascade Occupancy Network (CONet) to refine the coarse prediction, which\nrelatively enhances the performance by ~30% than the baseline. We hope the\nOpenOccupancy benchmark will boost the development of surrounding occupancy\nperception algorithms.",
        "authors": [
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenbo Xu",
            "Yunpeng Zhang",
            "Yi Wei",
            "Xu Chi",
            "Yun Ye",
            "Dalong Du",
            "Jiwen Lu",
            "Xingang Wang"
        ]
    },
    {
        "title": "Weakly-Supervised Text-Driven Contrastive Learning for Facial Behavior Understanding",
        "url": "http://arxiv.org/abs/2304.00058",
        "abstract": "Contrastive learning has shown promising potential for learning robust\nrepresentations by utilizing unlabeled data. However, constructing effective\npositive-negative pairs for contrastive learning on facial behavior datasets\nremains challenging. This is because such pairs inevitably encode the\nsubject-ID information, and the randomly constructed pairs may push similar\nfacial images away due to the limited number of subjects in facial behavior\ndatasets. To address this issue, we propose to utilize activity descriptions,\ncoarse-grained information provided in some datasets, which can provide\nhigh-level semantic information about the image sequences but is often\nneglected in previous studies. More specifically, we introduce a two-stage\nContrastive Learning with Text-Embeded framework for Facial behavior\nunderstanding (CLEF). The first stage is a weakly-supervised contrastive\nlearning method that learns representations from positive-negative pairs\nconstructed using coarse-grained activity information. The second stage aims to\ntrain the recognition of facial expressions or facial action units by\nmaximizing the similarity between image and the corresponding text label names.\nThe proposed CLEF achieves state-of-the-art performance on three in-the-lab\ndatasets for AU recognition and three in-the-wild datasets for facial\nexpression recognition.",
        "authors": [
            "Xiang Zhang",
            "Taoyue Wang",
            "Xiaotian Li",
            "Huiyuan Yang",
            "Lijun Yin"
        ]
    },
    {
        "title": "PRIOR: Prototype Representation Joint Learning from Medical Images and Reports",
        "url": "http://arxiv.org/abs/2307.12577",
        "abstract": "Contrastive learning based vision-language joint pre-training has emerged as\na successful representation learning strategy. In this paper, we present a\nprototype representation learning framework incorporating both global and local\nalignment between medical images and reports. In contrast to standard global\nmulti-modality alignment methods, we employ a local alignment module for\nfine-grained representation. Furthermore, a cross-modality conditional\nreconstruction module is designed to interchange information across modalities\nin the training phase by reconstructing masked images and reports. For\nreconstructing long reports, a sentence-wise prototype memory bank is\nconstructed, enabling the network to focus on low-level localized visual and\nhigh-level clinical linguistic features. Additionally, a non-auto-regressive\ngeneration paradigm is proposed for reconstructing non-sequential reports.\nExperimental results on five downstream tasks, including supervised\nclassification, zero-shot classification, image-to-text retrieval, semantic\nsegmentation, and object detection, show the proposed method outperforms other\nstate-of-the-art methods across multiple datasets and under different dataset\nsize settings. The code is available at https://github.com/QtacierP/PRIOR.",
        "authors": [
            "Pujin Cheng",
            "Li Lin",
            "Junyan Lyu",
            "Yijin Huang",
            "Wenhan Luo",
            "Xiaoying Tang"
        ]
    },
    {
        "title": "WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis",
        "url": "http://arxiv.org/abs/2303.07543",
        "abstract": "Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score based on class-specific and class-agnostic information.\nSpecifically, the approach utilizes Whitened Linear Discriminant Analysis to\nproject features into two subspaces - the discriminative and residual subspaces\n- for which the in-distribution (ID) classes are maximally separated and\nclosely clustered, respectively. The OOD score is then determined by combining\nthe deviation from the input data to the ID pattern in both subspaces. The\nefficacy of our method, named WDiscOOD, is verified on the large-scale\nImageNet-1k benchmark, with six OOD datasets that cover a variety of\ndistribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that WDiscOOD more effectively detects\nnovel concepts in representation spaces trained with contrastive objectives,\nincluding supervised contrastive loss and multi-modality contrastive loss.",
        "authors": [
            "Yiye Chen",
            "Yunzhi Lin",
            "Ruinian Xu",
            "Patricio A. Vela"
        ]
    },
    {
        "title": "Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching",
        "url": "http://arxiv.org/abs/2308.09346",
        "abstract": "Class prototype construction and matching are core aspects of few-shot action\nrecognition. Previous methods mainly focus on designing spatiotemporal relation\nmodeling modules or complex temporal alignment algorithms. Despite the\npromising results, they ignored the value of class prototype construction and\nmatching, leading to unsatisfactory performance in recognizing similar\ncategories in every task. In this paper, we propose GgHM, a new framework with\nGraph-guided Hybrid Matching. Concretely, we learn task-oriented features by\nthe guidance of a graph neural network during class prototype construction,\noptimizing the intra- and inter-class feature correlation explicitly. Next, we\ndesign a hybrid matching strategy, combining frame-level and tuple-level\nmatching to classify videos with multivariate styles. We additionally propose a\nlearnable dense temporal modeling module to enhance the video feature temporal\nrepresentation to build a more solid foundation for the matching process. GgHM\nshows consistent improvements over other challenging baselines on several\nfew-shot datasets, demonstrating the effectiveness of our method. The code will\nbe publicly available at https://github.com/jiazheng-xing/GgHM.",
        "authors": [
            "Jiazheng Xing",
            "Mengmeng Wang",
            "Yudi Ruan",
            "Bofan Chen",
            "Yaowei Guo",
            "Boyu Mu",
            "Guang Dai",
            "Jingdong Wang",
            "Yong Liu"
        ]
    },
    {
        "title": "Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging",
        "url": "http://arxiv.org/abs/2307.07693",
        "abstract": "We propose a novel neural deformable model (NDM) targeting at the\nreconstruction and modeling of 3D bi-ventricular shape of the heart from 2D\nsparse cardiac magnetic resonance (CMR) imaging data. We model the\nbi-ventricular shape using blended deformable superquadrics, which are\nparameterized by a set of geometric parameter functions and are capable of\ndeforming globally and locally. While global geometric parameter functions and\ndeformations capture gross shape features from visual data, local deformations,\nparameterized as neural diffeomorphic point flows, can be learned to recover\nthe detailed heart shape.Different from iterative optimization methods used in\nconventional deformable model formulations, NDMs can be trained to learn such\ngeometric parameter functions, global and local deformations from a shape\ndistribution manifold. Our NDM can learn to densify a sparse cardiac point\ncloud with arbitrary scales and generate high-quality triangular meshes\nautomatically. It also enables the implicit learning of dense correspondences\namong different heart shape instances for accurate cardiac shape registration.\nFurthermore, the parameters of NDM are intuitive, and can be used by a\nphysician without sophisticated post-processing. Experimental results on a\nlarge CMR dataset demonstrate the improved performance of NDM over conventional\nmethods.",
        "authors": [
            "Meng Ye",
            "Dong Yang",
            "Mikael Kanski",
            "Leon Axel",
            "Dimitris Metaxas"
        ]
    },
    {
        "title": "Nonrigid Object Contact Estimation With Regional Unwrapping Transformer",
        "url": "http://arxiv.org/abs/2308.14074",
        "abstract": "Acquiring contact patterns between hands and nonrigid objects is a common\nconcern in the vision and robotics community. However, existing learning-based\nmethods focus more on contact with rigid ones from monocular images. When\nadopting them for nonrigid contact, a major problem is that the existing\ncontact representation is restricted by the geometry of the object.\nConsequently, contact neighborhoods are stored in an unordered manner and\ncontact features are difficult to align with image cues. At the core of our\napproach lies a novel hand-object contact representation called RUPs (Region\nUnwrapping Profiles), which unwrap the roughly estimated hand-object surfaces\nas multiple high-resolution 2D regional profiles. The region grouping strategy\nis consistent with the hand kinematic bone division because they are the\nprimitive initiators for a composite contact pattern. Based on this\nrepresentation, our Regional Unwrapping Transformer (RUFormer) learns the\ncorrelation priors across regions from monocular inputs and predicts\ncorresponding contact and deformed transformations. Our experiments demonstrate\nthat the proposed framework can robustly estimate the deformed degrees and\ndeformed transformations, which makes it suitable for both nonrigid and rigid\ncontact.",
        "authors": [
            "Wei Xie",
            "Zimeng Zhao",
            "Shiying Li",
            "Binghui Zuo",
            "Yangang Wang"
        ]
    },
    {
        "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods",
        "url": "http://arxiv.org/abs/2308.06248",
        "abstract": "The field of explainable artificial intelligence (XAI) aims to uncover the\ninner workings of complex deep neural models. While being crucial for\nsafety-critical domains, XAI inherently lacks ground-truth explanations, making\nits automatic evaluation an unsolved problem. We address this challenge by\nproposing a novel synthetic vision dataset, named FunnyBirds, and accompanying\nautomatic evaluation protocols. Our dataset allows performing semantically\nmeaningful image interventions, e.g., removing individual object parts, which\nhas three important implications. First, it enables analyzing explanations on a\npart level, which is closer to human comprehension than existing methods that\nevaluate on a pixel level. Second, by comparing the model output for inputs\nwith removed parts, we can estimate ground-truth part importances that should\nbe reflected in the explanations. Third, by mapping individual explanations\ninto a common space of part importances, we can analyze a variety of different\nexplanation types in a single common framework. Using our tools, we report\nresults for 24 different combinations of neural models and XAI methods,\ndemonstrating the strengths and weaknesses of the assessed methods in a fully\nautomatic and systematic manner.",
        "authors": [
            "Robin Hesse",
            "Simone Schaub-Meyer",
            "Stefan Roth"
        ]
    },
    {
        "title": "Deformable Neural Radiance Fields using RGB and Event Cameras",
        "url": "http://arxiv.org/abs/2309.08416",
        "abstract": "Modeling Neural Radiance Fields for fast-moving deformable objects from\nvisual data alone is a challenging problem. A major issue arises due to the\nhigh deformation and low acquisition rates. To address this problem, we propose\nto use event cameras that offer very fast acquisition of visual change in an\nasynchronous manner. In this work, we develop a novel method to model the\ndeformable neural radiance fields using RGB and event cameras. The proposed\nmethod uses the asynchronous stream of events and calibrated sparse RGB frames.\nIn our setup, the camera pose at the individual events required to integrate\nthem into the radiance fields remains unknown. Our method jointly optimizes\nthese poses and the radiance field. This happens efficiently by leveraging the\ncollection of events at once and actively sampling the events during learning.\nExperiments conducted on both realistically rendered graphics and real-world\ndatasets demonstrate a significant benefit of the proposed method over the\nstate-of-the-art and the compared baseline.\n  This shows a promising direction for modeling deformable neural radiance\nfields in real-world dynamic scenes.",
        "authors": [
            "Qi Ma",
            "Danda Pani Paudel",
            "Ajad Chhatkuli",
            "Luc Van Gool"
        ]
    },
    {
        "title": "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction",
        "url": "http://arxiv.org/abs/2211.14304",
        "abstract": "Stochastic human motion prediction (HMP) has generally been tackled with\ngenerative adversarial networks and variational autoencoders. Most prior works\naim at predicting highly diverse movements in terms of the skeleton joints'\ndispersion. This has led to methods predicting fast and motion-divergent\nmovements, which are often unrealistic and incoherent with past motion. Such\nmethods also neglect contexts that need to anticipate diverse low-range\nbehaviors, or actions, with subtle joint displacements. To address these\nissues, we present BeLFusion, a model that, for the first time, leverages\nlatent diffusion models in HMP to sample from a latent space where behavior is\ndisentangled from pose and motion. As a result, diversity is encouraged from a\nbehavioral perspective. Thanks to our behavior coupler's ability to transfer\nsampled behavior to ongoing motion, BeLFusion's predictions display a variety\nof behaviors that are significantly more realistic than the state of the art.\nTo support it, we introduce two metrics, the Area of the Cumulative Motion\nDistribution, and the Average Pairwise Distance Error, which are correlated to\nour definition of realism according to a qualitative study with 126\nparticipants. Finally, we prove BeLFusion's generalization power in a new\ncross-dataset scenario for stochastic HMP.",
        "authors": [
            "German Barquero",
            "Sergio Escalera",
            "Cristina Palmero"
        ]
    },
    {
        "title": "Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation",
        "url": "http://arxiv.org/abs/2303.11516",
        "abstract": "Most modern image-based 6D object pose estimation methods learn to predict\n2D-3D correspondences, from which the pose can be obtained using a PnP solver.\nBecause of the non-differentiable nature of common PnP solvers, these methods\nare supervised via the individual correspondences. To address this, several\nmethods have designed differentiable PnP strategies, thus imposing supervision\non the pose obtained after the PnP step. Here, we argue that this conflicts\nwith the averaging nature of the PnP problem, leading to gradients that may\nencourage the network to degrade the accuracy of individual correspondences. To\naddress this, we derive a loss function that exploits the ground truth pose\nbefore solving the PnP problem. Specifically, we linearize the PnP solver\naround the ground-truth pose and compute the covariance of the resulting pose\ndistribution. We then define our loss based on the diagonal covariance\nelements, which entails considering the final pose estimate yet not suffering\nfrom the PnP averaging issue. Our experiments show that our loss consistently\nimproves the pose estimation accuracy for both dense and sparse correspondence\nbased methods, achieving state-of-the-art results on both Linemod-Occluded and\nYCB-Video.",
        "authors": [
            "Fulin Liu",
            "Yinlin Hu",
            "Mathieu Salzmann"
        ]
    },
    {
        "title": "RLSAC: Reinforcement Learning Enhanced Sample Consensus for End-to-End Robust Estimation",
        "url": "http://arxiv.org/abs/2308.05318",
        "abstract": "Robust estimation is a crucial and still challenging task, which involves\nestimating model parameters in noisy environments. Although conventional\nsampling consensus-based algorithms sample several times to achieve robustness,\nthese algorithms cannot use data features and historical information\neffectively. In this paper, we propose RLSAC, a novel Reinforcement Learning\nenhanced SAmple Consensus framework for end-to-end robust estimation. RLSAC\nemploys a graph neural network to utilize both data and memory features to\nguide exploring directions for sampling the next minimum set. The feedback of\ndownstream tasks serves as the reward for unsupervised training. Therefore,\nRLSAC can avoid differentiating to learn the features and the feedback of\ndownstream tasks for end-to-end robust estimation. In addition, RLSAC\nintegrates a state transition module that encodes both data and memory\nfeatures. Our experimental results demonstrate that RLSAC can learn from\nfeatures to gradually explore a better hypothesis. Through analysis, it is\napparent that RLSAC can be easily transferred to other sampling consensus-based\nrobust estimation tasks. To the best of our knowledge, RLSAC is also the first\nmethod that uses reinforcement learning to sample consensus for end-to-end\nrobust estimation. We release our codes at https://github.com/IRMVLab/RLSAC.",
        "authors": [
            "Chang Nie",
            "Guangming Wang",
            "Zhe Liu",
            "Luca Cavalli",
            "Marc Pollefeys",
            "Hesheng Wang"
        ]
    },
    {
        "title": "CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning",
        "url": "http://arxiv.org/abs/2303.03323",
        "abstract": "Multimodal contrastive pretraining has been used to train multimodal\nrepresentation models, such as CLIP, on large amounts of paired image-text\ndata. However, previous studies have revealed that such models are vulnerable\nto backdoor attacks. Specifically, when trained on backdoored examples, CLIP\nlearns spurious correlations between the embedded backdoor trigger and the\ntarget label, aligning their representations in the joint embedding space.\nInjecting even a small number of poisoned examples, such as 75 examples in 3\nmillion pretraining data, can significantly manipulate the model's behavior,\nmaking it difficult to detect or unlearn such correlations. To address this\nissue, we propose CleanCLIP, a finetuning framework that weakens the learned\nspurious associations introduced by backdoor attacks by independently\nre-aligning the representations for individual modalities. We demonstrate that\nunsupervised finetuning using a combination of multimodal contrastive and\nunimodal self-supervised objectives for individual modalities can significantly\nreduce the impact of the backdoor attack. Additionally, we show that supervised\nfinetuning on task-specific labeled image data removes the backdoor trigger\nfrom the CLIP vision encoder. We show empirically that CleanCLIP maintains\nmodel performance on benign examples while erasing a range of backdoor attacks\non multimodal contrastive learning. The code and checkpoints are available at\nhttps://github.com/nishadsinghi/CleanCLIP.",
        "authors": [
            "Hritik Bansal",
            "Nishad Singhi",
            "Yu Yang",
            "Fan Yin",
            "Aditya Grover",
            "Kai-Wei Chang"
        ]
    },
    {
        "title": "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild",
        "url": "http://arxiv.org/abs/2211.12352",
        "abstract": "Most in-the-wild images are stored in Low Dynamic Range (LDR) form, serving\nas a partial observation of the High Dynamic Range (HDR) visual world. Despite\nlimited dynamic range, these LDR images are often captured with different\nexposures, implicitly containing information about the underlying HDR image\ndistribution. Inspired by this intuition, in this work we present, to the best\nof our knowledge, the first method for learning a generative model of HDR\nimages from in-the-wild LDR image collections in a fully unsupervised manner.\nThe key idea is to train a generative adversarial network (GAN) to generate HDR\nimages which, when projected to LDR under various exposures, are\nindistinguishable from real LDR images. The projection from HDR to LDR is\nachieved via a camera model that captures the stochasticity in exposure and\ncamera response function. Experiments show that our method GlowGAN can\nsynthesize photorealistic HDR images in many challenging cases such as\nlandscapes, lightning, or windows, where previous supervised generative models\nproduce overexposed images. We further demonstrate the new application of\nunsupervised inverse tone mapping (ITM) enabled by GlowGAN. Our ITM method does\nnot need HDR images or paired multi-exposure images for training, yet it\nreconstructs more plausible information for overexposed regions than\nstate-of-the-art supervised learning models trained on such data.",
        "authors": [
            "Chao Wang",
            "Ana Serrano",
            "Xingang Pan",
            "Bin Chen",
            "Hans-Peter Seidel",
            "Christian Theobalt",
            "Karol Myszkowski",
            "Thomas Leimkuehler"
        ]
    },
    {
        "title": "Cumulative Spatial Knowledge Distillation for Vision Transformers",
        "url": "http://arxiv.org/abs/2307.08500",
        "abstract": "Distilling knowledge from convolutional neural networks (CNNs) is a\ndouble-edged sword for vision transformers (ViTs). It boosts the performance\nsince the image-friendly local-inductive bias of CNN helps ViT learn faster and\nbetter, but leading to two problems: (1) Network designs of CNN and ViT are\ncompletely different, which leads to different semantic levels of intermediate\nfeatures, making spatial-wise knowledge transfer methods (e.g., feature\nmimicking) inefficient. (2) Distilling knowledge from CNN limits the network\nconvergence in the later training period since ViT's capability of integrating\nglobal information is suppressed by CNN's local-inductive-bias supervision. To\nthis end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKD\ndistills spatial-wise knowledge to all patch tokens of ViT from the\ncorresponding spatial responses of CNN, without introducing intermediate\nfeatures. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF)\nmodule, which introduces the global response of CNN and increasingly emphasizes\nits importance during the training. Applying CKF leverages CNN's local\ninductive bias in the early training period and gives full play to ViT's global\ncapability in the later one. Extensive experiments and analysis on ImageNet-1k\nand downstream datasets demonstrate the superiority of our CSKD. Code will be\npublicly available.",
        "authors": [
            "Borui Zhao",
            "Renjie Song",
            "Jiajun Liang"
        ]
    },
    {
        "title": "Less is More: Focus Attention for Efficient DETR",
        "url": "http://arxiv.org/abs/2307.12612",
        "abstract": "DETR-like models have significantly boosted the performance of detectors and\neven outperformed classical convolutional models. However, all tokens are\ntreated equally without discrimination brings a redundant computational burden\nin the traditional encoder structure. The recent sparsification strategies\nexploit a subset of informative tokens to reduce attention complexity\nmaintaining performance through the sparse encoder. But these methods tend to\nrely on unreliable model statistics. Moreover, simply reducing the token\npopulation hinders the detection performance to a large extent, limiting the\napplication of these sparse models. We propose Focus-DETR, which focuses\nattention on more informative tokens for a better trade-off between computation\nefficiency and model accuracy. Specifically, we reconstruct the encoder with\ndual attention, which includes a token scoring mechanism that considers both\nlocalization and category semantic information of the objects from multi-scale\nfeature maps. We efficiently abandon the background queries and enhance the\nsemantic interaction of the fine-grained object queries based on the scores.\nCompared with the state-of-the-art sparse DETR-like detectors under the same\nsetting, our Focus-DETR gets comparable complexity while achieving 50.4AP\n(+2.2) on COCO. The code is available at\nhttps://github.com/huawei-noah/noah-research/tree/master/Focus-DETR and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR.",
        "authors": [
            "Dehua Zheng",
            "Wenhui Dong",
            "Hailin Hu",
            "Xinghao Chen",
            "Yunhe Wang"
        ]
    },
    {
        "title": "Efficient Controllable Multi-Task Architectures",
        "url": "http://arxiv.org/abs/2308.11744",
        "abstract": "We aim to train a multi-task model such that users can adjust the desired\ncompute budget and relative importance of task performances after deployment,\nwithout retraining. This enables optimizing performance for dynamically varying\nuser needs, without heavy computational overhead to train and save models for\nvarious scenarios. To this end, we propose a multi-task model consisting of a\nshared encoder and task-specific decoders where both encoder and decoder\nchannel widths are slimmable. Our key idea is to control the task importance by\nvarying the capacities of task-specific decoders, while controlling the total\ncomputational cost by jointly adjusting the encoder capacity. This improves\noverall accuracy by allowing a stronger encoder for a given budget, increases\ncontrol over computational cost, and delivers high-quality slimmed\nsub-architectures based on user's constraints. Our training strategy involves a\nnovel 'Configuration-Invariant Knowledge Distillation' loss that enforces\nbackbone representations to be invariant under different runtime width\nconfigurations to enhance accuracy. Further, we present a simple but effective\nsearch algorithm that translates user constraints to runtime width\nconfigurations of both the shared encoder and task decoders, for sampling the\nsub-architectures. The key rule for the search algorithm is to provide a larger\ncomputational budget to the higher preferred task decoder, while searching a\nshared encoder configuration that enhances the overall MTL performance. Various\nexperiments on three multi-task benchmarks (PASCALContext, NYUDv2, and\nCIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of\nour approach. For example, our method shows a higher controllability by ~33.5%\nin the NYUD-v2 dataset over prior methods, while incurring much less compute\ncost.",
        "authors": [
            "Abhishek Aich",
            "Samuel Schulter",
            "Amit K. Roy-Chowdhury",
            "Manmohan Chandraker",
            "Yumin Suh"
        ]
    },
    {
        "title": "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation",
        "url": "http://arxiv.org/abs/2304.04269",
        "abstract": "Controllable human image generation (HIG) has numerous real-life\napplications. State-of-the-art solutions, such as ControlNet and T2I-Adapter,\nintroduce an additional learnable branch on top of the frozen pre-trained\nstable diffusion (SD) model, which can enforce various conditions, including\nskeleton guidance of HIG. While such a plug-and-play approach is appealing, the\ninevitable and uncertain conflicts between the original images produced from\nthe frozen SD branch and the given condition incur significant challenges for\nthe learnable branch, which essentially conducts image feature editing for\ncondition enforcement. In this work, we propose a native skeleton-guided\ndiffusion model for controllable HIG called HumanSD. Instead of performing\nimage editing with dual-branch diffusion, we fine-tune the original SD model\nusing a novel heatmap-guided denoising loss. This strategy effectively and\nefficiently strengthens the given skeleton condition during model training\nwhile mitigating the catastrophic forgetting effects. HumanSD is fine-tuned on\nthe assembly of three large-scale human-centric datasets with text-image-pose\ninformation, two of which are established in this work. As shown in Figure 1,\nHumanSD outperforms ControlNet in terms of accurate pose control and image\nquality, particularly when the given skeleton guidance is sophisticated.",
        "authors": [
            "Xuan Ju",
            "Ailing Zeng",
            "Chenchen Zhao",
            "Jianan Wang",
            "Lei Zhang",
            "Qiang Xu"
        ]
    },
    {
        "title": "Alignment-free HDR Deghosting with Semantics Consistent Transformer",
        "url": "http://arxiv.org/abs/2305.18135",
        "abstract": "High dynamic range (HDR) imaging aims to retrieve information from multiple\nlow-dynamic range inputs to generate realistic output. The essence is to\nleverage the contextual information, including both dynamic and static\nsemantics, for better image generation. Existing methods often focus on the\nspatial misalignment across input frames caused by the foreground and/or camera\nmotion. However, there is no research on jointly leveraging the dynamic and\nstatic context in a simultaneous manner. To delve into this problem, we propose\na novel alignment-free network with a Semantics Consistent Transformer (SCTNet)\nwith both spatial and channel attention modules in the network. The spatial\nattention aims to deal with the intra-image correlation to model the dynamic\nmotion, while the channel attention enables the inter-image intertwining to\nenhance the semantic consistency across frames. Aside from this, we introduce a\nnovel realistic HDR dataset with more variations in foreground objects,\nenvironmental factors, and larger motions. Extensive comparisons on both\nconventional datasets and ours validate the effectiveness of our method,\nachieving the best trade-off on the performance and the computational cost.",
        "authors": [
            "Steven Tel",
            "Zongwei Wu",
            "Yulun Zhang",
            "Barth\u00e9l\u00e9my Heyrman",
            "C\u00e9dric Demonceaux",
            "Radu Timofte",
            "Dominique Ginhac"
        ]
    },
    {
        "title": "Semantic-Aware Implicit Template Learning via Part Deformation Consistency",
        "url": "http://arxiv.org/abs/2308.11916",
        "abstract": "Learning implicit templates as neural fields has recently shown impressive\nperformance in unsupervised shape correspondence. Despite the success, we\nobserve current approaches, which solely rely on geometric information, often\nlearn suboptimal deformation across generic object shapes, which have high\nstructural variability. In this paper, we highlight the importance of part\ndeformation consistency and propose a semantic-aware implicit template learning\nframework to enable semantically plausible deformation. By leveraging semantic\nprior from a self-supervised feature extractor, we suggest local conditioning\nwith novel semantic-aware deformation code and deformation consistency\nregularizations regarding part deformation, global deformation, and global\nscaling. Our extensive experiments demonstrate the superiority of the proposed\nmethod over baselines in various tasks: keypoint transfer, part label transfer,\nand texture transfer. More interestingly, our framework shows a larger\nperformance gain under more challenging settings. We also provide qualitative\nanalyses to validate the effectiveness of semantic-aware deformation. The code\nis available at https://github.com/mlvlab/PDC.",
        "authors": [
            "Sihyeon Kim",
            "Minseok Joo",
            "Jaewon Lee",
            "Juyeon Ko",
            "Juhan Cha",
            "Hyunwoo J. Kim"
        ]
    },
    {
        "title": "Multi3DRefer: Grounding Text Description to Multiple 3D Objects",
        "url": "http://arxiv.org/abs/2309.05251",
        "abstract": "We introduce the task of localizing a flexible number of objects in\nreal-world 3D scenes using natural language descriptions. Existing 3D visual\ngrounding tasks focus on localizing a unique object given a text description.\nHowever, such a strict setting is unnatural as localizing potentially multiple\nobjects is a common need in real-world scenarios and robotic tasks (e.g.,\nvisual navigation and object rearrangement). To address this setting we propose\nMulti3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains\n61926 descriptions of 11609 objects, where zero, single or multiple target\nobjects are referenced by each description. We also introduce a new evaluation\nmetric and benchmark methods from prior work to enable further investigation of\nmulti-modal 3D scene understanding. Furthermore, we develop a better baseline\nleveraging 2D features from CLIP by rendering object proposals online with\ncontrastive learning, which outperforms the state of the art on the ScanRefer\nbenchmark.",
        "authors": [
            "Yiming Zhang",
            "ZeMing Gong",
            "Angel X. Chang"
        ]
    },
    {
        "title": "Examining Autoexposure for Challenging Scenes",
        "url": "http://arxiv.org/abs/2309.04542",
        "abstract": "Autoexposure (AE) is a critical step applied by camera systems to ensure\nproperly exposed images. While current AE algorithms are effective in well-lit\nenvironments with constant illumination, these algorithms still struggle in\nenvironments with bright light sources or scenes with abrupt changes in\nlighting. A significant hurdle in developing new AE algorithms for challenging\nenvironments, especially those with time-varying lighting, is the lack of\nsuitable image datasets. To address this issue, we have captured a new 4D\nexposure dataset that provides a large solution space (i.e., shutter speed\nrange from (1/500 to 15 seconds) over a temporal sequence with moving objects,\nbright lights, and varying lighting. In addition, we have designed a software\nplatform to allow AE algorithms to be used in a plug-and-play manner with the\ndataset. Our dataset and associate platform enable repeatable evaluation of\ndifferent AE algorithms and provide a much-needed starting point to develop\nbetter AE methods. We examine several existing AE strategies using our dataset\nand show that most users prefer a simple saliency method for challenging\nlighting conditions.",
        "authors": [
            "SaiKiran Tedla",
            "Beixuan Yang",
            "Michael S. Brown"
        ]
    },
    {
        "title": "DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment",
        "url": "http://arxiv.org/abs/2308.11206",
        "abstract": "Cross-modal garment synthesis and manipulation will significantly benefit the\nway fashion designers generate garments and modify their designs via flexible\nlinguistic interfaces.Current approaches follow the general text-to-image\nparadigm and mine cross-modal relations via simple cross-attention modules,\nneglecting the structural correspondence between visual and textual\nrepresentations in the fashion design domain. In this work, we instead\nintroduce DiffCloth, a diffusion-based pipeline for cross-modal garment\nsynthesis and manipulation, which empowers diffusion models with flexible\ncompositionality in the fashion domain by structurally aligning the cross-modal\nsemantics. Specifically, we formulate the part-level cross-modal alignment as a\nbipartite matching problem between the linguistic Attribute-Phrases (AP) and\nthe visual garment parts which are obtained via constituency parsing and\nsemantic segmentation, respectively. To mitigate the issue of attribute\nconfusion, we further propose a semantic-bundled cross-attention to preserve\nthe spatial structure similarities between the attention maps of attribute\nadjectives and part nouns in each AP. Moreover, DiffCloth allows for\nmanipulation of the generated results by simply replacing APs in the text\nprompts. The manipulation-irrelevant regions are recognized by blended masks\nobtained from the bundled attention maps of the APs and kept unchanged.\nExtensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth\nboth yields state-of-the-art garment synthesis results by leveraging the\ninherent structural information and supports flexible manipulation with region\nconsistency.",
        "authors": [
            "Xujie Zhang",
            "Binbin Yang",
            "Michael C. Kampffmeyer",
            "Wenqing Zhang",
            "Shiyue Zhang",
            "Guansong Lu",
            "Liang Lin",
            "Hang Xu",
            "Xiaodan Liang"
        ]
    },
    {
        "title": "Improved Visual Fine-tuning with Natural Language Supervision",
        "url": "http://arxiv.org/abs/2304.01489",
        "abstract": "Fine-tuning a visual pre-trained model can leverage the semantic information\nfrom large-scale pre-training data and mitigate the over-fitting problem on\ndownstream vision tasks with limited training examples. While the problem of\ncatastrophic forgetting in pre-trained backbone has been extensively studied\nfor fine-tuning, its potential bias from the corresponding pre-training task\nand data, attracts less attention. In this work, we investigate this problem by\ndemonstrating that the obtained classifier after fine-tuning will be close to\nthat induced by the pre-trained model. To reduce the bias in the classifier\neffectively, we introduce a reference distribution obtained from a fixed text\nclassifier, which can help regularize the learned vision classifier. The\nproposed method, Text Supervised fine-tuning (TeS), is evaluated with diverse\npre-trained vision models including ResNet and ViT, and text encoders including\nBERT and CLIP, on 11 downstream tasks. The consistent improvement with a clear\nmargin over distinct scenarios confirms the effectiveness of our proposal. Code\nis available at \\url{https://github.com/idstcv/TeS}.",
        "authors": [
            "Junyang Wang",
            "Yuanhong Xu",
            "Juhua Hu",
            "Ming Yan",
            "Jitao Sang",
            "Qi Qian"
        ]
    },
    {
        "title": "Person Re-Identification without Identification via Event anonymization",
        "url": "http://arxiv.org/abs/2308.04402",
        "abstract": "Wide-scale use of visual surveillance in public spaces puts individual\nprivacy at stake while increasing resource consumption (energy, bandwidth, and\ncomputation). Neuromorphic vision sensors (event-cameras) have been recently\nconsidered a valid solution to the privacy issue because they do not capture\ndetailed RGB visual information of the subjects in the scene. However, recent\ndeep learning architectures have been able to reconstruct images from event\ncameras with high fidelity, reintroducing a potential threat to privacy for\nevent-based vision applications. In this paper, we aim to anonymize\nevent-streams to protect the identity of human subjects against such image\nreconstruction attacks. To achieve this, we propose an end-to-end network\narchitecture jointly optimized for the twofold objective of preserving privacy\nand performing a downstream task such as person ReId. Our network learns to\nscramble events, enforcing the degradation of images recovered from the privacy\nattacker. In this work, we also bring to the community the first ever\nevent-based person ReId dataset gathered to evaluate the performance of our\napproach. We validate our approach with extensive experiments and report\nresults on the synthetic event data simulated from the publicly available\nSoftBio dataset and our proposed Event-ReId dataset.",
        "authors": [
            "Shafiq Ahmad",
            "Pietro Morerio",
            "Alessio Del Bue"
        ]
    },
    {
        "title": "Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning",
        "url": "http://arxiv.org/abs/2308.09534",
        "abstract": "The past few years have witnessed the immense success of object detection,\nwhile current excellent detectors struggle on tackling size-limited instances.\nConcretely, the well-known challenge of low overlaps between the priors and\nobject regions leads to a constrained sample pool for optimization, and the\npaucity of discriminative information further aggravates the recognition. To\nalleviate the aforementioned issues, we propose CFINet, a two-stage framework\ntailored for small object detection based on the Coarse-to-fine pipeline and\nFeature Imitation learning. Firstly, we introduce Coarse-to-fine RPN (CRPN) to\nensure sufficient and high-quality proposals for small objects through the\ndynamic anchor selection strategy and cascade regression. Then, we equip the\nconventional detection head with a Feature Imitation (FI) branch to facilitate\nthe region representations of size-limited instances that perplex the model in\nan imitation manner. Moreover, an auxiliary imitation loss following supervised\ncontrastive learning paradigm is devised to optimize this branch. When\nintegrated with Faster RCNN, CFINet achieves state-of-the-art performance on\nthe large-scale small object detection benchmarks, SODA-D and SODA-A,\nunderscoring its superiority over baseline detector and other mainstream\ndetection approaches.",
        "authors": [
            "Xiang Yuan",
            "Gong Cheng",
            "Kebing Yan",
            "Qinghua Zeng",
            "Junwei Han"
        ]
    },
    {
        "title": "Anomaly Detection Under Distribution Shift",
        "url": "http://arxiv.org/abs/2303.13845",
        "abstract": "Anomaly detection (AD) is a crucial machine learning task that aims to learn\npatterns from a set of normal training samples to identify abnormal samples in\ntest data. Most existing AD studies assume that the training and test data are\ndrawn from the same data distribution, but the test data can have large\ndistribution shifts arising in many real-world applications due to different\nnatural variations such as new lighting conditions, object poses, or background\nappearances, rendering existing AD methods ineffective in such cases. In this\npaper, we consider the problem of anomaly detection under distribution shift\nand establish performance benchmarks on four widely-used AD and\nout-of-distribution (OOD) generalization datasets. We demonstrate that simple\nadaptation of state-of-the-art OOD generalization methods to AD settings fails\nto work effectively due to the lack of labeled anomaly data. We further\nintroduce a novel robust AD approach to diverse distribution shifts by\nminimizing the distribution gap between in-distribution and OOD normal samples\nin both the training and inference stages in an unsupervised way. Our extensive\nempirical results on the four datasets show that our approach substantially\noutperforms state-of-the-art AD methods and OOD generalization methods on data\nwith various distribution shifts, while maintaining the detection accuracy on\nin-distribution data. Code and data are available at\nhttps://github.com/mala-lab/ADShift.",
        "authors": [
            "Tri Cao",
            "Jiawen Zhu",
            "Guansong Pang"
        ]
    },
    {
        "title": "Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery",
        "url": "http://arxiv.org/abs/2308.15081",
        "abstract": "Positive-unlabeled learning (PU learning) in hyperspectral remote sensing\nimagery (HSI) is aimed at learning a binary classifier from positive and\nunlabeled data, which has broad prospects in various earth vision applications.\nHowever, when PU learning meets limited labeled HSI, the unlabeled data may\ndominate the optimization process, which makes the neural networks overfit the\nunlabeled data. In this paper, a Taylor variational loss is proposed for HSI PU\nlearning, which reduces the weight of the gradient of the unlabeled data by\nTaylor series expansion to enable the network to find a balance between\noverfitting and underfitting. In addition, the self-calibrated optimization\nstrategy is designed to stabilize the training process. Experiments on 7\nbenchmark datasets (21 tasks in total) validate the effectiveness of the\nproposed method. Code is at: https://github.com/Hengwei-Zhao96/T-HOneCls.",
        "authors": [
            "Hengwei Zhao",
            "Xinyu Wang",
            "Jingtao Li",
            "Yanfei Zhong"
        ]
    },
    {
        "title": "Self-Feedback DETR for Temporal Action Detection",
        "url": "http://arxiv.org/abs/2308.10570",
        "abstract": "Temporal Action Detection (TAD) is challenging but fundamental for real-world\nvideo applications. Recently, DETR-based models have been devised for TAD but\nhave not performed well yet. In this paper, we point out the problem in the\nself-attention of DETR for TAD; the attention modules focus on a few key\nelements, called temporal collapse problem. It degrades the capability of the\nencoder and decoder since their self-attention modules play no role. To solve\nthe problem, we propose a novel framework, Self-DETR, which utilizes\ncross-attention maps of the decoder to reactivate self-attention modules. We\nrecover the relationship between encoder features by simple matrix\nmultiplication of the cross-attention map and its transpose. Likewise, we also\nget the information within decoder queries. By guiding collapsed self-attention\nmaps with the guidance map calculated, we settle down the temporal collapse of\nself-attention modules in the encoder and decoder. Our extensive experiments\ndemonstrate that Self-DETR resolves the temporal collapse problem by keeping\nhigh diversity of attention over all layers.",
        "authors": [
            "Jihwan Kim",
            "Miso Lee",
            "Jae-Pil Heo"
        ]
    },
    {
        "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing",
        "url": "http://arxiv.org/abs/2308.09592",
        "abstract": "Diffusion-based methods can generate realistic images and videos, but they\nstruggle to edit existing objects in a video while preserving their appearance\nover time. This prevents diffusion models from being applied to natural video\nediting in practical scenarios. In this paper, we tackle this problem by\nintroducing temporal dependency to existing text-driven diffusion models, which\nallows them to generate consistent appearance for the edited objects.\nSpecifically, we develop a novel inter-frame propagation mechanism for\ndiffusion video editing, which leverages the concept of layered representations\nto propagate the appearance information from one frame to the next. We then\nbuild up a text-driven video editing framework based on this mechanism, namely\nStableVideo, which can achieve consistency-aware video editing. Extensive\nexperiments demonstrate the strong editing capability of our approach. Compared\nwith state-of-the-art video editing methods, our approach shows superior\nqualitative and quantitative results. Our code is available at\n\\href{https://github.com/rese1f/StableVideo}{this https URL}.",
        "authors": [
            "Wenhao Chai",
            "Xun Guo",
            "Gaoang Wang",
            "Yan Lu"
        ]
    },
    {
        "title": "Multi-Label Knowledge Distillation",
        "url": "http://arxiv.org/abs/2308.06453",
        "abstract": "Existing knowledge distillation methods typically work by imparting the\nknowledge of output logits or intermediate feature maps from the teacher\nnetwork to the student network, which is very successful in multi-class\nsingle-label learning. However, these methods can hardly be extended to the\nmulti-label learning scenario, where each instance is associated with multiple\nsemantic labels, because the prediction probabilities do not sum to one and\nfeature maps of the whole example may ignore minor classes in such a scenario.\nIn this paper, we propose a novel multi-label knowledge distillation method. On\none hand, it exploits the informative semantic knowledge from the logits by\ndividing the multi-label learning problem into a set of binary classification\nproblems; on the other hand, it enhances the distinctiveness of the learned\nfeature representations by leveraging the structural information of label-wise\nembeddings. Experimental results on multiple benchmark datasets validate that\nthe proposed method can avoid knowledge counteraction among labels, thus\nachieving superior performance against diverse comparing methods. Our code is\navailable at: https://github.com/penghui-yang/L2D",
        "authors": [
            "Penghui Yang",
            "Ming-Kun Xie",
            "Chen-Chen Zong",
            "Lei Feng",
            "Gang Niu",
            "Masashi Sugiyama",
            "Sheng-Jun Huang"
        ]
    },
    {
        "title": "Towards Geospatial Foundation Models via Continual Pretraining",
        "url": "http://arxiv.org/abs/2302.04476",
        "abstract": "Geospatial technologies are becoming increasingly essential in our world for\na wide range of applications, including agriculture, urban planning, and\ndisaster response. To help improve the applicability and performance of deep\nlearning models on these geospatial tasks, various works have begun\ninvestigating foundation models for this domain. Researchers have explored two\nprominent approaches for introducing such models in geospatial applications,\nbut both have drawbacks in terms of limited performance benefit or prohibitive\ntraining cost. Therefore, in this work, we propose a novel paradigm for\nbuilding highly effective geospatial foundation models with minimal resource\ncost and carbon impact. We first construct a compact yet diverse dataset from\nmultiple sources to promote feature diversity, which we term GeoPile. Then, we\ninvestigate the potential of continual pretraining from large-scale\nImageNet-22k models and propose a multi-objective continual pretraining\nparadigm, which leverages the strong representations of ImageNet while\nsimultaneously providing the freedom to learn valuable in-domain features. Our\napproach outperforms previous state-of-the-art geospatial pretraining methods\nin an extensive evaluation on seven downstream datasets covering various tasks\nsuch as change detection, classification, multi-label classification, semantic\nsegmentation, and super-resolution.",
        "authors": [
            "Matias Mendieta",
            "Boran Han",
            "Xingjian Shi",
            "Yi Zhu",
            "Chen Chen"
        ]
    },
    {
        "title": "ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis",
        "url": "http://arxiv.org/abs/2308.13324",
        "abstract": "Whole slide image (WSI) analysis has become increasingly important in the\nmedical imaging community, enabling automated and objective diagnosis,\nprognosis, and therapeutic-response prediction. However, in clinical practice,\nthe ever-evolving environment hamper the utility of WSI analysis models. In\nthis paper, we propose the FIRST continual learning framework for WSI analysis,\nnamed ConSlide, to tackle the challenges of enormous image size, utilization of\nhierarchical structure, and catastrophic forgetting by progressive model\nupdating on multiple sequential datasets. Our framework contains three key\ncomponents. The Hierarchical Interaction Transformer (HIT) is proposed to model\nand utilize the hierarchical structural knowledge of WSI. The\nBreakup-Reorganize (BuRo) rehearsal method is developed for WSI data replay\nwith efficient region storing buffer and WSI reorganizing operation. The\nasynchronous updating mechanism is devised to encourage the network to learn\ngeneric and specific knowledge respectively during the replay stage, based on a\nnested cross-scale similarity learning (CSSL) module. We evaluated the proposed\nConSlide on four public WSI datasets from TCGA projects. It performs best over\nother state-of-the-art methods with a fair WSI-based continual learning setting\nand achieves a better trade-off of the overall performance and forgetting on\nprevious task",
        "authors": [
            "Yanyan Huang",
            "Weiqin Zhao",
            "Shujun Wang",
            "Yu Fu",
            "Yuming Jiang",
            "Lequan Yu"
        ]
    },
    {
        "title": "UMC: A Unified Bandwidth-efficient and Multi-resolution based Collaborative Perception Framework",
        "url": "http://arxiv.org/abs/2303.12400",
        "abstract": "Multi-agent collaborative perception (MCP) has recently attracted much\nattention. It includes three key processes: communication for sharing,\ncollaboration for integration, and reconstruction for different downstream\ntasks. Existing methods pursue designing the collaboration process alone,\nignoring their intrinsic interactions and resulting in suboptimal performance.\nIn contrast, we aim to propose a Unified Collaborative perception framework\nnamed UMC, optimizing the communication, collaboration, and reconstruction\nprocesses with the Multi-resolution technique. The communication introduces a\nnovel trainable multi-resolution and selective-region (MRSR) mechanism,\nachieving higher quality and lower bandwidth. Then, a graph-based collaboration\nis proposed, conducting on each resolution to adapt the MRSR. Finally, the\nreconstruction integrates the multi-resolution collaborative features for\ndownstream tasks. Since the general metric can not reflect the performance\nenhancement brought by MCP systematically, we introduce a brand-new evaluation\nmetric that evaluates the MCP from different perspectives. To verify our\nalgorithm, we conducted experiments on the V2X-Sim and OPV2V datasets. Our\nquantitative and qualitative experiments prove that the proposed UMC greatly\noutperforms the state-of-the-art collaborative perception approaches.",
        "authors": [
            "Tianhang Wang",
            "Guang Chen",
            "Kai Chen",
            "Zhengfa Liu",
            "Bo Zhang",
            "Alois Knoll",
            "Changjun Jiang"
        ]
    },
    {
        "title": "SATR: Zero-Shot Semantic Segmentation of 3D Shapes",
        "url": "http://arxiv.org/abs/2304.04909",
        "abstract": "We explore the task of zero-shot semantic segmentation of 3D shapes by using\nlarge-scale off-the-shelf 2D image recognition models. Surprisingly, we find\nthat modern zero-shot 2D object detectors are better suited for this task than\ncontemporary text/image similarity predictors or even zero-shot 2D segmentation\nnetworks. Our key finding is that it is possible to extract accurate 3D\nsegmentation maps from multi-view bounding box predictions by using the\ntopological properties of the underlying surface. For this, we develop the\nSegmentation Assignment with Topological Reweighting (SATR) algorithm and\nevaluate it on ShapeNetPart and our proposed FAUST benchmarks. SATR achieves\nstate-of-the-art performance and outperforms a baseline algorithm by 1.3% and\n4% average mIoU on the FAUST coarse and fine-grained benchmarks, respectively,\nand by 5.2% average mIoU on the ShapeNetPart benchmark. Our source code and\ndata will be publicly released. Project webpage:\nhttps://samir55.github.io/SATR/.",
        "authors": [
            "Ahmed Abdelreheem",
            "Ivan Skorokhodov",
            "Maks Ovsjanikov",
            "Peter Wonka"
        ]
    },
    {
        "title": "Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation",
        "url": "http://arxiv.org/abs/2308.10016",
        "abstract": "Most self-supervised 6D object pose estimation methods can only work with\nadditional depth information or rely on the accurate annotation of 2D\nsegmentation masks, limiting their application range. In this paper, we propose\na 6D object pose estimation method that can be trained with pure RGB images\nwithout any auxiliary information. We first obtain a rough pose initialization\nfrom networks trained on synthetic images rendered from the target's 3D mesh.\nThen, we introduce a refinement strategy leveraging the geometry constraint in\nsynthetic-to-real image pairs from multiple different views. We formulate this\ngeometry constraint as pixel-level flow consistency between the training images\nwith dynamically generated pseudo labels. We evaluate our method on three\nchallenging datasets and demonstrate that it outperforms state-of-the-art\nself-supervised methods significantly, with neither 2D annotations nor\nadditional depth images.",
        "authors": [
            "Yang Hai",
            "Rui Song",
            "Jiaojiao Li",
            "David Ferstl",
            "Yinlin Hu"
        ]
    },
    {
        "title": "Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning",
        "url": "http://arxiv.org/abs/2303.12424",
        "abstract": "Event-based cameras offer reliable measurements for preforming computer\nvision tasks in high-dynamic range environments and during fast motion\nmaneuvers. However, adopting deep learning in event-based vision faces the\nchallenge of annotated data scarcity due to recency of event cameras.\nTransferring the knowledge that can be obtained from conventional camera\nannotated data offers a practical solution to this challenge. We develop an\nunsupervised domain adaptation algorithm for training a deep network for\nevent-based data image classification using contrastive learning and\nuncorrelated conditioning of data. Our solution outperforms the existing\nalgorithms for this purpose.",
        "authors": [
            "Dayuan Jian",
            "Mohammad Rostami"
        ]
    },
    {
        "title": "Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views",
        "url": "http://arxiv.org/abs/2304.06024",
        "abstract": "Automatic perception of human behaviors during social interactions is crucial\nfor AR/VR applications, and an essential component is estimation of plausible\n3D human pose and shape of our social partners from the egocentric view. One of\nthe biggest challenges of this task is severe body truncation due to close\nsocial distances in egocentric scenarios, which brings large pose ambiguities\nfor unseen body parts. To tackle this challenge, we propose a novel\nscene-conditioned diffusion method to model the body pose distribution.\nConditioned on the 3D scene geometry, the diffusion model generates bodies in\nplausible human-scene interactions, with the sampling guided by a physics-based\ncollision score to further resolve human-scene inter-penetrations. The\nclassifier-free training enables flexible sampling with different conditions\nand enhanced diversity. A visibility-aware graph convolution model guided by\nper-joint visibility serves as the diffusion denoiser to incorporate\ninter-joint dependencies and per-body-part control. Extensive evaluations show\nthat our method generates bodies in plausible interactions with 3D scenes,\nachieving both superior accuracy for visible joints and diversity for invisible\nbody parts. The code is available at\nhttps://sanweiliti.github.io/egohmr/egohmr.html.",
        "authors": [
            "Siwei Zhang",
            "Qianli Ma",
            "Yan Zhang",
            "Sadegh Aliakbarian",
            "Darren Cosker",
            "Siyu Tang"
        ]
    },
    {
        "title": "ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.09098",
        "abstract": "We propose ImGeoNet, a multi-view image-based 3D object detection framework\nthat models a 3D space by an image-induced geometry-aware voxel representation.\nUnlike previous methods which aggregate 2D features into 3D voxels without\nconsidering geometry, ImGeoNet learns to induce geometry from multi-view images\nto alleviate the confusion arising from voxels of free space, and during the\ninference phase, only images from multiple views are required. Besides, a\npowerful pre-trained 2D feature extractor can be leveraged by our\nrepresentation, leading to a more robust performance. To evaluate the\neffectiveness of ImGeoNet, we conduct quantitative and qualitative experiments\non three indoor datasets, namely ARKitScenes, ScanNetV2, and ScanNet200. The\nresults demonstrate that ImGeoNet outperforms the current state-of-the-art\nmulti-view image-based method, ImVoxelNet, on all three datasets in terms of\ndetection accuracy. In addition, ImGeoNet shows great data efficiency by\nachieving results comparable to ImVoxelNet with 100 views while utilizing only\n40 views. Furthermore, our studies indicate that our proposed image-induced\ngeometry-aware representation can enable image-based methods to attain superior\ndetection accuracy than the seminal point cloud-based method, VoteNet, in two\npractical scenarios: (1) scenarios where point clouds are sparse and noisy,\nsuch as in ARKitScenes, and (2) scenarios involve diverse object classes,\nparticularly classes of small objects, as in the case in ScanNet200.",
        "authors": [
            "Tao Tu",
            "Shun-Po Chuang",
            "Yu-Lun Liu",
            "Cheng Sun",
            "Ke Zhang",
            "Donna Roy",
            "Cheng-Hao Kuo",
            "Min Sun"
        ]
    },
    {
        "title": "DRAW: Defending Camera-shooted RAW Against Image Manipulation",
        "url": "http://arxiv.org/abs/2307.16418",
        "abstract": "RAW files are the initial measurement of scene radiance widely used in most\ncameras, and the ubiquitously-used RGB images are converted from RAW data\nthrough Image Signal Processing (ISP) pipelines. Nowadays, digital images are\nrisky of being nefariously manipulated. Inspired by the fact that innate\nimmunity is the first line of body defense, we propose DRAW, a novel scheme of\ndefending images against manipulation by protecting their sources, i.e.,\ncamera-shooted RAWs. Specifically, we design a lightweight Multi-frequency\nPartial Fusion Network (MPF-Net) friendly to devices with limited computing\nresources by frequency learning and partial feature fusion. It introduces\ninvisible watermarks as protective signal into the RAW data. The protection\ncapability can not only be transferred into the rendered RGB images regardless\nof the applied ISP pipeline, but also is resilient to post-processing\noperations such as blurring or compression. Once the image is manipulated, we\ncan accurately identify the forged areas with a localization network. Extensive\nexperiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD,\nindicate the effectiveness of our method. We hope that this technique can be\nused in future cameras as an option for image protection, which could\neffectively restrict image manipulation at the source.",
        "authors": [
            "Xiaoxiao Hu",
            "Qichao Ying",
            "Zhenxing Qian",
            "Sheng Li",
            "Xinpeng Zhang"
        ]
    },
    {
        "title": "Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.12350",
        "abstract": "Translating images from a source domain to a target domain for learning\ntarget models is one of the most common strategies in domain adaptive semantic\nsegmentation (DASS). However, existing methods still struggle to preserve\nsemantically-consistent local details between the original and translated\nimages. In this work, we present an innovative approach that addresses this\nchallenge by using source-domain labels as explicit guidance during image\ntranslation. Concretely, we formulate cross-domain image translation as a\ndenoising diffusion process and utilize a novel Semantic Gradient Guidance\n(SGG) method to constrain the translation process, conditioning it on the\npixel-wise source labels. Additionally, a Progressive Translation Learning\n(PTL) strategy is devised to enable the SGG method to work reliably across\ndomains with large gaps. Extensive experiments demonstrate the superiority of\nour approach over state-of-the-art methods.",
        "authors": [
            "Duo Peng",
            "Ping Hu",
            "Qiuhong Ke",
            "Jun Liu"
        ]
    },
    {
        "title": "SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields",
        "url": "http://arxiv.org/abs/2212.02501",
        "abstract": "3D reconstruction from a single 2D image was extensively covered in the\nliterature but relies on depth supervision at training time, which limits its\napplicability. To relax the dependence to depth we propose SceneRF, a\nself-supervised monocular scene reconstruction method using only posed image\nsequences for training. Fueled by the recent progress in neural radiance fields\n(NeRF) we optimize a radiance field though with explicit depth optimization and\na novel probabilistic sampling strategy to efficiently handle large scenes. At\ninference, a single input image suffices to hallucinate novel depth views which\nare fused together to obtain 3D scene reconstruction. Thorough experiments\ndemonstrate that we outperform all baselines for novel depth views synthesis\nand scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.\nCode is available at https://astra-vision.github.io/SceneRF .",
        "authors": [
            "Anh-Quan Cao",
            "Raoul de Charette"
        ]
    },
    {
        "title": "Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation",
        "url": "http://arxiv.org/abs/2308.06693",
        "abstract": "Recent leading zero-shot video object segmentation (ZVOS) works devote to\nintegrating appearance and motion information by elaborately designing feature\nfusion modules and identically applying them in multiple feature stages. Our\npreliminary experiments show that with the strong long-range dependency\nmodeling capacity of Transformer, simply concatenating the two modality\nfeatures and feeding them to vanilla Transformers for feature fusion can\ndistinctly benefit the performance but at a cost of heavy computation. Through\nfurther empirical analysis, we find that attention dependencies learned in\nTransformer in different stages exhibit completely different properties: global\nquery-independent dependency in the low-level stages and semantic-specific\ndependency in the high-level stages. Motivated by the observations, we propose\ntwo Transformer variants: i) Context-Sharing Transformer (CST) that learns the\nglobal-shared contextual information within image frames with a lightweight\ncomputation. ii) Semantic Gathering-Scattering Transformer (SGST) that models\nthe semantic correlation separately for the foreground and background and\nreduces the computation cost with a soft token merging mechanism. We apply CST\nand SGST for low-level and high-level feature fusions, respectively,\nformulating a level-isomerous Transformer framework for ZVOS task. Compared\nwith the baseline that uses vanilla Transformers for multi-stage fusion, ours\nsignificantly increase the speed by 13 times and achieves new state-of-the-art\nZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.",
        "authors": [
            "Yichen Yuan",
            "Yifan Wang",
            "Lijun Wang",
            "Xiaoqi Zhao",
            "Huchuan Lu",
            "Yu Wang",
            "Weibo Su",
            "Lei Zhang"
        ]
    },
    {
        "title": "CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.10316",
        "abstract": "We study the task of weakly-supervised point cloud semantic segmentation with\nsparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce\nthe expensive cost of dense annotations. Unfortunately, with extremely sparse\nannotated points, it is very difficult to extract both contextual and object\ninformation for scene understanding such as semantic segmentation. Motivated by\nmasked modeling (e.g., MAE) in image and video representation learning, we seek\nto endow the power of masked modeling to learn contextual information from\nsparsely-annotated points. However, directly applying MAE to 3D point clouds\nwith sparse annotations may fail to work. First, it is nontrivial to\neffectively mask out the informative visual context from 3D point clouds.\nSecond, how to fully exploit the sparse annotations for context modeling\nremains an open question. In this paper, we propose a simple yet effective\nContextual Point Cloud Modeling (CPCM) method that consists of two parts: a\nregion-wise masking (RegionMask) strategy and a contextual masked training\n(CMT) method. Specifically, RegionMask masks the point cloud continuously in\ngeometric space to construct a meaningful masked prediction task for subsequent\ncontext learning. CMT disentangles the learning of supervised segmentation and\nunsupervised masked context prediction for effectively learning the very\nlimited labeled points and mass unlabeled points, respectively. Extensive\nexperiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate\nthe superiority of CPCM over the state-of-the-art.",
        "authors": [
            "Lizhao Liu",
            "Zhuangwei Zhuang",
            "Shangxin Huang",
            "Xunlong Xiao",
            "Tianhang Xiang",
            "Cen Chen",
            "Jingdong Wang",
            "Mingkui Tan"
        ]
    },
    {
        "title": "PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting",
        "url": "http://arxiv.org/abs/2304.06107",
        "abstract": "Generative models such as StyleGAN2 and Stable Diffusion have achieved\nstate-of-the-art performance in computer vision tasks such as image synthesis,\ninpainting, and de-noising. However, current generative models for face\ninpainting often fail to preserve fine facial details and the identity of the\nperson, despite creating aesthetically convincing image structures and\ntextures. In this work, we propose Person Aware Tuning (PAT) of Mask-Aware\nTransformer (MAT) for face inpainting, which addresses this issue. Our proposed\nmethod, PATMAT, effectively preserves identity by incorporating reference\nimages of a subject and fine-tuning a MAT architecture trained on faces. By\nusing ~40 reference images, PATMAT creates anchor points in MAT's style module,\nand tunes the model using the fixed anchors to adapt the model to a new face\nidentity. Moreover, PATMAT's use of multiple images per anchor during training\nallows the model to use fewer reference images than competing methods. We\ndemonstrate that PATMAT outperforms state-of-the-art models in terms of image\nquality, the preservation of person-specific details, and the identity of the\nsubject. Our results suggest that PATMAT can be a promising approach for\nimproving the quality of personalized face inpainting.",
        "authors": [
            "Saman Motamed",
            "Jianjin Xu",
            "Chen Henry Wu",
            "Fernando De la Torre"
        ]
    },
    {
        "title": "Adaptive Nonlinear Latent Transformation for Conditional Face Editing",
        "url": "http://arxiv.org/abs/2307.07790",
        "abstract": "Recent works for face editing usually manipulate the latent space of StyleGAN\nvia the linear semantic directions. However, they usually suffer from the\nentanglement of facial attributes, need to tune the optimal editing strength,\nand are limited to binary attributes with strong supervision signals. This\npaper proposes a novel adaptive nonlinear latent transformation for\ndisentangled and conditional face editing, termed AdaTrans. Specifically, our\nAdaTrans divides the manipulation process into several finer steps; i.e., the\ndirection and size at each step are conditioned on both the facial attributes\nand the latent codes. In this way, AdaTrans describes an adaptive nonlinear\ntransformation trajectory to manipulate the faces into target attributes while\nkeeping other attributes unchanged. Then, AdaTrans leverages a predefined\ndensity model to constrain the learned trajectory in the distribution of latent\ncodes by maximizing the likelihood of transformed latent code. Moreover, we\nalso propose a disentangled learning strategy under a mutual information\nframework to eliminate the entanglement among attributes, which can further\nrelax the need for labeled data. Consequently, AdaTrans enables a controllable\nface editing with the advantages of disentanglement, flexibility with\nnon-binary attributes, and high fidelity. Extensive experimental results on\nvarious facial attributes demonstrate the qualitative and quantitative\neffectiveness of the proposed AdaTrans over existing state-of-the-art methods,\nespecially in the most challenging scenarios with a large age gap and few\nlabeled examples. The source code is available at\nhttps://github.com/Hzzone/AdaTrans.",
        "authors": [
            "Zhizhong Huang",
            "Siteng Ma",
            "Junping Zhang",
            "Hongming Shan"
        ]
    },
    {
        "title": "Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network",
        "url": "http://arxiv.org/abs/2307.13254",
        "abstract": "Many studies in vision tasks have aimed to create effective embedding spaces\nfor single-label object prediction within an image. However, in reality, most\nobjects possess multiple specific attributes, such as shape, color, and length,\nwith each attribute composed of various classes. To apply models in real-world\nscenarios, it is essential to be able to distinguish between the granular\ncomponents of an object. Conventional approaches to embedding multiple specific\nattributes into a single network often result in entanglement, where\nfine-grained features of each attribute cannot be identified separately. To\naddress this problem, we propose a Conditional Cross-Attention Network that\ninduces disentangled multi-space embeddings for various specific attributes\nwith only a single backbone. Firstly, we employ a cross-attention mechanism to\nfuse and switch the information of conditions (specific attributes), and we\ndemonstrate its effectiveness through a diverse visualization example.\nSecondly, we leverage the vision transformer for the first time to a\nfine-grained image retrieval task and present a simple yet effective framework\ncompared to existing methods. Unlike previous studies where performance varied\ndepending on the benchmark dataset, our proposed method achieved consistent\nstate-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K\nbenchmark datasets.",
        "authors": [
            "Chull Hwan Song",
            "Taebaek Hwang",
            "Jooyoung Yoon",
            "Shunghyun Choi",
            "Yeong Hyeon Gu"
        ]
    },
    {
        "title": "Muscles in Action",
        "url": "http://arxiv.org/abs/2212.02978",
        "abstract": "Human motion is created by, and constrained by, our muscles. We take a first\nstep at building computer vision methods that represent the internal muscle\nactivity that causes motion. We present a new dataset, Muscles in Action (MIA),\nto learn to incorporate muscle activity into human motion representations. The\ndataset consists of 12.5 hours of synchronized video and surface\nelectromyography (sEMG) data of 10 subjects performing various exercises. Using\nthis dataset, we learn a bidirectional representation that predicts muscle\nactivation from video, and conversely, reconstructs motion from muscle\nactivation. We evaluate our model on in-distribution subjects and exercises, as\nwell as on out-of-distribution subjects and exercises. We demonstrate how\nadvances in modeling both modalities jointly can serve as conditioning for\nmuscularly consistent motion generation. Putting muscles into computer vision\nsystems will enable richer models of virtual humans, with applications in\nsports, fitness, and AR/VR.",
        "authors": [
            "Mia Chiquier",
            "Carl Vondrick"
        ]
    },
    {
        "title": "Large-Scale Person Detection and Localization Using Overhead Fisheye Cameras",
        "url": "http://arxiv.org/abs/2307.08252",
        "abstract": "Location determination finds wide applications in daily life. Instead of\nexisting efforts devoted to localizing tourist photos captured by perspective\ncameras, in this article, we focus on devising person positioning solutions\nusing overhead fisheye cameras. Such solutions are advantageous in large field\nof view (FOV), low cost, anti-occlusion, and unaggressive work mode (without\nthe necessity of cameras carried by persons). However, related studies are\nquite scarce, due to the paucity of data. To stimulate research in this\nexciting area, we present LOAF, the first large-scale overhead fisheye dataset\nfor person detection and localization. LOAF is built with many essential\nfeatures, e.g., i) the data cover abundant diversities in scenes, human pose,\ndensity, and location; ii) it contains currently the largest number of\nannotated pedestrian, i.e., 457K bounding boxes with groundtruth location\ninformation; iii) the body-boxes are labeled as radius-aligned so as to fully\naddress the positioning challenge. To approach localization, we build a fisheye\nperson detection network, which exploits the fisheye distortions by a\nrotation-equivariant training strategy and predict radius-aligned human boxes\nend-to-end. Then, the actual locations of the detected persons are calculated\nby a numerical solution on the fisheye model and camera altitude data.\nExtensive experiments on LOAF validate the superiority of our fisheye detector\nw.r.t. previous methods, and show that our whole fisheye positioning solution\nis able to locate all persons in FOV with an accuracy of 0.5 m, within 0.1 s.",
        "authors": [
            "Lu Yang",
            "Liulei Li",
            "Xueshi Xin",
            "Yifan Sun",
            "Qing Song",
            "Wenguan Wang"
        ]
    },
    {
        "title": "ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation",
        "url": "http://arxiv.org/abs/2308.16689",
        "abstract": "Vision-language pre-training (VLP) methods are blossoming recently, and its\ncrucial goal is to jointly learn visual and textual features via a\ntransformer-based architecture, demonstrating promising improvements on a\nvariety of vision-language tasks. Prior arts usually focus on how to align\nvisual and textual features, but strategies for improving the robustness of\nmodel and speeding up model convergence are left insufficiently explored.\n  In this paper, we propose a novel method ViLTA, comprising of two components\nto further facilitate the model to learn fine-grained representations among\nimage-text pairs. For Masked Language Modeling (MLM), we propose a\ncross-distillation method to generate soft labels to enhance the robustness of\nmodel, which alleviates the problem of treating synonyms of masked words as\nnegative samples in one-hot labels. For Image-Text Matching (ITM), we leverage\nthe current language encoder to synthesize hard negatives based on the context\nof language input, encouraging the model to learn high-quality representations\nby increasing the difficulty of the ITM task. By leveraging the above\ntechniques, our ViLTA can achieve better performance on various vision-language\ntasks. Extensive experiments on benchmark datasets demonstrate that the\neffectiveness of ViLTA and its promising potential for vision-language\npre-training.",
        "authors": [
            "Weihan Wang",
            "Zhen Yang",
            "Bin Xu",
            "Juanzi Li",
            "Yankui Sun"
        ]
    },
    {
        "title": "All-to-Key Attention for Arbitrary Style Transfer",
        "url": "http://arxiv.org/abs/2212.04105",
        "abstract": "Attention-based arbitrary style transfer studies have shown promising\nperformance in synthesizing vivid local style details. They typically use the\nall-to-all attention mechanism -- each position of content features is fully\nmatched to all positions of style features. However, all-to-all attention tends\nto generate distorted style patterns and has quadratic complexity, limiting the\neffectiveness and efficiency of arbitrary style transfer. In this paper, we\npropose a novel all-to-key attention mechanism -- each position of content\nfeatures is matched to stable key positions of style features -- that is more\nin line with the characteristics of style transfer. Specifically, it integrates\ntwo newly proposed attention forms: distributed and progressive attention.\nDistributed attention assigns attention to key style representations that\ndepict the style distribution of local regions; Progressive attention pays\nattention from coarse-grained regions to fine-grained key positions. The\nresultant module, dubbed StyA2K, shows extraordinary performance in preserving\nthe semantic structure and rendering consistent style patterns. Qualitative and\nquantitative comparisons with state-of-the-art methods demonstrate the superior\nperformance of our approach.",
        "authors": [
            "Mingrui Zhu",
            "Xiao He",
            "Nannan Wang",
            "Xiaoyu Wang",
            "Xinbo Gao"
        ]
    },
    {
        "title": "Learning to Distill Global Representation for Sparse-View CT",
        "url": "http://arxiv.org/abs/2308.08463",
        "abstract": "Sparse-view computed tomography (CT) -- using a small number of projections\nfor tomographic reconstruction -- enables much lower radiation dose to patients\nand accelerated data acquisition. The reconstructed images, however, suffer\nfrom strong artifacts, greatly limiting their diagnostic value. Current trends\nfor sparse-view CT turn to the raw data for better information recovery. The\nresultant dual-domain methods, nonetheless, suffer from secondary artifacts,\nespecially in ultra-sparse view scenarios, and their generalization to other\nscanners/protocols is greatly limited. A crucial question arises: have the\nimage post-processing methods reached the limit? Our answer is not yet. In this\npaper, we stick to image post-processing methods due to great flexibility and\npropose global representation (GloRe) distillation framework for sparse-view\nCT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution,\nso each element in GloRe has an image-wide receptive field. Second, unlike\nmethods that only use the full-view images for supervision, we propose to\ndistill GloRe from intermediate-view reconstructed images that are readily\navailable but not explored in previous literature. The success of GloRe\ndistillation is attributed to two key components: representation directional\ndistillation to align the GloRe directions, and band-pass-specific contrastive\ndistillation to gain clinically important details. Extensive experiments\ndemonstrate the superiority of the proposed GloReDi over the state-of-the-art\nmethods, including dual-domain ones. The source code is available at\nhttps://github.com/longzilicart/GloReDi.",
        "authors": [
            "Zilong Li",
            "Chenglong Ma",
            "Jie Chen",
            "Junping Zhang",
            "Hongming Shan"
        ]
    },
    {
        "title": "FocalFormer3D: Focusing on Hard Instance for 3D Object Detection",
        "url": "http://arxiv.org/abs/2308.04556",
        "abstract": "False negatives (FN) in 3D object detection, {\\em e.g.}, missing predictions\nof pedestrians, vehicles, or other obstacles, can lead to potentially dangerous\nsituations in autonomous driving. While being fatal, this issue is understudied\nin many current 3D detection methods. In this work, we propose Hard Instance\nProbing (HIP), a general pipeline that identifies \\textit{FN} in a multi-stage\nmanner and guides the models to focus on excavating difficult instances. For 3D\nobject detection, we instantiate this method as FocalFormer3D, a simple yet\neffective detector that excels at excavating difficult objects and improving\nprediction recall. FocalFormer3D features a multi-stage query generation to\ndiscover hard objects and a box-level transformer decoder to efficiently\ndistinguish objects from massive object candidates. Experimental results on the\nnuScenes and Waymo datasets validate the superior performance of FocalFormer3D.\nThe advantage leads to strong performance on both detection and tracking, in\nboth LiDAR and multi-modal settings. Notably, FocalFormer3D achieves a 70.5 mAP\nand 73.9 NDS on nuScenes detection benchmark, while the nuScenes tracking\nbenchmark shows 72.1 AMOTA, both ranking 1st place on the nuScenes LiDAR\nleaderboard. Our code is available at\n\\url{https://github.com/NVlabs/FocalFormer3D}.",
        "authors": [
            "Yilun Chen",
            "Zhiding Yu",
            "Yukang Chen",
            "Shiyi Lan",
            "Animashree Anandkumar",
            "Jiaya Jia",
            "Jose Alvarez"
        ]
    },
    {
        "title": "Teaching CLIP to Count to Ten",
        "url": "http://arxiv.org/abs/2302.12066",
        "abstract": "Large vision-language models (VLMs), such as CLIP, learn rich joint\nimage-text representations, facilitating advances in numerous downstream tasks,\nincluding zero-shot classification and text-to-image generation. Nevertheless,\nexisting VLMs exhibit a prominent well-documented limitation - they fail to\nencapsulate compositional concepts such as counting. We introduce a simple yet\neffective method to improve the quantitative understanding of VLMs, while\nmaintaining their overall performance on common benchmarks. Specifically, we\npropose a new counting-contrastive loss used to finetune a pre-trained VLM in\ntandem with its original objective. Our counting loss is deployed over\nautomatically-created counterfactual examples, each consisting of an image and\na caption containing an incorrect object count. For example, an image depicting\nthree dogs is paired with the caption \"Six dogs playing in the yard\". Our loss\nencourages discrimination between the correct caption and its counterfactual\nvariant which serves as a hard negative example. To the best of our knowledge,\nthis work is the first to extend CLIP's capabilities to object counting.\nFurthermore, we introduce \"CountBench\" - a new image-text counting benchmark\nfor evaluating a model's understanding of object counting. We demonstrate a\nsignificant improvement over state-of-the-art baseline models on this task.\nFinally, we leverage our count-aware CLIP model for image retrieval and\ntext-conditioned image generation, demonstrating that our model can produce\nspecific counts of objects more reliably than existing ones.",
        "authors": [
            "Roni Paiss",
            "Ariel Ephrat",
            "Omer Tov",
            "Shiran Zada",
            "Inbar Mosseri",
            "Michal Irani",
            "Tali Dekel"
        ]
    },
    {
        "title": "TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting",
        "url": "http://arxiv.org/abs/2309.07910",
        "abstract": "Existing volumetric methods for predicting 3D human pose estimation are\naccurate, but computationally expensive and optimized for single time-step\nprediction. We present TEMPO, an efficient multi-view pose estimation model\nthat learns a robust spatiotemporal representation, improving pose accuracy\nwhile also tracking and forecasting human pose. We significantly reduce\ncomputation compared to the state-of-the-art by recurrently computing\nper-person 2D pose features, fusing both spatial and temporal information into\na single representation. In doing so, our model is able to use spatiotemporal\ncontext to predict more accurate human poses without sacrificing efficiency. We\nfurther use this representation to track human poses over time as well as\npredict future poses. Finally, we demonstrate that our model is able to\ngeneralize across datasets without scene-specific fine-tuning. TEMPO achieves\n10$\\%$ better MPJPE with a 33$\\times$ improvement in FPS compared to TesseTrack\non the challenging CMU Panoptic Studio dataset.",
        "authors": [
            "Rohan Choudhury",
            "Kris Kitani",
            "Laszlo A. Jeni"
        ]
    },
    {
        "title": "DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation",
        "url": "http://arxiv.org/abs/2307.16687",
        "abstract": "Denoising diffusion probabilistic models that were initially proposed for\nrealistic image generation have recently shown success in various perception\ntasks (e.g., object detection and image segmentation) and are increasingly\ngaining attention in computer vision. However, extending such models to\nmulti-frame human pose estimation is non-trivial due to the presence of the\nadditional temporal dimension in videos. More importantly, learning\nrepresentations that focus on keypoint regions is crucial for accurate\nlocalization of human joints. Nevertheless, the adaptation of the\ndiffusion-based methods remains unclear on how to achieve such objective. In\nthis paper, we present DiffPose, a novel diffusion architecture that formulates\nvideo-based human pose estimation as a conditional heatmap generation problem.\nFirst, to better leverage temporal information, we propose SpatioTemporal\nRepresentation Learner which aggregates visual evidences across frames and uses\nthe resulting features in each denoising step as a condition. In addition, we\npresent a mechanism called Lookup-based MultiScale Feature Interaction that\ndetermines the correlations between local joints and global contexts across\nmultiple scales. This mechanism generates delicate representations that focus\non keypoint regions. Altogether, by extending diffusion models, we show two\nunique characteristics from DiffPose on pose estimation task: (i) the ability\nto combine multiple sets of pose estimates to improve prediction accuracy,\nparticularly for challenging joints, and (ii) the ability to adjust the number\nof iterative steps for feature refinement without retraining the model.\nDiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017,\nPoseTrack2018, and PoseTrack21.",
        "authors": [
            "Runyang Feng",
            "Yixing Gao",
            "Tze Ho Elden Tse",
            "Xueqing Ma",
            "Hyung Jin Chang"
        ]
    },
    {
        "title": "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2302.13848",
        "abstract": "In addition to the unprecedented ability in imaginary creation, large\ntext-to-image models are expected to take customized concepts in image\ngeneration. Existing works generally learn such concepts in an\noptimization-based manner, yet bringing excessive computation or memory burden.\nIn this paper, we instead propose a learning-based encoder, which consists of a\nglobal and a local mapping networks for fast and accurate customized\ntext-to-image generation. In specific, the global mapping network projects the\nhierarchical features of a given image into multiple new words in the textual\nword embedding space, i.e., one primary word for well-editable concept and\nother auxiliary words to exclude irrelevant disturbances (e.g., background). In\nthe meantime, a local mapping network injects the encoded patch features into\ncross attention layers to provide omitted details, without sacrificing the\neditability of primary concepts. We compare our method with existing\noptimization-based approaches on a variety of user-defined concepts, and\ndemonstrate that our method enables high-fidelity inversion and more robust\neditability with a significantly faster encoding process. Our code is publicly\navailable at https://github.com/csyxwei/ELITE.",
        "authors": [
            "Yuxiang Wei",
            "Yabo Zhang",
            "Zhilong Ji",
            "Jinfeng Bai",
            "Lei Zhang",
            "Wangmeng Zuo"
        ]
    },
    {
        "title": "Text2Performer: Text-Driven Human Video Generation",
        "url": "http://arxiv.org/abs/2304.08483",
        "abstract": "Text-driven content creation has evolved to be a transformative technique\nthat revolutionizes creativity. Here we study the task of text-driven human\nvideo generation, where a video sequence is synthesized from texts describing\nthe appearance and motions of a target performer. Compared to general\ntext-driven video generation, human-centric video generation requires\nmaintaining the appearance of synthesized human while performing complex\nmotions. In this work, we present Text2Performer to generate vivid human videos\nwith articulated motions from texts. Text2Performer has two novel designs: 1)\ndecomposed human representation and 2) diffusion-based motion sampler. First,\nwe decompose the VQVAE latent space into human appearance and pose\nrepresentation in an unsupervised manner by utilizing the nature of human\nvideos. In this way, the appearance is well maintained along the generated\nframes. Then, we propose continuous VQ-diffuser to sample a sequence of pose\nembeddings. Unlike existing VQ-based methods that operate in the discrete\nspace, continuous VQ-diffuser directly outputs the continuous pose embeddings\nfor better motion modeling. Finally, motion-aware masking strategy is designed\nto mask the pose embeddings spatial-temporally to enhance the temporal\ncoherence. Moreover, to facilitate the task of text-driven human video\ngeneration, we contribute a Fashion-Text2Video dataset with manually annotated\naction labels and text descriptions. Extensive experiments demonstrate that\nText2Performer generates high-quality human videos (up to 512x256 resolution)\nwith diverse appearances and flexible motions.",
        "authors": [
            "Yuming Jiang",
            "Shuai Yang",
            "Tong Liang Koh",
            "Wayne Wu",
            "Chen Change Loy",
            "Ziwei Liu"
        ]
    },
    {
        "title": "4D Myocardium Reconstruction with Decoupled Motion and Shape Model",
        "url": "http://arxiv.org/abs/2308.14083",
        "abstract": "Estimating the shape and motion state of the myocardium is essential in\ndiagnosing cardiovascular diseases.However, cine magnetic resonance (CMR)\nimaging is dominated by 2D slices, whose large slice spacing challenges\ninter-slice shape reconstruction and motion acquisition.To address this\nproblem, we propose a 4D reconstruction method that decouples motion and shape,\nwhich can predict the inter-/intra- shape and motion estimation from a given\nsparse point cloud sequence obtained from limited slices. Our framework\ncomprises a neural motion model and an end-diastolic (ED) shape model. The\nimplicit ED shape model can learn a continuous boundary and encourage the\nmotion model to predict without the supervision of ground truth deformation,\nand the motion model enables canonical input of the shape model by deforming\nany point from any phase to the ED phase. Additionally, the constructed\nED-space enables pre-training of the shape model, thereby guiding the motion\nmodel and addressing the issue of data scarcity. We propose the first 4D\nmyocardial dataset as we know and verify our method on the proposed, public,\nand cross-modal datasets, showing superior reconstruction performance and\nenabling various clinical applications.",
        "authors": [
            "Xiaohan Yuan",
            "Cong Liu",
            "Yangang Wang"
        ]
    },
    {
        "title": "Robust Monocular Depth Estimation under Challenging Conditions",
        "url": "http://arxiv.org/abs/2308.09711",
        "abstract": "While state-of-the-art monocular depth estimation approaches achieve\nimpressive results in ideal settings, they are highly unreliable under\nchallenging illumination and weather conditions, such as at nighttime or in the\npresence of rain. In this paper, we uncover these safety-critical issues and\ntackle them with md4all: a simple and effective solution that works reliably\nunder both adverse and ideal conditions, as well as for different types of\nlearning supervision. We achieve this by exploiting the efficacy of existing\nmethods under perfect settings. Therefore, we provide valid training signals\nindependently of what is in the input. First, we generate a set of complex\nsamples corresponding to the normal training ones. Then, we train the model by\nguiding its self- or full-supervision by feeding the generated samples and\ncomputing the standard losses on the corresponding original images. Doing so\nenables a single model to recover information across diverse conditions without\nmodifications at inference time. Extensive experiments on two challenging\npublic datasets, namely nuScenes and Oxford RobotCar, demonstrate the\neffectiveness of our techniques, outperforming prior works by a large margin in\nboth standard and challenging conditions. Source code and data are available\nat: https://md4all.github.io.",
        "authors": [
            "Stefano Gasperini",
            "Nils Morbitzer",
            "HyunJun Jung",
            "Nassir Navab",
            "Federico Tombari"
        ]
    },
    {
        "title": "MSI: Maximize Support-Set Information for Few-Shot Segmentation",
        "url": "http://arxiv.org/abs/2212.04673",
        "abstract": "FSS(Few-shot segmentation) aims to segment a target class using a small\nnumber of labeled images(support set). To extract information relevant to the\ntarget class, a dominant approach in best-performing FSS methods removes\nbackground features using a support mask. We observe that this feature excision\nthrough a limiting support mask introduces an information bottleneck in several\nchallenging FSS cases, e.g., for small targets and/or inaccurate target\nboundaries. To this end, we present a novel method(MSI), which maximizes the\nsupport-set information by exploiting two complementary sources of features to\ngenerate super correlation maps. We validate the effectiveness of our approach\nby instantiating it into three recent and strong FSS methods. Experimental\nresults on several publicly available FSS benchmarks show that our proposed\nmethod consistently improves performance by visible margins and leads to faster\nconvergence. Our code and trained models are available at:\nhttps://github.com/moonsh/MSI-Maximize-Support-Set-Information",
        "authors": [
            "Seonghyeon Moon",
            "Samuel S. Sohn",
            "Honglu Zhou",
            "Sejong Yoon",
            "Vladimir Pavlovic",
            "Muhammad Haris Khan",
            "Mubbasir Kapadia"
        ]
    },
    {
        "title": "Global Features are All You Need for Image Retrieval and Reranking",
        "url": "http://arxiv.org/abs/2308.06954",
        "abstract": "Image retrieval systems conventionally use a two-stage paradigm, leveraging\nglobal features for initial retrieval and local features for reranking.\nHowever, the scalability of this method is often limited due to the significant\nstorage and computation cost incurred by local feature matching in the\nreranking stage. In this paper, we present SuperGlobal, a novel approach that\nexclusively employs global features for both stages, improving efficiency\nwithout sacrificing accuracy. SuperGlobal introduces key enhancements to the\nretrieval system, specifically focusing on the global feature extraction and\nreranking processes. For extraction, we identify sub-optimal performance when\nthe widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are\ncombined and propose several new modules to improve GeM pooling. In the\nreranking stage, we introduce a novel method to update the global features of\nthe query and top-ranked images by only considering feature refinement with a\nsmall set of images, thus being very compute and memory efficient. Our\nexperiments demonstrate substantial improvements compared to the state of the\nart in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset,\nour single-stage results improve by 7.1%, while our two-stage gain reaches 3.7%\nwith a strong 64,865x speedup. Our two-stage system surpasses the current\nsingle-stage state-of-the-art by 16.3%, offering a scalable, accurate\nalternative for high-performing image retrieval systems with minimal time\noverhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.",
        "authors": [
            "Shihao Shao",
            "Kaifeng Chen",
            "Arjun Karpur",
            "Qinghua Cui",
            "Andre Araujo",
            "Bingyi Cao"
        ]
    },
    {
        "title": "Yes, we CANN: Constrained Approximate Nearest Neighbors for Local Feature-Based Visual Localization",
        "url": "http://arxiv.org/abs/2306.09012",
        "abstract": "Large-scale visual localization systems continue to rely on 3D point clouds\nbuilt from image collections using structure-from-motion. While the 3D points\nin these models are represented using local image features, directly matching a\nquery image's local features against the point cloud is challenging due to the\nscale of the nearest-neighbor search problem. Many recent approaches to visual\nlocalization have thus proposed a hybrid method, where first a global (per\nimage) embedding is used to retrieve a small subset of database images, and\nlocal features of the query are matched only against those. It seems to have\nbecome common belief that global embeddings are critical for said\nimage-retrieval in visual localization, despite the significant downside of\nhaving to compute two feature types for each query image. In this paper, we\ntake a step back from this assumption and propose Constrained Approximate\nNearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both\nthe geometry and appearance space using only local features. We first derive\nthe theoretical foundation for k-nearest-neighbor retrieval across multiple\nmetrics and then showcase how CANN improves visual localization. Our\nexperiments on public localization benchmarks demonstrate that our method\nsignificantly outperforms both state-of-the-art global feature-based retrieval\nand approaches using local feature aggregation schemes. Moreover, it is an\norder of magnitude faster in both index and query time than feature aggregation\nschemes for these datasets. Code:\n\\url{https://github.com/google-research/google-research/tree/master/cann}",
        "authors": [
            "Dror Aiger",
            "Andr\u00e9 Araujo",
            "Simon Lynen"
        ]
    },
    {
        "title": "Multi-Object Navigation with Dynamically Learned Neural Implicit Representations",
        "url": "http://arxiv.org/abs/2210.05129",
        "abstract": "Understanding and mapping a new environment are core abilities of any\nautonomously navigating agent. While classical robotics usually estimates maps\nin a stand-alone manner with SLAM variants, which maintain a topological or\nmetric representation, end-to-end learning of navigation keeps some form of\nmemory in a neural network. Networks are typically imbued with inductive\nbiases, which can range from vectorial representations to birds-eye metric\ntensors or topological structures. In this work, we propose to structure neural\nnetworks with two neural implicit representations, which are learned\ndynamically during each episode and map the content of the scene: (i) the\nSemantic Finder predicts the position of a previously seen queried object; (ii)\nthe Occupancy and Exploration Implicit Representation encapsulates information\nabout explored area and obstacles, and is queried with a novel global read\nmechanism which directly maps from function space to a usable embedding space.\nBoth representations are leveraged by an agent trained with Reinforcement\nLearning (RL) and learned online during each episode. We evaluate the agent on\nMulti-Object Navigation and show the high impact of using neural implicit\nrepresentations as a memory source.",
        "authors": [
            "Pierre Marza",
            "Laetitia Matignon",
            "Olivier Simonin",
            "Christian Wolf"
        ]
    },
    {
        "title": "NPC: Neural Point Characters from Video",
        "url": "http://arxiv.org/abs/2304.02013",
        "abstract": "High-fidelity human 3D models can now be learned directly from videos,\ntypically by combining a template-based surface model with neural\nrepresentations. However, obtaining a template surface requires expensive\nmulti-view capture systems, laser scans, or strictly controlled conditions.\nPrevious methods avoid using a template but rely on a costly or ill-posed\nmapping from observation to canonical space. We propose a hybrid point-based\nrepresentation for reconstructing animatable characters that does not require\nan explicit surface model, while being generalizable to novel poses. For a\ngiven video, our method automatically produces an explicit set of 3D points\nrepresenting approximate canonical geometry, and learns an articulated\ndeformation model that produces pose-dependent point transformations. The\npoints serve both as a scaffold for high-frequency neural features and an\nanchor for efficiently mapping between observation and canonical space. We\ndemonstrate on established benchmarks that our representation overcomes\nlimitations of prior work operating in either canonical or in observation\nspace. Moreover, our automatic point extraction approach enables learning\nmodels of human and animal characters alike, matching the performance of the\nmethods using rigged surface templates despite being more general. Project\nwebsite: https://lemonatsu.github.io/npc/",
        "authors": [
            "Shih-Yang Su",
            "Timur Bagautdinov",
            "Helge Rhodin"
        ]
    },
    {
        "title": "MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition",
        "url": "http://arxiv.org/abs/2305.14758",
        "abstract": "Multilingual text recognition (MLTR) systems typically focus on a fixed set\nof languages, which makes it difficult to handle newly added languages or adapt\nto ever-changing data distribution. In this paper, we propose the Incremental\nMLTR (IMLTR) task in the context of incremental learning (IL), where different\nlanguages are introduced in batches. IMLTR is particularly challenging due to\nrehearsal-imbalance, which refers to the uneven distribution of sample\ncharacters in the rehearsal set, used to retain a small amount of old data as\npast memories. To address this issue, we propose a Multiplexed Routing Network\n(MRN). MRN trains a recognizer for each language that is currently seen.\nSubsequently, a language domain predictor is learned based on the rehearsal set\nto weigh the recognizers. Since the recognizers are derived from the original\ndata, MRN effectively reduces the reliance on older data and better fights\nagainst catastrophic forgetting, the core issue in IL. We extensively evaluate\nMRN on MLT17 and MLT19 datasets. It outperforms existing general-purpose IL\nmethods by large margins, with average accuracy improvements ranging from 10.3%\nto 35.8% under different settings. Code is available at\nhttps://github.com/simplify23/MRN.",
        "authors": [
            "Tianlun Zheng",
            "Zhineng Chen",
            "BingChen Huang",
            "Wei Zhang",
            "Yu-Gang Jiang"
        ]
    },
    {
        "title": "MOST: Multiple Object Localization with Self-Supervised Transformers for Object Discovery",
        "url": "http://arxiv.org/abs/2304.05387",
        "abstract": "We tackle the challenging task of unsupervised object localization in this\nwork. Recently, transformers trained with self-supervised learning have been\nshown to exhibit object localization properties without being trained for this\ntask. In this work, we present Multiple Object localization with\nSelf-supervised Transformers (MOST) that uses features of transformers trained\nusing self-supervised learning to localize multiple objects in real world\nimages. MOST analyzes the similarity maps of the features using box counting; a\nfractal analysis tool to identify tokens lying on foreground patches. The\nidentified tokens are then clustered together, and tokens of each cluster are\nused to generate bounding boxes on foreground regions. Unlike recent\nstate-of-the-art object localization methods, MOST can localize multiple\nobjects per image and outperforms SOTA algorithms on several object\nlocalization and discovery benchmarks on PASCAL-VOC 07, 12 and COCO20k\ndatasets. Additionally, we show that MOST can be used for self-supervised\npre-training of object detectors, and yields consistent improvements on fully,\nsemi-supervised object detection and unsupervised region proposal generation.",
        "authors": [
            "Sai Saketh Rambhatla",
            "Ishan Misra",
            "Rama Chellappa",
            "Abhinav Shrivastava"
        ]
    },
    {
        "title": "Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection",
        "url": "http://arxiv.org/abs/2205.09613",
        "abstract": "Modern object detectors have taken the advantages of backbone networks\npre-trained on large scale datasets. Except for the backbone networks, however,\nother components such as the detector head and the feature pyramid network\n(FPN) remain trained from scratch, which hinders fully tapping the potential of\nrepresentation models. In this study, we propose to integrally migrate\npre-trained transformer encoder-decoders (imTED) to a detector, constructing a\nfeature extraction path which is ``fully pre-trained\" so that detectors'\ngeneralization capacity is maximized. The essential differences between imTED\nwith the baseline detector are twofold: (1) migrating the pre-trained\ntransformer decoder to the detector head while removing the randomly\ninitialized FPN from the feature extraction path; and (2) defining a\nmulti-scale feature modulator (MFM) to enhance scale adaptability. Such designs\nnot only reduce randomly initialized parameters significantly but also unify\ndetector training with representation learning intendedly. Experiments on the\nMS COCO object detection dataset show that imTED consistently outperforms its\ncounterparts by $\\sim$2.4 AP. Without bells and whistles, imTED improves the\nstate-of-the-art of few-shot object detection by up to 7.6 AP. Code is\navailable at https://github.com/LiewFeng/imTED.",
        "authors": [
            "Feng Liu",
            "Xiaosong Zhang",
            "Zhiliang Peng",
            "Zonghao Guo",
            "Fang Wan",
            "Xiangyang Ji",
            "Qixiang Ye"
        ]
    },
    {
        "title": "CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition",
        "url": "http://arxiv.org/abs/2303.17778",
        "abstract": "We present CrossLoc3D, a novel 3D place recognition method that solves a\nlarge-scale point matching problem in a cross-source setting. Cross-source\npoint cloud data corresponds to point sets captured by depth sensors with\ndifferent accuracies or from different distances and perspectives. We address\nthe challenges in terms of developing 3D place recognition methods that account\nfor the representation gap between points captured by different sources. Our\nmethod handles cross-source data by utilizing multi-grained features and\nselecting convolution kernel sizes that correspond to most prominent features.\nInspired by the diffusion models, our method uses a novel iterative refinement\nprocess that gradually shifts the embedding spaces from different sources to a\nsingle canonical space for better metric learning. In addition, we present\nCS-Campus3D, the first 3D aerial-ground cross-source dataset consisting of\npoint cloud data from both aerial and ground LiDAR scans. The point clouds in\nCS-Campus3D have representation gaps and other features like different views,\npoint densities, and noise patterns. We show that our CrossLoc3D algorithm can\nachieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall\non our CS-Campus3D benchmark and achieves performance comparable to\nstate-of-the-art 3D place recognition method on the Oxford RobotCar. The code\nand CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d.",
        "authors": [
            "Tianrui Guan",
            "Aswath Muthuselvam",
            "Montana Hoover",
            "Xijun Wang",
            "Jing Liang",
            "Adarsh Jagan Sathyamoorthy",
            "Damon Conover",
            "Dinesh Manocha"
        ]
    },
    {
        "title": "Recursive Video Lane Detection",
        "url": "http://arxiv.org/abs/2308.11106",
        "abstract": "A novel algorithm to detect road lanes in videos, called recursive video lane\ndetector (RVLD), is proposed in this paper, which propagates the state of a\ncurrent frame recursively to the next frame. RVLD consists of an intra-frame\nlane detector (ILD) and a predictive lane detector (PLD). First, we design ILD\nto localize lanes in a still frame. Second, we develop PLD to exploit the\ninformation of the previous frame for lane detection in a current frame. To\nthis end, we estimate a motion field and warp the previous output to the\ncurrent frame. Using the warped information, we refine the feature map of the\ncurrent frame to detect lanes more reliably. Experimental results show that\nRVLD outperforms existing detectors on video lane datasets. Our codes are\navailable at https://github.com/dongkwonjin/RVLD.",
        "authors": [
            "Dongkwon Jin",
            "Dahyun Kim",
            "Chang-Su Kim"
        ]
    },
    {
        "title": "GECCO: Geometrically-Conditioned Point Diffusion Models",
        "url": "http://arxiv.org/abs/2303.05916",
        "abstract": "Diffusion models generating images conditionally on text, such as Dall-E 2\nand Stable Diffusion, have recently made a splash far beyond the computer\nvision community. Here, we tackle the related problem of generating point\nclouds, both unconditionally, and conditionally with images. For the latter, we\nintroduce a novel geometrically-motivated conditioning scheme based on\nprojecting sparse image features into the point cloud and attaching them to\neach individual point, at every step in the denoising process. This approach\nimproves geometric consistency and yields greater fidelity than current methods\nrelying on unstructured, global latent codes. Additionally, we show how to\napply recent continuous-time diffusion schemes. Our method performs on par or\nabove the state of art on conditional and unconditional experiments on\nsynthetic data, while being faster, lighter, and delivering tractable\nlikelihoods. We show it can also scale to diverse indoors scenes.",
        "authors": [
            "Micha\u0142 J. Tyszkiewicz",
            "Pascal Fua",
            "Eduard Trulls"
        ]
    },
    {
        "title": "Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding",
        "url": "http://arxiv.org/abs/2303.09706",
        "abstract": "Predicting attention regions of interest is an important yet challenging task\nfor self-driving systems. Existing methodologies rely on large-scale labeled\ntraffic datasets that are labor-intensive to obtain. Besides, the huge domain\ngap between natural scenes and traffic scenes in current datasets also limits\nthe potential for model training. To address these challenges, we are the first\nto introduce an unsupervised way to predict self-driving attention by\nuncertainty modeling and driving knowledge integration. Our approach's\nUncertainty Mining Branch (UMB) discovers commonalities and differences from\nmultiple generated pseudo-labels achieved from models pre-trained on natural\nscenes by actively measuring the uncertainty. Meanwhile, our Knowledge\nEmbedding Block (KEB) bridges the domain gap by incorporating driving knowledge\nto adaptively refine the generated pseudo-labels. Quantitative and qualitative\nresults with equivalent or even more impressive performance compared to\nfully-supervised state-of-the-art approaches across all three public datasets\ndemonstrate the effectiveness of the proposed method and the potential of this\ndirection. The code will be made publicly available.",
        "authors": [
            "Pengfei Zhu",
            "Mengshi Qi",
            "Xia Li",
            "Weijian Li",
            "Huadong Ma"
        ]
    },
    {
        "title": "PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images",
        "url": "http://arxiv.org/abs/2206.01256",
        "abstract": "In this paper, we propose PETRv2, a unified framework for 3D perception from\nmulti-view images. Based on PETR, PETRv2 explores the effectiveness of temporal\nmodeling, which utilizes the temporal information of previous frames to boost\n3D object detection. More specifically, we extend the 3D position embedding (3D\nPE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on\nobject position of different frames. A feature-guided position encoder is\nfurther introduced to improve the data adaptability of 3D PE. To support for\nmulti-task learning (e.g., BEV segmentation and 3D lane detection), PETRv2\nprovides a simple yet effective solution by introducing task-specific queries,\nwhich are initialized under different spaces. PETRv2 achieves state-of-the-art\nperformance on 3D object detection, BEV segmentation and 3D lane detection.\nDetailed robustness analysis is also conducted on PETR framework. We hope\nPETRv2 can serve as a strong baseline for 3D perception. Code is available at\n\\url{https://github.com/megvii-research/PETR}.",
        "authors": [
            "Yingfei Liu",
            "Junjie Yan",
            "Fan Jia",
            "Shuailin Li",
            "Aqi Gao",
            "Tiancai Wang",
            "Xiangyu Zhang",
            "Jian Sun"
        ]
    },
    {
        "title": "Out-of-Domain GAN Inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation",
        "url": "http://arxiv.org/abs/2212.09262",
        "abstract": "The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by\nOut-Of-Domain (OOD) areas (e.g., background, accessories) in the image.\nDetecting the OOD areas beyond the generation ability of the pre-trained model\nand blending these regions with the input image can enhance fidelity. The\n\"invertibility mask\" figures out these OOD areas, and existing methods predict\nthe mask with the reconstruction error. However, the estimated mask is usually\ninaccurate due to the influence of the reconstruction error in the In-Domain\n(ID) area. In this paper, we propose a novel framework that enhances the\nfidelity of human face inversion by designing a new module to decompose the\ninput images to ID and OOD partitions with invertibility masks. Unlike previous\nworks, our invertibility detector is simultaneously learned with a spatial\nalignment module. We iteratively align the generated features to the input\ngeometry and reduce the reconstruction error in the ID regions. Thus, the OOD\nareas are more distinguishable and can be precisely predicted. Then, we improve\nthe fidelity of our results by blending the OOD areas from the input image with\nthe ID GAN inversion results. Our method produces photo-realistic results for\nreal-world human face image inversion and manipulation. Extensive experiments\ndemonstrate our method's superiority over existing methods in the quality of\nGAN inversion and attribute manipulation.",
        "authors": [
            "Xin Yang",
            "Xiaogang Xu",
            "Yingcong Chen"
        ]
    },
    {
        "title": "SAFE: Machine Unlearning With Shard Graphs",
        "url": "http://arxiv.org/abs/2304.13169",
        "abstract": "We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large\nmodels on a diverse collection of data while minimizing the expected cost to\nremove the influence of training samples from the trained model. This process,\nalso known as selective forgetting or unlearning, is often conducted by\npartitioning a dataset into shards, training fully independent models on each,\nthen ensembling the resulting models. Increasing the number of shards reduces\nthe expected cost to forget but at the same time it increases inference cost\nand reduces the final accuracy of the model since synergistic information\nbetween samples is lost during the independent model training. Rather than\ntreating each shard as independent, SAFE introduces the notion of a shard\ngraph, which allows incorporating limited information from other shards during\ntraining, trading off a modest increase in expected forgetting cost with a\nsignificant increase in accuracy, all while still attaining complete removal of\nresidual influence after forgetting. SAFE uses a lightweight system of adapters\nwhich can be trained while reusing most of the computations. This allows SAFE\nto be trained on shards an order-of-magnitude smaller than current\nstate-of-the-art methods (thus reducing the forgetting costs) while also\nmaintaining high accuracy, as we demonstrate empirically on fine-grained\ncomputer vision datasets.",
        "authors": [
            "Yonatan Dukler",
            "Benjamin Bowman",
            "Alessandro Achille",
            "Aditya Golatkar",
            "Ashwin Swaminathan",
            "Stefano Soatto"
        ]
    },
    {
        "title": "Learning Trajectory-Word Alignments for Video-Language Tasks",
        "url": "http://arxiv.org/abs/2301.01953",
        "abstract": "In a video, an object usually appears as the trajectory, i.e., it spans over\na few spatial but longer temporal patches, that contains abundant\nspatiotemporal contexts. However, modern Video-Language BERTs (VDL-BERTs)\nneglect this trajectory characteristic that they usually follow image-language\nBERTs (IL-BERTs) to deploy the patch-to-word (P2W) attention that may\nover-exploit trivial spatial contexts and neglect significant temporal\ncontexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word\nalignment by a newly designed trajectory-to-word (T2W) attention for solving\nvideo-language tasks. Moreover, previous VDL-BERTs usually uniformly sample a\nfew frames into the model while different trajectories have diverse graininess,\ni.e., some trajectories span longer frames and some span shorter, and using a\nfew frames will lose certain useful temporal contexts. However, simply sampling\nmore frames will also make pre-training infeasible due to the largely increased\ntraining burdens. To alleviate the problem, during the fine-tuning stage, we\ninsert a novel Hierarchical Frame-Selector (HFS) module into the video encoder.\nHFS gradually selects the suitable frames conditioned on the text context for\nthe later cross-modal encoder to learn better trajectory-word alignments. By\nthe proposed T2W attention and HFS, our TW-BERT achieves SOTA performances on\ntext-to-video retrieval tasks, and comparable performances on video\nquestion-answering tasks with some VDL-BERTs trained on much more data. The\ncode will be available in the supplementary material.",
        "authors": [
            "Xu Yang",
            "Zhangzikang Li",
            "Haiyang Xu",
            "Hanwang Zhang",
            "Qinghao Ye",
            "Chenliang Li",
            "Ming Yan",
            "Yu Zhang",
            "Fei Huang",
            "Songfang Huang"
        ]
    },
    {
        "title": "NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing",
        "url": "http://arxiv.org/abs/2303.11219",
        "abstract": "We present a novel method, called NeTO, for capturing 3D geometry of solid\ntransparent objects from 2D images via volume rendering. Reconstructing\ntransparent objects is a very challenging task, which is ill-suited for\ngeneral-purpose reconstruction techniques due to the specular light transport\nphenomena. Although existing refraction-tracing based methods, designed\nspecially for this task, achieve impressive results, they still suffer from\nunstable optimization and loss of fine details, since the explicit surface\nrepresentation they adopted is difficult to be optimized, and the\nself-occlusion problem is ignored for refraction-tracing. In this paper, we\npropose to leverage implicit Signed Distance Function (SDF) as surface\nrepresentation, and optimize the SDF field via volume rendering with a\nself-occlusion aware refractive ray tracing. The implicit representation\nenables our method to be capable of reconstructing high-quality reconstruction\neven with a limited set of images, and the self-occlusion aware strategy makes\nit possible for our method to accurately reconstruct the self-occluded regions.\nExperiments show that our method achieves faithful reconstruction results and\noutperforms prior works by a large margin. Visit our project page at\nhttps://www.xxlong.site/NeTO/",
        "authors": [
            "Zongcheng Li",
            "Xiaoxiao Long",
            "Yusen Wang",
            "Tuo Cao",
            "Wenping Wang",
            "Fei Luo",
            "Chunxia Xiao"
        ]
    },
    {
        "title": "DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution",
        "url": "http://arxiv.org/abs/2301.02031",
        "abstract": "We propose an effective lightweight dynamic local and global self-attention\nnetwork (DLGSANet) to solve image super-resolution. Our method explores the\nproperties of Transformers while having low computational costs. Motivated by\nthe network designs of Transformers, we develop a simple yet effective\nmulti-head dynamic local self-attention (MHDLSA) module to extract local\nfeatures efficiently. In addition, we note that existing Transformers usually\nexplore all similarities of the tokens between the queries and keys for the\nfeature aggregation. However, not all the tokens from the queries are relevant\nto those in keys, using all the similarities does not effectively facilitate\nthe high-resolution image reconstruction. To overcome this problem, we develop\na sparse global self-attention (SparseGSA) module to select the most useful\nsimilarity values so that the most useful global features can be better\nutilized for the high-resolution image reconstruction. We develop a hybrid\ndynamic-Transformer block(HDTB) that integrates the MHDLSA and SparseGSA for\nboth local and global feature exploration. To ease the network training, we\nformulate the HDTBs into a residual hybrid dynamic-Transformer group (RHDTG).\nBy embedding the RHDTGs into an end-to-end trainable network, we show that our\nproposed method has fewer network parameters and lower computational costs\nwhile achieving competitive performance against state-of-the-art ones in terms\nof accuracy. More information is available at\nhttps://neonleexiang.github.io/DLGSANet/",
        "authors": [
            "Xiang Li",
            "Jinshan Pan",
            "Jinhui Tang",
            "Jiangxin Dong"
        ]
    },
    {
        "title": "Adaptive Reordering Sampler with Neurally Guided MAGSAC",
        "url": "http://arxiv.org/abs/2111.14093",
        "abstract": "We propose a new sampler for robust estimators that always selects the sample\nwith the highest probability of consisting only of inliers. After every\nunsuccessful iteration, the inlier probabilities are updated in a principled\nway via a Bayesian approach. The probabilities obtained by the deep network are\nused as prior (so-called neural guidance) inside the sampler. Moreover, we\nintroduce a new loss that exploits, in a geometrically justifiable manner, the\norientation and scale that can be estimated for any type of feature, e.g., SIFT\nor SuperPoint, to estimate two-view geometry. The new loss helps to learn\nhigher-order information about the underlying scene geometry. Benefiting from\nthe new sampler and the proposed loss, we combine the neural guidance with the\nstate-of-the-art MAGSAC++. Adaptive Reordering Sampler with Neurally Guided\nMAGSAC (ARS-MAGSAC) is superior to the state-of-the-art in terms of accuracy\nand run-time on the PhotoTourism and KITTI datasets for essential and\nfundamental matrix estimation. The code and trained models are available at\nhttps://github.com/weitong8591/ars_magsac.",
        "authors": [
            "Tong Wei",
            "Jiri Matas",
            "Daniel Barath"
        ]
    },
    {
        "title": "Black-Box Unsupervised Domain Adaptation with Bi-Directional Atkinson-Shiffrin Memory",
        "url": "http://arxiv.org/abs/2308.13236",
        "abstract": "Black-box unsupervised domain adaptation (UDA) learns with source predictions\nof target data without accessing either source data or source models during\ntraining, and it has clear superiority in data privacy and flexibility in\ntarget network selection. However, the source predictions of target data are\noften noisy and training with them is prone to learning collapses. We propose\nBiMem, a bi-directional memorization mechanism that learns to remember useful\nand representative information to correct noisy pseudo labels on the fly,\nleading to robust black-box UDA that can generalize across different visual\nrecognition tasks. BiMem constructs three types of memory, including sensory\nmemory, short-term memory, and long-term memory, which interact in a\nbi-directional manner for comprehensive and robust memorization of learnt\nfeatures. It includes a forward memorization flow that identifies and stores\nuseful features and a backward calibration flow that rectifies features' pseudo\nlabels progressively. Extensive experiments show that BiMem achieves superior\ndomain adaptation performance consistently across various visual recognition\ntasks such as image classification, semantic segmentation and object detection.",
        "authors": [
            "Jingyi Zhang",
            "Jiaxing Huang",
            "Xueying Jiang",
            "Shijian Lu"
        ]
    },
    {
        "title": "Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning",
        "url": "http://arxiv.org/abs/2309.07911",
        "abstract": "Recently, large-scale pre-trained language-image models like CLIP have shown\nextraordinary capabilities for understanding spatial contents, but naively\ntransferring such models to video recognition still suffers from unsatisfactory\ntemporal modeling capabilities. Existing methods insert tunable structures into\nor in parallel with the pre-trained model, which either requires\nback-propagation through the whole pre-trained model and is thus\nresource-demanding, or is limited by the temporal reasoning capability of the\npre-trained structure. In this work, we present DiST, which disentangles the\nlearning of spatial and temporal aspects of videos. Specifically, DiST uses a\ndual-encoder structure, where a pre-trained foundation model acts as the\nspatial encoder, and a lightweight network is introduced as the temporal\nencoder. An integration branch is inserted between the encoders to fuse\nspatio-temporal information. The disentangled spatial and temporal learning in\nDiST is highly efficient because it avoids the back-propagation of massive\npre-trained parameters. Meanwhile, we empirically show that disentangled\nlearning with an extra network for integration benefits both spatial and\ntemporal understanding. Extensive experiments on five benchmarks show that DiST\ndelivers better performance than existing state-of-the-art methods by\nconvincing gaps. When pre-training on the large-scale Kinetics-710, we achieve\n89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalability\nof DiST. Codes and models can be found in\nhttps://github.com/alibaba-mmai-research/DiST.",
        "authors": [
            "Zhiwu Qing",
            "Shiwei Zhang",
            "Ziyuan Huang",
            "Yingya Zhang",
            "Changxin Gao",
            "Deli Zhao",
            "Nong Sang"
        ]
    },
    {
        "title": "A Skeletonization Algorithm for Gradient-Based Optimization",
        "url": "http://arxiv.org/abs/2309.02527",
        "abstract": "The skeleton of a digital image is a compact representation of its topology,\ngeometry, and scale. It has utility in many computer vision applications, such\nas image description, segmentation, and registration. However, skeletonization\nhas only seen limited use in contemporary deep learning solutions. Most\nexisting skeletonization algorithms are not differentiable, making it\nimpossible to integrate them with gradient-based optimization. Compatible\nalgorithms based on morphological operations and neural networks have been\nproposed, but their results often deviate from the geometry and topology of the\ntrue medial axis. This work introduces the first three-dimensional\nskeletonization algorithm that is both compatible with gradient-based\noptimization and preserves an object's topology. Our method is exclusively\nbased on matrix additions and multiplications, convolutional operations, basic\nnon-linear functions, and sampling from a uniform probability distribution,\nallowing it to be easily implemented in any major deep learning library. In\nbenchmarking experiments, we prove the advantages of our skeletonization\nalgorithm compared to non-differentiable, morphological, and\nneural-network-based baselines. Finally, we demonstrate the utility of our\nalgorithm by integrating it with two medical image processing applications that\nuse gradient-based optimization: deep-learning-based blood vessel segmentation,\nand multimodal registration of the mandible in computed tomography and magnetic\nresonance images.",
        "authors": [
            "Martin J. Menten",
            "Johannes C. Paetzold",
            "Veronika A. Zimmer",
            "Suprosanna Shit",
            "Ivan Ezhov",
            "Robbie Holland",
            "Monika Probst",
            "Julia A. Schnabel",
            "Daniel Rueckert"
        ]
    },
    {
        "title": "V3Det: Vast Vocabulary Visual Detection Dataset",
        "url": "http://arxiv.org/abs/2304.03752",
        "abstract": "Recent advances in detecting arbitrary objects in the real world are trained\nand evaluated on object detection datasets with a relatively restricted\nvocabulary. To facilitate the development of more general visual object\ndetection, we propose V3Det, a vast vocabulary visual detection dataset with\nprecisely annotated bounding boxes on massive images. V3Det has several\nappealing properties: 1) Vast Vocabulary: It contains bounding boxes of objects\nfrom 13,204 categories on real-world images, which is 10 times larger than the\nexisting large vocabulary object detection dataset, e.g., LVIS. 2) Hierarchical\nCategory Organization: The vast vocabulary of V3Det is organized by a\nhierarchical category tree which annotates the inclusion relationship among\ncategories, encouraging the exploration of category relationships in vast and\nopen vocabulary object detection. 3) Rich Annotations: V3Det comprises\nprecisely annotated objects in 243k images and professional descriptions of\neach category written by human experts and a powerful chatbot. By offering a\nvast exploration space, V3Det enables extensive benchmarks on both vast and\nopen vocabulary object detection, leading to new observations, practices, and\ninsights for future research. It has the potential to serve as a cornerstone\ndataset for developing more general visual perception systems. V3Det is\navailable at https://v3det.openxlab.org.cn/.",
        "authors": [
            "Jiaqi Wang",
            "Pan Zhang",
            "Tao Chu",
            "Yuhang Cao",
            "Yujie Zhou",
            "Tong Wu",
            "Bin Wang",
            "Conghui He",
            "Dahua Lin"
        ]
    },
    {
        "title": "Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation",
        "url": "http://arxiv.org/abs/2308.05438",
        "abstract": "One critical challenge in 6D object pose estimation from a single RGBD image\nis efficient integration of two different modalities, i.e., color and depth. In\nthis work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)\nblock that can aggregate cross-modality features for improving pose estimation.\nUnlike existing fusion methods, the proposed DFTr can better model\ncross-modality semantic correlation by leveraging their semantic similarity,\nsuch that globally enhanced features from different modalities can be better\nintegrated for improved information extraction. Moreover, to further improve\nrobustness and efficiency, we introduce a novel weighted vector-wise voting\nalgorithm that employs a non-iterative global optimization strategy for precise\n3D keypoint localization while achieving near real-time inference. Extensive\nexperiments show the effectiveness and strong generalization capability of our\nproposed 3D keypoint voting algorithm. Results on four widely used benchmarks\nalso demonstrate that our method outperforms the state-of-the-art methods by\nlarge margins.",
        "authors": [
            "Jun Zhou",
            "Kai Chen",
            "Linlin Xu",
            "Qi Dou",
            "Jing Qin"
        ]
    },
    {
        "title": "DDP: Diffusion Model for Dense Visual Prediction",
        "url": "http://arxiv.org/abs/2303.17559",
        "abstract": "We propose a simple, efficient, yet powerful framework for dense visual\npredictions based on the conditional diffusion pipeline. Our approach follows a\n\"noise-to-map\" generative paradigm for prediction by progressively removing\nnoise from a random Gaussian distribution, guided by the image. The method,\ncalled DDP, efficiently extends the denoising diffusion process into the modern\nperception pipeline. Without task-specific design and architecture\ncustomization, DDP is easy to generalize to most dense prediction tasks, e.g.,\nsemantic segmentation and depth estimation. In addition, DDP shows attractive\nproperties such as dynamic inference and uncertainty awareness, in contrast to\nprevious single-step discriminative methods. We show top results on three\nrepresentative tasks with six diverse benchmarks, without tricks, DDP achieves\nstate-of-the-art or competitive performance on each task compared to the\nspecialist counterparts. For example, semantic segmentation (83.9 mIoU on\nCityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation\n(0.05 REL on KITTI). We hope that our approach will serve as a solid baseline\nand facilitate future research",
        "authors": [
            "Yuanfeng Ji",
            "Zhe Chen",
            "Enze Xie",
            "Lanqing Hong",
            "Xihui Liu",
            "Zhaoqiang Liu",
            "Tong Lu",
            "Zhenguo Li",
            "Ping Luo"
        ]
    },
    {
        "title": "Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning",
        "url": "http://arxiv.org/abs/2212.06486",
        "abstract": "In contrastive self-supervised learning, the common way to learn\ndiscriminative representation is to pull different augmented \"views\" of the\nsame image closer while pushing all other images further apart, which has been\nproven to be effective. However, it is unavoidable to construct undesirable\nviews containing different semantic concepts during the augmentation procedure.\nIt would damage the semantic consistency of representation to pull these\naugmentations closer in the feature space indiscriminately. In this study, we\nintroduce feature-level augmentation and propose a novel semantics-consistent\nfeature search (SCFS) method to mitigate this negative effect. The main idea of\nSCFS is to adaptively search semantics-consistent features to enhance the\ncontrast between semantics-consistent regions in different augmentations. Thus,\nthe trained model can learn to focus on meaningful object regions, improving\nthe semantic representation ability. Extensive experiments conducted on\ndifferent datasets and tasks demonstrate that SCFS effectively improves the\nperformance of self-supervised learning and achieves state-of-the-art\nperformance on different downstream tasks.",
        "authors": [
            "Kaiyou Song",
            "Shan Zhang",
            "Zihao An",
            "Zimeng Luo",
            "Tong Wang",
            "Jin Xie"
        ]
    },
    {
        "title": "GridMM: Grid Memory Map for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2307.12907",
        "abstract": "Vision-and-language navigation (VLN) enables the agent to navigate to a\nremote location following the natural language instruction in 3D environments.\nTo represent the previously visited environment, most approaches for VLN\nimplement memory using recurrent states, topological maps, or top-down semantic\nmaps. In contrast to these approaches, we build the top-down egocentric and\ndynamically growing Grid Memory Map (i.e., GridMM) to structure the visited\nenvironment. From a global perspective, historical observations are projected\ninto a unified grid map in a top-down view, which can better represent the\nspatial relations of the environment. From a local perspective, we further\npropose an instruction relevance aggregation method to capture fine-grained\nvisual clues in each grid region. Extensive experiments are conducted on both\nthe REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE\ndataset in the continuous environments, showing the superiority of our proposed\nmethod.",
        "authors": [
            "Zihan Wang",
            "Xiangyang Li",
            "Jiahao Yang",
            "Yeqi Liu",
            "Shuqiang Jiang"
        ]
    },
    {
        "title": "LAC - Latent Action Composition for Skeleton-based Action Segmentation",
        "url": "http://arxiv.org/abs/2308.14500",
        "abstract": "Skeleton-based action segmentation requires recognizing composable actions in\nuntrimmed videos. Current approaches decouple this problem by first extracting\nlocal visual features from skeleton sequences and then processing them by a\ntemporal model to classify frame-wise actions. However, their performances\nremain limited as the visual features cannot sufficiently express composable\nactions. In this context, we propose Latent Action Composition (LAC), a novel\nself-supervised framework aiming at learning from synthesized composable\nmotions for skeleton-based action segmentation. LAC is composed of a novel\ngeneration module towards synthesizing new sequences. Specifically, we design a\nlinear latent space in the generator to represent primitive motion. New\ncomposed motions can be synthesized by simply performing arithmetic operations\non latent representations of multiple input skeleton sequences. LAC leverages\nsuch synthesized sequences, which have large diversity and complexity, for\nlearning visual representations of skeletons in both sequence and frame spaces\nvia contrastive learning. The resulting visual encoder has a high expressive\npower and can be effectively transferred onto action segmentation tasks by\nend-to-end fine-tuning without the need for additional temporal models. We\nconduct a study focusing on transfer-learning and we show that representations\nlearned from pre-trained LAC outperform the state-of-the-art by a large margin\non TSU, Charades, PKU-MMD datasets.",
        "authors": [
            "Di Yang",
            "Yaohui Wang",
            "Antitza Dantcheva",
            "Quan Kong",
            "Lorenzo Garattoni",
            "Gianpiero Francesca",
            "Francois Bremond"
        ]
    },
    {
        "title": "Learning Vision-and-Language Navigation from YouTube Videos",
        "url": "http://arxiv.org/abs/2307.11984",
        "abstract": "Vision-and-language navigation (VLN) requires an embodied agent to navigate\nin realistic 3D environments using natural language instructions. Existing VLN\nmethods suffer from training on small-scale environments or unreasonable\npath-instruction datasets, limiting the generalization to unseen environments.\nThere are massive house tour videos on YouTube, providing abundant real\nnavigation experiences and layout information. However, these videos have not\nbeen explored for VLN before. In this paper, we propose to learn an agent from\nthese videos by creating a large-scale dataset which comprises reasonable\npath-instruction pairs from house tour videos and pre-training the agent on it.\nTo achieve this, we have to tackle the challenges of automatically constructing\npath-instruction pairs and exploiting real layout knowledge from raw and\nunlabeled videos. To address these, we first leverage an entropy-based method\nto construct the nodes of a path trajectory. Then, we propose an action-aware\ngenerator for generating instructions from unlabeled trajectories. Last, we\ndevise a trajectory judgment pretext task to encourage the agent to mine the\nlayout knowledge. Experimental results show that our method achieves\nstate-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code\nis available at https://github.com/JeremyLinky/YouTube-VLN",
        "authors": [
            "Kunyang Lin",
            "Peihao Chen",
            "Diwei Huang",
            "Thomas H. Li",
            "Mingkui Tan",
            "Chuang Gan"
        ]
    },
    {
        "title": "Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting",
        "url": "http://arxiv.org/abs/2307.08243",
        "abstract": "Hand trajectory forecasting from egocentric views is crucial for enabling a\nprompt understanding of human intentions when interacting with AR/VR systems.\nHowever, existing methods handle this problem in a 2D image space which is\ninadequate for 3D real-world applications. In this paper, we set up an\negocentric 3D hand trajectory forecasting task that aims to predict hand\ntrajectories in a 3D space from early observed RGB videos in a first-person\nview. To fulfill this goal, we propose an uncertainty-aware state space\nTransformer (USST) that takes the merits of the attention mechanism and\naleatoric uncertainty within the framework of the classical state-space model.\nThe model can be further enhanced by the velocity constraint and visual prompt\ntuning (VPT) on large vision transformers. Moreover, we develop an annotation\nworkflow to collect 3D hand trajectories with high quality. Experimental\nresults on H2O and EgoPAT3D datasets demonstrate the superiority of USST for\nboth 2D and 3D trajectory forecasting. The code and datasets are publicly\nreleased: https://actionlab-cv.github.io/EgoHandTrajPred.",
        "authors": [
            "Wentao Bao",
            "Lele Chen",
            "Libing Zeng",
            "Zhong Li",
            "Yi Xu",
            "Junsong Yuan",
            "Yu Kong"
        ]
    },
    {
        "title": "Pretrained Language Models as Visual Planners for Human Assistance",
        "url": "http://arxiv.org/abs/2304.09179",
        "abstract": "In our pursuit of advancing multi-modal AI assistants capable of guiding\nusers to achieve complex multi-step goals, we propose the task of \"Visual\nPlanning for Assistance (VPA)\". Given a succinct natural language goal, e.g.,\n\"make a shelf\", and a video of the user's progress so far, the aim of VPA is to\ndevise a plan, i.e., a sequence of actions such as \"sand shelf\", \"paint shelf\",\netc. to realize the specified goal. This requires assessing the user's progress\nfrom the (untrimmed) video, and relating it to the requirements of natural\nlanguage goal, i.e., which actions to select and in what order? Consequently,\nthis requires handling long video history and arbitrarily complex action\ndependencies. To address these challenges, we decompose VPA into video action\nsegmentation and forecasting. Importantly, we experiment by formulating the\nforecasting step as a multi-modal sequence modeling problem, allowing us to\nleverage the strength of pre-trained LMs (as the sequence model). This novel\napproach, which we call Visual Language Model based Planner (VLaMP),\noutperforms baselines across a suite of metrics that gauge the quality of the\ngenerated plans. Furthermore, through comprehensive ablations, we also isolate\nthe value of each component--language pre-training, visual observations, and\ngoal information. We have open-sourced all the data, model checkpoints, and\ntraining code.",
        "authors": [
            "Dhruvesh Patel",
            "Hamid Eghbalzadeh",
            "Nitin Kamra",
            "Michael Louis Iuzzolino",
            "Unnat Jain",
            "Ruta Desai"
        ]
    },
    {
        "title": "Dynamic Point Fields",
        "url": "http://arxiv.org/abs/2304.02626",
        "abstract": "Recent years have witnessed significant progress in the field of neural\nsurface reconstruction. While the extensive focus was put on volumetric and\nimplicit approaches, a number of works have shown that explicit graphics\nprimitives such as point clouds can significantly reduce computational\ncomplexity, without sacrificing the reconstructed surface quality. However,\nless emphasis has been put on modeling dynamic surfaces with point primitives.\nIn this work, we present a dynamic point field model that combines the\nrepresentational benefits of explicit point-based graphics with implicit\ndeformation networks to allow efficient modeling of non-rigid 3D surfaces.\nUsing explicit surface primitives also allows us to easily incorporate\nwell-established constraints such as-isometric-as-possible regularisation.\nWhile learning this deformation model is prone to local optima when trained in\na fully unsupervised manner, we propose to additionally leverage semantic\ninformation such as keypoint dynamics to guide the deformation learning. We\ndemonstrate our model with an example application of creating an expressive\nanimatable human avatar from a collection of 3D scans. Here, previous methods\nmostly rely on variants of the linear blend skinning paradigm, which\nfundamentally limits the expressivity of such models when dealing with complex\ncloth appearances such as long skirts. We show the advantages of our dynamic\npoint field framework in terms of its representational power, learning\nefficiency, and robustness to out-of-distribution novel poses.",
        "authors": [
            "Sergey Prokudin",
            "Qianli Ma",
            "Maxime Raafat",
            "Julien Valentin",
            "Siyu Tang"
        ]
    },
    {
        "title": "Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping",
        "url": "http://arxiv.org/abs/2308.06112",
        "abstract": "Visual Speech Recognition (VSR) differs from the common perception tasks as\nit requires deeper reasoning over the video sequence, even by human experts.\nDespite the recent advances in VSR, current approaches rely on labeled data to\nfully train or finetune their models predicting the target speech. This hinders\ntheir ability to generalize well beyond the training set and leads to\nperformance degeneration under out-of-distribution challenging scenarios.\nUnlike previous works that involve auxiliary losses or complex training\nprocedures and architectures, we propose a simple approach, named Lip2Vec that\nis based on learning a prior model. Given a robust visual speech encoder, this\nnetwork maps the encoded latent representations of the lip sequence to their\ncorresponding latents from the audio pair, which are sufficiently invariant for\neffective text decoding. The generated audio representation is then decoded to\ntext using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed\nmodel compares favorably with fully-supervised learning methods on the LRS3\ndataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable\nperformance on the VoxCeleb test set. We believe that reprogramming the VSR as\nan ASR task narrows the performance gap between the two and paves the way for\nmore flexible formulations of lip reading.",
        "authors": [
            "Yasser Abdelaziz Dahou Djilali",
            "Sanath Narayan",
            "Haithem Boussaid",
            "Ebtessam Almazrouei",
            "Merouane Debbah"
        ]
    },
    {
        "title": "Random Boxes Are Open-world Object Detectors",
        "url": "http://arxiv.org/abs/2307.08249",
        "abstract": "We show that classifiers trained with random region proposals achieve\nstate-of-the-art Open-world Object Detection (OWOD): they can not only maintain\nthe accuracy of the known objects (w/ training labels), but also considerably\nimprove the recall of unknown ones (w/o training labels). Specifically, we\npropose RandBox, a Fast R-CNN based architecture trained on random proposals at\neach training iteration, surpassing existing Faster R-CNN and Transformer based\nOWOD. Its effectiveness stems from the following two benefits introduced by\nrandomness. First, as the randomization is independent of the distribution of\nthe limited known objects, the random proposals become the instrumental\nvariable that prevents the training from being confounded by the known objects.\nSecond, the unbiased training encourages more proposal explorations by using\nour proposed matching score that does not penalize the random proposals whose\nprediction scores do not match the known objects. On two benchmarks:\nPascal-VOC/MS-COCO and LVIS, RandBox significantly outperforms the previous\nstate-of-the-art in all metrics. We also detail the ablations on randomization\nand loss designs. Codes are available at https://github.com/scuwyh2000/RandBox.",
        "authors": [
            "Yanghao Wang",
            "Zhongqi Yue",
            "Xian-Sheng Hua",
            "Hanwang Zhang"
        ]
    },
    {
        "title": "DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models",
        "url": "http://arxiv.org/abs/2211.12131",
        "abstract": "Scene extrapolation -- the idea of generating novel views by flying into a\ngiven image -- is a promising, yet challenging task. For each predicted frame,\na joint inpainting and 3D refinement problem has to be solved, which is ill\nposed and includes a high level of ambiguity. Moreover, training data for\nlong-range scenes is difficult to obtain and usually lacks sufficient views to\ninfer accurate camera poses. We introduce DiffDreamer, an unsupervised\nframework capable of synthesizing novel views depicting a long camera\ntrajectory while training solely on internet-collected images of nature scenes.\nUtilizing the stochastic nature of the guided denoising steps, we train the\ndiffusion models to refine projected RGBD images but condition the denoising\nsteps on multiple past and future frames for inference. We demonstrate that\nimage-conditioned diffusion models can effectively perform long-range scene\nextrapolation while preserving consistency significantly better than prior\nGAN-based methods. DiffDreamer is a powerful and efficient solution for scene\nextrapolation, producing impressive results despite limited supervision.\nProject page: https://primecai.github.io/diffdreamer.",
        "authors": [
            "Shengqu Cai",
            "Eric Ryan Chan",
            "Songyou Peng",
            "Mohamad Shahbazi",
            "Anton Obukhov",
            "Luc Van Gool",
            "Gordon Wetzstein"
        ]
    },
    {
        "title": "Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images",
        "url": "http://arxiv.org/abs/2308.11015",
        "abstract": "We propose a novel transformer-based framework that reconstructs two high\nfidelity hands from multi-view RGB images. Unlike existing hand pose estimation\nmethods, where one typically trains a deep network to regress hand model\nparameters from single RGB image, we consider a more challenging problem\nsetting where we directly regress the absolute root poses of two-hands with\nextended forearm at high resolution from egocentric view. As existing datasets\nare either infeasible for egocentric viewpoints or lack background variations,\nwe create a large-scale synthetic dataset with diverse scenarios and collect a\nreal dataset from multi-calibrated camera setup to verify our proposed\nmulti-view image feature fusion strategy. To make the reconstruction physically\nplausible, we propose two strategies: (i) a coarse-to-fine spectral graph\nconvolution decoder to smoothen the meshes during upsampling and (ii) an\noptimisation-based refinement stage at inference to prevent self-penetrations.\nThrough extensive quantitative and qualitative evaluations, we show that our\nframework is able to produce realistic two-hand reconstructions and demonstrate\nthe generalisation of synthetic-trained models to real data, as well as\nreal-time AR/VR applications.",
        "authors": [
            "Tze Ho Elden Tse",
            "Franziska Mueller",
            "Zhengyang Shen",
            "Danhang Tang",
            "Thabo Beeler",
            "Mingsong Dou",
            "Yinda Zhang",
            "Sasa Petrovic",
            "Hyung Jin Chang",
            "Jonathan Taylor",
            "Bardia Doosti"
        ]
    },
    {
        "title": "SMMix: Self-Motivated Image Mixing for Vision Transformers",
        "url": "http://arxiv.org/abs/2212.12977",
        "abstract": "CutMix is a vital augmentation strategy that determines the performance and\ngeneralization ability of vision transformers (ViTs). However, the\ninconsistency between the mixed images and the corresponding labels harms its\nefficacy. Existing CutMix variants tackle this problem by generating more\nconsistent mixed images or more precise mixed labels, but inevitably introduce\nheavy training overhead or require extra information, undermining ease of use.\nTo this end, we propose an novel and effective Self-Motivated image Mixing\nmethod (SMMix), which motivates both image and label enhancement by the model\nunder training itself. Specifically, we propose a max-min attention region\nmixing approach that enriches the attention-focused objects in the mixed\nimages. Then, we introduce a fine-grained label assignment technique that\nco-trains the output tokens of mixed images with fine-grained supervision.\nMoreover, we devise a novel feature consistency constraint to align features\nfrom mixed and unmixed images. Due to the subtle designs of the self-motivated\nparadigm, our SMMix is significant in its smaller training overhead and better\nperformance than other CutMix variants. In particular, SMMix improves the\naccuracy of DeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L by more than +1% on\nImageNet-1k. The generalization capability of our method is also demonstrated\non downstream tasks and out-of-distribution datasets. Our project is\nanonymously available at https://github.com/ChenMnZ/SMMix.",
        "authors": [
            "Mengzhao Chen",
            "Mingbao Lin",
            "ZhiHang Lin",
            "Yuxin Zhang",
            "Fei Chao",
            "Rongrong Ji"
        ]
    },
    {
        "title": "Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2308.04061",
        "abstract": "Adversarial robustness is a research area that has recently received a lot of\nattention in the quest for trustworthy artificial intelligence. However, recent\nworks on adversarial robustness have focused on supervised learning where it is\nassumed that labeled data is plentiful. In this paper, we investigate\nsemi-supervised adversarial training where labeled data is scarce. We derive\ntwo upper bounds for the robust risk and propose a regularization term for\nunlabeled data motivated by these two upper bounds. Then, we develop a\nsemi-supervised adversarial training algorithm that combines the proposed\nregularization term with knowledge distillation using a semi-supervised teacher\n(i.e., a teacher model trained using a semi-supervised learning algorithm). Our\nexperiments show that our proposed algorithm achieves state-of-the-art\nperformance with significant margins compared to existing algorithms. In\nparticular, compared to supervised learning algorithms, performance of our\nproposed algorithm is not much worse even when the amount of labeled data is\nvery small. For example, our algorithm with only 8\\% labeled data is comparable\nto supervised adversarial training algorithms that use all labeled data, both\nin terms of standard and robust accuracies on CIFAR-10.",
        "authors": [
            "Dongyoon Yang",
            "Insung Kong",
            "Yongdai Kim"
        ]
    },
    {
        "title": "Instance Neural Radiance Field",
        "url": "http://arxiv.org/abs/2304.04395",
        "abstract": "This paper presents one of the first learning-based NeRF 3D instance\nsegmentation pipelines, dubbed as Instance Neural Radiance Field, or Instance\nNeRF. Taking a NeRF pretrained from multi-view RGB images as input, Instance\nNeRF can learn 3D instance segmentation of a given scene, represented as an\ninstance field component of the NeRF model. To this end, we adopt a 3D\nproposal-based mask prediction network on the sampled volumetric features from\nNeRF, which generates discrete 3D instance masks. The coarse 3D mask prediction\nis then projected to image space to match 2D segmentation masks from different\nviews generated by existing panoptic segmentation models, which are used to\nsupervise the training of the instance field. Notably, beyond generating\nconsistent 2D segmentation maps from novel views, Instance NeRF can query\ninstance information at any 3D point, which greatly enhances NeRF object\nsegmentation and manipulation. Our method is also one of the first to achieve\nsuch results in pure inference. Experimented on synthetic and real-world NeRF\ndatasets with complex indoor scenes, Instance NeRF surpasses previous NeRF\nsegmentation works and competitive 2D segmentation methods in segmentation\nperformance on unseen views. Watch the demo video at\nhttps://youtu.be/wW9Bme73coI. Code and data are available at\nhttps://github.com/lyclyc52/Instance_NeRF.",
        "authors": [
            "Yichen Liu",
            "Benran Hu",
            "Junkai Huang",
            "Yu-Wing Tai",
            "Chi-Keung Tang"
        ]
    },
    {
        "title": "One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training",
        "url": "http://arxiv.org/abs/2308.07934",
        "abstract": "Deep neural networks (DNNs) are widely deployed on real-world devices.\nConcerns regarding their security have gained great attention from researchers.\nRecently, a new weight modification attack called bit flip attack (BFA) was\nproposed, which exploits memory fault inject techniques such as row hammer to\nattack quantized models in the deployment stage. With only a few bit flips, the\ntarget model can be rendered useless as a random guesser or even be implanted\nwith malicious functionalities. In this work, we seek to further reduce the\nnumber of bit flips. We propose a training-assisted bit flip attack, in which\nthe adversary is involved in the training stage to build a high-risk model to\nrelease. This high-risk model, obtained coupled with a corresponding malicious\nmodel, behaves normally and can escape various detection methods. The results\non benchmark datasets show that an adversary can easily convert this high-risk\nbut normal model to a malicious one on victim's side by \\textbf{flipping only\none critical bit} on average in the deployment stage. Moreover, our attack\nstill poses a significant threat even when defenses are employed. The codes for\nreproducing main experiments are available at\n\\url{https://github.com/jianshuod/TBA}.",
        "authors": [
            "Jianshuo Dong",
            "Han Qiu",
            "Yiming Li",
            "Tianwei Zhang",
            "Yuanjie Li",
            "Zeqi Lai",
            "Chao Zhang",
            "Shu-Tao Xia"
        ]
    },
    {
        "title": "CLIPTER: Looking at the Bigger Picture in Scene Text Recognition",
        "url": "http://arxiv.org/abs/2301.07464",
        "abstract": "Reading text in real-world scenarios often requires understanding the context\nsurrounding it, especially when dealing with poor-quality text. However,\ncurrent scene text recognizers are unaware of the bigger picture as they\noperate on cropped text images. In this study, we harness the representative\ncapabilities of modern vision-language models, such as CLIP, to provide\nscene-level information to the crop-based recognizer. We achieve this by fusing\na rich representation of the entire image, obtained from the vision-language\nmodel, with the recognizer word-level features via a gated cross-attention\nmechanism. This component gradually shifts to the context-enhanced\nrepresentation, allowing for stable fine-tuning of a pretrained recognizer. We\ndemonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP\nTExt Recognition), on leading text recognition architectures and achieve\nstate-of-the-art results across multiple benchmarks. Furthermore, our analysis\nhighlights improved robustness to out-of-vocabulary words and enhanced\ngeneralization in low-data regimes.",
        "authors": [
            "Aviad Aberdam",
            "David Bensa\u00efd",
            "Alona Golts",
            "Roy Ganz",
            "Oren Nuriel",
            "Royee Tichauer",
            "Shai Mazor",
            "Ron Litman"
        ]
    },
    {
        "title": "Revisiting Scene Text Recognition: A Data Perspective",
        "url": "http://arxiv.org/abs/2307.08723",
        "abstract": "This paper aims to re-assess scene text recognition (STR) from a\ndata-oriented perspective. We begin by revisiting the six commonly used\nbenchmarks in STR and observe a trend of performance saturation, whereby only\n2.91% of the benchmark images cannot be accurately recognized by an ensemble of\n13 representative models. While these results are impressive and suggest that\nSTR could be considered solved, however, we argue that this is primarily due to\nthe less challenging nature of the common benchmarks, thus concealing the\nunderlying issues that STR faces. To this end, we consolidate a large-scale\nreal STR dataset, namely Union14M, which comprises 4 million labeled images and\n10 million unlabeled images, to assess the performance of STR models in more\ncomplex real-world scenarios. Our experiments demonstrate that the 13 models\ncan only achieve an average accuracy of 66.53% on the 4 million labeled images,\nindicating that STR still faces numerous challenges in the real world. By\nanalyzing the error patterns of the 13 models, we identify seven open\nchallenges in STR and develop a challenge-driven benchmark consisting of eight\ndistinct subsets to facilitate further progress in the field. Our exploration\ndemonstrates that STR is far from being solved and leveraging data may be a\npromising solution. In this regard, we find that utilizing the 10 million\nunlabeled images through self-supervised pre-training can significantly improve\nthe robustness of STR model in real-world scenarios and leads to\nstate-of-the-art performance.",
        "authors": [
            "Qing Jiang",
            "Jiapeng Wang",
            "Dezhi Peng",
            "Chongyu Liu",
            "Lianwen Jin"
        ]
    },
    {
        "title": "The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion",
        "url": "http://arxiv.org/abs/2309.04509",
        "abstract": "In recent years, video generation has become a prominent generative tool and\nhas drawn significant attention. However, there is little consideration in\naudio-to-video generation, though audio contains unique qualities like temporal\nsemantics and magnitude. Hence, we propose The Power of Sound (TPoS) model to\nincorporate audio input that includes both changeable temporal semantics and\nmagnitude. To generate video frames, TPoS utilizes a latent stable diffusion\nmodel with textual semantic information, which is then guided by the sequential\naudio embedding from our pretrained Audio Encoder. As a result, this method\nproduces audio reactive video contents. We demonstrate the effectiveness of\nTPoS across various tasks and compare its results with current state-of-the-art\ntechniques in the field of audio-to-video generation. More examples are\navailable at https://ku-vai.github.io/TPoS/",
        "authors": [
            "Yujin Jeong",
            "Wonjeong Ryoo",
            "Seunghyun Lee",
            "Dabin Seo",
            "Wonmin Byeon",
            "Sangpil Kim",
            "Jinkyu Kim"
        ]
    },
    {
        "title": "SOCS: Semantically-Aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations",
        "url": "http://arxiv.org/abs/2303.10346",
        "abstract": "Most learning-based approaches to category-level 6D pose estimation are\ndesign around normalized object coordinate space (NOCS). While being\nsuccessful, NOCS-based methods become inaccurate and less robust when handling\nobjects of a category containing significant intra-category shape variations.\nThis is because the object coordinates induced by global and rigid alignment of\nobjects are semantically incoherent, making the coordinate regression hard to\nlearn and generalize. We propose Semantically-aware Object Coordinate Space\n(SOCS) built by warping-and-aligning the objects guided by a sparse set of\nkeypoints with semantically meaningful correspondence. SOCS is semantically\ncoherent: Any point on the surface of a object can be mapped to a semantically\nmeaningful location in SOCS, allowing for accurate pose and size estimation\nunder large shape variations. To learn effective coordinate regression to SOCS,\nwe propose a novel multi-scale coordinate-based attention network. Evaluations\ndemonstrate that our method is easy to train, well-generalizing for large\nintra-category shape variations and robust to inter-object occlusions.",
        "authors": [
            "Boyan Wan",
            "Yifei Shi",
            "Kai Xu"
        ]
    },
    {
        "title": "DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars",
        "url": "http://arxiv.org/abs/2303.09375",
        "abstract": "We present DINAR, an approach for creating realistic rigged fullbody avatars\nfrom single RGB images. Similarly to previous works, our method uses neural\ntextures combined with the SMPL-X body model to achieve photo-realistic quality\nof avatars while keeping them easy to animate and fast to infer. To restore the\ntexture, we use a latent diffusion model and show how such model can be trained\nin the neural texture space. The use of the diffusion model allows us to\nrealistically reconstruct large unseen regions such as the back of a person\ngiven the frontal view. The models in our pipeline are trained using 2D images\nand videos only. In the experiments, our approach achieves state-of-the-art\nrendering quality and good generalization to new poses and viewpoints. In\nparticular, the approach improves state-of-the-art on the SnapshotPeople public\nbenchmark.",
        "authors": [
            "David Svitov",
            "Dmitrii Gudkov",
            "Renat Bashirov",
            "Victor Lempitsky"
        ]
    },
    {
        "title": "ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices",
        "url": "http://arxiv.org/abs/2303.09730",
        "abstract": "Neural Architecture Search (NAS) has shown promising performance in the\nautomatic design of vision transformers (ViT) exceeding 1G FLOPs. However,\ndesigning lightweight and low-latency ViT models for diverse mobile devices\nremains a big challenge. In this work, we propose ElasticViT, a two-stage NAS\napproach that trains a high-quality ViT supernet over a very large search space\nthat supports a wide range of mobile devices, and then searches an optimal\nsub-network (subnet) for direct deployment. However, prior supernet training\nmethods that rely on uniform sampling suffer from the gradient conflict issue:\nthe sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G\nFLOPs), leading to different optimization directions and inferior performance.\nTo address this challenge, we propose two novel sampling techniques:\ncomplexity-aware sampling and performance-aware sampling. Complexity-aware\nsampling limits the FLOPs difference among the subnets sampled across adjacent\ntraining steps, while covering different-sized subnets in the search space.\nPerformance-aware sampling further selects subnets that have good accuracy,\nwhich can reduce gradient conflicts and improve supernet quality. Our\ndiscovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to\n80.0% on ImageNet from 60M to 800M FLOPs without extra retraining,\noutperforming all prior CNNs and ViTs in terms of accuracy and latency. Our\ntiny and small models are also the first ViT models that surpass\nstate-of-the-art CNNs with significantly lower latency on mobile devices. For\ninstance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher\naccuracy.",
        "authors": [
            "Chen Tang",
            "Li Lyna Zhang",
            "Huiqiang Jiang",
            "Jiahang Xu",
            "Ting Cao",
            "Quanlu Zhang",
            "Yuqing Yang",
            "Zhi Wang",
            "Mao Yang"
        ]
    },
    {
        "title": "OmniLabel: A Challenging Benchmark for Language-Based Object Detection",
        "url": "http://arxiv.org/abs/2304.11463",
        "abstract": "Language-based object detection is a promising direction towards building a\nnatural interface to describe objects in images that goes far beyond plain\ncategory names. While recent methods show great progress in that direction,\nproper evaluation is lacking. With OmniLabel, we propose a novel task\ndefinition, dataset, and evaluation metric. The task subsumes standard- and\nopen-vocabulary detection as well as referring expressions. With more than 28K\nunique object descriptions on over 25K images, OmniLabel provides a challenging\nbenchmark with diverse and complex object descriptions in a naturally\nopen-vocabulary setting. Moreover, a key differentiation to existing benchmarks\nis that our object descriptions can refer to one, multiple or even no object,\nhence, providing negative examples in free-form text. The proposed evaluation\nhandles the large label space and judges performance via a modified average\nprecision metric, which we validate by evaluating strong language-based\nbaselines. OmniLabel indeed provides a challenging test bed for future research\non language-based detection.",
        "authors": [
            "Samuel Schulter",
            "Vijay Kumar B G",
            "Yumin Suh",
            "Konstantinos M. Dafnis",
            "Zhixing Zhang",
            "Shiyu Zhao",
            "Dimitris Metaxas"
        ]
    },
    {
        "title": "Noise-Aware Learning from Web-Crawled Image-Text Data for Image Captioning",
        "url": "http://arxiv.org/abs/2212.13563",
        "abstract": "Image captioning is one of the straightforward tasks that can take advantage\nof large-scale web-crawled data which provides rich knowledge about the visual\nworld for a captioning model. However, since web-crawled data contains\nimage-text pairs that are aligned at different levels, the inherent noises\n(e.g., misaligned pairs) make it difficult to learn a precise captioning model.\nWhile the filtering strategy can effectively remove noisy data, it leads to a\ndecrease in learnable knowledge and sometimes brings about a new problem of\ndata deficiency. To take the best of both worlds, we propose a Noise-aware\nCaptioning (NoC) framework, which learns rich knowledge from the whole\nweb-crawled data while being less affected by the noises. This is achieved by\nthe proposed alignment-level-controllable captioner, which is learned using\nalignment levels of the image-text pairs as a control signal during training.\nThe alignment-level-conditioned training allows the model to generate\nhigh-quality captions by simply setting the control signal to the desired\nalignment level at inference time. An in-depth analysis shows the effectiveness\nof our framework in handling noise. With two tasks of zero-shot captioning and\ntext-to-image retrieval using generated captions (i.e., self-retrieval), we\nalso demonstrate our model can produce high-quality captions in terms of\ndescriptiveness and distinctiveness. The code is available at\n\\url{https://github.com/kakaobrain/noc}.",
        "authors": [
            "Wooyoung Kang",
            "Jonghwan Mun",
            "Sungjun Lee",
            "Byungseok Roh"
        ]
    },
    {
        "title": "3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability",
        "url": "http://arxiv.org/abs/2307.14051",
        "abstract": "Shape generation is the practice of producing 3D shapes as various\nrepresentations for 3D content creation. Previous studies on 3D shape\ngeneration have focused on shape quality and structure, without or less\nconsidering the importance of semantic information. Consequently, such\ngenerative models often fail to preserve the semantic consistency of shape\nstructure or enable manipulation of the semantic attributes of shapes during\ngeneration. In this paper, we proposed a novel semantic generative model named\n3D Semantic Subspace Traverser that utilizes semantic attributes for\ncategory-specific 3D shape generation and editing. Our method utilizes implicit\nfunctions as the 3D shape representation and combines a novel latent-space GAN\nwith a linear subspace model to discover semantic dimensions in the local\nlatent space of 3D shapes. Each dimension of the subspace corresponds to a\nparticular semantic attribute, and we can edit the attributes of generated\nshapes by traversing the coefficients of those dimensions. Experimental results\ndemonstrate that our method can produce plausible shapes with complex\nstructures and enable the editing of semantic attributes. The code and trained\nmodels are available at\nhttps://github.com/TrepangCat/3D_Semantic_Subspace_Traverser",
        "authors": [
            "Ruowei Wang",
            "Yu Liu",
            "Pei Su",
            "Jianwei Zhang",
            "Qijun Zhao"
        ]
    },
    {
        "title": "Inherent Redundancy in Spiking Neural Networks",
        "url": "http://arxiv.org/abs/2308.08227",
        "abstract": "Spiking Neural Networks (SNNs) are well known as a promising energy-efficient\nalternative to conventional artificial neural networks. Subject to the\npreconceived impression that SNNs are sparse firing, the analysis and\noptimization of inherent redundancy in SNNs have been largely overlooked, thus\nthe potential advantages of spike-based neuromorphic computing in accuracy and\nenergy efficiency are interfered. In this work, we pose and focus on three key\nquestions regarding the inherent redundancy in SNNs. We argue that the\nredundancy is induced by the spatio-temporal invariance of SNNs, which enhances\nthe efficiency of parameter utilization but also invites lots of noise spikes.\nFurther, we analyze the effect of spatio-temporal invariance on the\nspatio-temporal dynamics and spike firing of SNNs. Then, motivated by these\nanalyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs'\nredundancy, which can adaptively optimize their membrane potential distribution\nby a pair of individual spatial attention sub-modules. In this way, noise spike\nfeatures are accurately regulated. Experimental results demonstrate that the\nproposed method can significantly drop the spike firing with better performance\nthan state-of-the-art SNN baselines. Our code is available in\n\\url{https://github.com/BICLab/ASA-SNN}.",
        "authors": [
            "Man Yao",
            "Jiakui Hu",
            "Guangshe Zhao",
            "Yaoyuan Wang",
            "Ziyang Zhang",
            "Bo Xu",
            "Guoqi Li"
        ]
    },
    {
        "title": "On the Robustness of Normalizing Flows for Inverse Problems in Imaging",
        "url": "http://arxiv.org/abs/2212.04319",
        "abstract": "Conditional normalizing flows can generate diverse image samples for solving\ninverse problems. Most normalizing flows for inverse problems in imaging employ\nthe conditional affine coupling layer that can generate diverse images quickly.\nHowever, unintended severe artifacts are occasionally observed in the output of\nthem. In this work, we address this critical issue by investigating the origins\nof these artifacts and proposing the conditions to avoid them. First of all, we\nempirically and theoretically reveal that these problems are caused by\n\"exploding inverse\" in the conditional affine coupling layer for certain\nout-of-distribution (OOD) conditional inputs. Then, we further validated that\nthe probability of causing erroneous artifacts in pixels is highly correlated\nwith a Mahalanobis distance-based OOD score for inverse problems in imaging.\nLastly, based on our investigations, we propose a remark to avoid exploding\ninverse and then based on it, we suggest a simple remedy that substitutes the\naffine coupling layers with the modified rational quadratic spline coupling\nlayers in normalizing flows, to encourage the robustness of generated image\nsamples. Our experimental results demonstrated that our suggested methods\neffectively suppressed critical artifacts occurring in normalizing flows for\nsuper-resolution space generation and low-light image enhancement.",
        "authors": [
            "Seongmin Hong",
            "Inbum Park",
            "Se Young Chun"
        ]
    },
    {
        "title": "Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels",
        "url": "http://arxiv.org/abs/2307.08809",
        "abstract": "Many existing FL methods assume clients with fully-labeled data, while in\nrealistic settings, clients have limited labels due to the expensive and\nlaborious process of labeling. Limited labeled local data of the clients often\nleads to their local model having poor generalization abilities to their larger\nunlabeled local data, such as having class-distribution mismatch with the\nunlabeled data. As a result, clients may instead look to benefit from the\nglobal model trained across clients to leverage their unlabeled data, but this\nalso becomes difficult due to data heterogeneity across clients. In our work,\nwe propose FedLabel where clients selectively choose the local or global model\nto pseudo-label their unlabeled data depending on which is more of an expert of\nthe data. We further utilize both the local and global models' knowledge via\nglobal-local consistency regularization which minimizes the divergence between\nthe two models' outputs when they have identical pseudo-labels for the\nunlabeled data. Unlike other semi-supervised FL baselines, our method does not\nrequire additional experts other than the local or global model, nor require\nadditional parameters to be communicated. We also do not assume any\nserver-labeled data or fully labeled clients. For both cross-device and\ncross-silo settings, we show that FedLabel outperforms other semi-supervised FL\nbaselines by $8$-$24\\%$, and even outperforms standard fully supervised FL\nbaselines ($100\\%$ labeled data) with only $5$-$20\\%$ of labeled data.",
        "authors": [
            "Yae Jee Cho",
            "Gauri Joshi",
            "Dimitrios Dimitriadis"
        ]
    },
    {
        "title": "PoseFix: Correcting 3D Human Poses with Natural Language",
        "url": "http://arxiv.org/abs/2309.08480",
        "abstract": "Automatically producing instructions to modify one's posture could open the\ndoor to endless applications, such as personalized coaching and in-home\nphysical therapy. Tackling the reverse problem (i.e., refining a 3D pose based\non some natural language feedback) could help for assisted 3D character\nanimation or robot teaching, for instance. Although a few recent works explore\nthe connections between natural language and 3D human pose, none focus on\ndescribing 3D body pose differences. In this paper, we tackle the problem of\ncorrecting 3D human poses with natural language. To this end, we introduce the\nPoseFix dataset, which consists of several thousand paired 3D poses and their\ncorresponding text feedback, that describe how the source pose needs to be\nmodified to obtain the target pose. We demonstrate the potential of this\ndataset on two tasks: (1) text-based pose editing, that aims at generating\ncorrected 3D body poses given a query pose and a text modifier; and (2)\ncorrectional text generation, where instructions are generated based on the\ndifferences between two body poses.",
        "authors": [
            "Ginger Delmas",
            "Philippe Weinzaepfel",
            "Francesc Moreno-Noguer",
            "Gr\u00e9gory Rogez"
        ]
    },
    {
        "title": "TAPIR: Tracking Any Point with Per-Frame Initialization and Temporal Refinement",
        "url": "http://arxiv.org/abs/2306.08637",
        "abstract": "We present a novel model for Tracking Any Point (TAP) that effectively tracks\nany queried point on any physical surface throughout a video sequence. Our\napproach employs two stages: (1) a matching stage, which independently locates\na suitable candidate point match for the query point on every other frame, and\n(2) a refinement stage, which updates both the trajectory and query features\nbased on local correlations. The resulting model surpasses all baseline methods\nby a significant margin on the TAP-Vid benchmark, as demonstrated by an\napproximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model\nfacilitates fast inference on long and high-resolution video sequences. On a\nmodern GPU, our implementation has the capacity to track points faster than\nreal-time, and can be flexibly extended to higher-resolution videos. Given the\nhigh-quality trajectories extracted from a large dataset, we demonstrate a\nproof-of-concept diffusion model which generates trajectories from static\nimages, enabling plausible animations. Visualizations, source code, and\npretrained models can be found on our project webpage.",
        "authors": [
            "Carl Doersch",
            "Yi Yang",
            "Mel Vecerik",
            "Dilara Gokay",
            "Ankush Gupta",
            "Yusuf Aytar",
            "Joao Carreira",
            "Andrew Zisserman"
        ]
    },
    {
        "title": "SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM",
        "url": "http://arxiv.org/abs/2308.09891",
        "abstract": "Integrating CNNs and RNNs to capture spatiotemporal dependencies is a\nprevalent strategy for spatiotemporal prediction tasks. However, the property\nof CNNs to learn local spatial information decreases their efficiency in\ncapturing spatiotemporal dependencies, thereby limiting their prediction\naccuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which\nintegrates Swin Transformer blocks and the simplified LSTM, an extension that\nreplaces the convolutional structure in ConvLSTM with the self-attention\nmechanism. Furthermore, we construct a network with SwinLSTM cell as the core\nfor spatiotemporal prediction. Without using unique tricks, SwinLSTM\noutperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and\nKTH datasets. In particular, it exhibits a significant improvement in\nprediction accuracy compared to ConvLSTM. Our competitive experimental results\ndemonstrate that learning global spatial dependencies is more advantageous for\nmodels to capture spatiotemporal dependencies. We hope that SwinLSTM can serve\nas a solid baseline to promote the advancement of spatiotemporal prediction\naccuracy. The codes are publicly available at\nhttps://github.com/SongTang-x/SwinLSTM.",
        "authors": [
            "Song Tang",
            "Chuang Li",
            "Pu Zhang",
            "RongNian Tang"
        ]
    },
    {
        "title": "Coarse-to-Fine Amodal Segmentation with Shape Prior",
        "url": "http://arxiv.org/abs/2308.16825",
        "abstract": "Amodal object segmentation is a challenging task that involves segmenting\nboth visible and occluded parts of an object. In this paper, we propose a novel\napproach, called Coarse-to-Fine Segmentation (C2F-Seg), that addresses this\nproblem by progressively modeling the amodal segmentation. C2F-Seg initially\nreduces the learning space from the pixel-level image space to the\nvector-quantized latent space. This enables us to better handle long-range\ndependencies and learn a coarse-grained amodal segment from visual features and\nvisible segments. However, this latent space lacks detailed information about\nthe object, which makes it difficult to provide a precise segmentation\ndirectly. To address this issue, we propose a convolution refine module to\ninject fine-grained information and provide a more precise amodal object\nsegmentation based on visual features and coarse-predicted segmentation. To\nhelp the studies of amodal object segmentation, we create a synthetic amodal\ndataset, named as MOViD-Amodal (MOViD-A), which can be used for both image and\nvideo amodal object segmentation. We extensively evaluate our model on two\nbenchmark datasets: KINS and COCO-A. Our empirical results demonstrate the\nsuperiority of C2F-Seg. Moreover, we exhibit the potential of our approach for\nvideo amodal object segmentation tasks on FISHBOWL and our proposed MOViD-A.\nProject page at: http://jianxgao.github.io/C2F-Seg.",
        "authors": [
            "Jianxiong Gao",
            "Xuelin Qian",
            "Yikai Wang",
            "Tianjun Xiao",
            "Tong He",
            "Zheng Zhang",
            "Yanwei Fu"
        ]
    },
    {
        "title": "DEDRIFT: Robust Similarity Search under Content Drift",
        "url": "http://arxiv.org/abs/2308.02752",
        "abstract": "The statistical distribution of content uploaded and searched on media\nsharing sites changes over time due to seasonal, sociological and technical\nfactors. We investigate the impact of this \"content drift\" for large-scale\nsimilarity search tools, based on nearest neighbor search in embedding space.\nUnless a costly index reconstruction is performed frequently, content drift\ndegrades the search accuracy and efficiency. The degradation is especially\nsevere since, in general, both the query and database distributions change.\n  We introduce and analyze real-world image and video datasets for which\ntemporal information is available over a long time period. Based on the\nlearnings, we devise DeDrift, a method that updates embedding quantizers to\ncontinuously adapt large-scale indexing structures on-the-fly. DeDrift almost\neliminates the accuracy degradation due to the query and database content drift\nwhile being up to 100x faster than a full index reconstruction.",
        "authors": [
            "Dmitry Baranchuk",
            "Matthijs Douze",
            "Yash Upadhyay",
            "I. Zeki Yalniz"
        ]
    },
    {
        "title": "AdVerb: Visually Guided Audio Dereverberation",
        "url": "http://arxiv.org/abs/2308.12370",
        "abstract": "We present AdVerb, a novel audio-visual dereverberation framework that uses\nvisual cues in addition to the reverberant sound to estimate clean audio.\nAlthough audio-only dereverberation is a well-studied problem, our approach\nincorporates the complementary visual modality to perform audio\ndereverberation. Given an image of the environment where the reverberated sound\nsignal has been recorded, AdVerb employs a novel geometry-aware cross-modal\ntransformer architecture that captures scene geometry and audio-visual\ncross-modal relationship to generate a complex ideal ratio mask, which, when\napplied to the reverberant audio predicts the clean sound. The effectiveness of\nour method is demonstrated through extensive quantitative and qualitative\nevaluations. Our approach significantly outperforms traditional audio-only and\naudio-visual baselines on three downstream tasks: speech enhancement, speech\nrecognition, and speaker verification, with relative improvements in the range\nof 18% - 82% on the LibriSpeech test-clean set. We also achieve highly\nsatisfactory RT60 error scores on the AVSpeech dataset.",
        "authors": [
            "Sanjoy Chowdhury",
            "Sreyan Ghosh",
            "Subhrajyoti Dasgupta",
            "Anton Ratnarajah",
            "Utkarsh Tyagi",
            "Dinesh Manocha"
        ]
    },
    {
        "title": "Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment",
        "url": "http://arxiv.org/abs/2307.12964",
        "abstract": "Text-to-video retrieval systems have recently made significant progress by\nutilizing pre-trained models trained on large-scale image-text pairs. However,\nmost of the latest methods primarily focus on the video modality while\ndisregarding the audio signal for this task. Nevertheless, a recent advancement\nby ECLIPSE has improved long-range text-to-video retrieval by developing an\naudiovisual video representation. Nonetheless, the objective of the\ntext-to-video retrieval task is to capture the complementary audio and video\ninformation that is pertinent to the text query rather than simply achieving\nbetter audio and video alignment. To address this issue, we introduce TEFAL, a\nTExt-conditioned Feature ALignment method that produces both audio and video\nrepresentations conditioned on the text query. Instead of using only an\naudiovisual attention block, which could suppress the audio information\nrelevant to the text query, our approach employs two independent cross-modal\nattention blocks that enable the text to attend to the audio and video\nrepresentations separately. Our proposed method's efficacy is demonstrated on\nfour benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, and\nCharades, and achieves better than state-of-the-art performance consistently\nacross the four datasets. This is attributed to the additional\ntext-query-conditioned audio representation and the complementary information\nit adds to the text-query-conditioned video representation.",
        "authors": [
            "Sarah Ibrahimi",
            "Xiaohang Sun",
            "Pichao Wang",
            "Amanmeet Garg",
            "Ashutosh Sanan",
            "Mohamed Omar"
        ]
    },
    {
        "title": "Open-vocabulary Object Segmentation with Diffusion Models",
        "url": "http://arxiv.org/abs/2301.05221",
        "abstract": "The goal of this paper is to extract the visual-language correspondence from\na pre-trained text-to-image diffusion model, in the form of segmentation map,\ni.e., simultaneously generating images and segmentation masks for the\ncorresponding visual entities described in the text prompt. We make the\nfollowing contributions: (i) we pair the existing Stable Diffusion model with a\nnovel grounding module, that can be trained to align the visual and textual\nembedding space of the diffusion model with only a small number of object\ncategories; (ii) we establish an automatic pipeline for constructing a dataset,\nthat consists of {image, segmentation mask, text prompt} triplets, to train the\nproposed grounding module; (iii) we evaluate the performance of open-vocabulary\ngrounding on images generated from the text-to-image diffusion model and show\nthat the module can well segment the objects of categories beyond seen ones at\ntraining time; (iv) we adopt the augmented diffusion model to build a synthetic\nsemantic segmentation dataset, and show that, training a standard segmentation\nmodel on such dataset demonstrates competitive performance on the zero-shot\nsegmentation(ZS3) benchmark, which opens up new opportunities for adopting the\npowerful diffusion model for discriminative tasks.",
        "authors": [
            "Ziyi Li",
            "Qinye Zhou",
            "Xiaoyun Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ]
    },
    {
        "title": "Human-centric Scene Understanding for 3D Large-scale Scenarios",
        "url": "http://arxiv.org/abs/2307.14392",
        "abstract": "Human-centric scene understanding is significant for real-world applications,\nbut it is extremely challenging due to the existence of diverse human poses and\nactions, complex human-environment interactions, severe occlusions in crowds,\netc. In this paper, we present a large-scale multi-modal dataset for\nhuman-centric scene understanding, dubbed HuCenLife, which is collected in\ndiverse daily-life scenarios with rich and fine-grained annotations. Our\nHuCenLife can benefit many 3D perception tasks, such as segmentation,\ndetection, action recognition, etc., and we also provide benchmarks for these\ntasks to facilitate related research. In addition, we design novel modules for\nLiDAR-based segmentation and action recognition, which are more applicable for\nlarge-scale human-centric scenarios and achieve state-of-the-art performance.",
        "authors": [
            "Yiteng Xu",
            "Peishan Cong",
            "Yichen Yao",
            "Runnan Chen",
            "Yuenan Hou",
            "Xinge Zhu",
            "Xuming He",
            "Jingyi Yu",
            "Yuexin Ma"
        ]
    },
    {
        "title": "With a Little Help from Your Own Past: Prototypical Memory Networks for Image Captioning",
        "url": "http://arxiv.org/abs/2308.12383",
        "abstract": "Image captioning, like many tasks involving vision and language, currently\nrelies on Transformer-based architectures for extracting the semantics in an\nimage and translating it into linguistically coherent descriptions. Although\nsuccessful, the attention operator only considers a weighted summation of\nprojections of the current input sample, therefore ignoring the relevant\nsemantic information which can come from the joint observation of other\nsamples. In this paper, we devise a network which can perform attention over\nactivations obtained while processing other training samples, through a\nprototypical memory model. Our memory models the distribution of past keys and\nvalues through the definition of prototype vectors which are both\ndiscriminative and compact. Experimentally, we assess the performance of the\nproposed model on the COCO dataset, in comparison with carefully designed\nbaselines and state-of-the-art approaches, and by investigating the role of\neach of the proposed components. We demonstrate that our proposal can increase\nthe performance of an encoder-decoder Transformer by 3.7 CIDEr points both when\ntraining in cross-entropy only and when fine-tuning with self-critical sequence\ntraining. Source code and trained models are available at:\nhttps://github.com/aimagelab/PMA-Net.",
        "authors": [
            "Manuele Barraco",
            "Sara Sarto",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ]
    },
    {
        "title": "SimMatchV2: Semi-Supervised Learning with Graph Consistency",
        "url": "http://arxiv.org/abs/2308.06692",
        "abstract": "Semi-Supervised image classification is one of the most fundamental problem\nin computer vision, which significantly reduces the need for human labor. In\nthis paper, we introduce a new semi-supervised learning algorithm - SimMatchV2,\nwhich formulates various consistency regularizations between labeled and\nunlabeled data from the graph perspective. In SimMatchV2, we regard the\naugmented view of a sample as a node, which consists of a label and its\ncorresponding representation. Different nodes are connected with the edges,\nwhich are measured by the similarity of the node representations. Inspired by\nthe message passing and node classification in graph theory, we propose four\ntypes of consistencies, namely 1) node-node consistency, 2) node-edge\nconsistency, 3) edge-edge consistency, and 4) edge-node consistency. We also\nuncover that a simple feature normalization can reduce the gaps of the feature\nnorm between different augmented views, significantly improving the performance\nof SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised\nlearning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of\ntraining, SimMatchV2 achieves 71.9\\% and 76.2\\% Top-1 Accuracy with 1\\% and\n10\\% labeled examples on ImageNet, which significantly outperforms the previous\nmethods and achieves state-of-the-art performance. Code and pre-trained models\nare available at\n\\href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.",
        "authors": [
            "Mingkai Zheng",
            "Shan You",
            "Lang Huang",
            "Chen Luo",
            "Fei Wang",
            "Chen Qian",
            "Chang Xu"
        ]
    },
    {
        "title": "Reinforced Disentanglement for Face Swapping without Skip Connection",
        "url": "http://arxiv.org/abs/2307.07928",
        "abstract": "The SOTA face swap models still suffer the problem of either target identity\n(i.e., shape) being leaked or the target non-identity attributes (i.e.,\nbackground, hair) failing to be fully preserved in the final results. We show\nthat this insufficient disentanglement is caused by two flawed designs that\nwere commonly adopted in prior models: (1) counting on only one compressed\nencoder to represent both the semantic-level non-identity facial\nattributes(i.e., pose) and the pixel-level non-facial region details, which is\ncontradictory to satisfy at the same time; (2) highly relying on long\nskip-connections between the encoder and the final generator, leaking a certain\namount of target face identity into the result. To fix them, we introduce a new\nface swap framework called 'WSC-swap' that gets rid of skip connections and\nuses two target encoders to respectively capture the pixel-level non-facial\nregion attributes and the semantic non-identity attributes in the face region.\nTo further reinforce the disentanglement learning for the target encoder, we\nemploy both identity removal loss via adversarial training (i.e., GAN) and the\nnon-identity preservation loss via prior 3DMM models like [11]. Extensive\nexperiments on both FaceForensics++ and CelebA-HQ show that our results\nsignificantly outperform previous works on a rich set of metrics, including one\nnovel metric for measuring identity consistency that was completely neglected\nbefore.",
        "authors": [
            "Xiaohang Ren",
            "Xingyu Chen",
            "Pengfei Yao",
            "Heung-Yeung Shum",
            "Baoyuan Wang"
        ]
    },
    {
        "title": "PDiscoNet: Semantically consistent part discovery for fine-grained recognition",
        "url": "http://arxiv.org/abs/2309.03173",
        "abstract": "Fine-grained classification often requires recognizing specific object parts,\nsuch as beak shape and wing patterns for birds. Encouraging a fine-grained\nclassification model to first detect such parts and then using them to infer\nthe class could help us gauge whether the model is indeed looking at the right\ndetails better than with interpretability methods that provide a single\nattribution map. We propose PDiscoNet to discover object parts by using only\nimage-level class labels along with priors encouraging the parts to be:\ndiscriminative, compact, distinct from each other, equivariant to rigid\ntransforms, and active in at least some of the images. In addition to using the\nappropriate losses to encode these priors, we propose to use part-dropout,\nwhere full part feature vectors are dropped at once to prevent a single part\nfrom dominating in the classification, and part feature vector modulation,\nwhich makes the information coming from each part distinct from the perspective\nof the classifier. Our results on CUB, CelebA, and PartImageNet show that the\nproposed method provides substantially better part discovery performance than\nprevious methods while not requiring any additional hyper-parameter tuning and\nwithout penalizing the classification performance. The code is available at\nhttps://github.com/robertdvdk/part_detection.",
        "authors": [
            "Robert van der Klis",
            "Stephan Alaniz",
            "Massimiliano Mancini",
            "Cassio F. Dantas",
            "Dino Ienco",
            "Zeynep Akata",
            "Diego Marcos"
        ]
    },
    {
        "title": "Privacy-Preserving Face Recognition Using Random Frequency Components",
        "url": "http://arxiv.org/abs/2308.10461",
        "abstract": "The ubiquitous use of face recognition has sparked increasing privacy\nconcerns, as unauthorized access to sensitive face images could compromise the\ninformation of individuals. This paper presents an in-depth study of the\nprivacy protection of face images' visual information and against recovery.\nDrawing on the perceptual disparity between humans and models, we propose to\nconceal visual information by pruning human-perceivable low-frequency\ncomponents. For impeding recovery, we first elucidate the seeming paradox\nbetween reducing model-exploitable information and retaining high recognition\naccuracy. Based on recent theoretical insights and our observation on model\nattention, we propose a solution to the dilemma, by advocating for the training\nand inference of recognition models on randomly selected frequency components.\nWe distill our findings into a novel privacy-preserving face recognition\nmethod, PartialFace. Extensive experiments demonstrate that PartialFace\neffectively balances privacy protection goals and recognition accuracy. Code is\navailable at: https://github.com/Tencent/TFace.",
        "authors": [
            "Yuxi Mi",
            "Yuge Huang",
            "Jiazhen Ji",
            "Minyi Zhao",
            "Jiaxiang Wu",
            "Xingkun Xu",
            "Shouhong Ding",
            "Shuigeng Zhou"
        ]
    },
    {
        "title": "How to Choose your Best Allies for a Transferable Attack?",
        "url": "http://arxiv.org/abs/2304.02312",
        "abstract": "The transferability of adversarial examples is a key issue in the security of\ndeep neural networks. The possibility of an adversarial example crafted for a\nsource model fooling another targeted model makes the threat of adversarial\nattacks more realistic. Measuring transferability is a crucial problem, but the\nAttack Success Rate alone does not provide a sound evaluation. This paper\nproposes a new methodology for evaluating transferability by putting distortion\nin a central position. This new tool shows that transferable attacks may\nperform far worse than a black box attack if the attacker randomly picks the\nsource model. To address this issue, we propose a new selection mechanism,\ncalled FiT, which aims at choosing the best source model with only a few\npreliminary queries to the target. Our experimental results show that FiT is\nhighly effective at selecting the best source model for multiple scenarios such\nas single-model attacks, ensemble-model attacks and multiple attacks (Code\navailable at: https://github.com/t-maho/transferability_measure_fit).",
        "authors": [
            "Thibault Maho",
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Teddy Furon"
        ]
    },
    {
        "title": "CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction",
        "url": "http://arxiv.org/abs/2304.14633",
        "abstract": "Recent advances in neural reconstruction using posed image sequences have\nmade remarkable progress. However, due to the lack of depth information,\nexisting volumetric-based techniques simply duplicate 2D image features of the\nobject surface along the entire camera ray. We contend this duplication\nintroduces noise in empty and occluded spaces, posing challenges for producing\nhigh-quality 3D geometry. Drawing inspiration from traditional multi-view\nstereo methods, we propose an end-to-end 3D neural reconstruction framework\nCVRecon, designed to exploit the rich geometric embedding in the cost volumes\nto facilitate 3D geometric feature learning. Furthermore, we present\nRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature\nrepresentation that encodes view-dependent information with improved integrity\nand robustness. Through comprehensive experiments, we demonstrate that our\napproach significantly improves the reconstruction quality in various metrics\nand recovers clear fine details of the 3D geometries. Our extensive ablation\nstudies provide insights into the development of effective 3D geometric feature\nlearning schemes. Project page: https://cvrecon.ziyue.cool/",
        "authors": [
            "Ziyue Feng",
            "Liang Yang",
            "Pengsheng Guo",
            "Bing Li"
        ]
    },
    {
        "title": "Prior-guided Source-free Domain Adaptation for Human Pose Estimation",
        "url": "http://arxiv.org/abs/2308.13954",
        "abstract": "Domain adaptation methods for 2D human pose estimation typically require\ncontinuous access to the source data during adaptation, which can be\nchallenging due to privacy, memory, or computational constraints. To address\nthis limitation, we focus on the task of source-free domain adaptation for pose\nestimation, where a source model must adapt to a new target domain using only\nunlabeled target data. Although recent advances have introduced source-free\nmethods for classification tasks, extending them to the regression task of pose\nestimation is non-trivial. In this paper, we present Prior-guided Self-training\n(POST), a pseudo-labeling approach that builds on the popular Mean Teacher\nframework to compensate for the distribution shift. POST leverages\nprediction-level and feature-level consistency between a student and teacher\nmodel against certain image transformations. In the absence of source data,\nPOST utilizes a human pose prior that regularizes the adaptation process by\ndirecting the model to generate more accurate and anatomically plausible pose\npseudo-labels. Despite being simple and intuitive, our framework can deliver\nsignificant performance gains compared to applying the source model directly to\nthe target data, as demonstrated in our extensive experiments and ablation\nstudies. In fact, our approach achieves comparable performance to recent\nstate-of-the-art methods that use source data for adaptation.",
        "authors": [
            "Dripta S. Raychaudhuri",
            "Calvin-Khang Ta",
            "Arindam Dutta",
            "Rohit Lal",
            "Amit K. Roy-Chowdhury"
        ]
    },
    {
        "title": "ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment",
        "url": "http://arxiv.org/abs/2308.09987",
        "abstract": "We present ClothesNet: a large-scale dataset of 3D clothes objects with\ninformation-rich annotations. Our dataset consists of around 4400 models\ncovering 11 categories annotated with clothes features, boundary lines, and\nkeypoints. ClothesNet can be used to facilitate a variety of computer vision\nand robot interaction tasks. Using our dataset, we establish benchmark tasks\nfor clothes perception, including classification, boundary line segmentation,\nand keypoint detection, and develop simulated clothes environments for robotic\ninteraction tasks, including rearranging, folding, hanging, and dressing. We\nalso demonstrate the efficacy of our ClothesNet in real-world experiments.\nSupplemental materials and dataset are available on our project webpage.",
        "authors": [
            "Bingyang Zhou",
            "Haoyu Zhou",
            "Tianhai Liang",
            "Qiaojun Yu",
            "Siheng Zhao",
            "Yuwei Zeng",
            "Jun Lv",
            "Siyuan Luo",
            "Qiancai Wang",
            "Xinyuan Yu",
            "Haonan Chen",
            "Cewu Lu",
            "Lin Shao"
        ]
    },
    {
        "title": "Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction",
        "url": "http://arxiv.org/abs/2308.08942",
        "abstract": "Exploring spatial-temporal dependencies from observed motions is one of the\ncore challenges of human motion prediction. Previous methods mainly focus on\ndedicated network structures to model the spatial and temporal dependencies.\nThis paper considers a new direction by introducing a model learning framework\nwith auxiliary tasks. In our auxiliary tasks, partial body joints' coordinates\nare corrupted by either masking or adding noise and the goal is to recover\ncorrupted coordinates depending on the rest coordinates. To work with auxiliary\ntasks, we propose a novel auxiliary-adapted transformer, which can handle\nincomplete, corrupted motion data and achieve coordinate recovery via capturing\nspatial-temporal dependencies. Through auxiliary tasks, the auxiliary-adapted\ntransformer is promoted to capture more comprehensive spatial-temporal\ndependencies among body joints' coordinates, leading to better feature\nlearning. Extensive experimental results have shown that our method outperforms\nstate-of-the-art methods by remarkable margins of 7.2%, 3.7%, and 9.4% in terms\nof 3D mean per joint position error (MPJPE) on the Human3.6M, CMU Mocap, and\n3DPW datasets, respectively. We also demonstrate that our method is more robust\nunder data missing cases and noisy data cases. Code is available at\nhttps://github.com/MediaBrain-SJTU/AuxFormer.",
        "authors": [
            "Chenxin Xu",
            "Robby T. Tan",
            "Yuhong Tan",
            "Siheng Chen",
            "Xinchao Wang",
            "Yanfeng Wang"
        ]
    },
    {
        "title": "StyleLipSync: Style-based Personalized Lip-sync Video Generation",
        "url": "http://arxiv.org/abs/2305.00521",
        "abstract": "In this paper, we present StyleLipSync, a style-based personalized lip-sync\nvideo generative model that can generate identity-agnostic lip-synchronizing\nvideo from arbitrary audio. To generate a video of arbitrary identities, we\nleverage expressive lip prior from the semantically rich latent space of a\npre-trained StyleGAN, where we can also design a video consistency with a\nlinear transformation. In contrast to the previous lip-sync methods, we\nintroduce pose-aware masking that dynamically locates the mask to improve the\nnaturalness over frames by utilizing a 3D parametric mesh predictor frame by\nframe. Moreover, we propose a few-shot lip-sync adaptation method for an\narbitrary person by introducing a sync regularizer that preserves lip-sync\ngeneralization while enhancing the person-specific visual information.\nExtensive experiments demonstrate that our model can generate accurate lip-sync\nvideos even with the zero-shot setting and enhance characteristics of an unseen\nface using a few seconds of target video through the proposed adaptation\nmethod.",
        "authors": [
            "Taekyung Ki",
            "Dongchan Min"
        ]
    },
    {
        "title": "Cross Contrasting Feature Perturbation for Domain Generalization",
        "url": "http://arxiv.org/abs/2307.12502",
        "abstract": "Domain generalization (DG) aims to learn a robust model from source domains\nthat generalize well on unseen target domains. Recent studies focus on\ngenerating novel domain samples or features to diversify distributions\ncomplementary to source domains. Yet, these approaches can hardly deal with the\nrestriction that the samples synthesized from various domains can cause\nsemantic distortion. In this paper, we propose an online one-stage Cross\nContrasting Feature Perturbation (CCFP) framework to simulate domain shift by\ngenerating perturbed features in the latent space while regularizing the model\nprediction against domain shift. Different from the previous fixed synthesizing\nstrategy, we design modules with learnable feature perturbations and semantic\nconsistency constraints. In contrast to prior work, our method does not use any\ngenerative-based models or domain labels. We conduct extensive experiments on a\nstandard DomainBed benchmark with a strict evaluation protocol for a fair\ncomparison. Comprehensive experiments show that our method outperforms the\nprevious state-of-the-art, and quantitative analyses illustrate that our\napproach can alleviate the domain shift problem in out-of-distribution (OOD)\nscenarios.",
        "authors": [
            "Chenming Li",
            "Daoan Zhang",
            "Wenjian Huang",
            "Jianguo Zhang"
        ]
    },
    {
        "title": "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model",
        "url": "http://arxiv.org/abs/2303.09867",
        "abstract": "Existing text-video retrieval solutions are, in essence, discriminant models\nfocused on maximizing the conditional likelihood, i.e., p(candidates|query).\nWhile straightforward, this de facto paradigm overlooks the underlying data\ndistribution p(query), which makes it challenging to identify\nout-of-distribution data. To address this limitation, we creatively tackle this\ntask from a generative viewpoint and model the correlation between the text and\nthe video as their joint probability p(candidates,query). This is accomplished\nthrough a diffusion-based text-video retrieval framework (DiffusionRet), which\nmodels the retrieval task as a process of gradually generating joint\ndistribution from noise. During training, DiffusionRet is optimized from both\nthe generation and discrimination perspectives, with the generator being\noptimized by generation loss and the feature extractor trained with contrastive\nloss. In this way, DiffusionRet cleverly leverages the strengths of both\ngenerative and discriminative methods. Extensive experiments on five commonly\nused text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD,\nActivityNet Captions, and DiDeMo, with superior performances, justify the\nefficacy of our method. More encouragingly, without any modification,\nDiffusionRet even performs well in out-domain retrieval settings. We believe\nthis work brings fundamental insights into the related fields. Code is\navailable at https://github.com/jpthu17/DiffusionRet.",
        "authors": [
            "Peng Jin",
            "Hao Li",
            "Zesen Cheng",
            "Kehan Li",
            "Xiangyang Ji",
            "Chang Liu",
            "Li Yuan",
            "Jie Chen"
        ]
    },
    {
        "title": "Efficient 3D Semantic Segmentation with Superpoint Transformer",
        "url": "http://arxiv.org/abs/2306.08045",
        "abstract": "We introduce a novel superpoint-based transformer architecture for efficient\nsemantic segmentation of large-scale 3D scenes. Our method incorporates a fast\nalgorithm to partition point clouds into a hierarchical superpoint structure,\nwhich makes our preprocessing 7 times faster than existing superpoint-based\napproaches. Additionally, we leverage a self-attention mechanism to capture the\nrelationships between superpoints at multiple scales, leading to\nstate-of-the-art performance on three challenging benchmark datasets: S3DIS\n(76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%).\nWith only 212k parameters, our approach is up to 200 times more compact than\nother state-of-the-art models while maintaining similar performance.\nFurthermore, our model can be trained on a single GPU in 3 hours for a fold of\nthe S3DIS dataset, which is 7x to 70x fewer GPU-hours than the best-performing\nmethods. Our code and models are accessible at\ngithub.com/drprojects/superpoint_transformer.",
        "authors": [
            "Damien Robert",
            "Hugo Raguet",
            "Loic Landrieu"
        ]
    },
    {
        "title": "Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff",
        "url": "http://arxiv.org/abs/2308.16454",
        "abstract": "This paper addresses the tradeoff between standard accuracy on clean examples\nand robustness against adversarial examples in deep neural networks (DNNs).\nAlthough adversarial training (AT) improves robustness, it degrades the\nstandard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we\npropose a novel AT method called ARREST, which comprises three components: (i)\nadversarial finetuning (AFT), (ii) representation-guided knowledge distillation\n(RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples\nby initializing its parameters with a DNN that is standardly pretrained on\nclean examples. RGKD and NR respectively entail a regularization term and an\nalgorithm to preserve latent representations of clean examples during AFT. RGKD\npenalizes the distance between the representations of the standardly pretrained\nand AFT DNNs. NR switches input adversarial examples to nonadversarial ones\nwhen the representation changes significantly during AFT. By combining these\ncomponents, ARREST achieves both high standard accuracy and robustness.\nExperimental results demonstrate that ARREST mitigates the tradeoff more\neffectively than previous AT-based methods do.",
        "authors": [
            "Satoshi Suzuki",
            "Shin'ya Yamaguchi",
            "Shoichiro Takeda",
            "Sekitoshi Kanai",
            "Naoki Makishima",
            "Atsushi Ando",
            "Ryo Masumura"
        ]
    },
    {
        "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
        "url": "http://arxiv.org/abs/2303.06705",
        "abstract": "When enhancing low-light images, many deep learning algorithms are based on\nthe Retinex theory. However, the Retinex model does not consider the\ncorruptions hidden in the dark or introduced by the light-up process. Besides,\nthese methods usually require a tedious multi-stage training pipeline and rely\non convolutional neural networks, showing limitations in capturing long-range\ndependencies. In this paper, we formulate a simple yet principled One-stage\nRetinex-based Framework (ORF). ORF first estimates the illumination information\nto light up the low-light image and then restores the corruption to produce the\nenhanced image. We design an Illumination-Guided Transformer (IGT) that\nutilizes illumination representations to direct the modeling of non-local\ninteractions of regions with different lighting conditions. By plugging IGT\ninto ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative\nand qualitative experiments demonstrate that our Retinexformer significantly\noutperforms state-of-the-art methods on thirteen benchmarks. The user study and\napplication on low-light object detection also reveal the latent practical\nvalues of our method. Code, models, and results are available at\nhttps://github.com/caiyuanhao1998/Retinexformer",
        "authors": [
            "Yuanhao Cai",
            "Hao Bian",
            "Jing Lin",
            "Haoqian Wang",
            "Radu Timofte",
            "Yulun Zhang"
        ]
    },
    {
        "title": "Minimum Latency Deep Online Video Stabilization",
        "url": "http://arxiv.org/abs/2212.02073",
        "abstract": "We present a novel camera path optimization framework for the task of online\nvideo stabilization. Typically, a stabilization pipeline consists of three\nsteps: motion estimating, path smoothing, and novel view rendering. Most\nprevious methods concentrate on motion estimation, proposing various global or\nlocal motion models. In contrast, path optimization receives relatively less\nattention, especially in the important online setting, where no future frames\nare available. In this work, we adopt recent off-the-shelf high-quality deep\nmotion models for motion estimation to recover the camera trajectory and focus\non the latter two steps. Our network takes a short 2D camera path in a sliding\nwindow as input and outputs the stabilizing warp field of the last frame in the\nwindow, which warps the coming frame to its stabilized position. A hybrid loss\nis well-defined to constrain the spatial and temporal consistency. In addition,\nwe build a motion dataset that contains stable and unstable motion pairs for\nthe training. Extensive experiments demonstrate that our approach significantly\noutperforms state-of-the-art online methods both qualitatively and\nquantitatively and achieves comparable performance to offline methods. Our code\nand dataset are available at https://github.com/liuzhen03/NNDVS",
        "authors": [
            "Zhuofan Zhang",
            "Zhen Liu",
            "Ping Tan",
            "Bing Zeng",
            "Shuaicheng Liu"
        ]
    },
    {
        "title": "Speech2Lip: High-fidelity Speech to Lip Generation by Learning from a Short Video",
        "url": "http://arxiv.org/abs/2309.04814",
        "abstract": "Synthesizing realistic videos according to a given speech is still an open\nchallenge. Previous works have been plagued by issues such as inaccurate lip\nshape generation and poor image quality. The key reason is that only motions\nand appearances on limited facial areas (e.g., lip area) are mainly driven by\nthe input speech. Therefore, directly learning a mapping function from speech\nto the entire head image is prone to ambiguity, particularly when using a short\nvideo for training. We thus propose a decomposition-synthesis-composition\nframework named Speech to Lip (Speech2Lip) that disentangles speech-sensitive\nand speech-insensitive motion/appearance to facilitate effective learning from\nlimited training data, resulting in the generation of natural-looking videos.\nFirst, given a fixed head pose (i.e., canonical space), we present a\nspeech-driven implicit model for lip image generation which concentrates on\nlearning speech-sensitive motion and appearance. Next, to model the major\nspeech-insensitive motion (i.e., head movement), we introduce a geometry-aware\nmutual explicit mapping (GAMEM) module that establishes geometric mappings\nbetween different head poses. This allows us to paste generated lip images at\nthe canonical space onto head images with arbitrary poses and synthesize\ntalking videos with natural head movements. In addition, a Blend-Net and a\ncontrastive sync loss are introduced to enhance the overall synthesis\nperformance. Quantitative and qualitative results on three benchmarks\ndemonstrate that our model can be trained by a video of just a few minutes in\nlength and achieve state-of-the-art performance in both visual quality and\nspeech-visual synchronization. Code: https://github.com/CVMI-Lab/Speech2Lip.",
        "authors": [
            "Xiuzhe Wu",
            "Pengfei Hu",
            "Yang Wu",
            "Xiaoyang Lyu",
            "Yan-Pei Cao",
            "Ying Shan",
            "Wenming Yang",
            "Zhongqian Sun",
            "Xiaojuan Qi"
        ]
    },
    {
        "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models",
        "url": "http://arxiv.org/abs/2302.14383",
        "abstract": "We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.",
        "authors": [
            "Matthew Trager",
            "Pramuditha Perera",
            "Luca Zancato",
            "Alessandro Achille",
            "Parminder Bhatia",
            "Stefano Soatto"
        ]
    },
    {
        "title": "MULLER: Multilayer Laplacian Resizer for Vision",
        "url": "http://arxiv.org/abs/2304.02859",
        "abstract": "Image resizing operation is a fundamental preprocessing module in modern\ncomputer vision. Throughout the deep learning revolution, researchers have\noverlooked the potential of alternative resizing methods beyond the commonly\nused resizers that are readily available, such as nearest-neighbors, bilinear,\nand bicubic. The key question of our interest is whether the front-end resizer\naffects the performance of deep vision models? In this paper, we present an\nextremely lightweight multilayer Laplacian resizer with only a handful of\ntrainable parameters, dubbed MULLER resizer. MULLER has a bandpass nature in\nthat it learns to boost details in certain frequency subbands that benefit the\ndownstream recognition models. We show that MULLER can be easily plugged into\nvarious training pipelines, and it effectively boosts the performance of the\nunderlying vision task with little to no extra cost. Specifically, we select a\nstate-of-the-art vision Transformer, MaxViT, as the baseline, and show that, if\ntrained with MULLER, MaxViT gains up to 0.6% top-1 accuracy, and meanwhile\nenjoys 36% inference cost saving to achieve similar top-1 accuracy on\nImageNet-1k, as compared to the standard training scheme. Notably, MULLER's\nperformance also scales with model size and training data size such as\nImageNet-21k and JFT, and it is widely applicable to multiple vision tasks,\nincluding image classification, object detection and segmentation, as well as\nimage quality assessment.",
        "authors": [
            "Zhengzhong Tu",
            "Peyman Milanfar",
            "Hossein Talebi"
        ]
    },
    {
        "title": "Why do networks have inhibitory/negative connections?",
        "url": "http://arxiv.org/abs/2208.03211",
        "abstract": "Why do brains have inhibitory connections? Why do deep networks have negative\nweights? We propose an answer from the perspective of representation capacity.\nWe believe representing functions is the primary role of both (i) the brain in\nnatural intelligence, and (ii) deep networks in artificial intelligence. Our\nanswer to why there are inhibitory/negative weights is: to learn more\nfunctions. We prove that, in the absence of negative weights, neural networks\nwith non-decreasing activation functions are not universal approximators. While\nthis may be an intuitive result to some, to the best of our knowledge, there is\nno formal theory, in either machine learning or neuroscience, that demonstrates\nwhy negative weights are crucial in the context of representation capacity.\nFurther, we provide insights on the geometric properties of the representation\nspace that non-negative deep networks cannot represent. We expect these\ninsights will yield a deeper understanding of more sophisticated inductive\npriors imposed on the distribution of weights that lead to more efficient\nbiological and machine learning.",
        "authors": [
            "Qingyang Wang",
            "Michael A. Powell",
            "Ali Geisa",
            "Eric Bridgeford",
            "Carey E. Priebe",
            "Joshua T. Vogelstein"
        ]
    },
    {
        "title": "Model Calibration in Dense Classification with Adaptive Label Perturbation",
        "url": "http://arxiv.org/abs/2307.13539",
        "abstract": "For safety-related applications, it is crucial to produce trustworthy deep\nneural networks whose prediction is associated with confidence that can\nrepresent the likelihood of correctness for subsequent decision-making.\nExisting dense binary classification models are prone to being over-confident.\nTo improve model calibration, we propose Adaptive Stochastic Label Perturbation\n(ASLP) which learns a unique label perturbation level for each training image.\nASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,\nwhich unifies label perturbation processes including stochastic approaches\n(like DisturbLabel), and label smoothing, to correct calibration while\nmaintaining classification rates. ASLP follows Maximum Entropy Inference of\nclassic statistical mechanics to maximise prediction entropy with respect to\nmissing information. It performs this while: (1) preserving classification\naccuracy on known data as a conservative solution, or (2) specifically improves\nmodel calibration degree by minimising the gap between the prediction accuracy\nand expected confidence of the target training label. Extensive results\ndemonstrate that ASLP can significantly improve calibration degrees of dense\nbinary classification models on both in-distribution and out-of-distribution\ndata. The code is available on https://github.com/Carlisle-Liu/ASLP.",
        "authors": [
            "Jiawei Liu",
            "Changkun Ye",
            "Shan Wang",
            "Ruikai Cui",
            "Jing Zhang",
            "Kaihao Zhang",
            "Nick Barnes"
        ]
    },
    {
        "title": "Boosting Multi-modal Model Performance with Adaptive Gradient Modulation",
        "url": "http://arxiv.org/abs/2308.07686",
        "abstract": "While the field of multi-modal learning keeps growing fast, the deficiency of\nthe standard joint training paradigm has become clear through recent studies.\nThey attribute the sub-optimal performance of the jointly trained model to the\nmodality competition phenomenon. Existing works attempt to improve the jointly\ntrained model by modulating the training process. Despite their effectiveness,\nthose methods can only apply to late fusion models. More importantly, the\nmechanism of the modality competition remains unexplored. In this paper, we\nfirst propose an adaptive gradient modulation method that can boost the\nperformance of multi-modal models with various fusion strategies. Extensive\nexperiments show that our method surpasses all existing modulation methods.\nFurthermore, to have a quantitative understanding of the modality competition\nand the mechanism behind the effectiveness of our modulation method, we\nintroduce a novel metric to measure the competition strength. This metric is\nbuilt on the mono-modal concept, a function that is designed to represent the\ncompetition-less state of a modality. Through systematic investigation, our\nresults confirm the intuition that the modulation encourages the model to rely\non the more informative modality. In addition, we find that the jointly trained\nmodel typically has a preferred modality on which the competition is weaker\nthan other modalities. However, this preferred modality need not dominate\nothers. Our code will be available at\nhttps://github.com/lihong2303/AGM_ICCV2023.",
        "authors": [
            "Hong Li",
            "Xingyu Li",
            "Pengbo Hu",
            "Yinuo Lei",
            "Chunxiao Li",
            "Yi Zhou"
        ]
    },
    {
        "title": "Structure and Content-Guided Video Synthesis with Diffusion Models",
        "url": "http://arxiv.org/abs/2302.03011",
        "abstract": "Text-guided generative diffusion models unlock powerful image creation and\nediting tools. While these have been extended to video generation, current\napproaches that edit the content of existing footage while retaining structure\nrequire expensive re-training for every input or rely on error-prone\npropagation of image edits across frames. In this work, we present a structure\nand content-guided video diffusion model that edits videos based on visual or\ntextual descriptions of the desired output. Conflicts between user-provided\ncontent edits and structure representations occur due to insufficient\ndisentanglement between the two aspects. As a solution, we show that training\non monocular depth estimates with varying levels of detail provides control\nover structure and content fidelity. Our model is trained jointly on images and\nvideos which also exposes explicit control of temporal consistency through a\nnovel guidance method. Our experiments demonstrate a wide variety of successes;\nfine-grained control over output characteristics, customization based on a few\nreference images, and a strong user preference towards results by our model.",
        "authors": [
            "Patrick Esser",
            "Johnathan Chiu",
            "Parmida Atighehchian",
            "Jonathan Granskog",
            "Anastasis Germanidis"
        ]
    },
    {
        "title": "Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color",
        "url": "http://arxiv.org/abs/2309.05148",
        "abstract": "This paper strives to measure apparent skin color in computer vision, beyond\na unidimensional scale on skin tone. In their seminal paper Gender Shades,\nBuolamwini and Gebru have shown how gender classification systems can be biased\nagainst women with darker skin tones. Subsequently, fairness researchers and\npractitioners have adopted the Fitzpatrick skin type classification as a common\nmeasure to assess skin color bias in computer vision systems. While effective,\nthe Fitzpatrick scale only focuses on the skin tone ranging from light to dark.\nTowards a more comprehensive measure of skin color, we introduce the hue angle\nranging from red to yellow. When applied to images, the hue dimension reveals\nadditional biases related to skin color in both computer vision datasets and\nmodels. We then recommend multidimensional skin color scales, relying on both\nskin tone and hue, for fairness assessments.",
        "authors": [
            "William Thong",
            "Przemyslaw Joniak",
            "Alice Xiang"
        ]
    },
    {
        "title": "PODA: Prompt-driven Zero-shot Domain Adaptation",
        "url": "http://arxiv.org/abs/2212.03241",
        "abstract": "Domain adaptation has been vastly investigated in computer vision but still\nrequires access to target images at train time, which might be intractable in\nsome uncommon conditions. In this paper, we propose the task of `Prompt-driven\nZero-shot Domain Adaptation', where we adapt a model trained on a source domain\nusing only a general description in natural language of the target domain,\ni.e., a prompt. First, we leverage a pretrained contrastive vision-language\nmodel (CLIP) to optimize affine transformations of source features, steering\nthem towards the target text embedding while preserving their content and\nsemantics. To achieve this, we propose Prompt-driven Instance Normalization\n(PIN). Second, we show that these prompt-driven augmentations can be used to\nperform zero-shot domain adaptation for semantic segmentation. Experiments\ndemonstrate that our method significantly outperforms CLIP-based style transfer\nbaselines on several datasets for the downstream task at hand, even surpassing\none-shot unsupervised domain adaptation. A similar boost is observed on object\ndetection and image classification. The code is available at\nhttps://github.com/astra-vision/PODA .",
        "authors": [
            "Mohammad Fahes",
            "Tuan-Hung Vu",
            "Andrei Bursuc",
            "Patrick P\u00e9rez",
            "Raoul de Charette"
        ]
    },
    {
        "title": "Shatter and Gather: Learning Referring Image Segmentation with Text Supervision",
        "url": "http://arxiv.org/abs/2308.15512",
        "abstract": "Referring image segmentation, the task of segmenting any arbitrary entities\ndescribed in free-form texts, opens up a variety of vision applications.\nHowever, manual labeling of training data for this task is prohibitively\ncostly, leading to lack of labeled data for training. We address this issue by\na weakly supervised learning approach using text descriptions of training\nimages as the only source of supervision. To this end, we first present a new\nmodel that discovers semantic entities in input image and then combines such\nentities relevant to text query to predict the mask of the referent. We also\npresent a new loss function that allows the model to be trained without any\nfurther supervision. Our method was evaluated on four public benchmarks for\nreferring image segmentation, where it clearly outperformed the existing method\nfor the same task and recent open-vocabulary segmentation models on all the\nbenchmarks.",
        "authors": [
            "Dongwon Kim",
            "Namyup Kim",
            "Cuiling Lan",
            "Suha Kwak"
        ]
    },
    {
        "title": "Two-in-One Depth: Bridging the Gap Between Monocular and Binocular Self-Supervised Depth Estimation",
        "url": "http://arxiv.org/abs/2309.00933",
        "abstract": "Monocular and binocular self-supervised depth estimations are two important\nand related tasks in computer vision, which aim to predict scene depths from\nsingle images and stereo image pairs respectively. In literature, the two tasks\nare usually tackled separately by two different kinds of models, and binocular\nmodels generally fail to predict depth from single images, while the prediction\naccuracy of monocular models is generally inferior to binocular models. In this\npaper, we propose a Two-in-One self-supervised depth estimation network, called\nTiO-Depth, which could not only compatibly handle the two tasks, but also\nimprove the prediction accuracy. TiO-Depth employs a Siamese architecture and\neach sub-network of it could be used as a monocular depth estimation model. For\nbinocular depth estimation, a Monocular Feature Matching module is proposed for\nincorporating the stereo knowledge between the two images, and the full\nTiO-Depth is used to predict depths. We also design a multi-stage\njoint-training strategy for improving the performances of TiO-Depth in both two\ntasks by combining the relative advantages of them. Experimental results on the\nKITTI, Cityscapes, and DDAD datasets demonstrate that TiO-Depth outperforms\nboth the monocular and binocular state-of-the-art methods in most cases, and\nfurther verify the feasibility of a two-in-one network for monocular and\nbinocular depth estimation. The code is available at\nhttps://github.com/ZM-Zhou/TiO-Depth_pytorch.",
        "authors": [
            "Zhengming Zhou",
            "Qiulei Dong"
        ]
    },
    {
        "title": "Rethinking Pose Estimation in Crowds: Overcoming the Detection Information Bottleneck and Ambiguity",
        "url": "http://arxiv.org/abs/2306.07879",
        "abstract": "Frequent interactions between individuals are a fundamental challenge for\npose estimation algorithms. Current pipelines either use an object detector\ntogether with a pose estimator (top-down approach), or localize all body parts\nfirst and then link them to predict the pose of individuals (bottom-up). Yet,\nwhen individuals closely interact, top-down methods are ill-defined due to\noverlapping individuals, and bottom-up methods often falsely infer connections\nto distant bodyparts. Thus, we propose a novel pipeline called bottom-up\nconditioned top-down pose estimation (BUCTD) that combines the strengths of\nbottom-up and top-down methods. Specifically, we propose to use a bottom-up\nmodel as the detector, which in addition to an estimated bounding box provides\na pose proposal that is fed as condition to an attention-based top-down model.\nWe demonstrate the performance and efficiency of our approach on animal and\nhuman pose estimation benchmarks. On CrowdPose and OCHuman, we outperform\nprevious state-of-the-art models by a significant margin. We achieve 78.5 AP on\nCrowdPose and 48.5 AP on OCHuman, an improvement of 8.6% and 7.8% over the\nprior art, respectively. Furthermore, we show that our method strongly improves\nthe performance on multi-animal benchmarks involving fish and monkeys. The code\nis available at https://github.com/amathislab/BUCTD",
        "authors": [
            "Mu Zhou",
            "Lucas Stoffl",
            "Mackenzie Weygandt Mathis",
            "Alexander Mathis"
        ]
    },
    {
        "title": "Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking",
        "url": "http://arxiv.org/abs/2308.12549",
        "abstract": "Siamese network has been a de facto benchmark framework for 3D LiDAR object\ntracking with a shared-parametric encoder extracting features from template and\nsearch region, respectively. This paradigm relies heavily on an additional\nmatching network to model the cross-correlation/similarity of the template and\nsearch region. In this paper, we forsake the conventional Siamese paradigm and\npropose a novel single-branch framework, SyncTrack, synchronizing the feature\nextracting and matching to avoid forwarding encoder twice for template and\nsearch region as well as introducing extra parameters of matching network. The\nsynchronization mechanism is based on the dynamic affinity of the Transformer,\nand an in-depth analysis of the relevance is provided theoretically. Moreover,\nbased on the synchronization, we introduce a novel Attentive Points-Sampling\nstrategy into the Transformer layers (APST), replacing the random/Farthest\nPoints Sampling (FPS) method with sampling under the supervision of attentive\nrelations between the template and search region. It implies connecting\npoint-wise sampling with the feature learning, beneficial to aggregating more\ndistinctive and geometric features for tracking with sparse points. Extensive\nexperiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack\nachieves state-of-the-art performance in real-time tracking.",
        "authors": [
            "Teli Ma",
            "Mengmeng Wang",
            "Jimin Xiao",
            "Huifeng Wu",
            "Yong Liu"
        ]
    },
    {
        "title": "Leveraging Intrinsic Properties for Non-Rigid Garment Alignment",
        "url": "http://arxiv.org/abs/2308.09519",
        "abstract": "We address the problem of aligning real-world 3D data of garments, which\nbenefits many applications such as texture learning, physical parameter\nestimation, generative modeling of garments, etc. Existing extrinsic methods\ntypically perform non-rigid iterative closest point and struggle to align\ndetails due to incorrect closest matches and rigidity constraints. While\nintrinsic methods based on functional maps can produce high-quality\ncorrespondences, they work under isometric assumptions and become unreliable\nfor garment deformations which are highly non-isometric. To achieve\nwrinkle-level as well as texture-level alignment, we present a novel\ncoarse-to-fine two-stage method that leverages intrinsic manifold properties\nwith two neural deformation fields, in the 3D space and the intrinsic space,\nrespectively. The coarse stage performs a 3D fitting, where we leverage\nintrinsic manifold properties to define a manifold deformation field. The\ncoarse fitting then induces a functional map that produces an alignment of\nintrinsic embeddings. We further refine the intrinsic alignment with a second\nneural deformation field for higher accuracy. We evaluate our method with our\ncaptured garment dataset, GarmCap. The method achieves accurate wrinkle-level\nand texture-level alignment and works for difficult garment types such as long\ncoats. Our project page is\nhttps://jsnln.github.io/iccv2023_intrinsic/index.html.",
        "authors": [
            "Siyou Lin",
            "Boyao Zhou",
            "Zerong Zheng",
            "Hongwen Zhang",
            "Yebin Liu"
        ]
    },
    {
        "title": "Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity",
        "url": "http://arxiv.org/abs/2303.05689",
        "abstract": "There is a recently discovered and intriguing phenomenon called Neural\nCollapse: at the terminal phase of training a deep neural network for\nclassification, the within-class penultimate feature means and the associated\nclassifier vectors of all flat classes collapse to the vertices of a simplex\nEquiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon\nby fixing the related classifier weights to a pre-computed ETF to induce neural\ncollapse and maximize the separation of the learned features when training with\nimbalanced data. In this work, we propose to fix the linear classifier of a\ndeep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF,\nand use a cosine similarity-based auxiliary loss to learn hierarchy-aware\npenultimate features that collapse to the HAFrame. We demonstrate that our\napproach reduces the mistake severity of the model's predictions while\nmaintaining its top-1 accuracy on several datasets of varying scales with\nhierarchies of heights ranging from 3 to 12. Code:\nhttps://github.com/ltong1130ztr/HAFrame",
        "authors": [
            "Tong Liang",
            "Jim Davis"
        ]
    },
    {
        "title": "PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking",
        "url": "http://arxiv.org/abs/2303.07625",
        "abstract": "Planar object tracking is a critical computer vision problem and has drawn\nincreasing interest owing to its key roles in robotics, augmented reality, etc.\nDespite rapid progress, its further development, especially in the deep\nlearning era, is largely hindered due to the lack of large-scale challenging\nbenchmarks. Addressing this, we introduce PlanarTrack, a large-scale\nchallenging planar tracking benchmark. Specifically, PlanarTrack consists of\n1,000 videos with more than 490K images. All these videos are collected in\ncomplex unconstrained scenarios from the wild, which makes PlanarTrack,\ncompared with existing benchmarks, more challenging but realistic for\nreal-world applications. To ensure the high-quality annotation, each frame in\nPlanarTrack is manually labeled using four corners with multiple-round careful\ninspection and refinement. To our best knowledge, PlanarTrack, to date, is the\nlargest and most challenging dataset dedicated to planar object tracking. In\norder to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and\nconduct comprehensive comparisons and in-depth analysis. Our results, not\nsurprisingly, demonstrate that current top-performing planar trackers\ndegenerate significantly on the challenging PlanarTrack and more efforts are\nneeded to improve planar tracking in the future. In addition, we further derive\na variant named PlanarTrack$_{\\mathbf{BB}}$ for generic object tracking from\nPlanarTrack. Our evaluation of 10 excellent generic trackers on\nPlanarTrack$_{\\mathrm{BB}}$ manifests that, surprisingly,\nPlanarTrack$_{\\mathrm{BB}}$ is even more challenging than several popular\ngeneric tracking benchmarks and more attention should be paid to handle such\nplanar objects, though they are rigid. All benchmarks and evaluations will be\nreleased at the project webpage.",
        "authors": [
            "Xinran Liu",
            "Xiaoqiong Liu",
            "Ziruo Yi",
            "Xin Zhou",
            "Thanh Le",
            "Libo Zhang",
            "Yan Huang",
            "Qing Yang",
            "Heng Fan"
        ]
    },
    {
        "title": "Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation",
        "url": "http://arxiv.org/abs/2304.05669",
        "abstract": "Inverse path tracing has recently been applied to joint material and lighting\nestimation, given geometry and multi-view HDR observations of an indoor scene.\nHowever, it has two major limitations: path tracing is expensive to compute,\nand ambiguities exist between reflection and emission. Our Factorized Inverse\nPath Tracing (FIPT) addresses these challenges by using a factored light\ntransport formulation and finds emitters driven by rendering errors. Our\nalgorithm enables accurate material and lighting optimization faster than\nprevious work, and is more effective at resolving ambiguities. The exhaustive\nexperiments on synthetic scenes show that our method (1) outperforms\nstate-of-the-art indoor inverse rendering and relighting methods particularly\nin the presence of complex illumination effects; (2) speeds up inverse path\ntracing optimization to less than an hour. We further demonstrate robustness to\nnoisy inputs through material and lighting estimates that allow plausible\nrelighting in a real scene. The source code is available at:\nhttps://github.com/lwwu2/fipt",
        "authors": [
            "Liwen Wu",
            "Rui Zhu",
            "Mustafa B. Yaldiz",
            "Yinhao Zhu",
            "Hong Cai",
            "Janarbek Matai",
            "Fatih Porikli",
            "Tzu-Mao Li",
            "Manmohan Chandraker",
            "Ravi Ramamoorthi"
        ]
    },
    {
        "title": "P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds",
        "url": "http://arxiv.org/abs/2307.14726",
        "abstract": "Point cloud completion aims to recover the complete shape based on a partial\nobservation. Existing methods require either complete point clouds or multiple\npartial observations of the same object for learning. In contrast to previous\napproaches, we present Partial2Complete (P2C), the first self-supervised\nframework that completes point cloud objects using training samples consisting\nof only a single incomplete point cloud per object. Specifically, our framework\ngroups incomplete point clouds into local patches as input and predicts masked\npatches by learning prior information from different partial objects. We also\npropose Region-Aware Chamfer Distance to regularize shape mismatch without\nlimiting completion capability, and devise the Normal Consistency Constraint to\nincorporate a local planarity assumption, encouraging the recovered shape\nsurface to be continuous and complete. In this way, P2C no longer needs\nmultiple observations or complete point clouds as ground truth. Instead,\nstructural cues are learned from a category-specific dataset to complete\npartial point clouds of objects. We demonstrate the effectiveness of our\napproach on both synthetic ShapeNet data and real-world ScanNet data, showing\nthat P2C produces comparable results to methods trained with complete shapes,\nand outperforms methods learned with multiple partial observations. Code is\navailable at https://github.com/CuiRuikai/Partial2Complete.",
        "authors": [
            "Ruikai Cui",
            "Shi Qiu",
            "Saeed Anwar",
            "Jiawei Liu",
            "Chaoyue Xing",
            "Jing Zhang",
            "Nick Barnes"
        ]
    },
    {
        "title": "Overwriting Pretrained Bias with Finetuning Data",
        "url": "http://arxiv.org/abs/2303.06167",
        "abstract": "Transfer learning is beneficial by allowing the expressive features of models\npretrained on large-scale datasets to be finetuned for the target task of\nsmaller, more domain-specific datasets. However, there is a concern that these\npretrained models may come with their own biases which would propagate into the\nfinetuned model. In this work, we investigate bias when conceptualized as both\nspurious correlations between the target task and a sensitive attribute as well\nas underrepresentation of a particular group in the dataset. Under both notions\nof bias, we find that (1) models finetuned on top of pretrained models can\nindeed inherit their biases, but (2) this bias can be corrected for through\nrelatively minor interventions to the finetuning dataset, and often with a\nnegligible impact to performance. Our findings imply that careful curation of\nthe finetuning dataset is important for reducing biases on a downstream task,\nand doing so can even compensate for bias in the pretrained model.",
        "authors": [
            "Angelina Wang",
            "Olga Russakovsky"
        ]
    },
    {
        "title": "UMFuse: Unified Multi View Fusion for Human Editing Applications",
        "url": "http://arxiv.org/abs/2211.10157",
        "abstract": "Numerous pose-guided human editing methods have been explored by the vision\ncommunity due to their extensive practical applications. However, most of these\nmethods still use an image-to-image formulation in which a single image is\ngiven as input to produce an edited image as output. This objective becomes\nill-defined in cases when the target pose differs significantly from the input\npose. Existing methods then resort to in-painting or style transfer to handle\nocclusions and preserve content. In this paper, we explore the utilization of\nmultiple views to minimize the issue of missing information and generate an\naccurate representation of the underlying human model. To fuse knowledge from\nmultiple viewpoints, we design a multi-view fusion network that takes the pose\nkey points and texture from multiple source images and generates an explainable\nper-pixel appearance retrieval map. Thereafter, the encodings from a separate\nnetwork (trained on a single-view human reposing task) are merged in the latent\nspace. This enables us to generate accurate, precise, and visually coherent\nimages for different editing tasks. We show the application of our network on\ntwo newly proposed tasks - Multi-view human reposing and Mix&Match Human Image\ngeneration. Additionally, we study the limitations of single-view editing and\nscenarios in which multi-view provides a better alternative.",
        "authors": [
            "Rishabh Jain",
            "Mayur Hemani",
            "Duygu Ceylan",
            "Krishna Kumar Singh",
            "Jingwan Lu",
            "Mausoom Sarkar",
            "Balaji Krishnamurthy"
        ]
    },
    {
        "title": "CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation",
        "url": "http://arxiv.org/abs/2303.04869",
        "abstract": "Beyond novel view synthesis, Neural Radiance Fields are useful for\napplications that interact with the real world. In this paper, we use them as\nan implicit map of a given scene and propose a camera relocalization algorithm\ntailored for this representation. The proposed method enables to compute in\nreal-time the precise position of a device using a single RGB camera, during\nits navigation. In contrast with previous work, we do not rely on pose\nregression or photometric alignment but rather use dense local features\nobtained through volumetric rendering which are specialized on the scene with a\nself-supervised objective. As a result, our algorithm is more accurate than\ncompetitors, able to operate in dynamic outdoor environments with changing\nlightning conditions and can be readily integrated in any volumetric neural\nrenderer.",
        "authors": [
            "Arthur Moreau",
            "Nathan Piasco",
            "Moussab Bennehar",
            "Dzmitry Tsishkou",
            "Bogdan Stanciulescu",
            "Arnaud de La Fortelle"
        ]
    },
    {
        "title": "SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation",
        "url": "http://arxiv.org/abs/2304.10417",
        "abstract": "Our goal is to synthesize 3D human motions given textual inputs describing\nsimultaneous actions, for example 'waving hand' while 'walking' at the same\ntime. We refer to generating such simultaneous movements as performing 'spatial\ncompositions'. In contrast to temporal compositions that seek to transition\nfrom one action to another, spatial compositing requires understanding which\nbody parts are involved in which action, to be able to move them\nsimultaneously. Motivated by the observation that the correspondence between\nactions and body parts is encoded in powerful language models, we extract this\nknowledge by prompting GPT-3 with text such as \"what are the body parts\ninvolved in the action <action name>?\", while also providing the parts list and\nfew-shot examples. Given this action-part mapping, we combine body parts from\ntwo motions together and establish the first automated method to spatially\ncompose two actions. However, training data with compositional actions is\nalways limited by the combinatorics. Hence, we further create synthetic data\nwith this approach, and use it to train a new state-of-the-art text-to-motion\ngeneration model, called SINC (\"SImultaneous actioN Compositions for 3D human\nmotions\"). In our experiments, that training with such GPT-guided synthetic\ndata improves spatial composition generation over baselines. Our code is\npublicly available at https://sinc.is.tue.mpg.de/.",
        "authors": [
            "Nikos Athanasiou",
            "Mathis Petrovich",
            "Michael J. Black",
            "G\u00fcl Varol"
        ]
    },
    {
        "title": "ORC: Network Group-based Knowledge Distillation using Online Role Change",
        "url": "http://arxiv.org/abs/2206.01186",
        "abstract": "In knowledge distillation, since a single, omnipotent teacher network cannot\nsolve all problems, multiple teacher-based knowledge distillations have been\nstudied recently. However, sometimes their improvements are not as good as\nexpected because some immature teachers may transfer the false knowledge to the\nstudent. In this paper, to overcome this limitation and take the efficacy of\nthe multiple networks, we divide the multiple networks into teacher and student\ngroups, respectively. That is, the student group is a set of immature networks\nthat require learning the teacher's knowledge, while the teacher group consists\nof the selected networks that are capable of teaching successfully. We propose\nour online role change strategy where the top-ranked networks in the student\ngroup are able to promote to the teacher group at every iteration. After\ntraining the teacher group using the error samples of the student group to\nrefine the teacher group's knowledge, we transfer the collaborative knowledge\nfrom the teacher group to the student group successfully. We verify the\nsuperiority of the proposed method on CIFAR-10, CIFAR-100, and ImageNet which\nachieves high performance. We further show the generality of our method with\nvarious backbone architectures such as ResNet, WRN, VGG, Mobilenet, and\nShufflenet.",
        "authors": [
            "Junyong Choi",
            "Hyeon Cho",
            "Seokhwa Cheung",
            "Wonjun Hwang"
        ]
    },
    {
        "title": "Audiovisual Masked Autoencoders",
        "url": "http://arxiv.org/abs/2212.05922",
        "abstract": "Can we leverage the audiovisual information already present in video to\nimprove self-supervised representation learning? To answer this question, we\nstudy various pretraining architectures and objectives within the masked\nautoencoding framework, motivated by the success of similar methods in natural\nlanguage and image understanding. We show that we can achieve significant\nimprovements on audiovisual downstream classification tasks, surpassing the\nstate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our\naudiovisual pretraining scheme for multiple unimodal downstream tasks using a\nsingle audiovisual pretrained model. We additionally demonstrate the\ntransferability of our representations, achieving state-of-the-art audiovisual\nresults on Epic Kitchens without pretraining specifically for this dataset.",
        "authors": [
            "Mariana-Iuliana Georgescu",
            "Eduardo Fonseca",
            "Radu Tudor Ionescu",
            "Mario Lucic",
            "Cordelia Schmid",
            "Anurag Arnab"
        ]
    },
    {
        "title": "CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation",
        "url": "http://arxiv.org/abs/2308.10574",
        "abstract": "In daily life, humans utilize hands to manipulate objects. Modeling the shape\nof objects that are manipulated by the hand is essential for AI to comprehend\ndaily tasks and to learn manipulation skills. However, previous approaches have\nencountered difficulties in reconstructing the precise shapes of hand-held\nobjects, primarily owing to a deficiency in prior shape knowledge and\ninadequate data for training. As illustrated, given a particular type of tool,\nsuch as a mug, despite its infinite variations in shape and appearance, humans\nhave a limited number of 'effective' modes and poses for its manipulation. This\ncan be attributed to the fact that humans have mastered the shape prior of the\n'mug' category, and can quickly establish the corresponding relations between\ndifferent mug instances and the prior, such as where the rim and handle are\nlocated. In light of this, we propose a new method, CHORD, for Category-level\nHand-held Object Reconstruction via shape Deformation. CHORD deforms a\ncategorical shape prior for reconstructing the intra-class objects. To ensure\naccurate reconstruction, we empower CHORD with three types of awareness:\nappearance, shape, and interacting pose. In addition, we have constructed a new\ndataset, COMIC, of category-level hand-object interaction. COMIC contains a\nrich array of object instances, materials, hand interactions, and viewing\ndirections. Extensive evaluation shows that CHORD outperforms state-of-the-art\napproaches in both quantitative and qualitative measures. Code, model, and\ndatasets are available at https://kailinli.github.io/CHORD.",
        "authors": [
            "Kailin Li",
            "Lixin Yang",
            "Haoyu Zhen",
            "Zenan Lin",
            "Xinyu Zhan",
            "Licheng Zhong",
            "Jian Xu",
            "Kejian Wu",
            "Cewu Lu"
        ]
    },
    {
        "title": "Unmasking Anomalies in Road-Scene Segmentation",
        "url": "http://arxiv.org/abs/2307.13316",
        "abstract": "Anomaly segmentation is a critical task for driving applications, and it is\napproached traditionally as a per-pixel classification problem. However,\nreasoning individually about each pixel without considering their contextual\nsemantics results in high uncertainty around the objects' boundaries and\nnumerous false positives. We propose a paradigm change by shifting from a\nper-pixel classification to a mask classification. Our mask-based method,\nMask2Anomaly, demonstrates the feasibility of integrating an anomaly detection\nmethod in a mask-classification architecture. Mask2Anomaly includes several\ntechnical novelties that are designed to improve the detection of anomalies in\nmasks: i) a global masked attention module to focus individually on the\nforeground and background regions; ii) a mask contrastive learning that\nmaximizes the margin between an anomaly and known classes; and iii) a mask\nrefinement solution to reduce false positives. Mask2Anomaly achieves new\nstate-of-the-art results across a range of benchmarks, both in the per-pixel\nand component-level evaluations. In particular, Mask2Anomaly reduces the\naverage false positives rate by 60% wrt the previous state-of-the-art. Github\npage:\nhttps://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation.",
        "authors": [
            "Shyam Nandan Rai",
            "Fabio Cermelli",
            "Dario Fontanel",
            "Carlo Masone",
            "Barbara Caputo"
        ]
    },
    {
        "title": "DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization",
        "url": "http://arxiv.org/abs/2308.10285",
        "abstract": "Deep Neural Networks have exhibited considerable success in various visual\ntasks. However, when applied to unseen test datasets, state-of-the-art models\noften suffer performance degradation due to domain shifts. In this paper, we\nintroduce a novel approach for domain generalization from a novel perspective\nof enhancing the robustness of channels in feature maps to domain shifts. We\nobserve that models trained on source domains contain a substantial number of\nchannels that exhibit unstable activations across different domains, which are\ninclined to capture domain-specific features and behave abnormally when exposed\nto unseen target domains. To address the issue, we propose a DomainDrop\nframework to continuously enhance the channel robustness to domain shifts,\nwhere a domain discriminator is used to identify and drop unstable channels in\nfeature maps of each network layer during forward propagation. We theoretically\nprove that our framework could effectively lower the generalization bound.\nExtensive experiments on several benchmarks indicate that our framework\nachieves state-of-the-art performance compared to other competing methods. Our\ncode is available at https://github.com/lingeringlight/DomainDrop.",
        "authors": [
            "Jintao Guo",
            "Lei Qi",
            "Yinghuan Shi"
        ]
    },
    {
        "title": "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation",
        "url": "http://arxiv.org/abs/2308.16909",
        "abstract": "Unconditional video generation is a challenging task that involves\nsynthesizing high-quality videos that are both coherent and of extended\nduration. To address this challenge, researchers have used pretrained StyleGAN\nimage generators for high-quality frame synthesis and focused on motion\ngenerator design. The motion generator is trained in an autoregressive manner\nusing heavy 3D convolutional discriminators to ensure motion coherence during\nvideo generation. In this paper, we introduce a novel motion generator design\nthat uses a learning-based inversion network for GAN. The encoder in our method\ncaptures rich and smooth priors from encoding images to latents, and given the\nlatent of an initially generated frame as guidance, our method can generate\nsmooth future latent by modulating the inversion encoder temporally. Our method\nenjoys the advantage of sparse training and naturally constrains the generation\nspace of our motion generator with the inversion network guided by the initial\nframe, eliminating the need for heavy discriminators. Moreover, our method\nsupports style transfer with simple fine-tuning when the encoder is paired with\na pretrained StyleGAN generator. Extensive experiments conducted on various\nbenchmarks demonstrate the superiority of our method in generating long and\nhigh-resolution videos with decent single-frame quality and temporal\nconsistency.",
        "authors": [
            "Yuhan Wang",
            "Liming Jiang",
            "Chen Change Loy"
        ]
    },
    {
        "title": "Self-Calibrated Cross Attention Network for Few-Shot Segmentation",
        "url": "http://arxiv.org/abs/2308.09294",
        "abstract": "The key to the success of few-shot segmentation (FSS) lies in how to\neffectively utilize support samples. Most solutions compress support foreground\n(FG) features into prototypes, but lose some spatial details. Instead, others\nuse cross attention to fuse query features with uncompressed support FG. Query\nFG could be fused with support FG, however, query background (BG) cannot find\nmatched BG features in support FG, yet inevitably integrates dissimilar\nfeatures. Besides, as both query FG and BG are combined with support FG, they\nget entangled, thereby leading to ineffective segmentation. To cope with these\nissues, we design a self-calibrated cross attention (SCCA) block. For efficient\npatch-based attention, query and support features are firstly split into\npatches. Then, we design a patch alignment module to align each query patch\nwith its most similar support patch for better cross attention. Specifically,\nSCCA takes a query patch as Q, and groups the patches from the same query image\nand the aligned patches from the support image as K&V. In this way, the query\nBG features are fused with matched BG features (from query patches), and thus\nthe aforementioned issues will be mitigated. Moreover, when calculating SCCA,\nwe design a scaled-cosine mechanism to better utilize the support features for\nsimilarity calculation. Extensive experiments conducted on PASCAL-5^i and\nCOCO-20^i demonstrate the superiority of our model, e.g., the mIoU score under\n5-shot setting on COCO-20^i is 5.6%+ better than previous state-of-the-arts.\nThe code is available at https://github.com/Sam1224/SCCAN.",
        "authors": [
            "Qianxiong Xu",
            "Wenting Zhao",
            "Guosheng Lin",
            "Cheng Long"
        ]
    },
    {
        "title": "Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis",
        "url": "http://arxiv.org/abs/2302.05615",
        "abstract": "Self-supervised learning (SSL) has recently achieved promising performance\nfor 3D medical image analysis tasks. Most current methods follow existing SSL\nparadigm originally designed for photographic or natural images, which cannot\nexplicitly and thoroughly exploit the intrinsic similar anatomical structures\nacross varying medical images. This may in fact degrade the quality of learned\ndeep representations by maximizing the similarity among features containing\nspatial misalignment information and different anatomical semantics. In this\nwork, we propose a new self-supervised learning framework, namely Alice, that\nexplicitly fulfills Anatomical invariance modeling and semantic alignment via\nelaborately combining discriminative and generative objectives. Alice\nintroduces a new contrastive learning strategy which encourages the similarity\nbetween views that are diversely mined but with consistent high-level\nsemantics, in order to learn invariant anatomical features. Moreover, we design\na conditional anatomical feature alignment module to complement corrupted\nembeddings with globally matched semantics and inter-patch topology\ninformation, conditioned by the distribution of local image content, which\npermits to create better contrastive pairs. Our extensive quantitative\nexperiments on three 3D medical image analysis tasks demonstrate and validate\nthe performance superiority of Alice, surpassing the previous best SSL\ncounterpart methods and showing promising ability for united representation\nlearning. Codes are available at https://github.com/alibaba-damo-academy/alice.",
        "authors": [
            "Yankai Jiang",
            "Mingze Sun",
            "Heng Guo",
            "Xiaoyu Bai",
            "Ke Yan",
            "Le Lu",
            "Minfeng Xu"
        ]
    },
    {
        "title": "Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images",
        "url": "http://arxiv.org/abs/2308.16758",
        "abstract": "Generating 3D faces from textual descriptions has a multitude of\napplications, such as gaming, movie, and robotics. Recent progresses have\ndemonstrated the success of unconditional 3D face generation and text-to-3D\nshape generation. However, due to the limited text-3D face data pairs,\ntext-driven 3D face generation remains an open problem. In this paper, we\npropose a text-guided 3D faces generation method, refer as TG-3DFace, for\ngenerating realistic 3D faces using text guidance. Specifically, we adopt an\nunconditional 3D face generation framework and equip it with text conditions,\nwhich learns the text-guided 3D face generation with only text-2D face data. On\ntop of that, we propose two text-to-face cross-modal alignment techniques,\nincluding the global contrastive learning and the fine-grained alignment\nmodule, to facilitate high semantic consistency between generated 3D faces and\ninput texts. Besides, we present directional classifier guidance during the\ninference process, which encourages creativity for out-of-domain generations.\nCompared to the existing methods, TG-3DFace creates more realistic and\naesthetically pleasing 3D faces, boosting 9% multi-view consistency (MVIC) over\nLatent3D. The rendered face images generated by TG-3DFace achieve higher FID\nand CLIP score than text-to-2D face/image generation models, demonstrating our\nsuperiority in generating realistic and semantic-consistent textures.",
        "authors": [
            "Cuican Yu",
            "Guansong Lu",
            "Yihan Zeng",
            "Jian Sun",
            "Xiaodan Liang",
            "Huibin Li",
            "Zongben Xu",
            "Songcen Xu",
            "Wei Zhang",
            "Hang Xu"
        ]
    },
    {
        "title": "ENTL: Embodied Navigation Trajectory Learner",
        "url": "http://arxiv.org/abs/2304.02639",
        "abstract": "We propose Embodied Navigation Trajectory Learner (ENTL), a method for\nextracting long sequence representations for embodied navigation. Our approach\nunifies world modeling, localization and imitation learning into a single\nsequence prediction task. We train our model using vector-quantized predictions\nof future states conditioned on current states and actions. ENTL's generic\narchitecture enables sharing of the spatio-temporal sequence encoder for\nmultiple challenging embodied tasks. We achieve competitive performance on\nnavigation tasks using significantly less data than strong baselines while\nperforming auxiliary tasks such as localization and future frame prediction (a\nproxy for world modeling). A key property of our approach is that the model is\npre-trained without any explicit reward signal, which makes the resulting model\ngeneralizable to multiple tasks and environments.",
        "authors": [
            "Klemen Kotar",
            "Aaron Walsman",
            "Roozbeh Mottaghi"
        ]
    },
    {
        "title": "Learning Global-aware Kernel for Image Harmonization",
        "url": "http://arxiv.org/abs/2305.11676",
        "abstract": "Image harmonization aims to solve the visual inconsistency problem in\ncomposited images by adaptively adjusting the foreground pixels with the\nbackground as references. Existing methods employ local color transformation or\nregion matching between foreground and background, which neglects powerful\nproximity prior and independently distinguishes fore-/back-ground as a whole\npart for harmonization. As a result, they still show a limited performance\nacross varied foreground objects and scenes. To address this issue, we propose\na novel Global-aware Kernel Network (GKNet) to harmonize local regions with\ncomprehensive consideration of long-distance background references.\nSpecifically, GKNet includes two parts, \\ie, harmony kernel prediction and\nharmony kernel modulation branches. The former includes a Long-distance\nReference Extractor (LRE) to obtain long-distance context and Kernel Prediction\nBlocks (KPB) to predict multi-level harmony kernels by fusing global\ninformation with local features. To achieve this goal, a novel Selective\nCorrelation Fusion (SCF) module is proposed to better select relevant\nlong-distance background references for local harmonization. The latter employs\nthe predicted kernels to harmonize foreground regions with both local and\nglobal awareness. Abundant experiments demonstrate the superiority of our\nmethod for image harmonization over state-of-the-art methods, \\eg, achieving\n39.53dB PSNR that surpasses the best counterpart by +0.78dB $\\uparrow$;\ndecreasing fMSE/MSE by 11.5\\%$\\downarrow$/6.7\\%$\\downarrow$ compared with the\nSoTA method. Code will be available at\n\\href{https://github.com/XintianShen/GKNet}{here}.",
        "authors": [
            "Xintian Shen",
            "Jiangning Zhang",
            "Jun Chen",
            "Shipeng Bai",
            "Yue Han",
            "Yabiao Wang",
            "Chengjie Wang",
            "Yong Liu"
        ]
    },
    {
        "title": "ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer",
        "url": "http://arxiv.org/abs/2308.10147",
        "abstract": "In recent years, end-to-end scene text spotting approaches are evolving to\nthe Transformer-based framework. While previous studies have shown the crucial\nimportance of the intrinsic synergy between text detection and recognition,\nrecent advances in Transformer-based methods usually adopt an implicit synergy\nstrategy with shared query, which can not fully realize the potential of these\ntwo interactive tasks. In this paper, we argue that the explicit synergy\nconsidering distinct characteristics of text detection and recognition can\nsignificantly improve the performance text spotting. To this end, we introduce\na new model named Explicit Synergy-based Text Spotting Transformer framework\n(ESTextSpotter), which achieves explicit synergy by modeling discriminative and\ninteractive features for text detection and recognition within a single\ndecoder. Specifically, we decompose the conventional shared query into\ntask-aware queries for text polygon and content, respectively. Through the\ndecoder with the proposed vision-language communication module, the queries\ninteract with each other in an explicit manner while preserving discriminative\npatterns of text detection and recognition, thus improving performance\nsignificantly. Additionally, we propose a task-aware query initialization\nscheme to ensure stable training. Experimental results demonstrate that our\nmodel significantly outperforms previous state-of-the-art methods. Code is\navailable at https://github.com/mxin262/ESTextSpotter.",
        "authors": [
            "Mingxin Huang",
            "Jiaxin Zhang",
            "Dezhi Peng",
            "Hao Lu",
            "Can Huang",
            "Yuliang Liu",
            "Xiang Bai",
            "Lianwen Jin"
        ]
    },
    {
        "title": "UGC: Unified GAN Compression for Efficient Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2309.09310",
        "abstract": "Recent years have witnessed the prevailing progress of Generative Adversarial\nNetworks (GANs) in image-to-image translation. However, the success of these\nGAN models hinges on ponderous computational costs and labor-expensive training\ndata. Current efficient GAN learning techniques often fall into two orthogonal\naspects: i) model slimming via reduced calculation costs;\nii)data/label-efficient learning with fewer training data/labels. To combine\nthe best of both worlds, we propose a new learning paradigm, Unified GAN\nCompression (UGC), with a unified optimization objective to seamlessly prompt\nthe synergy of model-efficient and label-efficient learning. UGC sets up\nsemi-supervised-driven network architecture search and adaptive online\nsemi-supervised distillation stages sequentially, which formulates a\nheterogeneous mutual learning scheme to obtain an architecture-flexible,\nlabel-efficient, and performance-excellent model.",
        "authors": [
            "Yuxi Ren",
            "Jie Wu",
            "Peng Zhang",
            "Manlin Zhang",
            "Xuefeng Xiao",
            "Qian He",
            "Rui Wang",
            "Min Zheng",
            "Xin Pan"
        ]
    },
    {
        "title": "Efficient View Synthesis with Neural Radiance Distribution Field",
        "url": "http://arxiv.org/abs/2308.11130",
        "abstract": "Recent work on Neural Radiance Fields (NeRF) has demonstrated significant\nadvances in high-quality view synthesis. A major limitation of NeRF is its low\nrendering efficiency due to the need for multiple network forwardings to render\na single pixel. Existing methods to improve NeRF either reduce the number of\nrequired samples or optimize the implementation to accelerate the network\nforwarding. Despite these efforts, the problem of multiple sampling persists\ndue to the intrinsic representation of radiance fields. In contrast, Neural\nLight Fields (NeLF) reduce the computation cost of NeRF by querying only one\nsingle network forwarding per pixel. To achieve a close visual quality to NeRF,\nexisting NeLF methods require significantly larger network capacities which\nlimits their rendering efficiency in practice. In this work, we propose a new\nrepresentation called Neural Radiance Distribution Field (NeRDF) that targets\nefficient view synthesis in real-time. Specifically, we use a small network\nsimilar to NeRF while preserving the rendering speed with a single network\nforwarding per pixel as in NeLF. The key is to model the radiance distribution\nalong each ray with frequency basis and predict frequency weights using the\nnetwork. Pixel values are then computed via volume rendering on radiance\ndistributions. Experiments show that our proposed method offers a better\ntrade-off among speed, quality, and network size than existing methods: we\nachieve a ~254x speed-up over NeRF with similar network size, with only a\nmarginal performance decline. Our project page is at\nyushuang-wu.github.io/NeRDF.",
        "authors": [
            "Yushuang Wu",
            "Xiao Li",
            "Jinglu Wang",
            "Xiaoguang Han",
            "Shuguang Cui",
            "Yan Lu"
        ]
    },
    {
        "title": "MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition",
        "url": "http://arxiv.org/abs/2303.05309",
        "abstract": "Multi-media communications facilitate global interaction among people.\nHowever, despite researchers exploring cross-lingual translation techniques\nsuch as machine translation and audio speech translation to overcome language\nbarriers, there is still a shortage of cross-lingual studies on visual speech.\nThis lack of research is mainly due to the absence of datasets containing\nvisual speech and translated text pairs. In this paper, we present\n\\textbf{AVMuST-TED}, the first dataset for \\textbf{A}udio-\\textbf{V}isual\n\\textbf{Mu}ltilingual \\textbf{S}peech \\textbf{T}ranslation, derived from\n\\textbf{TED} talks. Nonetheless, visual speech is not as distinguishable as\naudio speech, making it difficult to develop a mapping from source speech\nphonemes to the target language text. To address this issue, we propose\nMixSpeech, a cross-modality self-learning framework that utilizes audio speech\nto regularize the training of visual speech tasks. To further minimize the\ncross-modality gap and its impact on knowledge transfer, we suggest adopting\nmixed speech, which is created by interpolating audio and visual streams, along\nwith a curriculum learning strategy to adjust the mixing ratio as needed.\nMixSpeech enhances speech translation in noisy environments, improving BLEU\nscores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves\nstate-of-the-art performance in lip reading on CMLR (11.1\\%), LRS2 (25.5\\%),\nand LRS3 (28.0\\%).",
        "authors": [
            "Xize Cheng",
            "Linjun Li",
            "Tao Jin",
            "Rongjie Huang",
            "Wang Lin",
            "Zehan Wang",
            "Huangdai Liu",
            "Ye Wang",
            "Aoxiong Yin",
            "Zhou Zhao"
        ]
    },
    {
        "title": "Chordal Averaging on Flag Manifolds and Its Applications",
        "url": "http://arxiv.org/abs/2303.13501",
        "abstract": "This paper presents a new, provably-convergent algorithm for computing the\nflag-mean and flag-median of a set of points on a flag manifold under the\nchordal metric. The flag manifold is a mathematical space consisting of flags,\nwhich are sequences of nested subspaces of a vector space that increase in\ndimension. The flag manifold is a superset of a wide range of known matrix\nspaces, including Stiefel and Grassmanians, making it a general object that is\nuseful in a wide variety computer vision problems.\n  To tackle the challenge of computing first order flag statistics, we first\ntransform the problem into one that involves auxiliary variables constrained to\nthe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and\nleveraging the numerical stability and efficiency of Stiefel-manifold\noptimization enables us to compute the flag-mean effectively. Through a series\nof experiments, we show the competence of our method in Grassmann and rotation\naveraging, as well as principal component analysis. We release our source code\nunder https://github.com/nmank/FlagAveraging.",
        "authors": [
            "Nathan Mankovich",
            "Tolga Birdal"
        ]
    },
    {
        "title": "Towards Building More Robust Models with Frequency Bias",
        "url": "http://arxiv.org/abs/2307.09763",
        "abstract": "The vulnerability of deep neural networks to adversarial samples has been a\nmajor impediment to their broad applications, despite their success in various\nfields. Recently, some works suggested that adversarially-trained models\nemphasize the importance of low-frequency information to achieve higher\nrobustness. While several attempts have been made to leverage this frequency\ncharacteristic, they have all faced the issue that applying low-pass filters\ndirectly to input images leads to irreversible loss of discriminative\ninformation and poor generalizability to datasets with distinct frequency\nfeatures. This paper presents a plug-and-play module called the Frequency\nPreference Control Module that adaptively reconfigures the low- and\nhigh-frequency components of intermediate feature representations, providing\nbetter utilization of frequency in robust learning. Empirical studies show that\nour proposed module can be easily incorporated into any adversarial training\nframework, further improving model robustness across different architectures\nand datasets. Additionally, experiments were conducted to examine how the\nfrequency bias of robust models impacts the adversarial training process and\nits final robustness, revealing interesting insights.",
        "authors": [
            "Qingwen Bu",
            "Dong Huang",
            "Heming Cui"
        ]
    },
    {
        "title": "SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos",
        "url": "http://arxiv.org/abs/2308.09244",
        "abstract": "Camera-based 3D object detection in BEV (Bird's Eye View) space has drawn\ngreat attention over the past few years. Dense detectors typically follow a\ntwo-stage pipeline by first constructing a dense BEV feature and then\nperforming object detection in BEV space, which suffers from complex view\ntransformations and high computation cost. On the other side, sparse detectors\nfollow a query-based paradigm without explicit dense BEV feature construction,\nbut achieve worse performance than the dense counterparts. In this paper, we\nfind that the key to mitigate this performance gap is the adaptability of the\ndetector in both BEV and image space. To achieve this goal, we propose\nSparseBEV, a fully sparse 3D object detector that outperforms the dense\ncounterparts. SparseBEV contains three key designs, which are (1)\nscale-adaptive self attention to aggregate features with adaptive receptive\nfield in BEV space, (2) adaptive spatio-temporal sampling to generate sampling\nlocations under the guidance of queries, and (3) adaptive mixing to decode the\nsampled features with dynamic weights from the queries. On the test split of\nnuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On\nthe val split, SparseBEV achieves 55.8 NDS while maintaining a real-time\ninference speed of 23.5 FPS. Code is available at\nhttps://github.com/MCG-NJU/SparseBEV.",
        "authors": [
            "Haisong Liu",
            "Yao Teng",
            "Tao Lu",
            "Haiguang Wang",
            "Limin Wang"
        ]
    },
    {
        "title": "When Prompt-based Incremental Learning Does Not Meet Strong Pretraining",
        "url": "http://arxiv.org/abs/2308.10445",
        "abstract": "Incremental learning aims to overcome catastrophic forgetting when learning\ndeep networks from sequential tasks. With impressive learning efficiency and\nperformance, prompt-based methods adopt a fixed backbone to sequential tasks by\nlearning task-specific prompts. However, existing prompt-based methods heavily\nrely on strong pretraining (typically trained on ImageNet-21k), and we find\nthat their models could be trapped if the potential gap between the pretraining\ntask and unknown future tasks is large. In this work, we develop a learnable\nAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval and\nprompt learning processes into a learnable prompt generator. Hence, the whole\nprompting process can be optimized to reduce the negative effects of the gap\nbetween tasks effectively. To make our APG avoid learning ineffective\nknowledge, we maintain a knowledge pool to regularize APG with the feature\ndistribution of each class. Extensive experiments show that our method\nsignificantly outperforms advanced methods in exemplar-free incremental\nlearning without (strong) pretraining. Besides, under strong retraining, our\nmethod also has comparable performance to existing prompt-based models, showing\nthat our method can still benefit from pretraining. Codes can be found at\nhttps://github.com/TOM-tym/APG",
        "authors": [
            "Yu-Ming Tang",
            "Yi-Xing Peng",
            "Wei-Shi Zheng"
        ]
    },
    {
        "title": "Exploring Transformers for Open-world Instance Segmentation",
        "url": "http://arxiv.org/abs/2308.04206",
        "abstract": "Open-world instance segmentation is a rising task, which aims to segment all\nobjects in the image by learning from a limited number of base-category\nobjects. This task is challenging, as the number of unseen categories could be\nhundreds of times larger than that of seen categories. Recently, the DETR-like\nmodels have been extensively studied in the closed world while stay unexplored\nin the open world. In this paper, we utilize the Transformer for open-world\ninstance segmentation and present SWORD. Firstly, we introduce to attach the\nstop-gradient operation before classification head and further add IoU heads\nfor discovering novel objects. We demonstrate that a simple stop-gradient\noperation not only prevents the novel objects from being suppressed as\nbackground, but also allows the network to enjoy the merit of heuristic label\nassignment. Secondly, we propose a novel contrastive learning framework to\nenlarge the representations between objects and background. Specifically, we\nmaintain a universal object queue to obtain the object center, and dynamically\nselect positive and negative samples from the object queries for contrastive\nlearning. While the previous works only focus on pursuing average recall and\nneglect average precision, we show the prominence of SWORD by giving\nconsideration to both criteria. Our models achieve state-of-the-art performance\nin various open-world cross-category and cross-dataset generalizations.\nParticularly, in VOC to non-VOC setup, our method sets new state-of-the-art\nresults of 40.0% on ARb100 and 34.9% on ARm100. For COCO to UVO generalization,\nSWORD significantly outperforms the previous best open-world model by 5.9% on\nAPm and 8.1% on ARm100.",
        "authors": [
            "Jiannan Wu",
            "Yi Jiang",
            "Bin Yan",
            "Huchuan Lu",
            "Zehuan Yuan",
            "Ping Luo"
        ]
    },
    {
        "title": "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers",
        "url": "http://arxiv.org/abs/2308.11662",
        "abstract": "Visual question answering is a task of predicting the answer to a question\nabout an image. Given that different people can provide different answers to a\nvisual question, we aim to better understand why with answer groundings. We\nintroduce the first dataset that visually grounds each unique answer to each\nvisual question, which we call VQAAnswerTherapy. We then propose two novel\nproblems of predicting whether a visual question has a single answer grounding\nand localizing all answer groundings. We benchmark modern algorithms for these\nnovel problems to show where they succeed and struggle. The dataset and\nevaluation server can be found publicly at\nhttps://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/.",
        "authors": [
            "Chongyan Chen",
            "Samreen Anjum",
            "Danna Gurari"
        ]
    },
    {
        "title": "Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2308.05911",
        "abstract": "Multi-object tracking (MOT) at low frame rates can reduce computational,\nstorage and power overhead to better meet the constraints of edge devices. Many\nexisting MOT methods suffer from significant performance degradation in\nlow-frame-rate videos due to significant location and appearance changes\nbetween adjacent frames. To this end, we propose to explore collaborative\ntracking learning (ColTrack) for frame-rate-insensitive MOT in a query-based\nend-to-end manner. Multiple historical queries of the same target jointly track\nit with richer temporal descriptions. Meanwhile, we insert an information\nrefinement module between every two temporal blocking decoders to better fuse\ntemporal clues and refine features. Moreover, a tracking object consistency\nloss is proposed to guide the interaction between historical queries. Extensive\nexperimental results demonstrate that in high-frame-rate videos, ColTrack\nobtains higher performance than state-of-the-art methods on large-scale\ndatasets Dancetrack and BDD100K, and outperforms the existing end-to-end\nmethods on MOT17. More importantly, ColTrack has a significant advantage over\nstate-of-the-art methods in low-frame-rate videos, which allows it to obtain\nfaster processing speeds by reducing frame-rate requirements while maintaining\nhigher performance. Code will be released at\nhttps://github.com/yolomax/ColTrack",
        "authors": [
            "Yiheng Liu",
            "Junta Wu",
            "Yi Fu"
        ]
    },
    {
        "title": "Tangent Model Composition for Ensembling and Continual Fine-tuning",
        "url": "http://arxiv.org/abs/2307.08114",
        "abstract": "Tangent Model Composition (TMC) is a method to combine component models\nindependently fine-tuned around a pre-trained point. Component models are\ntangent vectors to the pre-trained model that can be added, scaled, or\nsubtracted to support incremental learning, ensembling, or unlearning.\nComponent models are composed at inference time via scalar combination,\nreducing the cost of ensembling to that of a single model. TMC improves\naccuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a\n2.5x to 10x reduction of inference cost, growing linearly with the number of\ncomponent models. Each component model can be forgotten at zero cost, with no\nresidual effect on the resulting inference. When used for continual\nfine-tuning, TMC is not constrained by sequential bias and can be executed in\nparallel on federated data. TMC outperforms recently published continual\nfine-tuning methods almost uniformly on each setting -- task-incremental,\nclass-incremental, and data-incremental -- on a total of 13 experiments across\n3 benchmark datasets, despite not using any replay buffer. TMC is designed for\ncomposing models that are local to a pre-trained embedding, but could be\nextended to more general settings. The code is available at:\nhttps://github.com/tianyu139/tangent-model-composition",
        "authors": [
            "Tian Yu Liu",
            "Stefano Soatto"
        ]
    },
    {
        "title": "Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations",
        "url": "http://arxiv.org/abs/2303.08135",
        "abstract": "The field of visual representation learning has seen explosive growth in the\npast years, but its benefits in robotics have been surprisingly limited so far.\nPrior work uses generic visual representations as a basis to learn\n(task-specific) robot action policies (e.g., via behavior cloning). While the\nvisual representations do accelerate learning, they are primarily used to\nencode visual observations. Thus, action information has to be derived purely\nfrom robot data, which is expensive to collect! In this work, we present a\nscalable alternative where the visual representations can help directly infer\nrobot actions. We observe that vision encoders express relationships between\nimage observations as distances (e.g., via embedding dot product) that could be\nused to efficiently plan robot behavior. We operationalize this insight and\ndevelop a simple algorithm for acquiring a distance function and dynamics\npredictor, by fine-tuning a pre-trained representation on human collected video\nsequences. The final method is able to substantially outperform traditional\nrobot learning baselines (e.g., 70% success v.s. 50% for behavior cloning on\npick-place) on a suite of diverse real-world manipulation tasks. It can also\ngeneralize to novel objects, without using any robot demonstrations during\ntrain time. For visualizations of the learned policies please check:\nhttps://agi-labs.github.io/manipulate-by-seeing/.",
        "authors": [
            "Jianren Wang",
            "Sudeep Dasari",
            "Mohan Kumar Srirama",
            "Shubham Tulsiani",
            "Abhinav Gupta"
        ]
    },
    {
        "title": "Learning Human-Human Interactions in Images from Weak Textual Supervision",
        "url": "http://arxiv.org/abs/2304.14104",
        "abstract": "Interactions between humans are diverse and context-dependent, but previous\nworks have treated them as categorical, disregarding the heavy tail of possible\ninteractions. We propose a new paradigm of learning human-human interactions as\nfree text from a single still image, allowing for flexibility in modeling the\nunlimited space of situations and relationships between people. To overcome the\nabsence of data labelled specifically for this task, we use knowledge\ndistillation applied to synthetic caption data produced by a large language\nmodel without explicit supervision. We show that the pseudo-labels produced by\nthis procedure can be used to train a captioning model to effectively\nunderstand human-human interactions in images, as measured by a variety of\nmetrics that measure textual and semantic faithfulness and factual groundedness\nof our predictions. We further show that our approach outperforms SOTA image\ncaptioning and situation recognition models on this task. We will release our\ncode and pseudo-labels along with Waldo and Wenda, a manually-curated test set\nfor still image human-human interaction understanding.",
        "authors": [
            "Morris Alper",
            "Hadar Averbuch-Elor"
        ]
    },
    {
        "title": "Prompt-aligned Gradient for Prompt Tuning",
        "url": "http://arxiv.org/abs/2205.14865",
        "abstract": "Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we\ncan craft a zero-shot classifier by \"prompt\", e.g., the confidence score of an\nimage being \"[CLASS]\" can be obtained by using the VLM provided similarity\nmeasure between the image and the prompt sentence \"a photo of a [CLASS]\".\nTherefore, prompt shows a great potential for fast adaptation of VLMs to\ndownstream tasks if we fine-tune the prompt-based similarity measure. However,\nwe find a common failure that improper fine-tuning may not only undermine the\nprompt's inherent prediction for the task-related classes, but also for other\nclasses in the VLM vocabulary. Existing methods still address this problem by\nusing traditional anti-overfitting techniques such as early stopping and data\naugmentation, which lack a principled solution specific to prompt. We present\nPrompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from\nforgetting the the general knowledge learned from VLMs. In particular, ProGrad\nonly updates the prompt whose gradient is aligned (or non-conflicting) to the\n\"general direction\", which is represented as the gradient of the KL loss of the\npre-defined prompt prediction. Extensive experiments demonstrate the stronger\nfew-shot generalization ability of ProGrad over state-of-the-art prompt tuning\nmethods. Codes are available at https://github.com/BeierZhu/Prompt-align.",
        "authors": [
            "Beier Zhu",
            "Yulei Niu",
            "Yucheng Han",
            "Yue Wu",
            "Hanwang Zhang"
        ]
    },
    {
        "title": "Diffusion Action Segmentation",
        "url": "http://arxiv.org/abs/2303.17959",
        "abstract": "Temporal action segmentation is crucial for understanding long-form videos.\nPrevious works on this task commonly adopt an iterative refinement paradigm by\nusing multi-stage models. We propose a novel framework via denoising diffusion\nmodels, which nonetheless shares the same inherent spirit of such iterative\nrefinement. In this framework, action predictions are iteratively generated\nfrom random noise with input video features as conditions. To enhance the\nmodeling of three striking characteristics of human actions, including the\nposition prior, the boundary ambiguity, and the relational dependency, we\ndevise a unified masking strategy for the conditioning inputs in our framework.\nExtensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and\nBreakfast, are performed and the proposed method achieves superior or\ncomparable results to state-of-the-art methods, showing the effectiveness of a\ngenerative approach for action segmentation.",
        "authors": [
            "Daochang Liu",
            "Qiyue Li",
            "AnhDung Dinh",
            "Tingting Jiang",
            "Mubarak Shah",
            "Chang Xu"
        ]
    },
    {
        "title": "Exemplar-Free Continual Transformer with Convolutions",
        "url": "http://arxiv.org/abs/2308.11357",
        "abstract": "Continual Learning (CL) involves training a machine learning model in a\nsequential manner to learn new information while retaining previously learned\ntasks without the presence of previous training data. Although there has been\nsignificant interest in CL, most recent CL approaches in computer vision have\nfocused on convolutional architectures only. However, with the recent success\nof vision transformers, there is a need to explore their potential for CL.\nAlthough there have been some recent CL approaches for vision transformers,\nthey either store training instances of previous tasks or require a task\nidentifier during test time, which can be limiting. This paper proposes a new\nexemplar-free approach for class/task incremental learning called ConTraCon,\nwhich does not require task-id to be explicitly present during inference and\navoids the need for storing previous training instances. The proposed approach\nleverages the transformer architecture and involves re-weighting the key,\nquery, and value weights of the multi-head self-attention layers of a\ntransformer trained on a similar task. The re-weighting is done using\nconvolution, which enables the approach to maintain low parameter requirements\nper task. Additionally, an image augmentation-based entropic task\nidentification approach is used to predict tasks without requiring task-ids\nduring inference. Experiments on four benchmark datasets demonstrate that the\nproposed approach outperforms several competitive approaches while requiring\nfewer parameters.",
        "authors": [
            "Anurag Roy",
            "Vinay Kumar Verma",
            "Sravan Voonna",
            "Kripabandhu Ghosh",
            "Saptarshi Ghosh",
            "Abir Das"
        ]
    },
    {
        "title": "Scalable Video Object Segmentation with Simplified Framework",
        "url": "http://arxiv.org/abs/2308.09903",
        "abstract": "The current popular methods for video object segmentation (VOS) implement\nfeature matching through several hand-crafted modules that separately perform\nfeature extraction and matching. However, the above hand-crafted designs\nempirically cause insufficient target interaction, thus limiting the dynamic\ntarget-aware feature learning in VOS. To tackle these limitations, this paper\npresents a scalable Simplified VOS (SimVOS) framework to perform joint feature\nextraction and matching by leveraging a single transformer backbone.\nSpecifically, SimVOS employs a scalable ViT backbone for simultaneous feature\nextraction and matching between query and reference features. This design\nenables SimVOS to learn better target-ware features for accurate mask\nprediction. More importantly, SimVOS could directly apply well-pretrained ViT\nbackbones (e.g., MAE) for VOS, which bridges the gap between VOS and\nlarge-scale self-supervised pre-training. To achieve a better performance-speed\ntrade-off, we further explore within-frame attention and propose a new token\nrefinement module to improve the running speed and save computational cost.\nExperimentally, our SimVOS achieves state-of-the-art results on popular video\nobject segmentation benchmarks, i.e., DAVIS-2017 (88.0% J&F), DAVIS-2016 (92.9%\nJ&F) and YouTube-VOS 2019 (84.2% J&F), without applying any synthetic video or\nBL30K pre-training used in previous VOS approaches.",
        "authors": [
            "Qiangqiang Wu",
            "Tianyu Yang",
            "Wei WU",
            "Antoni Chan"
        ]
    },
    {
        "title": "Efficient Decision-based Black-box Patch Attacks on Video Recognition",
        "url": "http://arxiv.org/abs/2303.11917",
        "abstract": "Although Deep Neural Networks (DNNs) have demonstrated excellent performance,\nthey are vulnerable to adversarial patches that introduce perceptible and\nlocalized perturbations to the input. Generating adversarial patches on images\nhas received much attention, while adversarial patches on videos have not been\nwell investigated. Further, decision-based attacks, where attackers only access\nthe predicted hard labels by querying threat models, have not been well\nexplored on video models either, even if they are practical in real-world video\nrecognition scenes. The absence of such studies leads to a huge gap in the\nrobustness assessment for video models. To bridge this gap, this work first\nexplores decision-based patch attacks on video models. We analyze that the huge\nparameter space brought by videos and the minimal information returned by\ndecision-based models both greatly increase the attack difficulty and query\nburden. To achieve a query-efficient attack, we propose a spatial-temporal\ndifferential evolution (STDE) framework. First, STDE introduces target videos\nas patch textures and only adds patches on keyframes that are adaptively\nselected by temporal difference. Second, STDE takes minimizing the patch area\nas the optimization objective and adopts spatialtemporal mutation and crossover\nto search for the global optimum without falling into the local optimum.\nExperiments show STDE has demonstrated state-of-the-art performance in terms of\nthreat, efficiency and imperceptibility. Hence, STDE has the potential to be a\npowerful tool for evaluating the robustness of video recognition models.",
        "authors": [
            "Kaixun Jiang",
            "Zhaoyu Chen",
            "Hao Huang",
            "Jiafeng Wang",
            "Dingkang Yang",
            "Bo Li",
            "Yan Wang",
            "Wenqiang Zhang"
        ]
    },
    {
        "title": "Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV",
        "url": "http://arxiv.org/abs/2307.10713",
        "abstract": "Self-supervised monocular depth estimation (SS-MDE) has the potential to\nscale to vast quantities of data. Unfortunately, existing approaches limit\nthemselves to the automotive domain, resulting in models incapable of\ngeneralizing to complex environments such as natural or indoor settings.\n  To address this, we propose a large-scale SlowTV dataset curated from\nYouTube, containing an order of magnitude more data than existing automotive\ndatasets. SlowTV contains 1.7M images from a rich diversity of environments,\nsuch as worldwide seasonal hiking, scenic driving and scuba diving. Using this\ndataset, we train an SS-MDE model that provides zero-shot generalization to a\nlarge collection of indoor/outdoor datasets. The resulting model outperforms\nall existing SSL approaches and closes the gap on supervised SoTA, despite\nusing a more efficient architecture.\n  We additionally introduce a collection of best-practices to further maximize\nperformance and zero-shot generalization. This includes 1) aspect ratio\naugmentation, 2) camera intrinsic estimation, 3) support frame randomization\nand 4) flexible motion estimation. Code is available at\nhttps://github.com/jspenmar/slowtv_monodepth.",
        "authors": [
            "Jaime Spencer",
            "Chris Russell",
            "Simon Hadfield",
            "Richard Bowden"
        ]
    },
    {
        "title": "MetaGCD: Learning to Continually Learn in Generalized Category Discovery",
        "url": "http://arxiv.org/abs/2308.11063",
        "abstract": "In this paper, we consider a real-world scenario where a model that is\ntrained on pre-defined classes continually encounters unlabeled data that\ncontains both known and novel classes. The goal is to continually discover\nnovel classes while maintaining the performance in known classes. We name the\nsetting Continual Generalized Category Discovery (C-GCD). Existing methods for\nnovel class discovery cannot directly handle the C-GCD setting due to some\nunrealistic assumptions, such as the unlabeled data only containing novel\nclasses. Furthermore, they fail to discover novel classes in a continual\nfashion. In this work, we lift all these assumptions and propose an approach,\ncalled MetaGCD, to learn how to incrementally discover with less forgetting.\nOur proposed method uses a meta-learning framework and leverages the offline\nlabeled data to simulate the testing incremental learning process. A\nmeta-objective is defined to revolve around two conflicting learning objectives\nto achieve novel class discovery without forgetting. Furthermore, a soft\nneighborhood-based contrastive network is proposed to discriminate uncorrelated\nimages while attracting correlated images. We build strong baselines and\nconduct extensive experiments on three widely used benchmarks to demonstrate\nthe superiority of our method.",
        "authors": [
            "Yanan Wu",
            "Zhixiang Chi",
            "Yang Wang",
            "Songhe Feng"
        ]
    },
    {
        "title": "SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability",
        "url": "http://arxiv.org/abs/2208.09418",
        "abstract": "Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.\nDespite great efforts made by the Explainable AI (XAI) community, explanations\nlack robustness -- indistinguishable input perturbations may lead to different\nXAI results. Thus, it is vital to assess how robust DL interpretability is,\ngiven an XAI method. In this paper, we identify several challenges that the\nstate-of-the-art is unable to cope with collectively: i) existing metrics are\nnot comprehensive; ii) XAI techniques are highly heterogeneous; iii)\nmisinterpretations are normally rare events. To tackle these challenges, we\nintroduce two black-box evaluation methods, concerning the worst-case\ninterpretation discrepancy and a probabilistic notion of how robust in general,\nrespectively. Genetic Algorithm (GA) with bespoke fitness function is used to\nsolve constrained optimisation for efficient worst-case evaluation. Subset\nSimulation (SS), dedicated to estimate rare event probabilities, is used for\nevaluating overall robustness. Experiments show that the accuracy, sensitivity,\nand efficiency of our methods outperform the state-of-the-arts. Finally, we\ndemonstrate two applications of our methods: ranking robust XAI methods and\nselecting training schemes to improve both classification and interpretation\nrobustness.",
        "authors": [
            "Wei Huang",
            "Xingyu Zhao",
            "Gaojie Jin",
            "Xiaowei Huang"
        ]
    },
    {
        "title": "Towards General Low-Light Raw Noise Synthesis and Modeling",
        "url": "http://arxiv.org/abs/2307.16508",
        "abstract": "Modeling and synthesizing low-light raw noise is a fundamental problem for\ncomputational photography and image processing applications. Although most\nrecent works have adopted physics-based models to synthesize noise, the\nsignal-independent noise in low-light conditions is far more complicated and\nvaries dramatically across camera sensors, which is beyond the description of\nthese models. To address this issue, we introduce a new perspective to\nsynthesize the signal-independent noise by a generative model. Specifically, we\nsynthesize the signal-dependent and signal-independent noise in a physics- and\nlearning-based manner, respectively. In this way, our method can be considered\nas a general model, that is, it can simultaneously learn different noise\ncharacteristics for different ISO levels and generalize to various sensors.\nSubsequently, we present an effective multi-scale discriminator termed Fourier\ntransformer discriminator (FTD) to distinguish the noise distribution\naccurately. Additionally, we collect a new low-light raw denoising (LRD)\ndataset for training and benchmarking. Qualitative validation shows that the\nnoise generated by our proposed noise model can be highly similar to the real\nnoise in terms of distribution. Furthermore, extensive denoising experiments\ndemonstrate that our method performs favorably against state-of-the-art methods\non different sensors.",
        "authors": [
            "Feng Zhang",
            "Bin Xu",
            "Zhiqiang Li",
            "Xinran Liu",
            "Qingbo Lu",
            "Changxin Gao",
            "Nong Sang"
        ]
    },
    {
        "title": "What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Networks",
        "url": "http://arxiv.org/abs/2307.15860",
        "abstract": "In recent decades, Generative Adversarial Network (GAN) and its variants have\nachieved unprecedented success in image synthesis. However, well-trained GANs\nare under the threat of illegal steal or leakage. The prior studies on remote\nownership verification assume a black-box setting where the defender can query\nthe suspicious model with specific inputs, which we identify is not enough for\ngeneration tasks. To this end, in this paper, we propose a novel IP protection\nscheme for GANs where ownership verification can be done by checking outputs\nonly, without choosing the inputs (i.e., box-free setting). Specifically, we\nmake use of the unexploited potential of the discriminator to learn a\nhypersphere that captures the unique distribution learned by the paired\ngenerator. Extensive evaluations on two popular GAN tasks and more than 10 GAN\narchitectures demonstrate our proposed scheme to effectively verify the\nownership. Our proposed scheme shown to be immune to popular input-based\nremoval attacks and robust against other existing attacks. The source code and\nmodels are available at\nhttps://github.com/AbstractTeen/gan_ownership_verification",
        "authors": [
            "Ziheng Huang",
            "Boheng Li",
            "Yan Cai",
            "Run Wang",
            "Shangwei Guo",
            "Liming Fang",
            "Jing Chen",
            "Lina Wang"
        ]
    },
    {
        "title": "When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method",
        "url": "http://arxiv.org/abs/2211.10955",
        "abstract": "Real-world large-scale datasets are both noisily labeled and\nclass-imbalanced. The issues seriously hurt the generalization of trained\nmodels. It is hence significant to address the simultaneous incorrect labeling\nand class-imbalance, i.e., the problem of learning with noisy labels on\nlong-tailed data. Previous works develop several methods for the problem.\nHowever, they always rely on strong assumptions that are invalid or hard to be\nchecked in practice. In this paper, to handle the problem and address the\nlimitations of prior works, we propose a representation calibration method\nRCAL. Specifically, RCAL works with the representations extracted by\nunsupervised contrastive learning. We assume that without incorrect labeling\nand class imbalance, the representations of instances in each class conform to\na multivariate Gaussian distribution, which is much milder and easier to be\nchecked. Based on the assumption, we recover underlying representation\ndistributions from polluted ones resulting from mislabeled and class-imbalanced\ndata. Additional data points are then sampled from the recovered distributions\nto help generalization. Moreover, during classifier training, representation\nlearning takes advantage of representation robustness brought by contrastive\nlearning, which further improves the classifier performance. We derive\ntheoretical results to discuss the effectiveness of our representation\ncalibration. Experiments on multiple benchmarks justify our claims and confirm\nthe superiority of the proposed method.",
        "authors": [
            "Manyi Zhang",
            "Xuyang Zhao",
            "Jun Yao",
            "Chun Yuan",
            "Weiran Huang"
        ]
    },
    {
        "title": "Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement",
        "url": "http://arxiv.org/abs/2303.08983",
        "abstract": "We propose Dataset Reinforcement, a strategy to improve a dataset once such\nthat the accuracy of any model architecture trained on the reinforced dataset\nis improved at no additional training cost for users. We propose a Dataset\nReinforcement strategy based on data augmentation and knowledge distillation.\nOur generic strategy is designed based on extensive analysis across CNN- and\ntransformer-based models and performing large-scale study of distillation with\nstate-of-the-art models with various data augmentations. We create a reinforced\nversion of the ImageNet training dataset, called ImageNet+, as well as\nreinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained\nwith ImageNet+ are more accurate, robust, and calibrated, and transfer well to\ndownstream tasks (e.g., segmentation and detection). As an example, the\naccuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on\nImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the\nImageNet validation set is also reduced by 9.9%. Using this backbone with\nMask-RCNN for object detection on MS-COCO, the mean average precision improves\nby 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.\nFor MobileNetV3 and Swin-Tiny, we observe significant improvements on\nImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+\nand fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%\nimproved accuracy. The code, datasets, and pretrained models are available at\nhttps://github.com/apple/ml-dr.",
        "authors": [
            "Fartash Faghri",
            "Hadi Pouransari",
            "Sachin Mehta",
            "Mehrdad Farajtabar",
            "Ali Farhadi",
            "Mohammad Rastegari",
            "Oncel Tuzel"
        ]
    },
    {
        "title": "An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability",
        "url": "http://arxiv.org/abs/2308.02897",
        "abstract": "While the transferability property of adversarial examples allows the\nadversary to perform black-box attacks (i.e., the attacker has no knowledge\nabout the target model), the transfer-based adversarial attacks have gained\ngreat attention. Previous works mostly study gradient variation or image\ntransformations to amplify the distortion on critical parts of inputs. These\nmethods can work on transferring across models with limited differences, i.e.,\nfrom CNNs to CNNs, but always fail in transferring across models with wide\ndifferences, such as from CNNs to ViTs. Alternatively, model ensemble\nadversarial attacks are proposed to fuse outputs from surrogate models with\ndiverse architectures to get an ensemble loss, making the generated adversarial\nexample more likely to transfer to other models as it can fool multiple models\nconcurrently. However, existing ensemble attacks simply fuse the outputs of the\nsurrogate models evenly, thus are not efficacious to capture and amplify the\nintrinsic transfer information of adversarial examples. In this paper, we\npropose an adaptive ensemble attack, dubbed AdaEA, to adaptively control the\nfusion of the outputs from each model, via monitoring the discrepancy ratio of\ntheir contributions towards the adversarial objective. Furthermore, an extra\ndisparity-reduced filter is introduced to further synchronize the update\ndirection. As a result, we achieve considerable improvement over the existing\nensemble attacks on various datasets, and the proposed AdaEA can also boost\nexisting transfer-based attacks, which further demonstrates its efficacy and\nversatility.",
        "authors": [
            "Bin Chen",
            "Jia-Li Yin",
            "Shukai Chen",
            "Bo-Hao Chen",
            "Ximeng Liu"
        ]
    },
    {
        "title": "Incremental Generalized Category Discovery",
        "url": "http://arxiv.org/abs/2304.14310",
        "abstract": "We explore the problem of Incremental Generalized Category Discovery (IGCD).\nThis is a challenging category incremental learning setting where the goal is\nto develop models that can correctly categorize images from previously seen\ncategories, in addition to discovering novel ones. Learning is performed over a\nseries of time steps where the model obtains new labeled and unlabeled data,\nand discards old data, at each iteration. The difficulty of the problem is\ncompounded in our generalized setting as the unlabeled data can contain images\nfrom categories that may or may not have been observed before. We present a new\nmethod for IGCD which combines non-parametric categorization with efficient\nimage sampling to mitigate catastrophic forgetting. To quantify performance, we\npropose a new benchmark dataset named iNatIGCD that is motivated by a\nreal-world fine-grained visual categorization task. In our experiments we\noutperform existing related methods",
        "authors": [
            "Bingchen Zhao",
            "Oisin Mac Aodha"
        ]
    },
    {
        "title": "AccFlow: Backward Accumulation for Long-Range Optical Flow",
        "url": "http://arxiv.org/abs/2308.13133",
        "abstract": "Recent deep learning-based optical flow estimators have exhibited impressive\nperformance in generating local flows between consecutive frames. However, the\nestimation of long-range flows between distant frames, particularly under\ncomplex object deformation and large motion occlusion, remains a challenging\ntask. One promising solution is to accumulate local flows explicitly or\nimplicitly to obtain the desired long-range flow. Nevertheless, the\naccumulation errors and flow misalignment can hinder the effectiveness of this\napproach. This paper proposes a novel recurrent framework called AccFlow, which\nrecursively backward accumulates local flows using a deformable module called\nas AccPlus. In addition, an adaptive blending module is designed along with\nAccPlus to alleviate the occlusion effect by backward accumulation and rectify\nthe accumulation error. Notably, we demonstrate the superiority of backward\naccumulation over conventional forward accumulation, which to the best of our\nknowledge has not been explicitly established before. To train and evaluate the\nproposed AccFlow, we have constructed a large-scale high-quality dataset named\nCVO, which provides ground-truth optical flow labels between adjacent and\ndistant frames. Extensive experiments validate the effectiveness of AccFlow in\nhandling long-range optical flow estimation. Codes are available at\nhttps://github.com/mulns/AccFlow .",
        "authors": [
            "Guangyang Wu",
            "Xiaohong Liu",
            "Kunming Luo",
            "Xi Liu",
            "Qingqing Zheng",
            "Shuaicheng Liu",
            "Xinyang Jiang",
            "Guangtao Zhai",
            "Wenyi Wang"
        ]
    },
    {
        "title": "Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells",
        "url": "http://arxiv.org/abs/2307.09160",
        "abstract": "Learning-based multi-view stereo (MVS) methods deal with predicting accurate\ndepth maps to achieve an accurate and complete 3D representation. Despite the\nexcellent performance, existing methods ignore the fact that a suitable depth\ngeometry is also critical in MVS. In this paper, we demonstrate that different\ndepth geometries have significant performance gaps, even using the same depth\nprediction error. Therefore, we introduce an ideal depth geometry composed of\nSaddle-Shaped Cells, whose predicted depth map oscillates upward and downward\naround the ground-truth surface, rather than maintaining a continuous and\nsmooth depth plane. To achieve it, we develop a coarse-to-fine framework called\nDual-MVSNet (DMVSNet), which can produce an oscillating depth plane.\nTechnically, we predict two depth values for each pixel (Dual-Depth), and\npropose a novel loss function and a checkerboard-shaped selecting strategy to\nconstrain the predicted depth geometry. Compared to existing methods,DMVSNet\nachieves a high rank on the DTU benchmark and obtains the top performance on\nchallenging scenes of Tanks and Temples, demonstrating its strong performance\nand generalization ability. Our method also points to a new research direction\nfor considering depth geometry in MVS.",
        "authors": [
            "Xinyi Ye",
            "Weiyue Zhao",
            "Tianqi Liu",
            "Zihao Huang",
            "Zhiguo Cao",
            "Xin Li"
        ]
    },
    {
        "title": "SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-positive Mining",
        "url": "http://arxiv.org/abs/2201.04620",
        "abstract": "Training with sparse annotations is known to reduce the performance of object\ndetectors. Previous methods have focused on proxies for missing ground truth\nannotations in the form of pseudo-labels for unlabeled boxes. We observe that\nexisting methods suffer at higher levels of sparsity in the data due to noisy\npseudo-labels. To prevent this, we propose an end-to-end system that learns to\nseparate the proposals into labeled and unlabeled regions using Pseudo-positive\nmining. While the labeled regions are processed as usual, self-supervised\nlearning is used to process the unlabeled regions thereby preventing the\nnegative effects of noisy pseudo-labels. This novel approach has multiple\nadvantages such as improved robustness to higher sparsity when compared to\nexisting methods. We conduct exhaustive experiments on five splits on the\nPASCAL-VOC and COCO datasets achieving state-of-the-art performance. We also\nunify various splits used across literature for this task and present a\nstandardized benchmark. On average, we improve by $2.6$, $3.9$ and $9.6$ mAP\nover previous state-of-the-art methods on three splits of increasing sparsity\non COCO. Our project is publicly available at\nhttps://www.cs.umd.edu/~sakshams/SparseDet.",
        "authors": [
            "Saksham Suri",
            "Sai Saketh Rambhatla",
            "Rama Chellappa",
            "Abhinav Shrivastava"
        ]
    },
    {
        "title": "Among Us: Adversarially Robust Collaborative Perception by Consensus",
        "url": "http://arxiv.org/abs/2303.09495",
        "abstract": "Multiple robots could perceive a scene (e.g., detect objects) collaboratively\nbetter than individuals, although easily suffer from adversarial attacks when\nusing deep learning. This could be addressed by the adversarial defense, but\nits training requires the often-unknown attacking mechanism. Differently, we\npropose ROBOSAC, a novel sampling-based defense strategy generalizable to\nunseen attackers. Our key idea is that collaborative perception should lead to\nconsensus rather than dissensus in results compared to individual perception.\nThis leads to our hypothesize-and-verify framework: perception results with and\nwithout collaboration from a random subset of teammates are compared until\nreaching a consensus. In such a framework, more teammates in the sampled subset\noften entail better perception performance but require longer sampling time to\nreject potential attackers. Thus, we derive how many sampling trials are needed\nto ensure the desired size of an attacker-free subset, or equivalently, the\nmaximum size of such a subset that we can successfully sample within a given\nnumber of trials. We validate our method on the task of collaborative 3D object\ndetection in autonomous driving scenarios.",
        "authors": [
            "Yiming Li",
            "Qi Fang",
            "Jiamu Bai",
            "Siheng Chen",
            "Felix Juefei-Xu",
            "Chen Feng"
        ]
    },
    {
        "title": "BUS: Efficient and Effective Vision-Language Pre-Training with Bottom-Up Patch Summarization.",
        "url": "http://arxiv.org/abs/2307.08504",
        "abstract": "Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models have\ndemonstrated impressive performance in various tasks. However, the lengthy\nvisual token sequences fed into ViT can lead to training inefficiency and\nineffectiveness. Existing efforts address the challenge by either bottom-level\npatch extraction in the ViT backbone or top-level patch abstraction outside,\nnot balancing training efficiency and effectiveness well. Inspired by text\nsummarization in natural language processing, we propose a Bottom-Up Patch\nSummarization approach named BUS, coordinating bottom-level extraction and\ntop-level abstraction to learn a concise summary of lengthy visual token\nsequences efficiently. Specifically, We incorporate a Text-Semantics-Aware\nPatch Selector (TSPS) into the ViT backbone to perform a coarse-grained visual\ntoken extraction and then attach a flexible Transformer-based Patch Abstraction\nDecoder (PAD) upon the backbone for top-level visual abstraction. This\nbottom-up collaboration enables our BUS to yield high training efficiency while\nmaintaining or even improving effectiveness. We evaluate our approach on\nvarious visual-language understanding and generation tasks and show competitive\ndownstream task performance while boosting the training efficiency by 50\\%.\nAdditionally, our model achieves state-of-the-art performance on many\ndownstream tasks by increasing input image resolution without increasing\ncomputational costs over baselines.",
        "authors": [
            "Chaoya Jiang",
            "Haiyang Xu",
            "Wei Ye",
            "Qinghao Ye",
            "Chenliang Li",
            "Ming Yan",
            "Bin Bi",
            "Shikun Zhang",
            "Fei Huang",
            "Songfang Huang"
        ]
    },
    {
        "title": "DiffusionDet: Diffusion Model for Object Detection",
        "url": "http://arxiv.org/abs/2211.09788",
        "abstract": "We propose DiffusionDet, a new framework that formulates object detection as\na denoising diffusion process from noisy boxes to object boxes. During the\ntraining stage, object boxes diffuse from ground-truth boxes to random\ndistribution, and the model learns to reverse this noising process. In\ninference, the model refines a set of randomly generated boxes to the output\nresults in a progressive way. Our work possesses an appealing property of\nflexibility, which enables the dynamic number of boxes and iterative\nevaluation. The extensive experiments on the standard benchmarks show that\nDiffusionDet achieves favorable performance compared to previous\nwell-established detectors. For example, DiffusionDet achieves 5.3 AP and 4.8\nAP gains when evaluated with more boxes and iteration steps, under a zero-shot\ntransfer setting from COCO to CrowdHuman. Our code is available at\nhttps://github.com/ShoufaChen/DiffusionDet.",
        "authors": [
            "Shoufa Chen",
            "Peize Sun",
            "Yibing Song",
            "Ping Luo"
        ]
    },
    {
        "title": "CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2307.11526",
        "abstract": "Neural Radiance Fields (NeRF) have the potential to be a major representation\nof media. Since training a NeRF has never been an easy task, the protection of\nits model copyright should be a priority. In this paper, by analyzing the pros\nand cons of possible copyright protection solutions, we propose to protect the\ncopyright of NeRF models by replacing the original color representation in NeRF\nwith a watermarked color representation. Then, a distortion-resistant rendering\nscheme is designed to guarantee robust message extraction in 2D renderings of\nNeRF. Our proposed method can directly protect the copyright of NeRF models\nwhile maintaining high rendering quality and bit accuracy when compared among\noptional solutions.",
        "authors": [
            "Ziyuan Luo",
            "Qing Guo",
            "Ka Chun Cheung",
            "Simon See",
            "Renjie Wan"
        ]
    },
    {
        "title": "Creative Birds: Self-Supervised Single-View 3D Style Transfer",
        "url": "http://arxiv.org/abs/2307.14127",
        "abstract": "In this paper, we propose a novel method for single-view 3D style transfer\nthat generates a unique 3D object with both shape and texture transfer. Our\nfocus lies primarily on birds, a popular subject in 3D reconstruction, for\nwhich no existing single-view 3D transfer methods have been developed.The\nmethod we propose seeks to generate a 3D mesh shape and texture of a bird from\ntwo single-view images. To achieve this, we introduce a novel shape transfer\ngenerator that comprises a dual residual gated network (DRGNet), and a\nmulti-layer perceptron (MLP). DRGNet extracts the features of source and target\nimages using a shared coordinate gate unit, while the MLP generates spatial\ncoordinates for building a 3D mesh. We also introduce a semantic UV texture\ntransfer module that implements textural style transfer using semantic UV\nsegmentation, which ensures consistency in the semantic meaning of the\ntransferred regions. This module can be widely adapted to many existing\napproaches. Finally, our method constructs a novel 3D bird using a\ndifferentiable renderer. Experimental results on the CUB dataset verify that\nour method achieves state-of-the-art performance on the single-view 3D style\ntransfer task. Code is available in https://github.com/wrk226/creative_birds.",
        "authors": [
            "Renke Wang",
            "Guimin Que",
            "Shuo Chen",
            "Xiang Li",
            "Jun Li",
            "Jian Yang"
        ]
    },
    {
        "title": "DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection",
        "url": "http://arxiv.org/abs/2304.13031",
        "abstract": "In this paper, we study the problem of semi-supervised 3D object detection,\nwhich is of great importance considering the high annotation cost for cluttered\n3D indoor scenes. We resort to the robust and principled framework of\nselfteaching, which has triggered notable progress for semisupervised learning\nrecently. While this paradigm is natural for image-level or pixel-level\nprediction, adapting it to the detection problem is challenged by the issue of\nproposal matching. Prior methods are based upon two-stage pipelines, matching\nheuristically selected proposals generated in the first stage and resulting in\nspatially sparse training signals. In contrast, we propose the first\nsemisupervised 3D detection algorithm that works in the singlestage manner and\nallows spatially dense training signals. A fundamental issue of this new design\nis the quantization error caused by point-to-voxel discretization, which\ninevitably leads to misalignment between two transformed views in the voxel\ndomain. To this end, we derive and implement closed-form rules that compensate\nthis misalignment onthe-fly. Our results are significant, e.g., promoting\nScanNet mAP@0.5 from 35.2% to 48.5% using 20% annotation. Codes and data will\nbe publicly available.",
        "authors": [
            "Huan-ang Gao",
            "Beiwen Tian",
            "Pengfei Li",
            "Hao Zhao",
            "Guyue Zhou"
        ]
    },
    {
        "title": "Towards Inadequately Pre-trained Models in Transfer Learning",
        "url": "http://arxiv.org/abs/2203.04668",
        "abstract": "Pre-training has been a popular learning paradigm in deep learning era,\nespecially in annotation-insufficient scenario. Better ImageNet pre-trained\nmodels have been demonstrated, from the perspective of architecture, by\nprevious research to have better transferability to downstream tasks. However,\nin this paper, we found that during the same pre-training process, models at\nmiddle epochs, which is inadequately pre-trained, can outperform fully trained\nmodels when used as feature extractors (FE), while the fine-tuning (FT)\nperformance still grows with the source performance. This reveals that there is\nnot a solid positive correlation between top-1 accuracy on ImageNet and the\ntransferring result on target data. Based on the contradictory phenomenon\nbetween FE and FT that better feature extractor fails to be fine-tuned better\naccordingly, we conduct comprehensive analyses on features before softmax layer\nto provide insightful explanations. Our discoveries suggest that, during\npre-training, models tend to first learn spectral components corresponding to\nlarge singular values and the residual components contribute more when\nfine-tuning.",
        "authors": [
            "Andong Deng",
            "Xingjian Li",
            "Di Hu",
            "Tianyang Wang",
            "Haoyi Xiong",
            "Chengzhong Xu"
        ]
    },
    {
        "title": "Boosting Novel Category Discovery Over Domains with Soft Contrastive Learning and All in One Classifier",
        "url": "http://arxiv.org/abs/2211.11262",
        "abstract": "Unsupervised domain adaptation (UDA) has proven to be highly effective in\ntransferring knowledge from a label-rich source domain to a label-scarce target\ndomain. However, the presence of additional novel categories in the target\ndomain has led to the development of open-set domain adaptation (ODA) and\nuniversal domain adaptation (UNDA). Existing ODA and UNDA methods treat all\nnovel categories as a single, unified unknown class and attempt to detect it\nduring training. However, we found that domain variance can lead to more\nsignificant view-noise in unsupervised data augmentation, which affects the\neffectiveness of contrastive learning (CL) and causes the model to be\noverconfident in novel category discovery. To address these issues, a framework\nnamed Soft-contrastive All-in-one Network (SAN) is proposed for ODA and UNDA\ntasks. SAN includes a novel data-augmentation-based soft contrastive learning\n(SCL) loss to fine-tune the backbone for feature transfer and a more\nhuman-intuitive classifier to improve new class discovery capability. The SCL\nloss weakens the adverse effects of the data augmentation view-noise problem\nwhich is amplified in domain transfer tasks. The All-in-One (AIO) classifier\novercomes the overconfidence problem of current mainstream closed-set and\nopen-set classifiers. Visualization and ablation experiments demonstrate the\neffectiveness of the proposed innovations. Furthermore, extensive experiment\nresults on ODA and UNDA show that SAN outperforms existing state-of-the-art\nmethods.",
        "authors": [
            "Zelin Zang",
            "Lei Shang",
            "Senqiao Yang",
            "Fei Wang",
            "Baigui Sun",
            "Xuansong Xie",
            "Stan Z. Li"
        ]
    },
    {
        "title": "SegPrompt: Boosting Open-World Segmentation via Category-Level Prompt Learning",
        "url": "http://arxiv.org/abs/2308.06531",
        "abstract": "Current closed-set instance segmentation models rely on pre-defined class\nlabels for each mask during training and evaluation, largely limiting their\nability to detect novel objects. Open-world instance segmentation (OWIS) models\naddress this challenge by detecting unknown objects in a class-agnostic manner.\nHowever, previous OWIS approaches completely erase category information during\ntraining to keep the model's ability to generalize to unknown objects. In this\nwork, we propose a novel training mechanism termed SegPrompt that uses category\ninformation to improve the model's class-agnostic segmentation ability for both\nknown and unknown categories. In addition, the previous OWIS training setting\nexposes the unknown classes to the training set and brings information leakage,\nwhich is unreasonable in the real world. Therefore, we provide a new open-world\nbenchmark closer to a real-world scenario by dividing the dataset classes into\nknown-seen-unseen parts. For the first time, we focus on the model's ability to\ndiscover objects that never appear in the training set images.\n  Experiments show that SegPrompt can improve the overall and unseen detection\nperformance by 5.6% and 6.1% in AR on our new benchmark without affecting the\ninference efficiency. We further demonstrate the effectiveness of our method on\nexisting cross-dataset transfer and strongly supervised settings, leading to\n5.5% and 12.3% relative improvement.",
        "authors": [
            "Muzhi Zhu",
            "Hengtao Li",
            "Hao Chen",
            "Chengxiang Fan",
            "Weian Mao",
            "Chenchen Jing",
            "Yifan Liu",
            "Chunhua Shen"
        ]
    },
    {
        "title": "Search for or Navigate to? Dual Adaptive Thinking for Object Navigation",
        "url": "http://arxiv.org/abs/2208.00553",
        "abstract": "\"Search for\" or \"Navigate to\"? When finding an object, the two choices always\ncome up in our subconscious mind. Before seeing the target, we search for the\ntarget based on experience. After seeing the target, we remember the target\nlocation and navigate to. However, recently methods in object navigation field\nalmost only consider using object association to enhance \"search for\" phase\nwhile neglect the importance of \"navigate to\" phase. Therefore, this paper\nproposes the dual adaptive thinking (DAT) method to flexibly adjust the\ndifferent thinking strategies at different navigation stages. Dual thinking\nincludes search thinking with the object association ability and navigation\nthinking with the target location ability. To make the navigation thinking more\neffective, we design the target-oriented memory graph (TOMG) to store\nhistorical target information and the target-aware multi-scale aggregator\n(TAMSA) to encode the relative target position. We assess our methods on the\nAI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method\nreports 10.8%, 21.5% and 15.7% increase in success rate (SR), success weighted\nby path length (SPL) and success weighted by navigation efficiency (SNE),\nrespectively.",
        "authors": [
            "Ronghao Dang",
            "Liuyi Wang",
            "Zongtao He",
            "Shuai Su",
            "Chengju Liu",
            "Qijun Chen"
        ]
    },
    {
        "title": "Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat",
        "url": "http://arxiv.org/abs/2309.03237",
        "abstract": "We carefully evaluate a number of algorithms for learning in a federated\nenvironment, and test their utility for a variety of image classification\ntasks. We consider many issues that have not been adequately considered before:\nwhether learning over data sets that do not have diverse sets of images affects\nthe results; whether to use a pre-trained feature extraction \"backbone\"; how to\nevaluate learner performance (we argue that classification accuracy is not\nenough), among others. Overall, across a wide variety of settings, we find that\nvertically decomposing a neural network seems to give the best results, and\noutperforms more standard reconciliation-used methods.",
        "authors": [
            "Erdong Hu",
            "Yuxin Tang",
            "Anastasios Kyrillidis",
            "Chris Jermaine"
        ]
    },
    {
        "title": "HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video",
        "url": "http://arxiv.org/abs/2304.12281",
        "abstract": "We introduce HOSNeRF, a novel 360{\\deg} free-viewpoint rendering method that\nreconstructs neural radiance fields for dynamic human-object-scene from a\nsingle monocular in-the-wild video. Our method enables pausing the video at any\nframe and rendering all scene details (dynamic humans, objects, and\nbackgrounds) from arbitrary viewpoints. The first challenge in this task is the\ncomplex object motions in human-object interactions, which we tackle by\nintroducing the new object bones into the conventional human skeleton hierarchy\nto effectively estimate large object deformations in our dynamic human-object\nmodel. The second challenge is that humans interact with different objects at\ndifferent times, for which we introduce two new learnable object state\nembeddings that can be used as conditions for learning our human-object\nrepresentation and scene representation, respectively. Extensive experiments\nshow that HOSNeRF significantly outperforms SOTA approaches on two challenging\ndatasets by a large margin of 40% ~ 50% in terms of LPIPS. The code, data, and\ncompelling examples of 360{\\deg} free-viewpoint renderings from single videos\nwill be released in https://showlab.github.io/HOSNeRF.",
        "authors": [
            "Jia-Wei Liu",
            "Yan-Pei Cao",
            "Tianyuan Yang",
            "Eric Zhongcong Xu",
            "Jussi Keppo",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution",
        "url": "http://arxiv.org/abs/2308.08114",
        "abstract": "Omnidirectional images (ODIs) have become increasingly popular, as their\nlarge field-of-view (FoV) can offer viewers the chance to freely choose the\nview directions in immersive environments such as virtual reality. The M\\\"obius\ntransformation is typically employed to further provide the opportunity for\nmovement and zoom on ODIs, but applying it to the image level often results in\nblurry effect and aliasing problem. In this paper, we propose a novel deep\nlearning-based approach, called \\textbf{OmniZoomer}, to incorporate the\nM\\\"obius transformation into the network for movement and zoom on ODIs. By\nlearning various transformed feature maps under different conditions, the\nnetwork is enhanced to handle the increasing edge curvatures, which alleviates\nthe blurry effect. Moreover, to address the aliasing problem, we propose two\nkey components. Firstly, to compensate for the lack of pixels for describing\ncurves, we enhance the feature maps in the high-resolution (HR) space and\ncalculate the transformed index map with a spatial index generation module.\nSecondly, considering that ODIs are inherently represented in the spherical\nspace, we propose a spherical resampling module that combines the index map and\nHR feature maps to transform the feature maps for better spherical correlation.\nThe transformed feature maps are decoded to output a zoomed ODI. Experiments\nshow that our method can produce HR and high-quality ODIs with the flexibility\nto move and zoom in to the object of interest. Project page is available at\nhttp://vlislab22.github.io/OmniZoomer/.",
        "authors": [
            "Zidong Cao",
            "Hao Ai",
            "Yan-Pei Cao",
            "Ying Shan",
            "Xiaohu Qie",
            "Lin Wang"
        ]
    },
    {
        "title": "Knowing Where to Focus: Event-aware Transformer for Video Grounding",
        "url": "http://arxiv.org/abs/2308.06947",
        "abstract": "Recent DETR-based video grounding models have made the model directly predict\nmoment timestamps without any hand-crafted components, such as a pre-defined\nproposal or non-maximum suppression, by learning moment queries. However, their\ninput-agnostic moment queries inevitably overlook an intrinsic temporal\nstructure of a video, providing limited positional information. In this paper,\nwe formulate an event-aware dynamic moment query to enable the model to take\nthe input-specific content and positional information of the video into\naccount. To this end, we present two levels of reasoning: 1) Event reasoning\nthat captures distinctive event units constituting a given video using a slot\nattention mechanism; and 2) moment reasoning that fuses the moment queries with\na given sentence through a gated fusion transformer layer and learns\ninteractions between the moment queries and video-sentence representations to\npredict moment timestamps. Extensive experiments demonstrate the effectiveness\nand efficiency of the event-aware dynamic moment queries, outperforming\nstate-of-the-art approaches on several video grounding benchmarks.",
        "authors": [
            "Jinhyun Jang",
            "Jungin Park",
            "Jin Kim",
            "Hyeongjun Kwon",
            "Kwanghoon Sohn"
        ]
    },
    {
        "title": "Landscape Learning for Neural Network Inversion",
        "url": "http://arxiv.org/abs/2206.09027",
        "abstract": "Many machine learning methods operate by inverting a neural network at\ninference time, which has become a popular technique for solving inverse\nproblems in computer vision, robotics, and graphics. However, these methods\noften involve gradient descent through a highly non-convex loss landscape,\ncausing the optimization process to be unstable and slow. We introduce a method\nthat learns a loss landscape where gradient descent is efficient, bringing\nmassive improvement and acceleration to the inversion process. We demonstrate\nthis advantage on a number of methods for both generative and discriminative\ntasks, including GAN inversion, adversarial defense, and 3D human pose\nreconstruction.",
        "authors": [
            "Ruoshi Liu",
            "Chengzhi Mao",
            "Purva Tendulkar",
            "Hao Wang",
            "Carl Vondrick"
        ]
    },
    {
        "title": "Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision",
        "url": "http://arxiv.org/abs/2208.05110",
        "abstract": "Instance segmentation on 3D point clouds has been attracting increasing\nattention due to its wide applications, especially in scene understanding\nareas. However, most existing methods operate on fully annotated data while\nmanually preparing ground-truth labels at point-level is very cumbersome and\nlabor-intensive. To address this issue, we propose a novel weakly supervised\nmethod RWSeg that only requires labeling one object with one point. With these\nsparse weak labels, we introduce a unified framework with two branches to\npropagate semantic and instance information respectively to unknown regions\nusing self-attention and a cross-graph random walk method. Specifically, we\npropose a Cross-graph Competing Random Walks (CRW) algorithm that encourages\ncompetition among different instance graphs to resolve ambiguities in closely\nplaced objects, improving instance assignment accuracy. RWSeg generates\nhigh-quality instance-level pseudo labels. Experimental results on ScanNet-v2\nand S3DIS datasets show that our approach achieves comparable performance with\nfully-supervised methods and outperforms previous weakly-supervised methods by\na substantial margin.",
        "authors": [
            "Shichao Dong",
            "Ruibo Li",
            "Jiacheng Wei",
            "Fayao Liu",
            "Guosheng Lin"
        ]
    },
    {
        "title": "Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection",
        "url": "http://arxiv.org/abs/2308.05991",
        "abstract": "Recent progress in weakly supervised object detection is featured by a\ncombination of multiple instance detection networks (MIDN) and ordinal online\nrefinement. However, with only image-level annotation, MIDN inevitably assigns\nhigh scores to some unexpected region proposals when generating pseudo labels.\nThese inaccurate high-scoring region proposals will mislead the training of\nsubsequent refinement modules and thus hamper the detection performance. In\nthis work, we explore how to ameliorate the quality of pseudo-labeling in MIDN.\nFormally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervised\nobject detection pipeline, which optimizes MIDN with rank information from a\nreliable teacher network. Specifically, we obtain this teacher network by\nintroducing a weighted exponential moving average strategy to take advantage of\nvarious refinement modules. A novel class-specific ranking distillation\nalgorithm is proposed to leverage the output of weighted ensembled teacher\nnetwork for distilling MIDN with rank information. As a result, MIDN is guided\nto assign higher scores to accurate proposals among their neighboring ones,\nthus benefiting the subsequent pseudo labeling. Extensive experiments on the\nprevalent PASCAL VOC 2007 \\& 2012 and COCO datasets demonstrate the superior\nperformance of our CBL framework. Code will be available at\nhttps://github.com/Yinyf0804/WSOD-CBL/.",
        "authors": [
            "Yufei Yin",
            "Jiajun Deng",
            "Wengang Zhou",
            "Li Li",
            "Houqiang Li"
        ]
    },
    {
        "title": "Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation",
        "url": "http://arxiv.org/abs/2309.11160",
        "abstract": "Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a query\nvideo with the same category defined by a few annotated support images.\nHowever, this task was seldom explored. In this work, based on IPMT, a\nstate-of-the-art few-shot image segmentation method that combines external\nsupport guidance information with adaptive query guidance cues, we propose to\nleverage multi-grained temporal guidance information for handling the temporal\ncorrelation nature of video data. We decompose the query video information into\na clip prototype and a memory prototype for capturing local and long-term\ninternal temporal guidance, respectively. Frame prototypes are further used for\neach frame independently to handle fine-grained adaptive guidance and enable\nbidirectional clip-frame prototype communication. To reduce the influence of\nnoisy memory, we propose to leverage the structural similarity relation among\ndifferent predicted regions and the support for selecting reliable memory\nframes. Furthermore, a new segmentation loss is also proposed to enhance the\ncategory discriminability of the learned prototypes. Experimental results\ndemonstrate that our proposed video IPMT model significantly outperforms\nprevious models on two benchmark datasets. Code is available at\nhttps://github.com/nankepan/VIPMT.",
        "authors": [
            "Nian Liu",
            "Kepan Nan",
            "Wangbo Zhao",
            "Yuanwei Liu",
            "Xiwen Yao",
            "Salman Khan",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Junwei Han",
            "Fahad Shahbaz Khan"
        ]
    },
    {
        "title": "Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models",
        "url": "http://arxiv.org/abs/2303.08440",
        "abstract": "Diffusion models have become a popular approach for image generation and\nreconstruction due to their numerous advantages. However, most diffusion-based\ninverse problem-solving methods only deal with 2D images, and even recently\npublished 3D methods do not fully exploit the 3D distribution prior. To address\nthis, we propose a novel approach using two perpendicular pre-trained 2D\ndiffusion models to solve the 3D inverse problem. By modeling the 3D data\ndistribution as a product of 2D distributions sliced in different directions,\nour method effectively addresses the curse of dimensionality. Our experimental\nresults demonstrate that our method is highly effective for 3D medical image\nreconstruction tasks, including MRI Z-axis super-resolution, compressed sensing\nMRI, and sparse-view CT. Our method can generate high-quality voxel volumes\nsuitable for medical applications.",
        "authors": [
            "Suhyeon Lee",
            "Hyungjin Chung",
            "Minyoung Park",
            "Jonghyuk Park",
            "Wi-Sun Ryu",
            "Jong Chul Ye"
        ]
    },
    {
        "title": "Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations",
        "url": "http://arxiv.org/abs/2308.11796",
        "abstract": "Spatially dense self-supervised learning is a rapidly growing problem domain\nwith promising applications for unsupervised segmentation and pretraining for\ndense downstream tasks. Despite the abundance of temporal data in the form of\nvideos, this information-rich source has been largely overlooked. Our paper\naims to address this gap by proposing a novel approach that incorporates\ntemporal consistency in dense self-supervised learning. While methods designed\nsolely for images face difficulties in achieving even the same performance on\nvideos, our method improves not only the representation quality for videos-but\nalso images. Our approach, which we call time-tuning, starts from\nimage-pretrained models and fine-tunes them with a novel self-supervised\ntemporal-alignment clustering loss on unlabeled videos. This effectively\nfacilitates the transfer of high-level information from videos to image\nrepresentations. Time-tuning improves the state-of-the-art by 8-10% for\nunsupervised semantic segmentation on videos and matches it for images. We\nbelieve this method paves the way for further self-supervised scaling by\nleveraging the abundant availability of videos. The implementation can be found\nhere : https://github.com/SMSD75/Timetuning",
        "authors": [
            "Mohammadreza Salehi",
            "Efstratios Gavves",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ]
    },
    {
        "title": "CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow",
        "url": "http://arxiv.org/abs/2211.10408",
        "abstract": "Despite impressive performance for high-level downstream tasks,\nself-supervised pre-training methods have not yet fully delivered on dense\ngeometric vision tasks such as stereo matching or optical flow. The application\nof self-supervised concepts, such as instance discrimination or masked image\nmodeling, to geometric tasks is an active area of research. In this work, we\nbuild on the recent cross-view completion framework, a variation of masked\nimage modeling that leverages a second view from the same scene which makes it\nwell suited for binocular downstream tasks. The applicability of this concept\nhas so far been limited in at least two ways: (a) by the difficulty of\ncollecting real-world image pairs -- in practice only synthetic data have been\nused -- and (b) by the lack of generalization of vanilla transformers to dense\ndownstream tasks for which relative position is more meaningful than absolute\nposition. We explore three avenues of improvement. First, we introduce a method\nto collect suitable real-world image pairs at large scale. Second, we\nexperiment with relative positional embeddings and show that they enable vision\ntransformers to perform substantially better. Third, we scale up vision\ntransformer based cross-completion architectures, which is made possible by the\nuse of large amounts of data. With these improvements, we show for the first\ntime that state-of-the-art results on stereo matching and optical flow can be\nreached without using any classical task-specific techniques like correlation\nvolume, iterative estimation, image warping or multi-scale reasoning, thus\npaving the way towards universal vision models.",
        "authors": [
            "Philippe Weinzaepfel",
            "Thomas Lucas",
            "Vincent Leroy",
            "Yohann Cabon",
            "Vaibhav Arora",
            "Romain Br\u00e9gier",
            "Gabriela Csurka",
            "Leonid Antsfeld",
            "Boris Chidlovskii",
            "J\u00e9r\u00f4me Revaud"
        ]
    },
    {
        "title": "ExBluRF: Efficient Radiance Fields for Extreme Motion Blurred Images",
        "url": "http://arxiv.org/abs/2309.08957",
        "abstract": "We present ExBluRF, a novel view synthesis method for extreme motion blurred\nimages based on efficient radiance fields optimization. Our approach consists\nof two main components: 6-DOF camera trajectory-based motion blur formulation\nand voxel-based radiance fields. From extremely blurred images, we optimize the\nsharp radiance fields by jointly estimating the camera trajectories that\ngenerate the blurry images. In training, multiple rays along the camera\ntrajectory are accumulated to reconstruct single blurry color, which is\nequivalent to the physical motion blur operation. We minimize the\nphoto-consistency loss on blurred image space and obtain the sharp radiance\nfields with camera trajectories that explain the blur of all images. The joint\noptimization on the blurred image space demands painfully increasing\ncomputation and resources proportional to the blur size. Our method solves this\nproblem by replacing the MLP-based framework to low-dimensional 6-DOF camera\nposes and voxel-based radiance fields. Compared with the existing works, our\napproach restores much sharper 3D scenes from challenging motion blurred views\nwith the order of 10 times less training time and GPU memory consumption.",
        "authors": [
            "Dongwoo Lee",
            "Jeongtaek Oh",
            "Jaesung Rim",
            "Sunghyun Cho",
            "Kyoung Mu Lee"
        ]
    },
    {
        "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention",
        "url": "http://arxiv.org/abs/2211.13955",
        "abstract": "Secure multi-party computation (MPC) enables computation directly on\nencrypted data and protects both data and model privacy in deep learning\ninference. However, existing neural network architectures, including Vision\nTransformers (ViTs), are not designed or optimized for MPC and incur\nsignificant latency overhead. We observe Softmax accounts for the major latency\nbottleneck due to a high communication complexity, but can be selectively\nreplaced or linearized without compromising the model accuracy. Hence, in this\npaper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet\nefficient ViT inference in MPC. Based on a systematic latency and accuracy\nevaluation of the Softmax attention and other attention variants, we propose a\nheterogeneous attention optimization space. We also develop a simple yet\neffective MPC-aware neural architecture search algorithm for fast Pareto\noptimization. To further boost the inference efficiency, we propose MPCViT+, to\njointly optimize the Softmax attention and other network components, including\nGeLU, matrix multiplication, etc. With extensive experiments, we demonstrate\nthat MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2x, 2.9x and\n1.9x latency reduction compared with baseline ViT, MPCFormer and THE-X on the\nTiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto\nfront compared with MPCViT. The code and models for evaluation are available at\nhttps://github.com/PKU-SEC-Lab/mpcvit.",
        "authors": [
            "Wenxuan Zeng",
            "Meng Li",
            "Wenjie Xiong",
            "Tong Tong",
            "Wen-jie Lu",
            "Jin Tan",
            "Runsheng Wang",
            "Ru Huang"
        ]
    },
    {
        "title": "Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning",
        "url": "http://arxiv.org/abs/2308.09303",
        "abstract": "Continual learning aims to learn a model from a continuous stream of data,\nbut it mainly assumes a fixed number of data and tasks with clear task\nboundaries. However, in real-world scenarios, the number of input data and\ntasks is constantly changing in a statistical way, not a static way. Although\nrecently introduced incremental learning scenarios having blurry task\nboundaries somewhat address the above issues, they still do not fully reflect\nthe statistical properties of real-world situations because of the fixed ratio\nof disjoint and blurry samples. In this paper, we propose a new Stochastic\nincremental Blurry task boundary scenario, called Si-Blurry, which reflects the\nstochastic properties of the real-world. We find that there are two major\nchallenges in the Si-Blurry scenario: (1) inter- and intra-task forgettings and\n(2) class imbalance problem. To alleviate them, we introduce Mask and Visual\nPrompt tuning (MVP). In MVP, to address the inter- and intra-task forgetting\nissues, we propose a novel instance-wise logit masking and contrastive visual\nprompt tuning loss. Both of them help our model discern the classes to be\nlearned in the current batch. It results in consolidating the previous\nknowledge. In addition, to alleviate the class imbalance problem, we introduce\na new gradient similarity-based focal loss and adaptive feature scaling to ease\noverfitting to the major classes and underfitting to the minor classes.\nExtensive experiments show that our proposed MVP significantly outperforms the\nexisting state-of-the-art methods in our challenging Si-Blurry scenario.",
        "authors": [
            "Jun-Yeong Moon",
            "Keon-Hee Park",
            "Jung Uk Kim",
            "Gyeong-Moon Park"
        ]
    },
    {
        "title": "Masked Spiking Transformer",
        "url": "http://arxiv.org/abs/2210.01208",
        "abstract": "The combination of Spiking Neural Networks (SNNs) and Transformers has\nattracted significant attention due to their potential for high energy\nefficiency and high-performance nature. However, existing works on this topic\ntypically rely on direct training, which can lead to suboptimal performance. To\naddress this issue, we propose to leverage the benefits of the ANN-to-SNN\nconversion method to combine SNNs and Transformers, resulting in significantly\nimproved performance over existing state-of-the-art SNN models. Furthermore,\ninspired by the quantal synaptic failures observed in the nervous system, which\nreduces the number of spikes transmitted across synapses, we introduce a novel\nMasked Spiking Transformer (MST) framework that incorporates a Random Spike\nMasking (RSM) method to prune redundant spikes and reduce energy consumption\nwithout sacrificing performance. Our experimental results demonstrate that the\nproposed MST model achieves a significant reduction of 26.8% in power\nconsumption when the masking ratio is 75% while maintaining the same level of\nperformance as the unmasked model.",
        "authors": [
            "Ziqing Wang",
            "Yuetong Fang",
            "Jiahang Cao",
            "Qiang Zhang",
            "Zhongrui Wang",
            "Renjing Xu"
        ]
    },
    {
        "title": "Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives",
        "url": "http://arxiv.org/abs/2211.04894",
        "abstract": "The rapid increase in user-generated-content (UGC) videos calls for the\ndevelopment of effective video quality assessment (VQA) algorithms. However,\nthe objective of the UGC-VQA problem is still ambiguous and can be viewed from\ntwo perspectives: the technical perspective, measuring the perception of\ndistortions; and the aesthetic perspective, which relates to preference and\nrecommendation on contents. To understand how these two perspectives affect\noverall subjective opinions in UGC-VQA, we conduct a large-scale subjective\nstudy to collect human quality opinions on overall quality of videos as well as\nperceptions from aesthetic and technical perspectives. The collected\nDisentangled Video Quality Database (DIVIDE-3k) confirms that human quality\nopinions on UGC videos are universally and inevitably affected by both\naesthetic and technical perspectives. In light of this, we propose the\nDisentangled Objective Video Quality Evaluator (DOVER) to learn the quality of\nUGC videos based on the two perspectives. The DOVER proves state-of-the-art\nperformance in UGC-VQA under very high efficiency. With perspective opinions in\nDIVIDE-3k, we further propose DOVER++, the first approach to provide reliable\nclear-cut quality evaluations from a single aesthetic or technical perspective.\nCode at https://github.com/VQAssessment/DOVER.",
        "authors": [
            "Haoning Wu",
            "Erli Zhang",
            "Liang Liao",
            "Chaofeng Chen",
            "Jingwen Hou",
            "Annan Wang",
            "Wenxiu Sun",
            "Qiong Yan",
            "Weisi Lin"
        ]
    },
    {
        "title": "Distributed Bundle Adjustment with Block-Based Sparse Matrix Compression for Super Large Scale Datasets",
        "url": "http://arxiv.org/abs/2307.08383",
        "abstract": "We propose a distributed bundle adjustment (DBA) method using the exact\nLevenberg-Marquardt (LM) algorithm for super large-scale datasets. Most of the\nexisting methods partition the global map to small ones and conduct bundle\nadjustment in the submaps. In order to fit the parallel framework, they use\napproximate solutions instead of the LM algorithm. However, those methods often\ngive sub-optimal results. Different from them, we utilize the exact LM\nalgorithm to conduct global bundle adjustment where the formation of the\nreduced camera system (RCS) is actually parallelized and executed in a\ndistributed way. To store the large RCS, we compress it with a block-based\nsparse matrix compression format (BSMC), which fully exploits its block\nfeature. The BSMC format also enables the distributed storage and updating of\nthe global RCS. The proposed method is extensively evaluated and compared with\nthe state-of-the-art pipelines using both synthetic and real datasets.\nPreliminary results demonstrate the efficient memory usage and vast scalability\nof the proposed method compared with the baselines. For the first time, we\nconducted parallel bundle adjustment using LM algorithm on a real datasets with\n1.18 million images and a synthetic dataset with 10 million images (about 500\ntimes that of the state-of-the-art LM-based BA) on a distributed computing\nsystem.",
        "authors": [
            "Maoteng Zheng",
            "Nengcheng Chen",
            "Junfeng Zhu",
            "Xiaoru Zeng",
            "Huanbin Qiu",
            "Yuyao Jiang",
            "Xingyue Lu",
            "Hao Qu"
        ]
    },
    {
        "title": "Neural Interactive Keypoint Detection",
        "url": "http://arxiv.org/abs/2308.10174",
        "abstract": "This work proposes an end-to-end neural interactive keypoint detection\nframework named Click-Pose, which can significantly reduce more than 10 times\nlabeling costs of 2D keypoint annotation compared with manual-only annotation.\nClick-Pose explores how user feedback can cooperate with a neural keypoint\ndetector to correct the predicted keypoints in an interactive way for a faster\nand more effective annotation process. Specifically, we design the pose error\nmodeling strategy that inputs the ground truth pose combined with four typical\npose errors into the decoder and trains the model to reconstruct the correct\nposes, which enhances the self-correction ability of the model. Then, we attach\nan interactive human-feedback loop that allows receiving users' clicks to\ncorrect one or several predicted keypoints and iteratively utilizes the decoder\nto update all other keypoints with a minimum number of clicks (NoC) for\nefficient annotation. We validate Click-Pose in in-domain, out-of-domain\nscenes, and a new task of keypoint adaptation. For annotation, Click-Pose only\nneeds 1.97 and 6.45 NoC@95 (at precision 95%) on COCO and Human-Art, reducing\n31.4% and 36.3% efforts than the SOTA model (ViTPose) with manual correction,\nrespectively. Besides, without user clicks, Click-Pose surpasses the previous\nend-to-end model by 1.4 AP on COCO and 3.0 AP on Human-Art. The code is\navailable at https://github.com/IDEA-Research/Click-Pose.",
        "authors": [
            "Jie Yang",
            "Ailing Zeng",
            "Feng Li",
            "Shilong Liu",
            "Ruimao Zhang",
            "Lei Zhang"
        ]
    },
    {
        "title": "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models",
        "url": "http://arxiv.org/abs/2308.11186",
        "abstract": "Pre-trained vision-language models, e.g., CLIP, working with manually\ndesigned prompts have demonstrated great capacity of transfer learning.\nRecently, learnable prompts achieve state-of-the-art performance, which however\nare prone to overfit to seen classes, failing to generalize to unseen classes.\nIn this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for\nvision-language models. Our approach takes inspiration from human intelligence\nin which external knowledge is usually incorporated into recognizing novel\ncategories of objects. Specifically, we design two complementary types of\nknowledge-aware prompts for the text encoder to leverage the distinctive\ncharacteristics of category-related external knowledge. The discrete prompt\nextracts the key information from descriptions of an object category, and the\nlearned continuous prompt captures overall contexts. We further design an\nadaptation head for the visual encoder to aggregate salient attentive visual\ncues, which establishes discriminative and task-aware visual representations.\nWe conduct extensive experiments on 11 widely-used benchmark datasets and the\nresults verify the effectiveness in few-shot image classification, especially\nin generalizing to unseen categories. Compared with the state-of-the-art CoCoOp\nmethod, KAPT exhibits favorable performance and achieves an absolute gain of\n3.22% on new classes and 2.57% in terms of harmonic mean.",
        "authors": [
            "Baoshuo Kan",
            "Teng Wang",
            "Wenpeng Lu",
            "Xiantong Zhen",
            "Weili Guan",
            "Feng Zheng"
        ]
    },
    {
        "title": "Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement",
        "url": "http://arxiv.org/abs/2303.02091",
        "abstract": "Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in\nimage-based 3D reconstruction. However, their implicit volumetric\nrepresentations differ significantly from the widely-adopted polygonal meshes\nand lack support from common 3D software and hardware, making their rendering\nand manipulation inefficient. To overcome this limitation, we present a novel\nframework that generates textured surface meshes from images. Our approach\nbegins by efficiently initializing the geometry and view-dependency decomposed\nappearance with a NeRF. Subsequently, a coarse mesh is extracted, and an\niterative surface refining algorithm is developed to adaptively adjust both\nvertex positions and face density based on re-projected rendering errors. We\njointly refine the appearance with geometry and bake it into texture images for\nreal-time rendering. Extensive experiments demonstrate that our method achieves\nsuperior mesh quality and competitive rendering quality.",
        "authors": [
            "Jiaxiang Tang",
            "Hang Zhou",
            "Xiaokang Chen",
            "Tianshu Hu",
            "Errui Ding",
            "Jingdong Wang",
            "Gang Zeng"
        ]
    },
    {
        "title": "Leveraging Inpainting for Single-Image Shadow Removal",
        "url": "http://arxiv.org/abs/2302.05361",
        "abstract": "Fully-supervised shadow removal methods achieve the best restoration\nqualities on public datasets but still generate some shadow remnants. One of\nthe reasons is the lack of large-scale shadow & shadow-free image pairs.\nUnsupervised methods can alleviate the issue but their restoration qualities\nare much lower than those of fully-supervised methods. In this work, we find\nthat pretraining shadow removal networks on the image inpainting dataset can\nreduce the shadow remnants significantly: a naive encoder-decoder network gets\ncompetitive restoration quality w.r.t. the state-of-the-art methods via only\n10% shadow & shadow-free image pairs. After analyzing networks with/without\ninpainting pre-training via the information stored in the weight (IIW), we find\nthat inpainting pretraining improves restoration quality in non-shadow regions\nand enhances the generalization ability of networks significantly.\nAdditionally, shadow removal fine-tuning enables networks to fill in the\ndetails of shadow regions. Inspired by these observations we formulate shadow\nremoval as an adaptive fusion task that takes advantage of both shadow removal\nand image inpainting. Specifically, we develop an adaptive fusion network\nconsisting of two encoders, an adaptive fusion block, and a decoder. The two\nencoders are responsible for extracting the feature from the shadow image and\nthe shadow-masked image respectively. The adaptive fusion block is responsible\nfor combining these features in an adaptive manner. Finally, the decoder\nconverts the adaptive fused features to the desired shadow-free result. The\nextensive experiments show that our method empowered with inpainting\noutperforms all state-of-the-art methods.",
        "authors": [
            "Xiaoguang Li",
            "Qing Guo",
            "Rabab Abdelfattah",
            "Di Lin",
            "Wei Feng",
            "Ivor Tsang",
            "Song Wang"
        ]
    },
    {
        "title": "Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models",
        "url": "http://arxiv.org/abs/2309.01590",
        "abstract": "Assessing the fidelity and diversity of the generative model is a difficult\nbut important issue for technological advancement. So, recent papers have\nintroduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break\ndown the statistical distance into fidelity and diversity. While they provide\nan intuitive method, we thoroughly analyze these metrics and identify\noversimplified assumptions and undesirable properties of kNN that result in\nunreliable evaluation, such as susceptibility to outliers and insensitivity to\ndistributional changes. Thus, we propose novel metrics, P-precision and\nP-recall (PP\\&PR), based on a probabilistic approach that address the problems.\nThrough extensive investigations on toy experiments and state-of-the-art\ngenerative models, we show that our PP\\&PR provide more reliable estimates for\ncomparing fidelity and diversity than the existing metrics. The codes are\navailable at \\url{https://github.com/kdst-team/Probablistic_precision_recall}.",
        "authors": [
            "Dogyun Park",
            "Suhyun Kim"
        ]
    },
    {
        "title": "Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning",
        "url": "http://arxiv.org/abs/2308.07209",
        "abstract": "Structured pruning and quantization are promising approaches for reducing the\ninference time and memory footprint of neural networks. However, most existing\nmethods require the original training dataset to fine-tune the model. This not\nonly brings heavy resource consumption but also is not possible for\napplications with sensitive or proprietary data due to privacy and security\nconcerns. Therefore, a few data-free methods are proposed to address this\nproblem, but they perform data-free pruning and quantization separately, which\ndoes not explore the complementarity of pruning and quantization. In this\npaper, we propose a novel framework named Unified Data-Free Compression(UDFC),\nwhich performs pruning and quantization simultaneously without any data and\nfine-tuning process. Specifically, UDFC starts with the assumption that the\npartial information of a damaged(e.g., pruned or quantized) channel can be\npreserved by a linear combination of other channels, and then derives the\nreconstruction form from the assumption to restore the information loss due to\ncompression. Finally, we formulate the reconstruction error between the\noriginal network and its compressed network, and theoretically deduce the\nclosed-form solution. We evaluate the UDFC on the large-scale image\nclassification task and obtain significant improvements over various network\narchitectures and compression methods. For example, we achieve a 20.54%\naccuracy improvement on ImageNet dataset compared to SOTA method with 30%\npruning ratio and 6-bit quantization on ResNet-34.",
        "authors": [
            "Shipeng Bai",
            "Jun Chen",
            "Xintian Shen",
            "Yixuan Qian",
            "Yong Liu"
        ]
    },
    {
        "title": "SurroundOcc: Multi-camera 3D Occupancy Prediction for Autonomous Driving",
        "url": "http://arxiv.org/abs/2303.09551",
        "abstract": "3D scene understanding plays a vital role in vision-based autonomous driving.\nWhile most existing methods focus on 3D object detection, they have difficulty\ndescribing real-world objects of arbitrary shapes and infinite classes. Towards\na more comprehensive perception of a 3D scene, in this paper, we propose a\nSurroundOcc method to predict the 3D occupancy with multi-camera images. We\nfirst extract multi-scale features for each image and adopt spatial 2D-3D\nattention to lift them to the 3D volume space. Then we apply 3D convolutions to\nprogressively upsample the volume features and impose supervision on multiple\nlevels. To obtain dense occupancy prediction, we design a pipeline to generate\ndense occupancy ground truth without expansive occupancy annotations.\nSpecifically, we fuse multi-frame LiDAR scans of dynamic objects and static\nscenes separately. Then we adopt Poisson Reconstruction to fill the holes and\nvoxelize the mesh to get dense occupancy labels. Extensive experiments on\nnuScenes and SemanticKITTI datasets demonstrate the superiority of our method.\nCode and dataset are available at https://github.com/weiyithu/SurroundOcc",
        "authors": [
            "Yi Wei",
            "Linqing Zhao",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Jie Zhou",
            "Jiwen Lu"
        ]
    },
    {
        "title": "Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction",
        "url": "http://arxiv.org/abs/2304.00967",
        "abstract": "In this paper, we propose a new paradigm, named Historical Object Prediction\n(HoP) for multi-view 3D detection to leverage temporal information more\neffectively. The HoP approach is straightforward: given the current timestamp\nt, we generate a pseudo Bird's-Eye View (BEV) feature of timestamp t-k from its\nadjacent frames and utilize this feature to predict the object set at timestamp\nt-k. Our approach is motivated by the observation that enforcing the detector\nto capture both the spatial location and temporal motion of objects occurring\nat historical timestamps can lead to more accurate BEV feature learning. First,\nwe elaborately design short-term and long-term temporal decoders, which can\ngenerate the pseudo BEV feature for timestamp t-k without the involvement of\nits corresponding camera images. Second, an additional object decoder is\nflexibly attached to predict the object targets using the generated pseudo BEV\nfeature. Note that we only perform HoP during training, thus the proposed\nmethod does not introduce extra overheads during inference. As a plug-and-play\napproach, HoP can be easily incorporated into state-of-the-art BEV detection\nframeworks, including BEVFormer and BEVDet series. Furthermore, the auxiliary\nHoP approach is complementary to prevalent temporal modeling methods, leading\nto significant performance gains. Extensive experiments are conducted to\nevaluate the effectiveness of the proposed HoP on the nuScenes dataset. We\nchoose the representative methods, including BEVFormer and BEVDet4D-Depth to\nevaluate our method. Surprisingly, HoP achieves 68.5% NDS and 62.4% mAP with\nViT-L on nuScenes test, outperforming all the 3D object detectors on the\nleaderboard. Codes will be available at https://github.com/Sense-X/HoP.",
        "authors": [
            "Zhuofan Zong",
            "Dongzhi Jiang",
            "Guanglu Song",
            "Zeyue Xue",
            "Jingyong Su",
            "Hongsheng Li",
            "Yu Liu"
        ]
    },
    {
        "title": "PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects",
        "url": "http://arxiv.org/abs/2308.07391",
        "abstract": "We address the task of simultaneous part-level reconstruction and motion\nparameter estimation for articulated objects. Given two sets of multi-view\nimages of an object in two static articulation states, we decouple the movable\npart from the static part and reconstruct shape and appearance while predicting\nthe motion parameters. To tackle this problem, we present PARIS: a\nself-supervised, end-to-end architecture that learns part-level implicit shape\nand appearance models and optimizes motion parameters jointly without any 3D\nsupervision, motion, or semantic annotation. Our experiments show that our\nmethod generalizes better across object categories, and outperforms baselines\nand prior work that are given 3D point clouds as input. Our approach improves\nreconstruction relative to state-of-the-art baselines with a Chamfer-L1\ndistance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and\nachieves 5% error rate for motion estimation across 10 object categories.\n  Video summary at: https://youtu.be/tDSrROPCgUc",
        "authors": [
            "Jiayi Liu",
            "Ali Mahdavi-Amiri",
            "Manolis Savva"
        ]
    },
    {
        "title": "OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation",
        "url": "http://arxiv.org/abs/2307.09356",
        "abstract": "Referring video object segmentation (RVOS) aims at segmenting an object in a\nvideo following human instruction. Current state-of-the-art methods fall into\nan offline pattern, in which each clip independently interacts with text\nembedding for cross-modal understanding. They usually present that the offline\npattern is necessary for RVOS, yet model limited temporal association within\neach clip. In this work, we break up the previous offline belief and propose a\nsimple yet effective online model using explicit query propagation, named\nOnlineRefer. Specifically, our approach leverages target cues that gather\nsemantic information and position prior to improve the accuracy and ease of\nreferring predictions for the current frame. Furthermore, we generalize our\nonline model into a semi-online framework to be compatible with video-based\nbackbones. To show the effectiveness of our method, we evaluate it on four\nbenchmarks, \\ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and\nJHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L\nbackbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17,\noutperforming all other offline methods.",
        "authors": [
            "Dongming Wu",
            "Tiancai Wang",
            "Yuang Zhang",
            "Xiangyu Zhang",
            "Jianbing Shen"
        ]
    },
    {
        "title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement",
        "url": "http://arxiv.org/abs/2303.11722",
        "abstract": "The following three factors restrict the application of existing low-light\nimage enhancement methods: unpredictable brightness degradation and noise,\ninherent gap between metric-favorable and visual-friendly versions, and the\nlimited paired training data. To address these limitations, we propose an\nimplicit Neural Representation method for Cooperative low-light image\nenhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in\nan unsupervised manner. Concretely, NeRCo unifies the diverse degradation\nfactors of real-world scenes with a controllable fitting function, leading to\nbetter robustness. In addition, for the output results, we introduce\nsemantic-orientated supervision with priors from the pre-trained\nvision-language model. Instead of merely following reference images, it\nencourages results to meet subjective expectations, finding more\nvisual-friendly solutions. Further, to ease the reliance on paired data and\nreduce solution space, we develop a dual-closed-loop constrained enhancement\nmodule. It is trained cooperatively with other affiliated modules in a\nself-supervised manner. Finally, extensive experiments demonstrate the\nrobustness and superior effectiveness of our proposed NeRCo. Our code is\navailable at https://github.com/Ysz2022/NeRCo.",
        "authors": [
            "Shuzhou Yang",
            "Moxuan Ding",
            "Yanmin Wu",
            "Zihan Li",
            "Jian Zhang"
        ]
    },
    {
        "title": "Deep Multiview Clustering by Contrasting Cluster Assignments",
        "url": "http://arxiv.org/abs/2304.10769",
        "abstract": "Multiview clustering (MVC) aims to reveal the underlying structure of\nmultiview data by categorizing data samples into clusters. Deep learning-based\nmethods exhibit strong feature learning capabilities on large-scale datasets.\nFor most existing deep MVC methods, exploring the invariant representations of\nmultiple views is still an intractable problem. In this paper, we propose a\ncross-view contrastive learning (CVCL) method that learns view-invariant\nrepresentations and produces clustering results by contrasting the cluster\nassignments among multiple views. Specifically, we first employ deep\nautoencoders to extract view-dependent features in the pretraining stage. Then,\na cluster-level CVCL strategy is presented to explore consistent semantic label\ninformation among the multiple views in the fine-tuning stage. Thus, the\nproposed CVCL method is able to produce more discriminative cluster assignments\nby virtue of this learning strategy. Moreover, we provide a theoretical\nanalysis of soft cluster assignment alignment. Extensive experimental results\nobtained on several datasets demonstrate that the proposed CVCL method\noutperforms several state-of-the-art approaches.",
        "authors": [
            "Jie Chen",
            "Hua Mao",
            "Wai Lok Woo",
            "Xi Peng"
        ]
    },
    {
        "title": "Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation",
        "url": "http://arxiv.org/abs/2303.09036",
        "abstract": "Generating images with both photorealism and multiview 3D consistency is\ncrucial for 3D-aware GANs, yet existing methods struggle to achieve them\nsimultaneously. Improving the photorealism via CNN-based 2D super-resolution\ncan break the strict 3D consistency, while keeping the 3D consistency by\nlearning high-resolution 3D representations for direct rendering often\ncompromises image quality. In this paper, we propose a novel learning strategy,\nnamely 3D-to-2D imitation, which enables a 3D-aware GAN to generate\nhigh-quality images while maintaining their strict 3D consistency, by letting\nthe images synthesized by the generator's 3D rendering branch to mimic those\ngenerated by its 2D super-resolution branch. We also introduce 3D-aware\nconvolutions into the generator for better 3D representation learning, which\nfurther improves the image generation quality. With the above strategies, our\nmethod reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,\nrespectively, at 512x512 resolution, largely outperforming existing 3D-aware\nGANs using direct 3D rendering and coming very close to the previous\nstate-of-the-art method that leverages 2D super-resolution. Project website:\nhttps://seanchenxy.github.io/Mimic3DWeb.",
        "authors": [
            "Xingyu Chen",
            "Yu Deng",
            "Baoyuan Wang"
        ]
    },
    {
        "title": "Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation",
        "url": "http://arxiv.org/abs/2308.05493",
        "abstract": "Endeavors have been recently made to transfer knowledge from the labeled\npinhole image domain to the unlabeled panoramic image domain via Unsupervised\nDomain Adaptation (UDA). The aim is to tackle the domain gaps caused by the\nstyle disparities and distortion problem from the non-uniformly distributed\npixels of equirectangular projection (ERP). Previous works typically focus on\ntransferring knowledge based on geometric priors with specially designed\nmulti-branch network architectures. As a result, considerable computational\ncosts are induced, and meanwhile, their generalization abilities are profoundly\nhindered by the variation of distortion among pixels. In this paper, we find\nthat the pixels' neighborhood regions of the ERP indeed introduce less\ndistortion. Intuitively, we propose a novel UDA framework that can effectively\naddress the distortion problems for panoramic semantic segmentation. In\ncomparison, our method is simpler, easier to implement, and more\ncomputationally efficient. Specifically, we propose distortion-aware attention\n(DA) capturing the neighboring pixel distribution without using any geometric\nconstraints. Moreover, we propose a class-wise feature aggregation (CFA) module\nto iteratively update the feature representations with a memory bank. As such,\nthe feature similarity between two domains can be consistently optimized.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance while remarkably reducing 80% parameters.",
        "authors": [
            "Xu Zheng",
            "Tianbo Pan",
            "Yunhao Luo",
            "Lin Wang"
        ]
    },
    {
        "title": "Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack",
        "url": "http://arxiv.org/abs/2308.11894",
        "abstract": "In autonomous driving (AD), accurate perception is indispensable to achieving\nsafe and secure driving. Due to its safety-criticality, the security of AD\nperception has been widely studied. Among different attacks on AD perception,\nthe physical adversarial object evasion attacks are especially severe. However,\nwe find that all existing literature only evaluates their attack effect at the\ntargeted AI component level but not at the system level, i.e., with the entire\nsystem semantics and context such as the full AD pipeline. Thereby, this raises\na critical research question: can these existing researches effectively achieve\nsystem-level attack effects (e.g., traffic rule violations) in the real-world\nAD context? In this work, we conduct the first measurement study on whether and\nhow effectively the existing designs can lead to system-level effects,\nespecially for the STOP sign-evasion attacks due to their popularity and\nseverity. Our evaluation results show that all the representative prior works\ncannot achieve any system-level effects. We observe two design limitations in\nthe prior works: 1) physical model-inconsistent object size distribution in\npixel sampling and 2) lack of vehicle plant model and AD system model\nconsideration. Then, we propose SysAdv, a novel system-driven attack design in\nthe AD context and our evaluation results show that the system-level effects\ncan be significantly improved, i.e., the violation rate increases by around\n70%.",
        "authors": [
            "Ningfei Wang",
            "Yunpeng Luo",
            "Takami Sato",
            "Kaidi Xu",
            "Qi Alfred Chen"
        ]
    },
    {
        "title": "Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation",
        "url": "http://arxiv.org/abs/2303.11057",
        "abstract": "Understanding and manipulating deformable objects (e.g., ropes and fabrics)\nis an essential yet challenging task with broad applications. Difficulties come\nfrom complex states and dynamics, diverse configurations and high-dimensional\naction space of deformable objects. Besides, the manipulation tasks usually\nrequire multiple steps to accomplish, and greedy policies may easily lead to\nlocal optimal states. Existing studies usually tackle this problem using\nreinforcement learning or imitating expert demonstrations, with limitations in\nmodeling complex states or requiring hand-crafted expert policies. In this\npaper, we study deformable object manipulation using dense visual affordance,\nwith generalization towards diverse states, and propose a novel kind of\nforesightful dense affordance, which avoids local optima by estimating states'\nvalues for long-term manipulation. We propose a framework for learning this\nrepresentation, with novel designs such as multi-stage stable learning and\nefficient self-supervised data collection without experts. Experiments\ndemonstrate the superiority of our proposed foresightful dense affordance.\nProject page: https://hyperplane-lab.github.io/DeformableAffordance",
        "authors": [
            "Ruihai Wu",
            "Chuanruo Ning",
            "Hao Dong"
        ]
    },
    {
        "title": "Generalizable Neural Fields as Partially Observed Neural Processes",
        "url": "http://arxiv.org/abs/2309.06660",
        "abstract": "Neural fields, which represent signals as a function parameterized by a\nneural network, are a promising alternative to traditional discrete vector or\ngrid-based representations. Compared to discrete representations, neural\nrepresentations both scale well with increasing resolution, are continuous, and\ncan be many-times differentiable. However, given a dataset of signals that we\nwould like to represent, having to optimize a separate neural field for each\nsignal is inefficient, and cannot capitalize on shared information or\nstructures among signals. Existing generalization methods view this as a\nmeta-learning problem and employ gradient-based meta-learning to learn an\ninitialization which is then fine-tuned with test-time optimization, or learn\nhypernetworks to produce the weights of a neural field. We instead propose a\nnew paradigm that views the large-scale training of neural representations as a\npart of a partially-observed neural process framework, and leverage neural\nprocess algorithms to solve this task. We demonstrate that this approach\noutperforms both state-of-the-art gradient-based meta-learning approaches and\nhypernetwork approaches.",
        "authors": [
            "Jeffrey Gu",
            "Kuan-Chieh Wang",
            "Serena Yeung"
        ]
    },
    {
        "title": "CiteTracker: Correlating Image and Text for Visual Tracking",
        "url": "http://arxiv.org/abs/2308.11322",
        "abstract": "Existing visual tracking methods typically take an image patch as the\nreference of the target to perform tracking. However, a single image patch\ncannot provide a complete and precise concept of the target object as images\nare limited in their ability to abstract and can be ambiguous, which makes it\ndifficult to track targets with drastic variations. In this paper, we propose\nthe CiteTracker to enhance target modeling and inference in visual tracking by\nconnecting images and text. Specifically, we develop a text generation module\nto convert the target image patch into a descriptive text containing its class\nand attribute information, providing a comprehensive reference point for the\ntarget. In addition, a dynamic description module is designed to adapt to\ntarget variations for more effective target representation. We then associate\nthe target description and the search image using an attention-based\ncorrelation module to generate the correlated features for target state\nreference. Extensive experiments on five diverse datasets are conducted to\nevaluate the proposed algorithm and the favorable performance against the\nstate-of-the-art methods demonstrates the effectiveness of the proposed\ntracking method.",
        "authors": [
            "Xin Li",
            "Yuqing Huang",
            "Zhenyu He",
            "Yaowei Wang",
            "Huchuan Lu",
            "Ming-Hsuan Yang"
        ]
    },
    {
        "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2302.05543",
        "abstract": "We present ControlNet, a neural network architecture to add spatial\nconditioning controls to large, pretrained text-to-image diffusion models.\nControlNet locks the production-ready large diffusion models, and reuses their\ndeep and robust encoding layers pretrained with billions of images as a strong\nbackbone to learn a diverse set of conditional controls. The neural\narchitecture is connected with \"zero convolutions\" (zero-initialized\nconvolution layers) that progressively grow the parameters from zero and ensure\nthat no harmful noise could affect the finetuning. We test various conditioning\ncontrols, eg, edges, depth, segmentation, human pose, etc, with Stable\nDiffusion, using single or multiple conditions, with or without prompts. We\nshow that the training of ControlNets is robust with small (<50k) and large\n(>1m) datasets. Extensive results show that ControlNet may facilitate wider\napplications to control image diffusion models.",
        "authors": [
            "Lvmin Zhang",
            "Anyi Rao",
            "Maneesh Agrawala"
        ]
    },
    {
        "title": "Unleashing Text-to-Image Diffusion Models for Visual Perception",
        "url": "http://arxiv.org/abs/2303.02153",
        "abstract": "Diffusion models (DMs) have become the new trend of generative models and\nhave demonstrated a powerful ability of conditional synthesis. Among those,\ntext-to-image diffusion models pre-trained on large-scale image-text pairs are\nhighly controllable by customizable prompts. Unlike the unconditional\ngenerative models that focus on low-level attributes and details, text-to-image\ndiffusion models contain more high-level knowledge thanks to the\nvision-language pre-training. In this paper, we propose VPD (Visual Perception\nwith a pre-trained Diffusion model), a new framework that exploits the semantic\ninformation of a pre-trained text-to-image diffusion model in visual perception\ntasks. Instead of using the pre-trained denoising autoencoder in a\ndiffusion-based pipeline, we simply use it as a backbone and aim to study how\nto take full advantage of the learned knowledge. Specifically, we prompt the\ndenoising decoder with proper textual inputs and refine the text features with\nan adapter, leading to a better alignment to the pre-trained stage and making\nthe visual contents interact with the text prompts. We also propose to utilize\nthe cross-attention maps between the visual features and the text features to\nprovide explicit guidance. Compared with other pre-training methods, we show\nthat vision-language pre-trained diffusion models can be faster adapted to\ndownstream visual perception tasks using the proposed VPD. Extensive\nexperiments on semantic segmentation, referring image segmentation and depth\nestimation demonstrates the effectiveness of our method. Notably, VPD attains\n0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring\nimage segmentation, establishing new records on these two benchmarks. Code is\navailable at https://github.com/wl-zhao/VPD",
        "authors": [
            "Wenliang Zhao",
            "Yongming Rao",
            "Zuyan Liu",
            "Benlin Liu",
            "Jie Zhou",
            "Jiwen Lu"
        ]
    },
    {
        "title": "Iterative Superquadric Recomposition of 3D Objects from Multiple Views",
        "url": "http://arxiv.org/abs/2309.02102",
        "abstract": "Humans are good at recomposing novel objects, i.e. they can identify\ncommonalities between unknown objects from general structure to finer detail,\nan ability difficult to replicate by machines. We propose a framework, ISCO, to\nrecompose an object using 3D superquadrics as semantic parts directly from 2D\nviews without training a model that uses 3D supervision. To achieve this, we\noptimize the superquadric parameters that compose a specific instance of the\nobject, comparing its rendered 3D view and 2D image silhouette. Our ISCO\nframework iteratively adds new superquadrics wherever the reconstruction error\nis high, abstracting first coarse regions and then finer details of the target\nobject. With this simple coarse-to-fine inductive bias, ISCO provides\nconsistent superquadrics for related object parts, despite not having any\nsemantic supervision. Since ISCO does not train any neural network, it is also\ninherently robust to out-of-distribution objects. Experiments show that,\ncompared to recent single instance superquadrics reconstruction approaches,\nISCO provides consistently more accurate 3D reconstructions, even from images\nin the wild. Code available at https://github.com/ExplainableML/ISCO .",
        "authors": [
            "Stephan Alaniz",
            "Massimiliano Mancini",
            "Zeynep Akata"
        ]
    },
    {
        "title": "TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models",
        "url": "http://arxiv.org/abs/2308.05985",
        "abstract": "Robust pedestrian trajectory forecasting is crucial to developing safe\nautonomous vehicles. Although previous works have studied adversarial\nrobustness in the context of trajectory forecasting, some significant issues\nremain unaddressed. In this work, we try to tackle these crucial problems.\nFirstly, the previous definitions of robustness in trajectory prediction are\nambiguous. We thus provide formal definitions for two kinds of robustness,\nnamely label robustness and pure robustness. Secondly, as previous works fail\nto consider robustness about all points in a disturbance interval, we utilise a\nprobably approximately correct (PAC) framework for robustness verification.\nAdditionally, this framework can not only identify potential counterexamples,\nbut also provides interpretable analyses of the original methods. Our approach\nis applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the\nrobustness of four state-of-the-art trajectory prediction models --\nTrajectron++, MemoNet, AgentFormer, and MID -- on trajectories from five scenes\nof the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using our\nframework, we also experimentally study various factors that could influence\nrobustness performance.",
        "authors": [
            "Liang Zhang",
            "Nathaniel Xu",
            "Pengfei Yang",
            "Gaojie Jin",
            "Cheng-Chao Huang",
            "Lijun Zhang"
        ]
    },
    {
        "title": "Efficient Neural Supersampling on a Novel Gaming Dataset",
        "url": "http://arxiv.org/abs/2308.01483",
        "abstract": "Real-time rendering for video games has become increasingly challenging due\nto the need for higher resolutions, framerates and photorealism. Supersampling\nhas emerged as an effective solution to address this challenge. Our work\nintroduces a novel neural algorithm for supersampling rendered content that is\n4 times more efficient than existing methods while maintaining the same level\nof accuracy. Additionally, we introduce a new dataset which provides auxiliary\nmodalities such as motion vectors and depth generated using graphics rendering\nfeatures like viewport jittering and mipmap biasing at different resolutions.\nWe believe that this dataset fills a gap in the current dataset landscape and\ncan serve as a valuable resource to help measure progress in the field and\nadvance the state-of-the-art in super-resolution techniques for gaming content.",
        "authors": [
            "Antoine Mercier",
            "Ruan Erasmus",
            "Yashesh Savani",
            "Manik Dhingra",
            "Fatih Porikli",
            "Guillaume Berger"
        ]
    },
    {
        "title": "Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?",
        "url": "http://arxiv.org/abs/2305.09275",
        "abstract": "We revisit the common practice of evaluating adaptation of Online Continual\nLearning (OCL) algorithms through the metric of online accuracy, which measures\nthe accuracy of the model on the immediate next few samples. However, we show\nthat this metric is unreliable, as even vacuous blind classifiers, which do not\nuse input images for prediction, can achieve unrealistically high online\naccuracy by exploiting spurious label correlations in the data stream. Our\nstudy reveals that existing OCL algorithms can also achieve high online\naccuracy, but perform poorly in retaining useful information, suggesting that\nthey unintentionally learn spurious label correlations. To address this issue,\nwe propose a novel metric for measuring adaptation based on the accuracy on the\nnear-future samples, where spurious correlations are removed. We benchmark\nexisting OCL approaches using our proposed metric on large-scale datasets under\nvarious computational budgets and find that better generalization can be\nachieved by retaining and reusing past seen information. We believe that our\nproposed metric can aid in the development of truly adaptive OCL methods. We\nprovide code to reproduce our results at\nhttps://github.com/drimpossible/EvalOCL.",
        "authors": [
            "Hasan Abed Al Kader Hammoud",
            "Ameya Prabhu",
            "Ser-Nam Lim",
            "Philip H. S. Torr",
            "Adel Bibi",
            "Bernard Ghanem"
        ]
    },
    {
        "title": "Label-Efficient Online Continual Object Detection in Streaming Video",
        "url": "http://arxiv.org/abs/2206.00309",
        "abstract": "Humans can watch a continuous video stream and effortlessly perform continual\nacquisition and transfer of new knowledge with minimal supervision yet\nretaining previously learnt experiences. In contrast, existing continual\nlearning (CL) methods require fully annotated labels to effectively learn from\nindividual frames in a video stream. Here, we examine a more realistic and\nchallenging problem$\\unicode{x2014}$Label-Efficient Online Continual Object\nDetection (LEOCOD) in streaming video. We propose a plug-and-play module,\nEfficient-CLS, that can be easily inserted into and improve existing continual\nlearners for object detection in video streams with reduced data annotation\ncosts and model retraining time. We show that our method has achieved\nsignificant improvement with minimal forgetting across all supervision levels\non two challenging CL benchmarks for streaming real-world videos. Remarkably,\nwith only 25% annotated video frames, our method still outperforms the base CL\nlearners, which are trained with 100% annotations on all video frames. The data\nand source code will be publicly available at\nhttps://github.com/showlab/Efficient-CLS.",
        "authors": [
            "Jay Zhangjie Wu",
            "David Junhao Zhang",
            "Wynne Hsu",
            "Mengmi Zhang",
            "Mike Zheng Shou"
        ]
    },
    {
        "title": "Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation",
        "url": "http://arxiv.org/abs/2304.11705",
        "abstract": "The ability to deploy robots that can operate safely in diverse environments\nis crucial for developing embodied intelligent agents. As a community, we have\nmade tremendous progress in within-domain LiDAR semantic segmentation. However,\ndo these methods generalize across domains? To answer this question, we design\nthe first experimental setup for studying domain generalization (DG) for LiDAR\nsemantic segmentation (DG-LSS). Our results confirm a significant gap between\nmethods, evaluated in a cross-domain setting: for example, a model trained on\nthe source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data,\ncompared to $48.49$ mIoU obtained by the model trained on the target domain\n(nuScenes). To tackle this gap, we propose the first method specifically\ndesigned for DG-LSS, which obtains $34.88$ mIoU on the target domain,\noutperforming all baselines. Our method augments a sparse-convolutional\nencoder-decoder 3D segmentation network with an additional, dense 2D\nconvolutional decoder that learns to classify a birds-eye view of the point\ncloud. This simple auxiliary task encourages the 3D network to learn features\nthat are robust to sensor placement shifts and resolution, and are transferable\nacross domains. With this work, we aim to inspire the community to develop and\nevaluate future models in such cross-domain conditions.",
        "authors": [
            "Cristiano Saltori",
            "Aljo\u0161a O\u0161ep",
            "Elisa Ricci",
            "Laura Leal-Taix\u00e9"
        ]
    },
    {
        "title": "Diverse Cotraining Makes Strong Semi-Supervised Segmentor",
        "url": "http://arxiv.org/abs/2308.09281",
        "abstract": "Deep co-training has been introduced to semi-supervised segmentation and\nachieves impressive results, yet few studies have explored the working\nmechanism behind it. In this work, we revisit the core assumption that supports\nco-training: multiple compatible and conditionally independent views. By\ntheoretically deriving the generalization upper bound, we prove the prediction\nsimilarity between two models negatively impacts the model's generalization\nability. However, most current co-training models are tightly coupled together\nand violate this assumption. Such coupling leads to the homogenization of\nnetworks and confirmation bias which consequently limits the performance. To\nthis end, we explore different dimensions of co-training and systematically\nincrease the diversity from the aspects of input domains, different\naugmentations and model architectures to counteract homogenization. Our Diverse\nCo-training outperforms the state-of-the-art (SOTA) methods by a large margin\nacross different evaluation protocols on the Pascal and Cityscapes. For\nexample. we achieve the best mIoU of 76.2%, 77.7% and 80.2% on Pascal with only\n92, 183 and 366 labeled images, surpassing the previous best results by more\nthan 5%.",
        "authors": [
            "Yijiang Li",
            "Xinjiang Wang",
            "Lihe Yang",
            "Litong Feng",
            "Wayne Zhang",
            "Ying Gao"
        ]
    },
    {
        "title": "Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution",
        "url": "http://arxiv.org/abs/2303.08942",
        "abstract": "Guided depth map super-resolution (GDSR), as a hot topic in multi-modal image\nprocessing, aims to upsample low-resolution (LR) depth maps with additional\ninformation involved in high-resolution (HR) RGB images from the same scene.\nThe critical step of this task is to effectively extract domain-shared and\ndomain-private RGB/depth features. In addition, three detailed issues, namely\nblurry edges, noisy surfaces, and over-transferred RGB texture, need to be\naddressed. In this paper, we propose the Spherical Space feature Decomposition\nNetwork (SSDNet) to solve the above issues. To better model cross-modality\nfeatures, Restormer block-based RGB/depth encoders are employed for extracting\nlocal-global features. Then, the extracted features are mapped to the spherical\nspace to complete the separation of private features and the alignment of\nshared features. Shared features of RGB are fused with the depth features to\ncomplete the GDSR task. Subsequently, a spherical contrast refinement (SCR)\nmodule is proposed to further address the detail issues. Patches that are\nclassified according to imperfect categories are input into the SCR module,\nwhere the patch features are pulled closer to the ground truth and pushed away\nfrom the corresponding imperfect samples in the spherical feature space via\ncontrastive learning. Extensive experiments demonstrate that our method can\nachieve state-of-the-art results on four test datasets, as well as successfully\ngeneralize to real-world scenes. The code is available at\n\\url{https://github.com/Zhaozixiang1228/GDSR-SSDNet}.",
        "authors": [
            "Zixiang Zhao",
            "Jiangshe Zhang",
            "Xiang Gu",
            "Chengli Tan",
            "Shuang Xu",
            "Yulun Zhang",
            "Radu Timofte",
            "Luc Van Gool"
        ]
    },
    {
        "title": "Tiled Multiplane Images for Practical 3D Photography",
        "url": "http://arxiv.org/abs/2309.14291",
        "abstract": "The task of synthesizing novel views from a single image has useful\napplications in virtual reality and mobile computing, and a number of\napproaches to the problem have been proposed in recent years. A Multiplane\nImage (MPI) estimates the scene as a stack of RGBA layers, and can model\ncomplex appearance effects, anti-alias depth errors and synthesize soft edges\nbetter than methods that use textured meshes or layered depth images. And\nunlike neural radiance fields, an MPI can be efficiently rendered on graphics\nhardware. However, MPIs are highly redundant and require a large number of\ndepth layers to achieve plausible results. Based on the observation that the\ndepth complexity in local image regions is lower than that over the entire\nimage, we split an MPI into many small, tiled regions, each with only a few\ndepth planes. We call this representation a Tiled Multiplane Image (TMPI). We\npropose a method for generating a TMPI with adaptive depth planes for\nsingle-view 3D photography in the wild. Our synthesized results are comparable\nto state-of-the-art single-view MPI methods while having lower computational\noverhead.",
        "authors": [
            "Numair Khan",
            "Douglas Lanman",
            "Lei Xiao"
        ]
    },
    {
        "title": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
        "url": "http://arxiv.org/abs/2303.16058",
        "abstract": "Video Foundation Models (VFMs) have received limited exploration due to high\ncomputational costs and data scarcity. Previous VFMs rely on Image Foundation\nModels (IFMs), which face challenges in transferring to the video domain.\nAlthough VideoMAE has trained a robust ViT from limited data, its low-level\nreconstruction poses convergence difficulties and conflicts with high-level\ncross-modal alignment. This paper proposes a training-efficient method for\ntemporal-sensitive VFMs that integrates the benefits of existing methods. To\nincrease data efficiency, we mask out most of the low-semantics video tokens,\nbut selectively align the unmasked tokens with IFM, which serves as the\nUnMasked Teacher (UMT). By providing semantic guidance, our method enables\nfaster convergence and multimodal friendliness. With a progressive pre-training\nframework, our model can handle various tasks including scene-related,\ntemporal-related, and complex video-language understanding. Using only public\nsources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16\nachieves state-of-the-art performances on various video tasks. The code and\nmodels will be released at https://github.com/OpenGVLab/unmasked_teacher.",
        "authors": [
            "Kunchang Li",
            "Yali Wang",
            "Yizhuo Li",
            "Yi Wang",
            "Yinan He",
            "Limin Wang",
            "Yu Qiao"
        ]
    },
    {
        "title": "Explore and Tell: Embodied Visual Captioning in 3D Environments",
        "url": "http://arxiv.org/abs/2308.10447",
        "abstract": "While current visual captioning models have achieved impressive performance,\nthey often assume that the image is well-captured and provides a complete view\nof the scene. In real-world scenarios, however, a single image may not offer a\ngood viewpoint, hindering fine-grained scene understanding. To overcome this\nlimitation, we propose a novel task called Embodied Captioning, which equips\nvisual captioning models with navigation capabilities, enabling them to\nactively explore the scene and reduce visual ambiguity from suboptimal\nviewpoints. Specifically, starting at a random viewpoint, an agent must\nnavigate the environment to gather information from different viewpoints and\ngenerate a comprehensive paragraph describing all objects in the scene. To\nsupport this task, we build the ET-Cap dataset with Kubric simulator,\nconsisting of 10K 3D scenes with cluttered objects and three annotated\nparagraphs per scene. We propose a Cascade Embodied Captioning model (CaBOT),\nwhich comprises of a navigator and a captioner, to tackle this task. The\nnavigator predicts which actions to take in the environment, while the\ncaptioner generates a paragraph description based on the whole navigation\ntrajectory. Extensive experiments demonstrate that our model outperforms other\ncarefully designed baselines. Our dataset, codes and models are available at\nhttps://aim3-ruc.github.io/ExploreAndTell.",
        "authors": [
            "Anwen Hu",
            "Shizhe Chen",
            "Liang Zhang",
            "Qin Jin"
        ]
    },
    {
        "title": "FastViT: A Fast Hybrid Vision Transformer Using Structural Reparameterization",
        "url": "http://arxiv.org/abs/2303.14189",
        "abstract": "The recent amalgamation of transformer and convolutional designs has led to\nsteady improvements in accuracy and efficiency of the models. In this work, we\nintroduce FastViT, a hybrid vision transformer architecture that obtains the\nstate-of-the-art latency-accuracy trade-off. To this end, we introduce a novel\ntoken mixing operator, RepMixer, a building block of FastViT, that uses\nstructural reparameterization to lower the memory access cost by removing\nskip-connections in the network. We further apply train-time\noverparametrization and large kernel convolutions to boost accuracy and\nempirically show that these choices have minimal effect on latency. We show\nthat - our model is 3.5x faster than CMT, a recent state-of-the-art hybrid\ntransformer architecture, 4.9x faster than EfficientNet, and 1.9x faster than\nConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. At\nsimilar latency, our model obtains 4.2% better Top-1 accuracy on ImageNet than\nMobileOne. Our model consistently outperforms competing architectures across\nseveral tasks -- image classification, detection, segmentation and 3D mesh\nregression with significant improvement in latency on both a mobile device and\na desktop GPU. Furthermore, our model is highly robust to out-of-distribution\nsamples and corruptions, improving over competing robust models. Code and\nmodels are available at https://github.com/apple/ml-fastvit.",
        "authors": [
            "Pavan Kumar Anasosalu Vasu",
            "James Gabriel",
            "Jeff Zhu",
            "Oncel Tuzel",
            "Anurag Ranjan"
        ]
    },
    {
        "title": "Multi-view Self-supervised Disentanglement for General Image Denoising",
        "url": "http://arxiv.org/abs/2309.05049",
        "abstract": "With its significant performance improvements, the deep learning paradigm has\nbecome a standard tool for modern image denoisers. While promising performance\nhas been shown on seen noise distributions, existing approaches often suffer\nfrom generalisation to unseen noise types or general and real noise. It is\nunderstandable as the model is designed to learn paired mapping (e.g. from a\nnoisy image to its clean version). In this paper, we instead propose to learn\nto disentangle the noisy image, under the intuitive assumption that different\ncorrupted versions of the same clean image share a common latent space. A\nself-supervised learning framework is proposed to achieve the goal, without\nlooking at the latent clean image. By taking two different corrupted versions\nof the same image as input, the proposed Multi-view Self-supervised\nDisentanglement (MeD) approach learns to disentangle the latent clean features\nfrom the corruptions and recover the clean image consequently. Extensive\nexperimental analysis on both synthetic and real noise shows the superiority of\nthe proposed method over prior self-supervised approaches, especially on unseen\nnovel noise types. On real noise, the proposed method even outperforms its\nsupervised counterparts by over 3 dB.",
        "authors": [
            "Hao Chen",
            "Chenyuan Qu",
            "Yu Zhang",
            "Chen Chen",
            "Jianbo Jiao"
        ]
    },
    {
        "title": "Multi-Event Video-Text Retrieval",
        "url": "http://arxiv.org/abs/2308.11551",
        "abstract": "Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive\nvideo-text data on the Internet. A plethora of work characterized by using a\ntwo-stream Vision-Language model architecture that learns a joint\nrepresentation of video-text pairs has become a prominent approach for the VTR\ntask. However, these models operate under the assumption of bijective\nvideo-text correspondences and neglect a more practical scenario where video\ncontent usually encompasses multiple events, while texts like user queries or\nwebpage metadata tend to be specific and correspond to single events. This\nestablishes a gap between the previous training objective and real-world\napplications, leading to the potential performance degradation of earlier\nmodels during inference. In this study, we introduce the Multi-event Video-Text\nRetrieval (MeVTR) task, addressing scenarios in which each video contains\nmultiple different events, as a niche scenario of the conventional Video-Text\nRetrieval Task. We present a simple model, Me-Retriever, which incorporates key\nevent video representation and a new MeVTR loss for the MeVTR task.\nComprehensive experiments show that this straightforward framework outperforms\nother models in the Video-to-Text and Text-to-Video tasks, effectively\nestablishing a robust baseline for the MeVTR task. We believe this work serves\nas a strong foundation for future studies. Code is available at\nhttps://github.com/gengyuanmax/MeVTR.",
        "authors": [
            "Gengyuan Zhang",
            "Jisen Ren",
            "Jindong Gu",
            "Volker Tresp"
        ]
    },
    {
        "title": "SHERF: Generalizable Human NeRF from a Single Image",
        "url": "http://arxiv.org/abs/2303.12791",
        "abstract": "Existing Human NeRF methods for reconstructing 3D humans typically rely on\nmultiple 2D images from multi-view cameras or monocular videos captured from\nfixed camera views. However, in real-world scenarios, human images are often\ncaptured from random camera angles, presenting challenges for high-quality 3D\nhuman reconstruction. In this paper, we propose SHERF, the first generalizable\nHuman NeRF model for recovering animatable 3D humans from a single input image.\nSHERF extracts and encodes 3D human representations in canonical space,\nenabling rendering and animation from free views and poses. To achieve\nhigh-fidelity novel view and pose synthesis, the encoded 3D human\nrepresentations should capture both global appearance and local fine-grained\ntextures. To this end, we propose a bank of 3D-aware hierarchical features,\nincluding global, point-level, and pixel-aligned features, to facilitate\ninformative encoding. Global features enhance the information extracted from\nthe single input image and complement the information missing from the partial\n2D observation. Point-level features provide strong clues of 3D human\nstructure, while pixel-aligned features preserve more fine-grained details. To\neffectively integrate the 3D-aware hierarchical feature bank, we design a\nfeature fusion transformer. Extensive experiments on THuman, RenderPeople,\nZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art\nperformance, with better generalizability for novel view and pose synthesis.",
        "authors": [
            "Shoukang Hu",
            "Fangzhou Hong",
            "Liang Pan",
            "Haiyi Mei",
            "Lei Yang",
            "Ziwei Liu"
        ]
    },
    {
        "title": "MVPSNet: Fast Generalizable Multi-view Photometric Stereo",
        "url": "http://arxiv.org/abs/2305.11167",
        "abstract": "We propose a fast and generalizable solution to Multi-view Photometric Stereo\n(MVPS), called MVPSNet. The key to our approach is a feature extraction network\nthat effectively combines images from the same view captured under multiple\nlighting conditions to extract geometric features from shading cues for stereo\nmatching. We demonstrate these features, termed `Light Aggregated Feature Maps'\n(LAFM), are effective for feature matching even in textureless regions, where\ntraditional multi-view stereo methods fail. Our method produces similar\nreconstruction results to PS-NeRF, a state-of-the-art MVPS method that\noptimizes a neural network per-scene, while being 411$\\times$ faster (105\nseconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic\ndataset for MVPS, sMVPS, which is shown to be effective to train a\ngeneralizable MVPS method.",
        "authors": [
            "Dongxu Zhao",
            "Daniel Lichy",
            "Pierre-Nicolas Perrin",
            "Jan-Michael Frahm",
            "Soumyadip Sengupta"
        ]
    },
    {
        "title": "High Quality Entity Segmentation",
        "url": "http://arxiv.org/abs/2211.05776",
        "abstract": "Dense image segmentation tasks e.g., semantic, panoptic) are useful for image\nediting, but existing methods can hardly generalize well in an in-the-wild\nsetting where there are unrestricted image domains, classes, and image\nresolution and quality variations. Motivated by these observations, we\nconstruct a new entity segmentation dataset, with a strong focus on\nhigh-quality dense segmentation in the wild. The dataset contains images\nspanning diverse image domains and entities, along with plentiful\nhigh-resolution images and high-quality mask annotations for training and\ntesting. Given the high-quality and -resolution nature of the dataset, we\npropose CropFormer which is designed to tackle the intractability of\ninstance-level segmentation on high-resolution images. It improves mask\nprediction by fusing high-res image crops that provide more fine-grained image\ndetails and the full image. CropFormer is the first query-based Transformer\narchitecture that can effectively fuse mask predictions from multiple image\nviews, by learning queries that effectively associate the same entities across\nthe full image and its crop. With CropFormer, we achieve a significant AP gain\nof $1.9$ on the challenging entity segmentation task. Furthermore, CropFormer\nconsistently improves the accuracy of traditional segmentation tasks and\ndatasets. The dataset and code will be released at\nhttp://luqi.info/entityv2.github.io/.",
        "authors": [
            "Lu Qi",
            "Jason Kuen",
            "Weidong Guo",
            "Tiancheng Shen",
            "Jiuxiang Gu",
            "Jiaya Jia",
            "Zhe Lin",
            "Ming-Hsuan Yang"
        ]
    },
    {
        "title": "CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection",
        "url": "http://arxiv.org/abs/2309.01093",
        "abstract": "Task driven object detection aims to detect object instances suitable for\naffording a task in an image. Its challenge lies in object categories available\nfor the task being too diverse to be limited to a closed set of object\nvocabulary for traditional object detection. Simply mapping categories and\nvisual features of common objects to the task cannot address the challenge. In\nthis paper, we propose to explore fundamental affordances rather than object\ncategories, i.e., common attributes that enable different objects to accomplish\nthe same task. Moreover, we propose a novel multi-level chain-of-thought\nprompting (MLCoT) to extract the affordance knowledge from large language\nmodels, which contains multi-level reasoning steps from task to object examples\nto essential visual attributes with rationales. Furthermore, to fully exploit\nknowledge to benefit object recognition and localization, we propose a\nknowledge-conditional detection framework, namely CoTDet. It conditions the\ndetector from the knowledge to generate object queries and regress boxes.\nExperimental results demonstrate that our CoTDet outperforms state-of-the-art\nmethods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can\ngenerate rationales for why objects are detected to afford the task.",
        "authors": [
            "Jiajin Tang",
            "Ge Zheng",
            "Jingyi Yu",
            "Sibei Yang"
        ]
    },
    {
        "title": "Human from Blur: Human Pose Tracking from Blurry Images",
        "url": "http://arxiv.org/abs/2303.17209",
        "abstract": "We propose a method to estimate 3D human poses from substantially blurred\nimages. The key idea is to tackle the inverse problem of image deblurring by\nmodeling the forward problem with a 3D human model, a texture map, and a\nsequence of poses to describe human motion. The blurring process is then\nmodeled by a temporal image aggregation step. Using a differentiable renderer,\nwe can solve the inverse problem by backpropagating the pixel-wise reprojection\nerror to recover the best human motion representation that explains a single or\nmultiple input images. Since the image reconstruction loss alone is\ninsufficient, we present additional regularization terms. To the best of our\nknowledge, we present the first method to tackle this problem. Our method\nconsistently outperforms other methods on significantly blurry inputs since\nthey lack one or multiple key functionalities that our method unifies, i.e.\nimage deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid\nhuman motion.",
        "authors": [
            "Yiming Zhao",
            "Denys Rozumnyi",
            "Jie Song",
            "Otmar Hilliges",
            "Marc Pollefeys",
            "Martin R. Oswald"
        ]
    },
    {
        "title": "NerfAcc: Efficient Sampling Accelerates NeRFs",
        "url": "http://arxiv.org/abs/2305.04966",
        "abstract": "Optimizing and rendering Neural Radiance Fields is computationally expensive\ndue to the vast number of samples required by volume rendering. Recent works\nhave included alternative sampling approaches to help accelerate their methods,\nhowever, they are often not the focus of the work. In this paper, we\ninvestigate and compare multiple sampling approaches and demonstrate that\nimproved sampling is generally applicable across NeRF variants under an unified\nconcept of transmittance estimator. To facilitate future experiments, we\ndevelop NerfAcc, a Python toolbox that provides flexible APIs for incorporating\nadvanced sampling methods into NeRF related methods. We demonstrate its\nflexibility by showing that it can reduce the training time of several recent\nNeRF methods by 1.5x to 20x with minimal modifications to the existing\ncodebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be\nimplemented in native PyTorch using NerfAcc.",
        "authors": [
            "Ruilong Li",
            "Hang Gao",
            "Matthew Tancik",
            "Angjoo Kanazawa"
        ]
    },
    {
        "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance",
        "url": "http://arxiv.org/abs/2308.13504",
        "abstract": "We present accumulator-aware quantization (A2Q), a novel weight quantization\nmethod designed to train quantized neural networks (QNNs) to avoid overflow\nwhen using low-precision accumulators during inference. A2Q introduces a unique\nformulation inspired by weight normalization that constrains the L1-norm of\nmodel weights according to accumulator bit width bounds that we derive. Thus,\nin training QNNs for low-precision accumulation, A2Q also inherently promotes\nunstructured weight sparsity to guarantee overflow avoidance. We apply our\nmethod to deep learning-based computer vision tasks to show that A2Q can train\nQNNs for low-precision accumulators while maintaining model accuracy\ncompetitive with a floating-point baseline. In our evaluations, we consider the\nimpact of A2Q on both general-purpose platforms and programmable hardware.\nHowever, we primarily target model deployment on FPGAs because they can be\nprogrammed to fully exploit custom accumulator bit widths. Our experimentation\nshows accumulator bit width significantly impacts the resource efficiency of\nFPGA-based accelerators. On average across our benchmarks, A2Q offers up to a\n2.3x reduction in resource utilization over 32-bit accumulator counterparts\nwith 99.2% of the floating-point model accuracy.",
        "authors": [
            "Ian Colbert",
            "Alessandro Pappalardo",
            "Jakoba Petri-Koenig"
        ]
    },
    {
        "title": "ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic 3D Scenes",
        "url": "http://arxiv.org/abs/2304.04321",
        "abstract": "Understanding the continuous states of objects is essential for task learning\nand planning in the real world. However, most existing task learning benchmarks\nassume discrete (e.g., binary) object goal states, which poses challenges for\nthe learning of complex tasks and transferring learned policy from simulated\nenvironments to the real world. Furthermore, state discretization limits a\nrobot's ability to follow human instructions based on the grounding of actions\nand states. To tackle these challenges, we present ARNOLD, a benchmark that\nevaluates language-grounded task learning with continuous states in realistic\n3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve\nunderstanding object states and learning policies for continuous goals. To\npromote language-instructed learning, we provide expert demonstrations with\ntemplate-generated language descriptions. We assess task performance by\nutilizing the latest language-conditioned policy learning models. Our results\nindicate that current models for language-conditioned manipulations continue to\nexperience significant challenges in novel goal-state generalizations, scene\ngeneralizations, and object generalizations. These findings highlight the need\nto develop new algorithms that address this gap and underscore the potential\nfor further research in this area. Project website:\nhttps://arnold-benchmark.github.io.",
        "authors": [
            "Ran Gong",
            "Jiangyong Huang",
            "Yizhou Zhao",
            "Haoran Geng",
            "Xiaofeng Gao",
            "Qingyang Wu",
            "Wensi Ai",
            "Ziheng Zhou",
            "Demetri Terzopoulos",
            "Song-Chun Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ]
    },
    {
        "title": "Full-Body Articulated Human-Object Interaction",
        "url": "http://arxiv.org/abs/2212.10621",
        "abstract": "Fine-grained capturing of 3D HOI boosts human activity understanding and\nfacilitates downstream visual tasks, including action recognition, holistic\nscene reconstruction, and human motion synthesis. Despite its significance,\nexisting works mostly assume that humans interact with rigid objects using only\na few body parts, limiting their scope. In this paper, we address the\nchallenging problem of f-AHOI, wherein the whole human bodies interact with\narticulated objects, whose parts are connected by movable joints. We present\nCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hours\nof versatile interactions between 46 participants and 81 articulated and rigid\nsittable objects. CHAIRS provides 3D meshes of both humans and articulated\nobjects during the entire interactive process, as well as realistic and\nphysically plausible full-body interactions. We show the value of CHAIRS with\nobject pose estimation. By learning the geometrical relationships in HOI, we\ndevise the very first model that leverage human pose estimation to tackle the\nestimation of articulated object poses and shapes during whole-body\ninteractions. Given an image and an estimated human pose, our model first\nreconstructs the pose and shape of the object, then optimizes the\nreconstruction according to a learned interaction prior. Under both evaluation\nsettings (e.g., with or without the knowledge of objects'\ngeometries/structures), our model significantly outperforms baselines. We hope\nCHAIRS will promote the community towards finer-grained interaction\nunderstanding. We will make the data/code publicly available.",
        "authors": [
            "Nan Jiang",
            "Tengyu Liu",
            "Zhexuan Cao",
            "Jieming Cui",
            "Zhiyuan zhang",
            "Yixin Chen",
            "He Wang",
            "Yixin Zhu",
            "Siyuan Huang"
        ]
    },
    {
        "title": "FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models",
        "url": "http://arxiv.org/abs/2303.12786",
        "abstract": "Recent works on generalizable NeRFs have shown promising results on novel\nview synthesis from single or few images. However, such models have rarely been\napplied on other downstream tasks beyond synthesis such as semantic\nunderstanding and parsing. In this paper, we propose a novel framework named\nFeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision\nfoundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D\npre-trained foundation models to 3D space via neural rendering, and then\nextract deep features for 3D query points from NeRF MLPs. Consequently, it\nallows to map 2D images to continuous 3D semantic feature volumes, which can be\nused for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D\nsemantic keypoint transfer and 2D/3D object part segmentation. Our extensive\nexperiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D\nsemantic feature extractor. Our project page is available at\nhttps://jianglongye.com/featurenerf/ .",
        "authors": [
            "Jianglong Ye",
            "Naiyan Wang",
            "Xiaolong Wang"
        ]
    },
    {
        "title": "SRFormer: Permuted Self-Attention for Single Image Super-Resolution",
        "url": "http://arxiv.org/abs/2303.09735",
        "abstract": "Previous works have shown that increasing the window size for\nTransformer-based image super-resolution models (e.g., SwinIR) can\nsignificantly improve the model performance but the computation overhead is\nalso considerable. In this paper, we present SRFormer, a simple but novel\nmethod that can enjoy the benefit of large window self-attention but introduces\neven less computational burden. The core of our SRFormer is the permuted\nself-attention (PSA), which strikes an appropriate balance between the channel\nand spatial information for self-attention. Our PSA is simple and can be easily\napplied to existing super-resolution networks based on window self-attention.\nWithout any bells and whistles, we show that our SRFormer achieves a 33.86dB\nPSNR score on the Urban100 dataset, which is 0.46dB higher than that of SwinIR\nbut uses fewer parameters and computations. We hope our simple and effective\napproach can serve as a useful tool for future research in super-resolution\nmodel design.",
        "authors": [
            "Yupeng Zhou",
            "Zhen Li",
            "Chun-Le Guo",
            "Song Bai",
            "Ming-Ming Cheng",
            "Qibin Hou"
        ]
    },
    {
        "title": "Audio-Visual Glance Network for Efficient Video Recognition",
        "url": "http://arxiv.org/abs/2308.09322",
        "abstract": "Deep learning has made significant strides in video understanding tasks, but\nthe computation required to classify lengthy and massive videos using\nclip-level video classifiers remains impractical and prohibitively expensive.\nTo address this issue, we propose Audio-Visual Glance Network (AVGN), which\nleverages the commonly available audio and visual modalities to efficiently\nprocess the spatio-temporally important parts of a video. AVGN firstly divides\nthe video into snippets of image-audio clip pair and employs lightweight\nunimodal encoders to extract global visual features and audio features. To\nidentify the important temporal segments, we use an Audio-Visual Temporal\nSaliency Transformer (AV-TeST) that estimates the saliency scores of each\nframe. To further increase efficiency in the spatial dimension, AVGN processes\nonly the important patches instead of the whole images. We use an\nAudio-Enhanced Spatial Patch Attention (AESPA) module to produce a set of\nenhanced coarse visual features, which are fed to a policy network that\nproduces the coordinates of the important patches. This approach enables us to\nfocus only on the most important spatio-temporally parts of the video, leading\nto more efficient video recognition. Moreover, we incorporate various training\ntechniques and multi-modal feature fusion to enhance the robustness and\neffectiveness of our AVGN. By combining these strategies, our AVGN sets new\nstate-of-the-art performance in multiple video recognition benchmarks while\nachieving faster processing speed.",
        "authors": [
            "Muhammad Adi Nugroho",
            "Sangmin Woo",
            "Sumin Lee",
            "Changick Kim"
        ]
    },
    {
        "title": "Rendering Humans from Object-Occluded Monocular Videos",
        "url": "http://arxiv.org/abs/2308.04622",
        "abstract": "3D understanding and rendering of moving humans from monocular videos is a\nchallenging task. Despite recent progress, the task remains difficult in\nreal-world scenarios, where obstacles may block the camera view and cause\npartial occlusions in the captured videos. Existing methods cannot handle such\ndefects due to two reasons. First, the standard rendering strategy relies on\npoint-point mapping, which could lead to dramatic disparities between the\nvisible and occluded areas of the body. Second, the naive direct regression\napproach does not consider any feasibility criteria (ie, prior information) for\nrendering under occlusions. To tackle the above drawbacks, we present OccNeRF,\na neural rendering method that achieves better rendering of humans in severely\noccluded scenes. As direct solutions to the two drawbacks, we propose\nsurface-based rendering by integrating geometry and visibility priors. We\nvalidate our method on both simulated and real-world occlusions and demonstrate\nour method's superiority.",
        "authors": [
            "Tiange Xiang",
            "Adam Sun",
            "Jiajun Wu",
            "Ehsan Adeli",
            "Li Fei-Fei"
        ]
    },
    {
        "title": "Out-of-Distribution Detection for Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2308.06072",
        "abstract": "In monocular depth estimation, uncertainty estimation approaches mainly\ntarget the data uncertainty introduced by image noise. In contrast to prior\nwork, we address the uncertainty due to lack of knowledge, which is relevant\nfor the detection of data not represented by the training distribution, the\nso-called out-of-distribution (OOD) data. Motivated by anomaly detection, we\npropose to detect OOD images from an encoder-decoder depth estimation model\nbased on the reconstruction error. Given the features extracted with the fixed\ndepth encoder, we train an image decoder for image reconstruction using only\nin-distribution data. Consequently, OOD images result in a high reconstruction\nerror, which we use to distinguish between in- and out-of-distribution samples.\nWe built our experiments on the standard NYU Depth V2 and KITTI benchmarks as\nin-distribution data. Our post hoc method performs astonishingly well on\ndifferent models and outperforms existing uncertainty estimation approaches\nwithout modifying the trained encoder-decoder depth estimation model.",
        "authors": [
            "Julia Hornauer",
            "Adrian Holzbock",
            "Vasileios Belagiannis"
        ]
    },
    {
        "title": "STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos",
        "url": "http://arxiv.org/abs/2301.00794",
        "abstract": "We address the problem of extracting key steps from unlabeled procedural\nvideos, motivated by the potential of Augmented Reality (AR) headsets to\nrevolutionize job training and performance. We decompose the problem into two\nsteps: representation learning and key steps extraction. We propose a training\nobjective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn\ndiscriminative representations for various steps without any labels. Different\nfrom prior works, we develop techniques to train a light-weight temporal module\nwhich uses off-the-shelf features for self supervision. Our approach can\nseamlessly leverage information from multiple cues like optical flow, depth or\ngaze to learn discriminative features for key-steps, making it amenable for AR\napplications. We finally extract key steps via a tunable algorithm that\nclusters the representations and samples. We show significant improvements over\nprior works for the task of key step localization and phase classification.\nQualitative results demonstrate that the extracted key steps are meaningful and\nsuccinctly represent various steps of the procedural tasks.",
        "authors": [
            "Anshul Shah",
            "Benjamin Lundell",
            "Harpreet Sawhney",
            "Rama Chellappa"
        ]
    },
    {
        "title": "Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild",
        "url": "http://arxiv.org/abs/2309.08644",
        "abstract": "3D pose estimation is an invaluable task in computer vision with various\npractical applications. Especially, 3D pose estimation for multi-person from a\nmonocular video (3DMPPE) is particularly challenging and is still largely\nuncharted, far from applying to in-the-wild scenarios yet. We pose three\nunresolved issues with the existing methods: lack of robustness on unseen views\nduring training, vulnerability to occlusion, and severe jittering in the\noutput. As a remedy, we propose POTR-3D, the first realization of a\nsequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel\ngeometry-aware data augmentation strategy, capable of generating unbounded data\nwith a variety of views while caring about the ground plane and occlusions.\nThrough extensive experiments, we verify that the proposed model and data\naugmentation robustly generalizes to diverse unseen views, robustly recovers\nthe poses against heavy occlusions, and reliably generates more natural and\nsmoother outputs. The effectiveness of our approach is verified not only by\nachieving the state-of-the-art performance on public benchmarks, but also by\nqualitative results on more challenging in-the-wild videos. Demo videos are\navailable at https://www.youtube.com/@potr3d.",
        "authors": [
            "Sungchan Park",
            "Eunyi You",
            "Inhoe Lee",
            "Joonseok Lee"
        ]
    },
    {
        "title": "Reducing Training Time in Cross-Silo Federated Learning Using Multigraph Topology",
        "url": "http://arxiv.org/abs/2207.09657",
        "abstract": "Federated learning is an active research topic since it enables several\nparticipants to jointly train a model without sharing local data. Currently,\ncross-silo federated learning is a popular training setting that utilizes a few\nhundred reliable data silos with high-speed access links to training a model.\nWhile this approach has been widely applied in real-world scenarios, designing\na robust topology to reduce the training time remains an open problem. In this\npaper, we present a new multigraph topology for cross-silo federated learning.\nWe first construct the multigraph using the overlay graph. We then parse this\nmultigraph into different simple graphs with isolated nodes. The existence of\nisolated nodes allows us to perform model aggregation without waiting for other\nnodes, hence effectively reducing the training time. Intensive experiments on\nthree public datasets show that our proposed method significantly reduces the\ntraining time compared with recent state-of-the-art topologies while\nmaintaining the accuracy of the learned model. Our code can be found at\nhttps://github.com/aioz-ai/MultigraphFL",
        "authors": [
            "Tuong Do",
            "Binh X. Nguyen",
            "Vuong Pham",
            "Toan Tran",
            "Erman Tjiputra",
            "Quang D. Tran",
            "Anh Nguyen"
        ]
    },
    {
        "title": "Counting Crowds in Bad Weather",
        "url": "http://arxiv.org/abs/2306.01209",
        "abstract": "Crowd counting has recently attracted significant attention in the field of\ncomputer vision due to its wide applications to image understanding. Numerous\nmethods have been proposed and achieved state-of-the-art performance for\nreal-world tasks. However, existing approaches do not perform well under\nadverse weather such as haze, rain, and snow since the visual appearances of\ncrowds in such scenes are drastically different from those images in clear\nweather of typical datasets. In this paper, we propose a method for robust\ncrowd counting in adverse weather scenarios. Instead of using a two-stage\napproach that involves image restoration and crowd counting modules, our model\nlearns effective features and adaptive queries to account for large appearance\nvariations. With these weather queries, the proposed model can learn the\nweather information according to the degradation of the input image and\noptimize with the crowd counting module simultaneously. Experimental results\nshow that the proposed algorithm is effective in counting crowds under\ndifferent weather types on benchmark datasets. The source code and trained\nmodels will be made available to the public.",
        "authors": [
            "Zhi-Kai Huang",
            "Wei-Ting Chen",
            "Yuan-Chun Chiang",
            "Sy-Yen Kuo",
            "Ming-Hsuan Yang"
        ]
    },
    {
        "title": "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model",
        "url": "http://arxiv.org/abs/2303.09833",
        "abstract": "Recently, conditional diffusion models have gained popularity in numerous\napplications due to their exceptional generation ability. However, many\nexisting methods are training-required. They need to train a time-dependent\nclassifier or a condition-dependent score estimator, which increases the cost\nof constructing conditional diffusion models and is inconvenient to transfer\nacross different conditions. Some current works aim to overcome this limitation\nby proposing training-free solutions, but most can only be applied to a\nspecific category of tasks and not to more general conditions. In this work, we\npropose a training-Free conditional Diffusion Model (FreeDoM) used for various\nconditions. Specifically, we leverage off-the-shelf pre-trained networks, such\nas a face detection model, to construct time-independent energy functions,\nwhich guide the generation process without requiring training. Furthermore,\nbecause the construction of the energy function is very flexible and adaptable\nto various conditions, our proposed FreeDoM has a broader range of applications\nthan existing training-free methods. FreeDoM is advantageous in its simplicity,\neffectiveness, and low cost. Experiments demonstrate that FreeDoM is effective\nfor various conditions and suitable for diffusion models of diverse data\ndomains, including image and latent code domains.",
        "authors": [
            "Jiwen Yu",
            "Yinhuai Wang",
            "Chen Zhao",
            "Bernard Ghanem",
            "Jian Zhang"
        ]
    },
    {
        "title": "UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding",
        "url": "http://arxiv.org/abs/2212.00836",
        "abstract": "Performing 3D dense captioning and visual grounding requires a common and\nshared understanding of the underlying multimodal relationships. However,\ndespite some previous attempts on connecting these two related tasks with\nhighly task-specific neural modules, it remains understudied how to explicitly\ndepict their shared nature to learn them simultaneously. In this work, we\npropose UniT3D, a simple yet effective fully unified transformer-based\narchitecture for jointly solving 3D visual grounding and dense captioning.\nUniT3D enables learning a strong multimodal representation across the two tasks\nthrough a supervised joint pre-training scheme with bidirectional and\nseq-to-seq objectives. With a generic architecture design, UniT3D allows\nexpanding the pre-training scope to more various training sources such as the\nsynthesized data from 2D prior knowledge to benefit 3D vision-language tasks.\nExtensive experiments and analysis demonstrate that UniT3D obtains significant\ngains for 3D dense captioning and visual grounding.",
        "authors": [
            "Dave Zhenyu Chen",
            "Ronghang Hu",
            "Xinlei Chen",
            "Matthias Nie\u00dfner",
            "Angel X. Chang"
        ]
    },
    {
        "title": "Clustering based Point Cloud Representation Learning for 3D Analysis",
        "url": "http://arxiv.org/abs/2307.14605",
        "abstract": "Point cloud analysis (such as 3D segmentation and detection) is a challenging\ntask, because of not only the irregular geometries of many millions of\nunordered points, but also the great variations caused by depth, viewpoint,\nocclusion, etc. Current studies put much focus on the adaption of neural\nnetworks to the complex geometries of point clouds, but are blind to a\nfundamental question: how to learn an appropriate point embedding space that is\naware of both discriminative semantics and challenging variations? As a\nresponse, we propose a clustering based supervised learning scheme for point\ncloud analysis. Unlike current de-facto, scene-wise training paradigm, our\nalgorithm conducts within-class clustering on the point embedding space for\nautomatically discovering subclass patterns which are latent yet representative\nacross scenes. The mined patterns are, in turn, used to repaint the embedding\nspace, so as to respect the underlying distribution of the entire training\ndataset and improve the robustness to the variations. Our algorithm is\nprincipled and readily pluggable to modern point cloud segmentation networks\nduring training, without extra overhead during testing. With various 3D network\narchitectures (i.e., voxel-based, point-based, Transformer-based, automatically\nsearched), our algorithm shows notable improvements on famous point cloud\nsegmentation datasets (i.e.,2.0-2.6% on single-scan and 2.0-2.2% multi-scan of\nSemanticKITTI, 1.8-1.9% on S3DIS, in terms of mIoU). Our algorithm also\ndemonstrates utility in 3D detection, showing 2.0-3.4% mAP gains on KITTI.",
        "authors": [
            "Tuo Feng",
            "Wenguan Wang",
            "Xiaohan Wang",
            "Yi Yang",
            "Qinghua Zheng"
        ]
    },
    {
        "title": "Efficient Transformer-based 3D Object Detection with Dynamic Token Halting",
        "url": "http://arxiv.org/abs/2303.05078",
        "abstract": "Balancing efficiency and accuracy is a long-standing problem for deploying\ndeep learning models. The trade-off is even more important for real-time\nsafety-critical systems like autonomous vehicles. In this paper, we propose an\neffective approach for accelerating transformer-based 3D object detectors by\ndynamically halting tokens at different layers depending on their contribution\nto the detection task. Although halting a token is a non-differentiable\noperation, our method allows for differentiable end-to-end learning by\nleveraging an equivalent differentiable forward-pass. Furthermore, our\nframework allows halted tokens to be reused to inform the model's predictions\nthrough a straightforward token recycling mechanism. Our method significantly\nimproves the Pareto frontier of efficiency versus accuracy when compared with\nthe existing approaches. By halting tokens and increasing model capacity, we\nare able to improve the baseline model's performance without increasing the\nmodel's latency on the Waymo Open Dataset.",
        "authors": [
            "Mao Ye",
            "Gregory P. Meyer",
            "Yuning Chai",
            "Qiang Liu"
        ]
    },
    {
        "title": "Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation",
        "url": "http://arxiv.org/abs/2212.07585",
        "abstract": "Source-free domain adaptation (SFDA) aims to adapt a source model trained on\na fully-labeled source domain to an unlabeled target domain. Large-data\npre-trained networks are used to initialize source models during source\ntraining, and subsequently discarded. However, source training can cause the\nmodel to overfit to source data distribution and lose applicable target domain\nknowledge. We propose to integrate the pre-trained network into the target\nadaptation process as it has diversified features important for generalization\nand provides an alternate view of features and classification decisions\ndifferent from the source model. We propose to distil useful target domain\ninformation through a co-learning strategy to improve target pseudolabel\nquality for finetuning the source model. Evaluation on 4 benchmark datasets\nshow that our proposed strategy improves adaptation performance and can be\nsuccessfully integrated with existing SFDA methods. Leveraging modern\npre-trained networks that have stronger representation learning ability in the\nco-learning strategy further boosts performance.",
        "authors": [
            "Wenyu Zhang",
            "Li Shen",
            "Chuan-Sheng Foo"
        ]
    },
    {
        "title": "RLIPv2: Fast Scaling of Relational Language-Image Pre-Training",
        "url": "http://arxiv.org/abs/2308.09351",
        "abstract": "Relational Language-Image Pre-training (RLIP) aims to align vision\nrepresentations with relational texts, thereby advancing the capability of\nrelational reasoning in computer vision tasks. However, hindered by the slow\nconvergence of RLIPv1 architecture and the limited availability of existing\nscene graph data, scaling RLIPv1 is challenging. In this paper, we propose\nRLIPv2, a fast converging model that enables the scaling of relational\npre-training to large-scale pseudo-labelled scene graph data. To enable fast\nscaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanism\nthat facilitates earlier and deeper gated cross-modal fusion with sparsified\nlanguage encoding layers. ALIF leads to comparable or better performance than\nRLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain\nscene graph data at scale, we extend object detection datasets with free-form\nrelation labels by introducing a captioner (e.g., BLIP) and a designed Relation\nTagger. The Relation Tagger assigns BLIP-generated relation texts to region\npairs, thus enabling larger-scale relational pre-training. Through extensive\nexperiments conducted on Human-Object Interaction Detection and Scene Graph\nGeneration, RLIPv2 shows state-of-the-art performance on three benchmarks under\nfully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2\nachieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with\njust 1% data and yields 45.09mAP with 100% data. Code and models are publicly\navailable at https://github.com/JacobYuan7/RLIPv2.",
        "authors": [
            "Hangjie Yuan",
            "Shiwei Zhang",
            "Xiang Wang",
            "Samuel Albanie",
            "Yining Pan",
            "Tao Feng",
            "Jianwen Jiang",
            "Dong Ni",
            "Yingya Zhang",
            "Deli Zhao"
        ]
    },
    {
        "title": "TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective",
        "url": "http://arxiv.org/abs/2308.10133",
        "abstract": "Vision Transformers (ViTs) have demonstrated powerful representation ability\nin various visual tasks thanks to their intrinsic data-hungry nature. However,\nwe unexpectedly find that ViTs perform vulnerably when applied to face\nrecognition (FR) scenarios with extremely large datasets. We investigate the\nreasons for this phenomenon and discover that the existing data augmentation\napproach and hard sample mining strategy are incompatible with ViTs-based FR\nbackbone due to the lack of tailored consideration on preserving face\nstructural information and leveraging each local token information. To remedy\nthese problems, this paper proposes a superior FR model called TransFace, which\nemploys a patch-level data augmentation strategy named DPAP and a hard sample\nmining strategy named EHSM. Specially, DPAP randomly perturbs the amplitude\ninformation of dominant patches to expand sample diversity, which effectively\nalleviates the overfitting problem in ViTs. EHSM utilizes the information\nentropy in the local tokens to dynamically adjust the importance weight of easy\nand hard samples during training, leading to a more stable prediction.\nExperiments on several benchmarks demonstrate the superiority of our TransFace.\nCode and models are available at https://github.com/DanJun6737/TransFace.",
        "authors": [
            "Jun Dan",
            "Yang Liu",
            "Haoyu Xie",
            "Jiankang Deng",
            "Haoran Xie",
            "Xuansong Xie",
            "Baigui Sun"
        ]
    },
    {
        "title": "Exploring Model Transferability through the Lens of Potential Energy",
        "url": "http://arxiv.org/abs/2308.15074",
        "abstract": "Transfer learning has become crucial in computer vision tasks due to the vast\navailability of pre-trained deep learning models. However, selecting the\noptimal pre-trained model from a diverse pool for a specific downstream task\nremains a challenge. Existing methods for measuring the transferability of\npre-trained models rely on statistical correlations between encoded static\nfeatures and task labels, but they overlook the impact of underlying\nrepresentation dynamics during fine-tuning, leading to unreliable results,\nespecially for self-supervised models. In this paper, we present an insightful\nphysics-inspired approach named PED to address these challenges. We reframe the\nchallenge of model selection through the lens of potential energy and directly\nmodel the interaction forces that influence fine-tuning dynamics. By capturing\nthe motion of dynamic representations to decline the potential energy within a\nforce-driven physical model, we can acquire an enhanced and more stable\nobservation for estimating transferability. The experimental results on 10\ndownstream tasks and 12 self-supervised models demonstrate that our approach\ncan seamlessly integrate into existing ranking techniques and enhance their\nperformances, revealing its effectiveness for the model selection task and its\npotential for understanding the mechanism in transfer learning. Code will be\navailable at https://github.com/lixiaotong97/PED.",
        "authors": [
            "Xiaotong Li",
            "Zixuan Hu",
            "Yixiao Ge",
            "Ying Shan",
            "Ling-Yu Duan"
        ]
    },
    {
        "title": "Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving",
        "url": "http://arxiv.org/abs/2309.04422",
        "abstract": "Performing multiple heterogeneous visual tasks in dynamic scenes is a\nhallmark of human perception capability. Despite remarkable progress in image\nand video recognition via representation learning, current research still\nfocuses on designing specialized networks for singular, homogeneous, or simple\ncombination of tasks. We instead explore the construction of a unified model\nfor major image and video recognition tasks in autonomous driving with diverse\ninput and output structures. To enable such an investigation, we design a new\nchallenge, Video Task Decathlon (VTD), which includes ten representative image\nand video tasks spanning classification, segmentation, localization, and\nassociation of objects and pixels. On VTD, we develop our unified network,\nVTDNet, that uses a single structure and a single set of weights for all ten\ntasks. VTDNet groups similar tasks and employs task interaction stages to\nexchange information within and between task groups. Given the impracticality\nof labeling all tasks on all frames, and the performance degradation associated\nwith joint training of many tasks, we design a Curriculum training,\nPseudo-labeling, and Fine-tuning (CPF) scheme to successfully train VTDNet on\nall tasks and mitigate performance loss. Armed with CPF, VTDNet significantly\noutperforms its single-task counterparts on most tasks with only 20% overall\ncomputations. VTD is a promising new direction for exploring the unification of\nperception tasks in autonomous driving.",
        "authors": [
            "Thomas E. Huang",
            "Yifan Liu",
            "Luc Van Gool",
            "Fisher Yu"
        ]
    },
    {
        "title": "Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception",
        "url": "http://arxiv.org/abs/2306.06362",
        "abstract": "We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\nusing Aria glasses with extensive object, environment, and human level ground\ntruth. This ADT release contains 200 sequences of real-world activities\nconducted by Aria wearers in two real indoor scenes with 398 object instances\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\nsensor calibration; c) ground truth data including continuous\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\nexisting egocentric dataset with a level of accuracy, photo-realism and\ncomprehensiveness comparable to ADT. By contributing ADT to the research\ncommunity, our mission is to set a new standard for evaluation in the\negocentric machine perception domain, which includes very challenging research\nproblems such as 3D object detection and tracking, scene reconstruction and\nunderstanding, sim-to-real learning, human pose prediction - while also\ninspiring new machine perception tasks for augmented reality (AR) applications.\nTo kick start exploration of the ADT research use cases, we evaluated several\nexisting state-of-the-art methods for object detection, segmentation and image\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\ndataset.",
        "authors": [
            "Xiaqing Pan",
            "Nicholas Charron",
            "Yongqian Yang",
            "Scott Peters",
            "Thomas Whelan",
            "Chen Kong",
            "Omkar Parkhi",
            "Richard Newcombe",
            "Carl Yuheng Ren"
        ]
    },
    {
        "title": "PreSTU: Pre-Training for Scene-Text Understanding",
        "url": "http://arxiv.org/abs/2209.05534",
        "abstract": "The ability to recognize and reason about text embedded in visual inputs is\noften lacking in vision-and-language (V&L) models, perhaps because V&L\npre-training methods have often failed to include such an ability in their\ntraining objective. In this paper, we propose PreSTU, a novel pre-training\nrecipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware\npre-training objectives that encourage the model to recognize text from an\nimage and connect it to the rest of the image content. We implement PreSTU\nusing a simple transformer-based encoder-decoder architecture, combined with\nlarge-scale image-text datasets with scene text obtained from an off-the-shelf\nOCR system. We empirically demonstrate the effectiveness of this pre-training\napproach on eight visual question answering and four image captioning\nbenchmarks.",
        "authors": [
            "Jihyung Kil",
            "Soravit Changpinyo",
            "Xi Chen",
            "Hexiang Hu",
            "Sebastian Goodman",
            "Wei-Lun Chao",
            "Radu Soricut"
        ]
    }
]